2024-05-15 09:30:55.148 | INFO     | __main__:<module>:262 - Using evaluation function v2 to print results
2024-05-15 09:30:59.188 | INFO     | __main__:<module>:320 - model build
2024-05-15 09:31:10.312 | INFO     | __main__:train:37 - [Parameter containing:
tensor([[-0.0146, -0.0039, -0.0066,  ..., -0.0070, -0.0130,  0.0021],
        [-0.0165,  0.0127,  0.0296,  ...,  0.0056,  0.0020, -0.0008],
        [ 0.0086, -0.0143, -0.0048,  ..., -0.0151, -0.0058, -0.0208],
        ...,
        [-0.0014, -0.0116, -0.0034,  ..., -0.0181,  0.0034, -0.0155],
        [ 0.0091,  0.0107, -0.0240,  ..., -0.0009,  0.0183,  0.0085],
        [ 0.0078,  0.0050, -0.0247,  ...,  0.0250,  0.0013, -0.0091]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([2.0889, 1.9712, 2.1220, 2.0694, 1.7119, 1.9933, 2.0799, 2.1845, 2.1324,
        1.9399, 1.9302, 2.1218, 2.0308, 2.0338, 2.1017, 2.0951, 2.0867, 2.0461,
        1.9855, 2.1302, 2.0887, 2.0182, 1.9796, 2.0423, 2.0565, 2.1084, 2.0156,
        2.0070, 1.9619, 1.9993, 1.9641, 2.1471, 2.1692, 2.0407, 2.0561, 1.7148,
        1.9077, 2.0416, 2.0109, 2.0489, 2.0492, 1.9716, 1.9944, 2.1945, 2.1414,
        2.0624, 2.0382, 1.7156, 2.0872, 2.1689, 1.9913, 1.9874, 2.0611, 2.0384,
        2.0404, 2.0571, 2.1749, 1.6695, 1.9704, 2.0106, 2.1707, 2.1583, 1.7716,
        2.0785, 2.2589, 1.5433, 2.0498, 2.1556, 2.2475, 1.6193, 2.0502, 1.9964,
        1.9409, 2.1102, 2.1142, 1.8969, 2.0679, 1.9978, 1.9989, 1.9519, 1.8782,
        2.0690, 1.9627, 2.0572, 2.0304, 1.9690, 2.0986, 2.1040, 2.1931, 2.0631,
        2.0917, 2.0014, 2.0774, 1.8498, 1.8968, 2.1439, 1.9327, 2.0869, 2.1567,
        1.9760, 2.1401, 1.9547, 2.2018, 1.9345, 1.9655, 1.9996, 1.9677, 1.8891,
        2.1570, 2.0854, 1.7238, 2.0024, 2.1032, 2.0581, 1.9916, 2.1147, 1.9997,
        1.9987, 2.1948, 2.0187, 2.0479, 2.0436, 2.1438, 2.0421, 2.0650, 2.0503,
        2.0932, 2.1495, 2.1429, 2.2573, 2.2058, 2.0425, 1.9905, 2.0903, 2.0920,
        2.0021, 1.9449, 1.8799, 1.9051, 2.0264, 2.0167, 2.0677, 2.1733, 2.1022,
        2.0488, 1.9244, 2.0255, 1.9785, 2.0561, 2.1360, 2.0916, 1.9735, 2.0471,
        2.0240, 2.0103, 2.0339, 2.0940, 2.1347, 1.8951, 1.9661, 2.0123, 1.6906,
        3.3178, 1.8554, 2.0427, 2.0867, 2.0731, 2.2051, 2.0521, 1.8706, 2.1517,
        2.0741, 1.9758, 2.0706, 2.1696, 2.1816, 1.6692, 1.9433, 2.1614, 1.9843,
        2.0820, 2.0234, 2.0356, 1.9761, 2.0929, 2.0717, 1.9660, 1.7830, 2.1155,
        1.8459, 2.1288, 2.1077, 1.9489, 1.8428, 2.0365, 2.0475, 1.9203, 2.0346,
        1.1667, 1.9564, 2.0408, 2.1060, 1.9239, 2.1521, 2.0335, 1.3680, 1.7952,
        2.1439, 2.1176, 1.9656, 2.1946, 2.0850, 1.9810, 2.1021, 2.0342, 2.0878,
        2.1313, 1.8232, 2.0304, 2.0742, 2.0921, 2.0573, 2.1955, 2.1340, 1.9932,
        2.0807, 2.0696, 2.0617, 2.0044, 2.0714, 2.1352, 2.1044, 1.9892, 1.9525,
        2.2479, 1.9878, 2.0778, 2.0956, 2.1284, 2.0560, 2.0330, 2.0734, 2.1860,
        1.5747, 2.0099, 2.1652, 2.0593, 1.9409, 2.1205, 2.0521, 2.1560, 2.0191,
        1.9861, 2.0404, 2.0371, 2.1400, 2.0281, 2.0016, 2.1504, 1.8761, 1.9561,
        2.0932, 2.0882, 1.9444, 1.9052, 2.0580, 1.9558, 2.0516, 1.9564, 2.0826,
        2.0919, 2.1240, 1.9847, 2.0485, 2.0422, 2.0356, 1.8621, 2.1438, 2.1475,
        1.9393, 2.0578, 2.0111, 2.2414, 2.0166, 2.0661, 2.1818, 2.1259, 2.1101,
        1.9721, 1.9622, 1.9813, 2.1706, 1.9655, 1.9623, 1.9839, 2.0574, 2.1466,
        2.1507, 2.1680, 1.4906, 2.1190, 1.9092, 2.1146, 2.0637, 2.1615, 1.9260,
        1.9775, 1.5948, 2.1446, 1.5686, 1.9936, 2.1131, 1.9389, 2.0891, 2.0374,
        2.1002, 2.0653, 1.9287, 2.0661, 1.9063, 1.8354, 2.0778, 2.1387, 2.0642,
        2.0024, 1.9499, 2.0517, 2.1462, 1.8669, 2.0599, 2.1175, 2.1816, 2.1317,
        2.0351, 2.0384, 2.2207, 2.0393, 1.9515, 2.0600, 2.0513, 2.0241, 1.4406,
        2.0965, 2.0465, 2.1819, 2.0059, 1.9621, 2.0900, 1.9422, 1.9949, 2.1607,
        2.0713, 2.0750, 1.9913, 1.9107, 2.0848, 2.0837, 2.2114, 2.1490, 2.0948,
        2.0114, 2.1234, 1.8489, 1.9168, 1.8525, 2.0741, 1.9893, 2.0585, 1.9955,
        2.1011, 1.9959, 1.7628, 2.1660, 2.0795, 2.1166, 2.0747, 1.9695, 2.0061,
        2.1132, 2.0464, 2.0033, 1.9074, 2.1169, 2.0778, 2.1615, 2.0775, 2.0127,
        2.1388, 2.0702, 1.9780, 1.9862, 2.0076, 1.9101, 2.1197, 2.0821, 2.1660,
        1.6451, 2.1807, 2.1510, 2.0006, 2.1235, 1.9724, 2.0940, 1.9664, 1.9653,
        2.0226, 2.1127, 1.6622, 2.0052, 2.0679, 2.0634, 2.0308, 1.9230, 2.3308,
        2.0401, 2.0522, 2.1555, 1.6648, 2.1525, 1.9948, 2.0326, 2.1897, 1.9837,
        2.1599, 2.0922, 2.1043, 2.1130, 2.1266, 2.0320, 1.7905, 2.1134, 1.9856,
        2.1800, 2.0651, 2.2059, 1.6809, 2.0747, 1.8652, 1.9064, 1.9484, 2.2097,
        2.1325, 1.9957, 2.1914, 2.0445, 2.0817, 2.0053, 1.9492, 1.9617, 1.7545,
        2.0156, 2.0356, 2.1489, 2.0271, 1.9697, 2.0002, 2.1147, 2.1199, 2.0303,
        1.8091, 1.9793, 2.0675, 1.9589, 1.9105, 2.1151, 2.0203, 2.0771, 1.9647,
        2.0655, 2.0748, 2.0341, 1.9724, 2.0934, 1.9961, 0.3405, 1.9650, 2.0412,
        2.1093, 2.0053, 2.0580, 2.1316, 2.1214, 1.8425, 2.1187, 1.7724, 2.0784,
        2.0464, 1.7946, 2.1334, 2.0780, 2.1812, 2.1858, 2.1440, 2.1193, 2.0504,
        1.9470, 1.9928, 1.9448, 2.1769, 2.0388, 2.1214, 2.0781, 1.8957, 1.9711,
        2.0179, 1.9501, 2.0254, 2.0303, 2.0969, 2.0911, 1.3677, 1.9383, 1.8941,
        1.9511, 1.9579, 1.9971, 2.0300, 2.0202, 1.3250, 1.6754, 2.1596, 2.0482,
        2.0071, 2.1047, 1.7998, 1.9864, 1.8247, 2.0342, 1.9502, 2.0478, 2.0405,
        1.8985, 2.0594, 2.0769, 1.8037, 2.0254, 2.1384, 2.1213, 2.1802, 2.1189,
        2.0923, 1.9126, 2.1282, 2.1499, 1.9930, 2.0616, 1.7846, 1.9661, 2.2175,
        2.1521, 2.0787, 2.1360, 1.9558, 1.9896, 2.0940, 1.9441, 2.0977, 1.9642,
        2.1160, 1.8274, 1.9144, 2.1407, 2.0104, 2.0967, 2.0736, 1.9755, 2.1098,
        1.9763, 2.0681, 2.0670, 2.1317, 2.0077, 2.0983, 2.1026, 2.0234, 2.0871,
        2.0295, 1.9842, 1.9524, 1.9992, 2.2744, 2.0831, 2.0668, 2.0192, 1.9225,
        1.9729, 1.9364, 2.1503, 2.0694, 2.0884, 2.0392, 1.9470, 2.0753, 1.9972,
        1.9704, 2.0931, 2.1665, 2.0933, 1.9537, 2.0864, 2.0178, 1.9763, 2.0968,
        1.9622, 1.9361, 2.1092, 1.9623, 2.1026, 1.9378, 2.1092, 2.0311, 1.8948,
        1.9659, 1.9941, 2.0935, 2.2296, 2.2099, 1.9437, 2.0376, 2.0139, 1.9951,
        1.9778, 2.0384, 2.0941, 2.0700, 2.0591, 2.1484, 2.0313, 2.1585, 1.9633,
        2.1104, 2.0213, 2.0351, 1.5722, 2.0605, 2.1667, 2.1725, 2.0506, 2.0942,
        2.1590, 1.8917, 2.0401, 2.0299, 2.0578, 2.0229, 2.0145, 2.1082, 2.1442,
        1.9256, 2.1155, 1.5651, 2.1935, 2.0792, 2.1289, 2.0517, 2.0612, 1.9459,
        2.2552, 1.9714, 2.0947, 2.0609, 2.0716, 1.4566, 1.9967, 2.0037, 1.8967,
        2.1645, 1.9572, 2.0952, 2.1335, 2.1983, 1.9248, 2.0946, 2.0167, 1.9967,
        2.0781, 2.0708, 2.2572, 2.0508, 2.1440, 2.1336, 2.1300, 1.9258, 2.0525,
        2.0640, 2.1309, 1.9936, 2.0384, 2.1007, 1.3213, 2.1270, 2.0090, 2.1278,
        2.0224, 2.0256, 2.1023, 1.8309, 1.9433, 1.9495, 2.0881, 2.1420, 2.1381,
        2.0138, 1.8643, 2.0748, 1.9934, 2.0834, 1.9823, 2.1178, 2.2130, 2.0325,
        1.9908, 2.1174, 2.2464, 2.0913, 2.0609, 2.0776, 2.1028, 2.0192, 2.0302,
        1.9206, 2.1787, 2.0304, 1.9062, 3.4342, 1.9552, 1.9041, 2.0857, 2.0320,
        1.9365, 1.9956, 1.9609, 1.9605, 1.6706, 2.1107, 1.9736, 2.0025, 2.0189,
        1.9959, 2.0646, 2.1581, 1.8589, 2.0009, 2.2918, 2.0199, 1.9572, 1.8224,
        1.8725, 2.0706, 2.1529, 2.1153, 2.0897, 1.9255, 2.1299, 2.1575, 2.0383,
        2.0991, 2.0648, 2.1076, 2.0169, 1.8750, 2.1080, 2.1821, 1.9574, 2.1477,
        2.0282, 1.6939, 2.0273], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 2.8504e-02, -6.6268e-02,  2.0469e-02, -2.4063e-01, -7.6139e-01,
         2.2901e-01, -1.9181e-02, -1.6229e-01,  1.4640e-01, -7.2078e-02,
        -6.4448e-02,  1.3004e-01, -2.8852e-02, -2.2445e-01, -1.5333e-01,
        -2.0017e-02, -1.4645e-02,  1.3456e-01,  1.1367e-01,  8.5705e-04,
        -1.9154e-01, -1.1060e-01,  7.7911e-02,  1.0782e-03,  1.8011e-01,
        -6.9190e-02,  8.1234e-02,  4.6244e-03,  2.0738e-01,  2.1546e-01,
        -1.2781e-01,  2.1756e-01,  1.9662e-01,  6.6177e-02, -9.6555e-02,
         3.6276e-02,  3.0880e-01, -4.2114e-02, -1.7260e-01, -3.2207e-02,
         8.0750e-02,  1.4804e-01,  1.5997e-01, -1.2746e-02, -2.0021e-01,
         2.4657e-01, -1.5625e-01,  4.1060e-01,  1.1039e-02,  1.7342e-03,
        -2.8080e-01,  7.2084e-02, -9.5679e-03,  2.8248e-01, -1.3109e-01,
        -4.9942e-02, -7.1812e-03,  2.3124e-01,  3.6284e-01,  2.3893e-01,
         4.1646e-02,  1.7018e-01,  1.5995e-01, -1.8938e-02, -1.4604e-01,
         3.8497e-01, -2.0005e-01,  1.1361e-01, -8.6606e-02,  8.1025e-02,
         1.0023e-01,  1.3133e-02,  1.4986e-01, -1.4484e-01, -1.8983e-01,
         1.2057e-01,  6.8089e-02,  1.7732e-02, -1.4275e-02,  4.1753e-01,
        -1.7650e-01, -1.2653e-01, -1.9440e-01,  3.2383e-02,  6.3769e-03,
        -6.3109e-02,  3.4128e-01,  1.1140e-01,  5.1841e-02, -7.8993e-02,
         3.1750e-01, -2.7008e-01,  2.6093e-01, -4.3071e-02, -1.6392e-01,
        -5.4121e-02, -2.5731e-01,  1.8952e-01,  8.5067e-02, -6.1931e-02,
         1.1788e-01,  3.1476e-01,  9.4151e-02, -1.6559e-01,  1.5590e-01,
         1.8307e-01,  3.4897e-01,  3.2071e-01, -9.9986e-02,  1.2352e-01,
        -3.6178e-02,  1.7537e-01, -8.1322e-02, -3.0396e-02,  5.5304e-02,
        -2.4548e-02,  8.7976e-02, -1.4368e-01,  2.1969e-02,  1.4105e-01,
        -2.4492e-02, -2.6660e-01,  1.4491e-01,  1.0095e-01,  1.1811e-02,
         7.9764e-02,  1.2176e-01, -5.9894e-02,  6.5163e-02,  2.0217e-02,
         7.2439e-02,  9.9869e-02,  2.5884e-01, -1.3904e-01, -7.1748e-02,
        -1.6325e-01, -3.4289e-01, -4.1664e-02, -2.6442e-01, -2.1141e-01,
        -3.2832e-01, -1.9760e-01,  1.9742e-01,  6.4940e-02,  1.1064e-01,
         9.8830e-02, -5.5894e-02, -7.5366e-02, -2.5354e-02,  1.8192e-01,
        -1.5770e-01, -9.7434e-02, -2.0390e-02,  1.2097e-01,  1.3304e-01,
        -2.2001e-01, -1.9000e-01,  1.0213e-01,  2.8652e-01,  7.8545e-02,
        -4.1900e-01, -1.5198e-01,  8.8589e-01,  6.4870e-02,  1.9507e-01,
         1.1244e-01,  1.3662e-01, -5.5619e-03, -1.8421e-01,  3.6384e-01,
         1.0898e-01,  1.7704e-02, -1.4382e-01,  3.1085e-02,  4.4398e-02,
         2.6900e-01,  2.5362e-01, -2.0190e-01, -2.3754e-01, -2.3838e-01,
         6.8463e-02, -5.0972e-02, -7.0668e-02, -1.4725e-01, -1.0647e-01,
        -2.4313e-01, -2.0898e-02, -6.2361e-03, -9.2734e-02,  1.2089e-01,
         1.3466e-01, -1.5528e-01,  9.1784e-02,  3.1565e-01, -8.8425e-02,
         1.1853e-01,  2.9177e-01,  4.9970e-02,  2.9435e-01, -2.6363e-02,
        -3.5723e-03,  1.1744e-02, -3.8822e-01, -6.3808e-02, -2.5130e-02,
        -2.9781e-01,  2.4423e-01,  1.2359e-01, -2.0310e-01,  1.1666e-01,
        -1.0383e-01, -2.4320e-01,  1.0476e-01, -6.4406e-02, -1.1334e-01,
        -1.6384e-01,  4.5622e-02,  4.6126e-02, -2.3101e-01, -4.2995e-02,
        -3.2435e-02,  1.8009e-01,  1.0649e-01,  1.6906e-01,  2.8517e-01,
         1.5568e-01,  1.0221e-01,  1.8800e-01,  1.2888e-01,  1.1685e-01,
        -1.8799e-02, -8.4322e-02, -2.2413e-01, -3.1196e-01,  2.1720e-01,
        -2.6198e-01, -2.5196e-02,  9.2941e-02,  6.2847e-02,  2.4234e-01,
        -3.4218e-01, -1.8627e-01,  1.3247e-01, -3.4954e-01, -3.0751e-01,
        -1.5220e-01,  1.3842e-01, -7.7362e-02, -8.6115e-02,  8.0118e-02,
        -1.3368e-01, -1.9051e-01,  1.4877e-01,  1.5263e-01,  6.0897e-02,
        -2.5285e-01,  2.9866e-01,  1.6658e-01,  1.3632e-01, -3.1249e-01,
         5.9045e-02,  1.4344e-01, -8.2676e-02,  3.4193e-01, -1.9562e-01,
        -8.8454e-03,  8.0644e-02, -1.5615e-02,  1.6132e-01,  9.9109e-03,
        -2.1831e-01,  2.0527e-04,  2.0767e-01, -1.9229e-01, -6.2333e-02,
         2.8975e-01,  2.2820e-01,  5.4081e-02, -2.8020e-02, -7.5214e-02,
         1.3673e-01,  2.2390e-01,  8.9420e-02,  2.9266e-01,  8.7919e-03,
        -5.7573e-02,  2.4258e-01,  3.9646e-02,  3.2063e-01,  2.2784e-01,
         9.1253e-02,  1.3482e-01,  5.0732e-02,  8.2332e-02, -2.5443e-01,
        -5.3945e-02,  3.5411e-02,  1.5352e-01, -3.5287e-02,  6.9929e-02,
         5.6327e-02,  2.1955e-01, -1.0062e-01, -2.5025e-02,  1.6035e-01,
        -1.7752e-01,  3.2281e-03,  3.3873e-01, -2.4688e-02, -3.2176e-01,
         1.9693e-01, -7.4028e-02, -4.5915e-02, -7.9669e-02, -1.4538e-01,
         8.4735e-02, -9.0724e-02, -1.1218e-02, -5.2998e-02, -2.8057e-01,
         2.5006e-01, -7.6606e-02,  8.9481e-02,  2.1017e-01, -1.4844e-01,
        -1.1927e-01, -1.7030e-01, -2.6984e-02, -1.2455e-01, -1.7030e-02,
        -2.3333e-03, -3.4794e-02,  1.1147e-01,  6.8249e-02,  1.1888e-01,
        -6.2072e-03, -5.5677e-02,  2.5506e-02,  1.4834e-01,  2.8043e-01,
         7.5211e-02,  3.2190e-01,  1.0136e-01,  1.3366e-01, -1.1125e-01,
        -2.2113e-02, -1.9412e-01,  5.0625e-02,  1.1823e-01, -1.0454e-01,
        -9.8780e-03,  7.1319e-02, -1.9997e-02, -1.6023e-02, -3.0358e-01,
         4.9813e-02, -3.7291e-02,  4.1195e-02,  4.0840e-02,  2.5038e-01,
        -1.3593e-01, -3.9288e-02,  1.4432e-01,  1.5827e-01,  3.7552e-01,
         5.9903e-02,  3.5905e-01,  2.4094e-01,  8.8207e-02, -4.8586e-02,
         8.4939e-03, -1.3572e-02,  1.3245e-01, -2.6827e-01,  2.1792e-01,
         1.3221e-02, -1.5648e-01,  1.1359e-02, -1.0186e-01,  4.2942e-03,
         1.7744e-01,  2.8789e-02,  4.6753e-02, -9.7045e-02,  1.3429e-01,
        -9.7212e-02, -7.1507e-02, -3.4633e-03,  1.3883e-01, -2.0727e-01,
         5.2512e-03,  9.8836e-02, -1.7465e-01, -4.8214e-02,  2.0891e-01,
        -1.7837e-01,  1.4867e-01, -6.8707e-02,  2.1542e-01,  2.3465e-02,
        -1.6120e-01, -7.1607e-03,  6.9563e-02,  2.0662e-01, -1.3214e-02,
        -3.6621e-03,  3.9815e-02,  1.1840e-01, -9.3086e-04,  1.6077e-01,
         2.5887e-01,  3.3082e-02,  9.1206e-02, -8.1345e-02,  1.5747e-01,
         2.2100e-01,  1.4447e-01,  1.3298e-01,  5.6416e-02,  7.1703e-02,
         2.7277e-02, -3.4643e-02,  1.1375e-02, -1.2775e-01,  1.4280e-01,
        -1.1996e-03,  1.4385e-01,  6.2964e-02,  7.8224e-04,  3.1198e-01,
        -8.2213e-02,  3.0901e-01, -1.8916e-01,  6.4224e-02, -6.0739e-02,
         3.6575e-01,  2.7061e-01, -4.9588e-01,  3.7487e-02,  1.9776e-01,
         9.6408e-02, -2.3510e-02, -2.2246e-01,  1.2251e-01,  6.9865e-02,
        -1.0218e-01, -1.2041e-01,  3.6202e-02,  1.3946e-01,  1.2657e-01,
         4.5796e-01,  1.1498e-01,  1.9502e-01,  7.9131e-02, -2.3154e-01,
        -1.8005e-01, -9.3380e-02, -7.7939e-02,  2.3785e-03, -1.6296e-02,
        -2.1455e-01,  1.5535e-01,  6.0736e-02, -2.5630e-01, -5.4613e-02,
        -1.0549e-01,  2.1634e-01,  1.6198e-01,  1.9333e-02, -1.3153e-01,
        -7.9205e-02,  1.5459e-01,  1.2753e-01,  3.5562e-01, -2.0996e+00,
         1.8678e-01,  1.3896e-01,  1.3344e-01,  1.9373e-01, -1.3733e-01,
        -4.8426e-02,  1.0204e-01, -5.3012e-02,  6.2561e-02,  2.0782e-01,
        -2.7372e-02,  1.2175e-01,  9.3349e-02, -1.4915e-02,  1.4731e-01,
        -4.1648e-02,  1.9402e-01, -4.0652e-02,  1.3574e-01,  2.3930e-01,
         2.1554e-01,  1.9693e-01,  3.4045e-01,  5.3095e-03, -1.1799e-01,
         2.0984e-01,  1.4215e-01,  1.6345e-01, -9.9329e-02,  2.8398e-01,
         1.3910e-01, -1.9960e-02, -4.4554e-02, -4.9254e-02,  1.2946e-01,
         1.6621e-02, -1.4321e-01, -3.1374e-01,  1.4049e-01, -1.4755e-01,
         2.1822e-01, -1.5111e-01,  9.3312e-02,  7.1476e-01, -4.9110e-01,
         1.7405e-01,  9.4529e-02, -1.6380e-01,  2.5346e-01, -1.6801e-01,
         1.5994e-01, -1.4535e-01, -1.8851e-02,  4.0526e-01,  9.0625e-02,
         1.1590e-03,  7.2827e-02, -1.7893e-01,  6.2739e-03, -2.0410e-02,
         3.5180e-01, -1.2868e-01, -8.4570e-02,  8.2254e-02,  9.5615e-02,
        -4.3183e-02, -1.3872e-01,  2.5896e-02, -2.4962e-01,  1.2058e-01,
         4.3327e-02,  2.0339e-01,  2.3865e-02,  9.1863e-02,  5.4388e-02,
         1.1565e-01, -9.1775e-02,  1.7982e-01, -6.4217e-04, -6.8558e-02,
         2.7526e-01,  1.1384e-01,  3.2608e-01, -2.5336e-01, -4.9204e-01,
        -1.1743e-01, -6.1600e-03,  1.7388e-02, -1.4776e-01,  2.1700e-02,
         5.5266e-02, -9.0456e-02, -3.9565e-01,  5.1087e-02, -9.5472e-03,
         1.0449e-01,  8.0698e-02, -1.3287e-01, -1.2388e-02, -1.5503e-01,
         1.4343e-01, -1.7563e-01,  1.2543e-01, -1.0902e-01,  3.5237e-01,
         1.3982e-02, -1.6298e-01,  7.5430e-02,  1.4752e-01, -2.1335e-01,
         2.0591e-02,  8.5383e-02, -1.4727e-01,  1.1335e-01,  1.0891e-01,
         8.6156e-02,  2.1769e-02, -9.0225e-02, -6.0408e-02,  1.7196e-01,
         1.7566e-01,  5.9273e-02, -2.7534e-01,  1.5645e-01,  2.8370e-02,
         9.7209e-02,  1.5087e-01, -1.5383e-01, -1.1292e-01,  1.7840e-01,
         1.4085e-01,  3.9992e-01,  3.1400e-02, -3.0622e-01,  4.0452e-02,
         1.0277e-01,  2.9261e-01,  7.9035e-02, -1.6428e-01,  1.3205e-01,
        -5.9142e-02,  6.1661e-02, -2.2273e-01, -1.1950e-01,  1.1244e-02,
        -7.5128e-04,  6.2115e-02, -7.5307e-02, -1.9999e-01, -5.0906e-02,
         1.5986e-01, -1.5384e-01,  1.8486e-02,  5.3391e-02, -2.2856e-02,
         1.6538e-02,  1.6673e-01,  1.6526e-01,  3.7644e-01,  2.2116e-02,
         1.6101e-02,  8.4632e-02,  1.6122e-01, -1.2911e-01, -4.2537e-03,
         4.3188e-01, -1.6758e-01,  3.1329e-01, -3.9120e-02,  2.3113e-01,
        -4.7350e-02, -1.7430e-03, -4.6825e-03,  9.6474e-02, -1.3785e-02,
         3.6050e-01, -2.3145e-01,  9.9413e-02, -7.9733e-02,  2.7168e-01,
        -7.9808e-02, -9.3619e-02,  2.1798e-02, -7.2180e-02, -6.3289e-02,
         1.9584e-01, -4.4046e-02,  4.8561e-01,  1.6550e-01,  2.8046e-01,
        -2.8508e-01, -1.3410e-01, -3.0835e-01, -8.8017e-02, -8.2227e-02,
        -2.3335e-01, -3.8007e-01, -1.8516e-01, -8.0878e-02,  6.9986e-02,
         3.1223e-03, -1.6395e-01,  6.1631e-02,  1.0742e-01,  1.4890e-01,
         3.1640e-01,  3.0610e-01, -1.3100e-01, -2.2691e-01,  2.3236e-01,
         2.3160e-02, -2.4449e-02,  2.5452e-01,  9.4654e-02, -2.5230e-01,
         1.0485e-01, -3.9629e-02,  2.7753e-01, -8.5534e-02, -2.8560e-01,
        -1.0625e-01, -1.9122e-01, -7.7833e-02,  1.7479e-02,  1.1107e-01,
         1.9104e-01, -5.3806e-02,  1.4329e-01,  7.4348e-02, -3.7891e-02,
        -1.6470e-01,  2.7303e-01,  1.9698e-01, -3.4510e-01, -3.8307e-02,
         1.8732e-01, -7.2547e-02, -1.5918e-01, -1.2051e-01,  1.5882e-01,
        -2.4738e-01, -1.3463e-02,  2.5781e-01, -1.8179e-01,  3.0621e-01,
         3.7397e-02, -6.3673e-02, -6.0211e-02,  3.0543e-01,  1.8674e+00,
         2.9141e-01, -2.1947e-01, -4.4114e-02, -1.2505e-01,  3.7997e-01,
        -1.4247e-02, -3.6082e-01,  1.8770e-01,  9.6182e-03,  1.2768e-01,
        -1.4299e-01,  2.9161e-01, -3.4586e-01, -4.6834e-02,  1.1860e-01,
         5.2585e-03,  1.3938e-01, -3.3451e-01,  9.1791e-02,  2.3278e-01,
         4.8837e-03, -3.0325e-01,  3.3830e-01,  4.9341e-02,  1.1152e-02,
         9.8717e-02,  3.3592e-02,  4.5998e-01,  1.2436e-02,  1.7771e-01,
         8.9274e-02, -1.4505e-01, -4.3361e-03, -6.2916e-02, -2.6059e-01,
         3.9149e-01, -7.0545e-02,  7.1020e-02,  1.5872e-01,  1.6568e-01,
         1.2057e-01, -2.5532e-01,  1.2026e-01], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 0.0032,  0.0204, -0.0020,  ..., -0.0134,  0.0182,  0.0079],
        [ 0.0039,  0.0227,  0.0147,  ...,  0.0069, -0.0088,  0.0002],
        [ 0.0234,  0.0145,  0.0353,  ..., -0.0183, -0.0043, -0.0039],
        ...,
        [-0.0105,  0.0073,  0.0207,  ...,  0.0153, -0.0183, -0.0241],
        [ 0.0283,  0.0126, -0.0240,  ...,  0.0002, -0.0133,  0.0178],
        [-0.0102,  0.0041,  0.0092,  ...,  0.0073,  0.0087, -0.0314]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.0820,  0.2947, -0.0189,  ...,  0.0406, -0.0679, -0.0443],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0014, -0.0327, -0.0172,  ...,  0.0096, -0.0359,  0.0276],
        [-0.0273, -0.0061, -0.0044,  ..., -0.0093, -0.0338, -0.0004],
        [ 0.0194,  0.0110,  0.0128,  ...,  0.0142,  0.0174, -0.0026],
        ...,
        [-0.0471, -0.0134, -0.0056,  ...,  0.0004,  0.0095,  0.0284],
        [ 0.0096, -0.0149, -0.0111,  ..., -0.0030, -0.0090, -0.0091],
        [-0.0043, -0.0149, -0.0157,  ...,  0.0204,  0.0081,  0.0181]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-5.8228e-02,  9.1492e-02,  1.6858e-01,  2.8473e-02,  4.3750e-01,
        -7.2021e-02, -2.1652e-02, -9.1125e-02, -6.7215e-03,  4.0222e-02,
         4.4312e-02, -9.1797e-02, -6.2561e-02, -9.0454e-02,  1.1432e-01,
        -1.3196e-01,  5.2002e-02, -4.8706e-02, -9.7198e-03, -3.0615e-01,
         6.1760e-03,  2.2324e-02,  2.8467e-01, -2.8275e-02, -7.9956e-02,
        -5.9326e-02,  2.8210e-03, -1.2201e-01,  6.4514e-02, -5.7251e-02,
        -1.1969e-01,  1.6772e-01, -1.1429e-02, -2.2766e-01, -1.7120e-02,
         9.6497e-02, -9.4482e-02, -6.5796e-02,  8.2825e-02,  1.0254e-01,
         1.3342e-01,  1.1084e-01,  3.7720e-02,  1.1597e-01,  1.7407e-01,
         1.3196e-01, -5.4245e-03,  2.9572e-02,  1.1803e-02,  8.5220e-03,
         8.0383e-02, -2.5684e-01,  1.3562e-01, -4.3610e-02,  1.2646e-01,
         1.2489e-02, -5.5237e-02, -1.2744e-01,  8.9478e-02, -1.6980e-01,
         1.3574e-01, -4.9866e-02, -1.4001e-01, -2.0660e-02,  6.9214e-02,
        -6.4697e-02,  3.5547e-01, -3.0365e-02, -5.8868e-02,  1.6205e-02,
         1.5617e-02, -9.3933e-02,  1.4429e-01,  1.6769e-02, -2.1301e-02,
         1.3391e-01,  1.0559e-01, -1.5356e-01,  4.4189e-02, -2.2449e-03,
         9.0759e-02,  1.7395e-01, -4.8218e-03, -2.0233e-02, -4.1809e-02,
        -5.4810e-02,  1.4270e-01,  5.6488e-02, -9.1492e-02, -4.0222e-02,
         2.7740e-02, -4.9652e-02,  1.6858e-01,  1.0675e-01, -4.3945e-02,
         2.2293e-02,  1.4795e-01,  2.4377e-01, -1.6150e-01,  5.3650e-02,
         5.6976e-02, -1.5576e-01,  3.2959e-02,  1.8567e-01,  1.5454e-01,
         1.0437e-01, -2.0410e-01, -7.5798e-03,  1.5210e-01, -2.3022e-01,
         1.0480e-01,  5.8289e-02, -2.3041e-02,  6.0944e-02, -3.7323e-02,
        -2.0605e-01,  1.6260e-01,  7.9468e-02, -1.2671e-01,  8.2397e-02,
        -2.4463e-01,  1.0339e-01,  7.5562e-02,  1.0004e-01,  2.4573e-01,
        -2.6108e-02, -1.3513e-01,  1.2445e-01, -1.6321e-01,  2.8882e-01,
         5.3619e-02,  7.2571e-02, -1.2964e-01, -5.5237e-02,  2.8782e-03,
         2.3651e-03,  2.6764e-02,  3.5286e-03, -6.9580e-02, -1.9470e-02,
         7.2021e-02,  1.6602e-02, -7.2365e-03,  1.0645e-01, -1.3831e-01,
        -5.7892e-02,  1.5576e-01, -1.9287e-01,  6.3782e-03, -1.9714e-01,
        -1.0419e-01, -1.8665e-01,  1.1884e-01, -4.6173e-02, -8.6121e-02,
        -1.4722e-01, -3.5675e-02,  3.3374e-01, -7.7209e-02, -1.1346e-01,
        -9.3018e-02, -7.9468e-02,  2.8535e+00,  3.7811e-02, -9.5398e-02,
         8.0261e-02,  2.8591e-03, -2.5757e-02,  2.4185e-02,  1.0553e-01,
         6.7505e-02, -1.7297e-01,  1.6626e-01, -1.7505e-01, -2.6581e-02,
         1.6931e-01, -6.7566e-02,  2.5543e-02, -7.9163e-02, -9.2468e-02,
        -2.1130e-01, -1.6833e-01,  3.4149e-02,  1.6858e-01, -1.4880e-01,
        -9.3933e-02,  2.8000e-02,  6.6643e-03,  9.3323e-02, -4.2480e-02,
        -8.1787e-02,  2.1350e-01, -1.1328e-01, -1.4856e-01,  3.6011e-01,
         1.2314e-02, -1.8286e-01,  9.1797e-02, -1.3260e-02, -1.6101e-01,
         1.8054e-01, -1.7990e-02, -8.9539e-02, -4.3274e-02,  1.7993e-01,
        -5.6519e-02,  4.4495e-02, -6.6223e-02,  1.9678e-01,  2.0523e-02,
         1.8774e-01, -2.0374e-01,  5.2551e-02, -2.1033e-01, -2.0740e-01,
         1.9202e-01,  8.7952e-02,  1.6663e-02,  6.9397e-02,  2.4512e-01,
        -2.5681e-02,  8.7402e-02, -1.1890e-01, -1.2939e-01,  3.6621e-02,
        -6.2134e-02,  2.7115e-02,  8.1238e-02, -1.6418e-01, -1.2463e-01,
         1.0876e-01,  1.0577e-01,  1.5613e-01,  1.6516e-01, -8.8501e-03,
        -1.0059e-01,  9.1492e-02, -1.0480e-01,  3.3905e-02, -9.6054e-03,
         5.0262e-02, -7.0496e-02, -2.2437e-01,  1.1566e-01, -4.0649e-02,
         1.2708e-01,  5.4413e-02, -3.6011e-02,  1.9592e-02,  1.8945e-01,
         7.6965e-02,  2.3938e-01,  7.8735e-02,  2.8491e-01, -1.3708e-01,
        -5.0507e-02, -5.0171e-02, -3.5309e-02, -1.9373e-01,  9.1309e-02,
         4.2084e-02,  1.0658e-02, -4.9255e-02, -2.0422e-01,  1.0266e-01,
         9.4971e-02,  6.9641e-02,  6.3477e-02, -6.7322e-02, -5.6580e-02,
         3.9673e-02,  1.8860e-02,  1.3806e-01,  5.4626e-02, -2.3145e-01,
        -2.0032e-01,  1.3086e-01,  7.6233e-02, -2.4536e-02, -2.3453e-02,
        -1.0394e-01,  1.5649e-01, -7.0740e-02, -9.3811e-02, -1.2732e-01,
        -1.7175e-01, -1.4087e-01, -7.5623e-02, -6.1462e-02, -3.4058e-01,
        -1.1115e-01,  1.1787e-02, -3.9246e-02,  2.5269e-02,  8.7524e-02,
        -9.3842e-03,  1.1627e-01, -1.1115e-01,  4.2755e-02,  1.8188e-01,
        -5.9540e-02,  1.7908e-01,  4.2725e-02,  6.0760e-02, -5.7922e-02,
        -2.4011e-01, -1.2184e-02, -1.0999e-01, -5.3345e-02,  7.1533e-02,
         5.6244e-02, -4.5898e-02,  2.4487e-01,  7.8964e-03,  1.2585e-01,
        -9.2285e-02,  1.8652e-01, -5.9387e-02,  2.8076e-02, -1.2856e-02,
         1.7444e-01,  3.6373e-03,  3.7750e-02,  6.0272e-02,  8.0017e-02,
        -5.8075e-02,  1.0547e-01, -4.0588e-02, -1.3220e-01,  1.0645e-01,
         1.3599e-01,  5.1056e-02, -9.2529e-02,  1.2915e-01, -1.7993e-01,
        -6.0577e-02, -4.3488e-02, -2.8824e-02,  1.1823e-01, -1.9882e-02,
        -7.2449e-02, -1.6785e-02,  6.5247e-02, -4.7073e-03,  8.0444e-02,
        -6.8909e-02, -7.3486e-02,  4.6204e-02,  3.7598e-02, -5.6580e-02,
        -9.2102e-02, -1.3403e-01,  8.9905e-02, -4.8950e-02, -5.6854e-02,
         1.6617e-02,  5.0232e-02,  5.3741e-02,  6.7017e-02, -1.7114e-01,
         1.2390e-01, -6.1584e-02, -4.9225e-02,  7.4341e-02, -7.8430e-02,
         1.1859e-01, -1.8127e-02,  9.7942e-04,  1.1499e-01, -8.2947e-02,
         2.5043e-03, -7.2510e-02,  4.4373e-02,  2.0361e-01,  9.3567e-02,
         1.2164e-01,  9.6130e-02,  1.3733e-01,  8.3618e-02, -1.2573e-01,
        -5.1346e-03,  7.1350e-02, -5.2673e-02,  1.3770e-01, -7.1289e-02,
         2.0294e-02, -8.5754e-02,  3.6713e-02, -4.5197e-02, -4.0161e-01,
         1.0187e-01, -1.6760e-01,  5.4840e-02,  5.2338e-02, -1.0120e-01,
        -6.1707e-02,  8.3984e-02, -1.0864e-01,  3.5492e-02,  1.5149e-01,
         4.5410e-02,  2.1240e-02, -1.2988e-01, -1.4722e-01, -6.7406e-03,
        -1.7258e-02, -6.3477e-02,  6.5430e-02, -9.7290e-02, -1.2115e-01,
        -1.1426e-01,  1.0071e-01,  5.2490e-02,  4.4785e-03,  6.3538e-02,
        -1.0834e-01, -1.5417e-01,  1.1505e-01,  6.0822e-02,  2.7664e-02,
         5.8823e-03, -1.2610e-01,  6.7505e-02,  1.6823e-03, -6.2256e-02,
         1.6064e-01, -8.3923e-03,  8.4595e-02,  1.4200e-03,  7.7148e-02,
         8.1055e-02,  1.6068e-02, -5.9998e-02, -4.0588e-03,  2.3987e-01,
        -1.4062e-01, -3.8544e-02, -1.5723e-01,  1.7532e-02,  1.0724e-01,
        -1.0303e-01, -2.9022e-02, -3.3417e-02, -6.9763e-02, -4.3457e-02,
         1.1212e-01,  1.0406e-01,  1.4978e-01,  1.7517e-01, -1.1664e-01,
        -5.3375e-02, -1.3794e-02, -8.0139e-02, -8.2153e-02,  1.7322e-01,
        -1.9226e-02,  5.3589e-02,  7.4890e-02,  6.2805e-02,  2.8290e-02,
         8.4473e-02,  7.5073e-03, -4.3518e-02,  2.6270e-01, -3.5980e-02,
         2.6636e-01,  1.1414e-01,  1.8091e-01,  1.1169e-02, -4.5105e-02,
        -3.8788e-02,  8.1116e-02,  1.2131e-02, -1.3477e-01, -7.1875e+00,
        -6.6948e-04,  6.2073e-02, -1.0217e-01,  5.1819e-02, -1.5869e-01,
        -8.2336e-02, -8.9233e-02,  1.2091e-01,  8.8440e-02, -5.6244e-02,
        -1.1438e-01, -4.9561e-02, -1.1359e-01, -1.4734e-01, -1.4368e-01,
         6.4636e-02, -6.9702e-02,  3.9093e-02,  2.3727e-02,  2.1576e-02,
        -3.1647e-02, -3.7384e-02, -1.6968e-01, -1.0895e-01,  7.3425e-02,
         1.0376e-01, -2.7954e-02,  7.8430e-02,  1.5466e-01,  8.2520e-02,
         1.8262e-01, -1.7676e-01, -9.2346e-02, -3.9124e-02, -9.7168e-02,
        -9.1797e-02, -1.9019e-01,  3.2471e-02, -1.5845e-01,  8.7559e-05,
         1.1029e-01, -5.4283e-03, -1.0956e-01,  1.0370e-01, -6.5430e-02,
        -9.5642e-02,  2.7481e-02, -1.2177e-01, -7.0923e-02, -7.9834e-02,
        -2.0538e-02,  7.6904e-02,  7.3059e-02, -2.0508e-01,  1.0223e-01,
         2.0459e-01,  1.5503e-01,  1.3893e-02, -7.2449e-02, -1.5030e-02,
        -1.5747e-01, -1.8713e-01,  3.1036e-02,  8.1543e-02, -1.2535e-02,
         2.3605e-02, -6.7406e-03, -1.7578e-01,  1.0181e-01,  1.6003e-01,
        -2.9556e-02, -5.6877e-03,  5.2856e-02,  1.7859e-01, -2.2717e-01,
        -4.3396e-02,  4.4128e-02,  9.1980e-02, -6.7810e-02, -3.9795e-02,
         2.7649e-02,  5.9021e-02,  1.1407e-01, -5.8350e-02,  2.4109e-01,
         1.3379e-01, -1.2671e-01, -8.6060e-02, -8.9188e-03,  1.5100e-01,
        -1.6919e-01, -7.2144e-02,  3.1372e-02,  1.4722e-01,  2.0889e-02,
         7.8796e-02, -1.1115e-01, -1.5640e-02, -1.8872e-01,  6.1798e-02,
         9.5215e-03, -8.1787e-02, -3.3741e-03,  6.9275e-02, -5.0995e-02,
        -4.7882e-02,  1.1646e-01,  1.3098e-01,  4.6875e-02,  1.1987e-01,
        -7.9163e-02, -1.6675e-01, -9.0027e-02, -7.5150e-03, -8.6426e-02,
        -4.0009e-02, -4.3274e-02, -8.6792e-02,  1.4563e-01, -1.9653e-02,
        -3.6201e-03,  6.0425e-02,  1.8652e-01,  1.4319e-01, -2.4048e-02,
        -1.0248e-01, -1.7126e-01,  9.4238e-02,  1.2769e-01,  6.8481e-02,
        -7.2250e-03, -2.3804e-02,  4.5746e-02, -4.7974e-02,  4.1107e-02,
        -2.5925e-02, -1.0913e-01,  2.2995e-02, -1.8433e-01, -4.4678e-02,
        -4.7913e-02,  1.2396e-01,  2.5732e-01, -3.2654e-02,  2.1130e-01,
        -4.9438e-02,  1.1487e-01, -2.1240e-01, -5.9692e-02,  3.6224e-02,
        -9.7900e-02,  1.1742e-02, -1.3220e-01,  1.2268e-01,  1.2964e-01,
         1.8579e-01, -3.6278e-03,  1.5778e-02,  2.1326e-01,  3.6426e-01,
        -5.0415e-02,  3.1128e-02, -2.0715e-01,  9.7656e-02,  5.9631e-02,
         1.8799e-01,  1.7859e-01, -1.3708e-01,  8.4167e-02,  5.9967e-02,
         2.7075e-01,  1.2421e-02,  8.4778e-02, -2.0642e-01, -1.2195e-01,
        -1.5100e-01, -1.1253e-02,  1.1139e-01, -2.0068e-01,  2.1915e-03,
        -7.4890e-02,  5.7434e-02,  1.8335e-01,  1.2036e-01,  1.4185e-01,
         8.3801e-02,  7.1716e-02, -4.1931e-02,  1.2561e-01, -9.9060e-02,
         6.2141e-03, -1.6968e-02,  7.0923e-02,  9.8206e-02, -7.6538e-02,
         1.5515e-01, -6.4880e-02, -1.1786e-01, -1.8347e-01, -5.0468e-03,
        -3.6377e-02, -8.3862e-02, -8.6731e-02,  4.1168e-02,  1.9547e-02,
         1.6602e-01, -1.2854e-01, -9.2590e-02, -2.1729e-01,  2.9507e-03,
         4.5074e-02, -2.8076e-02,  1.9189e-01,  3.4698e-02, -9.9182e-02,
        -6.1096e-02,  9.2102e-02, -7.0129e-02,  1.2903e-01, -8.4473e-02,
        -3.5648e-03, -1.9250e-01,  1.5979e-01,  9.2407e-02, -2.2266e-01,
         2.2839e-01, -2.3155e-03,  7.7759e-02, -2.7481e-02, -1.5442e-01,
        -1.3928e-01,  2.9648e-02,  1.9943e-02, -1.4435e-02,  1.0114e-01,
        -6.4941e-02, -1.0944e-01, -6.1157e-02,  8.1253e-03,  8.9661e-02,
         2.9190e-02,  6.3293e-02, -2.1326e-01, -2.2949e-01, -1.9250e-01,
        -2.0825e-01,  9.8694e-02, -6.9153e-02,  2.5464e-01, -2.1606e-01,
         1.5637e-01, -6.4819e-02, -4.6539e-02,  3.7262e-02, -1.1328e-01,
         6.9275e-02, -3.6346e-02,  1.9379e-02,  1.6556e-03,  8.8074e-02,
         7.0923e-02,  8.2642e-02,  6.3086e-04, -1.6983e-02, -1.0858e-01,
         3.4644e-01,  1.2134e-01,  3.4973e-02,  4.4128e-02, -1.4160e-01,
        -1.7761e-01,  3.7384e-02,  1.3477e-01, -9.5398e-02, -7.1899e-02,
        -8.4900e-02, -3.9825e-02,  1.1810e-02, -4.6326e-02,  1.3867e-01,
         9.7107e-02, -1.0150e-01, -3.7441e-03,  1.1786e-01,  2.5803e-02,
        -1.5845e-01, -2.1252e-01,  8.8135e-02, -1.6309e-01,  3.8879e-02,
         2.0004e-02, -8.9111e-03, -2.5162e-02], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([1.8728, 1.7547, 1.8036, 1.8577, 2.1824, 1.6302, 1.7899, 1.8454, 1.8129,
        1.7273, 1.7220, 1.6906, 1.7052, 1.8640, 1.8814, 1.7893, 1.8372, 1.7668,
        1.8278, 1.8139, 1.6810, 1.7612, 1.9072, 1.9237, 1.7676, 1.7158, 1.7243,
        1.8889, 1.9533, 1.7293, 1.8389, 1.7399, 1.7465, 1.8322, 1.7277, 1.7053,
        1.8235, 1.7012, 1.8006, 1.9238, 1.7329, 1.9680, 1.7680, 1.6244, 1.6845,
        1.7426, 2.0288, 1.8557, 1.8263, 1.7483, 1.8057, 1.7374, 1.6137, 1.7142,
        1.8097, 1.7203, 1.6717, 2.2543, 1.7730, 1.9483, 1.7105, 1.6814, 2.1237,
        1.8089, 1.8255, 2.6967, 1.8107, 1.6820, 1.7113, 1.7730, 1.9706, 1.9000,
        1.8153, 1.8284, 1.6183, 1.8163, 1.7406, 1.7113, 2.0869, 1.7986, 1.9555,
        1.8223, 1.8276, 1.7193, 1.8223, 1.8769, 1.7694, 1.7360, 1.7613, 1.6688,
        1.7022, 1.7667, 1.7065, 1.6603, 1.8120, 1.7049, 1.9174, 1.6893, 1.7328,
        1.7920, 1.7548, 1.7317, 1.6880, 1.7914, 1.9288, 1.8672, 1.7392, 1.7381,
        1.7374, 1.6784, 1.6162, 1.7353, 1.8124, 1.8877, 1.6735, 1.8267, 1.7956,
        1.7927, 1.7485, 1.7837, 1.8291, 1.6297, 1.7247, 1.8656, 1.9152, 1.6948,
        1.8756, 1.7589, 1.7512, 1.7557, 1.6466, 1.7673, 1.6632, 1.8049, 1.8177,
        1.7658, 1.7416, 1.9632, 1.7444, 1.6705, 1.7454, 1.7714, 1.6047, 1.8853,
        1.8237, 1.7208, 1.8655, 1.7895, 1.6101, 1.8483, 1.8135, 1.9494, 1.7945,
        1.7738, 1.8106, 1.7011, 1.8042, 1.7543, 1.6437, 1.7617, 1.7073, 1.9108,
        2.2943, 1.7387, 1.8584, 1.7676, 1.8051, 1.6746, 1.8105, 1.8633, 1.7904,
        1.7631, 1.6917, 1.6902, 1.8191, 1.8009, 2.6139, 2.0224, 1.6584, 1.7414,
        1.8323, 1.8182, 1.6671, 1.7693, 1.8126, 1.9716, 1.7960, 1.8354, 1.7195,
        1.7754, 1.6959, 1.6688, 1.7538, 1.8228, 1.9487, 1.7090, 1.7782, 1.7023,
        2.8828, 1.8100, 1.6670, 1.7395, 1.7515, 1.8846, 1.7747, 1.7251, 1.9862,
        1.7202, 1.6708, 1.7447, 1.5925, 1.8382, 1.7598, 1.6398, 1.8064, 1.8713,
        1.7965, 1.8117, 1.7469, 1.7719, 1.6719, 1.7656, 1.7959, 1.7350, 1.9117,
        1.7505, 1.8743, 1.8043, 1.7795, 1.5903, 1.7346, 1.8705, 1.7257, 1.7646,
        1.7982, 1.7138, 1.7988, 1.6006, 1.7668, 1.8458, 1.8039, 1.7732, 1.7253,
        1.9010, 1.6667, 1.6973, 1.7982, 1.9759, 1.7445, 1.8074, 1.8802, 1.7144,
        1.8232, 1.8887, 1.8002, 1.8160, 1.7317, 1.9969, 1.5780, 1.8576, 1.7781,
        1.8187, 1.8604, 1.8072, 1.7960, 1.7139, 1.7889, 1.7381, 2.0270, 1.7207,
        1.7016, 1.8712, 1.9156, 1.7318, 1.8476, 1.9266, 1.8543, 1.7598, 1.7594,
        1.6991, 1.7971, 1.7460, 1.7098, 1.6603, 1.7289, 1.6441, 1.5901, 1.7972,
        1.8199, 1.8494, 2.0410, 1.7342, 1.9012, 2.0701, 1.7688, 1.8882, 1.8565,
        1.6923, 1.7557, 1.8527, 1.7336, 1.8296, 1.6045, 1.8099, 1.6580, 1.8173,
        1.9574, 2.4231, 1.6354, 2.2620, 1.8750, 1.7571, 1.8287, 1.7256, 1.9842,
        1.8685, 1.7660, 1.8027, 1.7770, 1.8104, 1.8142, 1.7283, 1.6328, 1.6241,
        1.8117, 1.7310, 1.6545, 1.6059, 1.8148, 1.8364, 1.8917, 1.6462, 1.8115,
        1.8170, 1.7312, 1.7646, 1.6505, 1.7751, 1.7892, 1.8509, 1.9477, 2.3132,
        1.8190, 1.9001, 1.8442, 1.6871, 1.8139, 1.8468, 2.0367, 1.7056, 1.7112,
        1.7670, 1.8652, 1.7420, 1.8898, 1.7194, 1.7252, 1.7193, 1.7331, 1.7921,
        1.8597, 1.6955, 1.8549, 1.8233, 1.9745, 1.8662, 1.7944, 1.7510, 1.7609,
        1.8620, 1.7772, 1.7749, 1.8237, 1.6827, 1.7449, 1.8918, 2.0390, 1.6744,
        1.6268, 1.9001, 1.7299, 1.8229, 1.7977, 1.6669, 1.6024, 1.8188, 1.7334,
        1.8002, 1.7300, 2.1726, 1.6700, 1.7431, 1.8026, 1.7612, 1.7199, 1.6923,
        1.6644, 1.7084, 1.9433, 1.7660, 1.6032, 1.7658, 1.7543, 1.7204, 1.8941,
        1.9148, 1.6203, 1.9567, 1.7905, 1.6558, 1.8582, 1.8381, 2.0396, 1.9045,
        1.7705, 1.7511, 1.9576, 1.7374, 1.7310, 1.8140, 1.7889, 1.6220, 1.7523,
        1.7130, 1.7670, 1.9267, 1.6834, 1.7794, 1.6861, 1.8663, 1.8656, 1.8087,
        1.7948, 1.7197, 1.5690, 2.2319, 1.8988, 1.7588, 1.8252, 1.7361, 1.7862,
        1.7354, 1.7093, 1.6775, 1.8468, 1.7330, 1.8936, 1.7130, 1.9545, 1.7298,
        1.6729, 1.7887, 1.6690, 1.8468, 1.7840, 1.6812, 1.7597, 1.7837, 1.6500,
        1.6694, 1.9539, 2.0421, 1.8921, 1.6931, 1.9127, 1.6547, 1.7876, 1.8291,
        1.7312, 1.7571, 1.8522, 1.6939, 1.7974, 1.7912, 0.8882, 1.6792, 1.9011,
        1.9224, 1.9471, 1.8223, 1.7650, 1.6055, 1.8410, 1.8062, 1.8044, 1.6980,
        1.6566, 1.8625, 1.6706, 1.8128, 1.6267, 1.6477, 1.7553, 1.9766, 1.7803,
        1.8333, 1.8784, 1.8143, 1.6159, 1.7287, 1.7773, 1.8109, 1.7313, 1.7073,
        1.7380, 1.8799, 1.9860, 1.7834, 1.7660, 1.7020, 1.7776, 1.7250, 1.7347,
        1.8215, 1.8359, 1.6774, 1.6558, 1.9027, 1.8354, 1.8742, 1.9091, 1.5760,
        1.7863, 1.7307, 1.7591, 1.8141, 1.7780, 1.6756, 1.8921, 1.7581, 1.7023,
        2.0466, 1.8074, 1.7839, 1.8754, 1.7286, 1.7876, 1.7002, 2.0919, 1.8398,
        1.8331, 1.8582, 1.6425, 1.7268, 1.7142, 1.6585, 1.9305, 1.7850, 1.8508,
        1.8482, 1.7189, 1.7981, 1.7123, 1.9352, 1.6617, 1.8423, 1.6198, 1.8271,
        1.5744, 1.5694, 1.9952, 1.7246, 1.7511, 1.7516, 1.8460, 1.7108, 1.6408,
        1.7795, 1.7720, 1.7731, 1.8246, 1.7610, 1.7569, 1.7468, 1.6235, 1.6202,
        1.8685, 1.9336, 1.8680, 1.6949, 1.6995, 1.7635, 1.9551, 1.7691, 1.8124,
        1.6820, 1.7588, 1.7007, 1.8804, 1.7494, 1.7261, 1.6887, 1.7230, 1.6999,
        1.8258, 1.8233, 1.7310, 1.7570, 1.8468, 1.8310, 1.8870, 1.8488, 1.6868,
        1.6953, 1.8011, 1.7912, 1.8304, 1.7616, 1.7405, 1.7687, 1.6651, 1.8245,
        1.9685, 1.7588, 1.9942, 1.7595, 1.7126, 1.8003, 1.7433, 1.8008, 1.6225,
        1.7806, 1.9816, 1.6298, 1.8152, 1.7408, 1.6533, 1.8026, 1.6771, 1.8380,
        1.6243, 1.6354, 1.7953, 1.6672, 1.8827, 1.7984, 1.7944, 1.8477, 1.7569,
        1.7022, 1.7916, 1.7132, 1.8886, 1.8595, 1.8478, 1.6918, 1.7125, 1.8510,
        1.7128, 1.6492, 2.3644, 1.7261, 1.7210, 1.7260, 1.6130, 1.7513, 1.8390,
        1.5612, 1.7037, 1.7211, 2.0017, 1.8191, 1.6772, 1.9493, 1.7455, 1.7580,
        1.8318, 1.8643, 1.8332, 1.6268, 1.7308, 1.7625, 1.8703, 1.9427, 1.6668,
        1.7555, 1.9277, 1.8045, 1.8333, 1.8089, 1.8974, 1.8570, 1.6222, 1.6393,
        1.8872, 1.7288, 1.7060, 1.8189, 1.8870, 1.5363, 1.7967, 1.8584, 1.8575,
        1.8540, 1.9067, 1.7273, 1.8267, 1.6695, 1.7934, 2.0291, 1.8780, 1.8184,
        1.7510, 2.0461, 1.7423, 1.9446, 1.7160, 1.8476, 1.7205, 1.5758, 1.7424,
        1.8978, 1.6879, 1.7293, 1.7713, 2.0305, 1.7728, 1.7149, 1.7348, 1.8070,
        1.7017, 1.7348, 1.7224, 1.6461, 3.0367, 2.0496, 1.9265, 1.7293, 1.8574,
        1.7300, 1.9038, 1.9109, 1.7025, 1.8784, 1.7179, 1.7195, 1.8131, 1.7751,
        1.5399, 1.6682, 1.6308, 1.8071, 1.6887, 1.8143, 1.6503, 1.7686, 2.0083,
        1.8872, 1.8092, 1.8261, 1.8082, 1.7445, 1.7919, 1.7134, 1.7019, 1.8723,
        1.7027, 1.6786, 1.9378, 1.6835, 1.7235, 1.7465, 1.7253, 1.5730, 1.7733,
        1.7328, 1.8270, 1.6885], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 5.2365e-01, -7.1796e-01, -5.0786e-01, -4.9917e-01, -7.3772e-01,
         6.9723e-01, -3.5357e-01,  1.9300e-01,  8.1534e-01, -4.2375e-01,
        -5.5178e-01,  1.4156e-01,  4.5392e-01, -3.9982e-01,  9.1892e-02,
         2.9702e-01, -1.6993e-01,  5.2459e-01,  3.2687e-01,  8.4204e-01,
        -3.4065e-01, -8.2934e-02, -9.8496e-02, -1.5335e-01,  1.2601e-01,
        -2.3323e-01,  4.7130e-02,  1.0745e-01,  5.4473e-01,  1.7098e-01,
        -2.7386e-02, -1.1410e-01,  6.8870e-01,  3.2801e-01,  4.2133e-02,
        -7.9934e-01,  7.5729e-01,  3.4921e-01, -6.0004e-01,  1.1502e-01,
        -1.5464e-01,  3.9600e-02, -1.5119e-01, -6.2914e-01, -4.5401e-01,
        -3.7856e-01, -9.9999e-01,  6.7442e-01, -3.6019e-01, -7.1740e-02,
        -1.2775e+00,  9.6797e-01,  1.9182e-01,  7.7241e-01, -3.6558e-01,
        -3.3722e-01,  6.6506e-01, -4.6364e-01,  5.5047e-01,  7.0849e-01,
        -4.1890e-01,  1.7221e-01, -5.1826e-01, -2.4845e-01, -3.9541e-01,
        -3.9335e-01, -1.0862e+00,  1.6473e-01,  1.7781e-01, -3.8569e-02,
        -3.6542e-01,  5.1494e-01, -8.3685e-01, -4.0427e-02, -4.5675e-01,
        -1.7859e-01, -3.8725e-01,  8.1883e-01,  7.5297e-01,  9.6626e-01,
         6.9364e-01, -5.0789e-01, -2.9852e-01,  1.2151e-01,  5.9669e-01,
        -3.1539e-01,  2.4300e-01, -9.6667e-02,  2.1359e-01,  6.6510e-02,
         1.2985e-01, -6.6999e-01, -1.6000e-01, -7.6950e-01,  3.1830e-01,
         2.8169e-01, -1.0485e+00, -1.2472e-01,  4.7124e-01, -4.2533e-02,
         4.8433e-01,  6.8543e-01,  3.5366e-01, -7.9901e-01,  1.6506e-01,
        -4.2159e-01,  9.2392e-01,  9.5828e-01, -5.4434e-01,  1.0712e+00,
        -2.4472e-01,  7.0082e-01,  1.6221e-01,  1.4343e-01,  1.1916e-01,
         6.4597e-01, -1.1901e+00, -3.2111e-01,  3.1017e-01,  2.6418e-02,
         7.9504e-01, -3.1324e-01,  1.2178e-01,  1.0480e-01, -4.9646e-01,
         2.0638e-01,  8.8050e-01, -6.3201e-01,  6.5160e-01, -9.2663e-01,
        -2.6332e-01, -2.4718e-01,  7.2113e-01, -7.5739e-02, -7.9578e-01,
         2.6718e-02, -8.4464e-01,  9.4241e-01, -3.8431e-01, -2.1190e-01,
        -8.9685e-01, -8.7849e-01,  3.6954e-01,  1.9354e-01,  5.9779e-01,
         5.4826e-01, -1.2341e-01,  5.3911e-01, -1.6569e-01,  8.4252e-01,
         2.7934e-01,  1.2656e-02, -5.4159e-01,  6.6726e-01,  4.8667e-01,
         6.7404e-01,  1.9638e-01, -9.7165e-01,  5.7017e-01,  4.4719e-01,
        -4.4060e-01,  4.2154e-02,  5.2086e-01,  5.6147e-01,  3.0391e-01,
         7.2406e-02,  7.2969e-01,  1.9448e-01, -5.1772e-01,  7.0601e-01,
        -6.6572e-02,  3.0350e-01, -6.1288e-01,  1.8081e-01, -2.8082e-01,
         4.0170e-01, -7.3233e-01,  7.8688e-01,  4.8578e-01,  5.3802e-02,
         7.2736e-01, -1.2462e-01, -4.1762e-01, -5.2198e-01,  7.9471e-01,
        -2.1844e-01, -3.9000e-01, -7.0649e-02,  1.5447e-01,  3.3449e-01,
        -1.9374e-01, -1.0065e+00,  5.1472e-01,  4.9429e-01, -1.0720e+00,
         7.3440e-02,  7.5565e-01, -3.9948e-01, -2.7195e-01,  1.0148e+00,
        -6.9053e-01, -3.2311e-01, -9.6137e-02, -1.9534e-01, -3.0459e-01,
         1.9061e-01,  1.8854e-01,  1.7647e-01, -8.0750e-01,  6.3126e-03,
        -3.9351e-01,  8.9944e-03, -4.0237e-01,  7.4425e-01,  4.2765e-01,
         7.7177e-02, -3.1802e-01, -4.0009e-01, -9.7929e-01, -7.0733e-01,
        -4.4645e-02,  1.8130e-01,  7.0742e-01,  6.4393e-01,  7.3340e-01,
         5.9089e-01,  2.3528e-01, -1.2541e-01,  6.3674e-01,  5.6344e-01,
        -5.0353e-01,  9.3068e-02, -1.0097e+00, -8.5259e-01,  5.3163e-01,
        -2.0954e-01, -6.7231e-01,  5.8950e-01, -1.1848e-01,  5.2578e-01,
        -1.2783e+00,  1.8730e-02,  1.1769e+00,  6.8309e-01, -4.2683e-01,
        -3.9154e-01, -3.4926e-01, -5.6174e-01, -3.3042e-01,  8.3069e-02,
        -8.3279e-01, -1.1924e+00,  2.7345e-01, -1.9008e-01,  6.7653e-01,
         5.9039e-01,  8.5830e-01,  9.9681e-01,  7.1562e-01, -3.2135e-01,
        -1.6521e-01,  3.2021e-01,  4.2866e-01,  7.8429e-01, -7.3655e-01,
        -5.8396e-01, -1.3525e-01,  4.8509e-01,  5.6491e-02, -1.0362e-01,
        -6.2418e-01,  5.4077e-01, -1.6874e-01, -6.9453e-01,  7.5331e-01,
         7.8803e-01,  4.4891e-01, -1.9796e-01,  2.5425e-01,  3.6405e-01,
         3.2291e-01, -1.2160e-01,  6.6354e-01,  8.8945e-01,  8.0281e-01,
         2.7114e-01,  8.3829e-01,  3.3341e-01,  5.7132e-01,  1.4304e+00,
        -2.1592e-01,  9.8776e-02,  1.9750e-01, -9.7931e-01, -2.5027e-01,
        -7.6657e-02,  2.8210e-02,  5.7256e-01, -3.3332e-01, -3.7285e-01,
        -5.8753e-02, -3.2304e-03, -7.2074e-01, -3.4369e-01,  3.6055e-01,
         6.7007e-01, -6.9041e-01, -8.7381e-01,  5.3381e-01,  5.5394e-02,
        -1.1908e-01,  2.9258e-01, -2.1546e-01, -8.9148e-02, -1.7491e-01,
         9.0487e-01, -5.3882e-01,  8.9418e-01, -1.2240e-01, -1.8015e-01,
         2.9930e-01, -3.0655e-02,  4.2033e-01,  3.7214e-01, -4.8523e-01,
        -3.4483e-03, -5.0961e-01,  7.6495e-02,  5.2234e-01,  3.0821e-01,
        -1.6111e-02, -1.8429e-01,  2.2425e-01, -1.6561e-01,  5.7110e-01,
        -5.3549e-01,  1.0812e-01, -2.6734e-01, -1.0017e-03,  2.8036e-01,
         4.8367e-02,  6.0758e-03, -1.0951e-01,  1.6691e-01,  1.4140e-01,
         8.8960e-02, -1.9603e-01, -2.5611e-01,  1.3564e-01,  4.0519e-01,
         3.0434e-01,  2.0968e-01,  3.4171e-01,  1.7606e-01, -6.2441e-01,
         9.6200e-02, -6.4161e-01,  1.8472e-02, -4.7658e-01,  8.7947e-01,
        -4.6602e-01,  4.7910e-02,  5.2541e-01, -5.8759e-02, -1.3030e-01,
         3.8045e-01,  2.3253e-01,  4.5987e-01, -3.4145e-01,  2.9264e-01,
        -2.1473e-01, -4.5932e-01,  3.1802e-01, -9.6810e-01, -2.1568e-01,
        -7.9277e-01,  2.5702e-01, -6.7151e-01, -4.9960e-01,  2.7052e-01,
         6.7502e-01, -3.3173e-01,  9.9372e-01, -4.2096e-01,  2.1906e-01,
         3.3975e-02,  4.1775e-01, -2.8452e-01,  8.6553e-01,  3.9230e-01,
         1.1056e-01,  5.2030e-01, -2.8196e-01, -2.2177e-01,  7.0446e-01,
        -8.6578e-02,  1.3139e-01, -8.7306e-02,  1.1427e-01,  8.5759e-02,
        -1.4070e-01, -2.9741e-01,  8.9071e-01,  4.8202e-01, -5.9489e-02,
         8.9789e-01,  5.3881e-01,  6.0686e-01, -2.2308e-01,  8.6642e-01,
         4.9077e-01, -5.5091e-02, -4.3449e-01, -6.7774e-01,  3.7506e-02,
         2.6588e-01,  1.5247e-01, -2.2813e-01,  9.3671e-02,  3.6551e-01,
         5.5127e-01,  1.6033e-01, -2.5638e-01, -4.1804e-02, -3.0825e-02,
         2.9872e-01,  6.0427e-03, -1.8824e-01, -1.0125e-02,  8.9432e-01,
        -5.2097e-01,  8.5012e-01, -2.2281e-01, -4.3597e-01, -8.0840e-01,
        -9.1855e-01,  3.0273e-01,  1.0803e-01,  1.1152e+00, -1.5719e-01,
         1.1975e-02,  3.6843e-01, -2.9271e-01,  4.6732e-01,  1.7604e-01,
        -3.3923e-01, -2.9831e-01, -4.8610e-01, -7.5712e-02,  4.3407e-01,
         1.1111e+00,  5.1410e-01,  6.9860e-01,  4.0695e-01, -1.0682e+00,
         1.8455e-01, -4.9250e-01, -3.1826e-01, -2.9163e-01, -2.3546e-01,
        -7.8442e-01,  9.2442e-01,  1.6520e-01, -1.2188e+00,  5.7878e-01,
        -8.5155e-01,  6.7063e-02,  2.6550e-02, -4.3127e-01, -2.7951e-01,
         1.8323e-02,  1.9658e-02,  4.7443e-01,  6.1131e-01, -2.3093e+00,
         3.5181e-01,  2.5726e-01,  4.7805e-01, -1.7803e-01, -1.2482e-01,
         4.9293e-01,  5.0750e-01, -3.7749e-01, -2.6515e-01,  2.8590e-01,
         5.1095e-01,  1.3173e-01, -8.0175e-03,  1.1047e-01,  8.3509e-01,
        -4.4624e-01,  4.7691e-01,  1.7419e-01, -4.7241e-01,  3.1154e-01,
         2.9991e-01,  4.3542e-01,  1.2321e+00,  2.3803e-01, -4.3119e-01,
         3.7148e-01,  4.9344e-01, -3.0547e-01, -5.3341e-01, -3.1853e-02,
        -2.7490e-01,  3.2135e-01,  2.6234e-01, -1.9682e-02,  7.0717e-01,
        -4.2983e-01,  4.6524e-01, -5.3194e-01,  9.8069e-01,  5.8870e-01,
         2.1646e-01, -3.4338e-01,  4.2862e-02,  2.7347e-01, -2.2594e-01,
         1.0674e+00,  4.2078e-01,  4.8330e-01,  6.1738e-01,  3.2063e-01,
         1.7535e-01,  9.7011e-02, -1.8358e-01,  9.3179e-01, -2.2381e-02,
        -9.4270e-01,  1.1792e-01, -2.6643e-01,  2.1232e-01,  4.9830e-01,
         7.6591e-01,  2.9870e-02, -4.3988e-01,  4.0557e-01,  2.6872e-01,
        -4.1879e-02,  7.5626e-02,  6.6198e-01, -8.3230e-01, -3.0211e-01,
        -2.0827e-01, -7.0032e-01, -1.4148e-01, -1.6967e-01,  7.0183e-01,
         8.5013e-02, -4.1698e-01, -6.0489e-02,  3.0557e-01,  3.9021e-02,
         6.1819e-01, -6.1305e-01,  3.1326e-01, -3.0995e-01, -1.5488e+00,
        -3.0525e-01,  8.8306e-02,  9.2133e-01, -6.2372e-01, -1.9223e-01,
         3.8362e-01,  5.9477e-01, -6.0004e-01,  1.2034e-01, -2.1234e-01,
         5.9456e-02,  6.9879e-01, -3.6176e-01,  6.5457e-01, -4.7436e-01,
         1.7996e-01,  3.0792e-01,  7.9678e-02, -6.7011e-01,  5.5580e-01,
         1.0828e-01, -1.7653e-01,  4.6241e-02,  2.5056e-01, -8.9252e-01,
         3.1166e-01,  7.0509e-01,  6.8284e-01, -6.6525e-03,  6.1851e-01,
        -5.9312e-02,  8.9731e-02,  2.2366e-01, -5.5600e-01,  1.0849e+00,
         2.4813e-01,  1.1594e-01, -1.2167e+00,  1.5649e-01, -8.0943e-02,
         1.0084e+00,  5.9600e-01, -7.4801e-01, -6.5032e-01,  2.3075e-01,
         3.7942e-01,  4.2537e-01,  1.2587e-01, -4.7321e-01,  3.1443e-01,
         5.0104e-01,  9.8989e-01,  8.0250e-01,  3.8222e-01,  7.4728e-01,
         5.6268e-01, -4.8270e-02, -1.1665e+00,  3.3048e-02, -7.2375e-01,
         2.0525e-01, -2.7832e-01,  8.6653e-02, -2.8562e-02,  3.1306e-01,
         7.6600e-01, -8.0847e-01,  5.4784e-01, -2.8227e-01, -6.6010e-01,
        -9.8385e-01,  2.1724e-01,  2.2076e-02,  7.8854e-01, -9.0521e-01,
         3.7701e-01,  1.6502e-01,  4.0696e-01, -7.0733e-01,  3.6993e-01,
         5.1732e-01, -8.8974e-01,  2.7491e-01, -6.1666e-01,  5.1971e-01,
        -7.9852e-01, -2.4639e-01, -7.7815e-02,  7.8357e-01,  8.9507e-01,
        -6.5967e-01, -5.7339e-01, -3.1713e-01,  4.5455e-01,  1.1265e-01,
         2.2680e-01, -1.7445e-01, -3.9281e-01, -2.5807e-01, -3.0213e-01,
         3.4656e-01, -7.4623e-01, -2.8558e-01,  1.7893e-01,  1.1342e-01,
        -8.0159e-01, -2.1829e-01, -9.9254e-01, -1.5305e-01, -1.0703e-03,
        -1.1041e+00, -4.0161e-01, -3.3409e-02,  1.1676e-02, -2.7472e-01,
         1.2675e-01, -4.5706e-01,  4.1187e-01, -7.0135e-01,  7.1590e-01,
         5.4015e-01,  1.0804e+00,  1.8346e-01,  6.5775e-01,  4.7076e-01,
        -1.9371e-01, -7.5276e-02,  4.3526e-01,  3.9449e-01,  4.0887e-01,
        -5.3782e-02, -4.4384e-01,  1.9879e-02, -9.1176e-02,  2.1702e-01,
         1.1800e-01,  5.0274e-01, -7.4890e-01, -4.1458e-01,  4.5222e-01,
        -3.2062e-01, -7.9956e-02,  6.9394e-01, -1.0899e+00,  3.4254e-01,
        -1.7928e-01,  6.2240e-01,  6.2699e-02, -1.3474e-01, -5.6386e-01,
         3.9947e-01,  1.0371e+00,  3.3629e-01, -4.3833e-01,  1.2022e-01,
        -5.9730e-01, -4.6715e-01,  1.3945e+00,  7.4473e-01,  8.4637e-01,
         6.8066e-01, -1.5404e-01,  3.6263e-01, -7.1079e-02, -2.0128e-01,
         1.5455e-02, -5.0949e-01,  1.2212e-01, -5.4024e-01,  9.5051e-01,
        -6.7772e-01, -5.8960e-01, -3.9299e-01,  3.8189e-01, -9.1321e-02,
        -8.0553e-01,  5.3645e-01, -1.0406e+00,  2.0083e-01,  1.0773e+00,
        -1.1015e+00, -3.1549e-01, -3.2300e-01,  2.5178e-01,  6.0958e-01,
         4.2411e-01,  1.3246e+00, -6.4755e-02,  4.4368e-02,  5.3248e-01,
         4.2212e-01, -2.6460e-01,  8.3685e-01, -5.3068e-01, -1.3308e-01,
        -1.4837e-01,  3.9950e-02,  5.2396e-02,  4.1660e-01, -4.7404e-01,
         9.1058e-01,  4.1668e-01, -2.8680e-02,  3.0489e-01, -5.8090e-02,
         3.7067e-01,  4.8497e-02,  4.4988e-01], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[-0.0061,  0.0216,  0.0378,  ...,  0.0087, -0.0147, -0.0091],
        [-0.0168, -0.0002,  0.0211,  ..., -0.0099,  0.0221, -0.0031],
        [-0.0080,  0.0057,  0.0215,  ..., -0.0246, -0.0132, -0.0026],
        ...,
        [-0.0203, -0.0034, -0.0117,  ...,  0.0065, -0.0158, -0.0084],
        [ 0.0024,  0.0246, -0.0107,  ...,  0.0023, -0.0117, -0.0079],
        [-0.0057, -0.0211, -0.0082,  ..., -0.0046,  0.0054, -0.0058]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.5361, -0.4248, -0.5415,  ..., -0.5884, -0.7373, -0.2561],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0039, -0.0219, -0.0056,  ...,  0.0122, -0.0174,  0.0043],
        [ 0.0055,  0.0005,  0.0034,  ...,  0.0172, -0.0199,  0.0144],
        [-0.0199, -0.0112, -0.0061,  ...,  0.0151, -0.0054,  0.0031],
        ...,
        [-0.0059,  0.0005,  0.0246,  ..., -0.0153,  0.0092,  0.0027],
        [ 0.0113,  0.0209,  0.0061,  ...,  0.0087,  0.0100, -0.0050],
        [-0.0006,  0.0108,  0.0045,  ...,  0.0284, -0.0053,  0.0162]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-2.9590e-01, -4.0253e-02, -1.9165e-01, -4.5410e-01, -2.3712e-02,
         4.4067e-02,  7.4768e-02,  4.4703e-04, -1.4130e-02,  4.3945e-02,
        -6.3904e-02, -1.5918e-01, -2.5528e-02, -1.0309e-01, -8.4351e-02,
         3.9581e-02, -1.4819e-01,  3.3569e-02,  7.2998e-02, -1.4124e-01,
        -2.0667e-01, -1.0364e-01,  3.0542e-01, -2.5732e-01, -4.1077e-02,
        -8.7158e-02,  9.7717e-02, -1.2891e-01,  1.2901e-02, -4.3030e-02,
        -2.5903e-01,  7.9117e-03, -1.3269e-01, -1.2366e-01, -3.4546e-02,
         1.7041e-01, -2.7969e-02, -5.4169e-02,  7.0862e-02, -2.1399e-01,
         8.4991e-03, -7.7248e-03,  3.8574e-02,  1.6992e-01, -9.3384e-02,
         2.0813e-01,  3.1281e-02,  1.2964e-01,  2.5574e-02, -4.7272e-02,
         1.1328e-01,  3.9856e-02,  1.1298e-01, -4.8553e-02,  3.2776e-02,
         3.4351e-03, -3.6011e-02,  1.0506e-02, -3.4790e-02,  2.0508e-01,
        -5.3040e-02,  4.5258e-02,  1.4977e-02,  3.6804e-02,  2.9343e-02,
         1.5283e-01,  3.9978e-02,  3.1952e-02,  1.9760e-02,  1.4709e-01,
        -1.2500e-01, -7.8857e-02,  1.5894e-01, -7.4463e-02, -6.4331e-02,
         2.6636e-01,  2.1851e-01, -2.4927e-01, -5.5908e-02, -1.9379e-02,
        -1.1237e-01,  3.1555e-02,  6.7291e-03,  1.0150e-01, -1.0187e-01,
         2.2903e-02,  2.2144e-01,  8.0200e-02,  1.8555e-01, -7.0839e-03,
         7.8552e-02, -8.8196e-02,  8.1360e-02,  6.9946e-02, -1.7773e-01,
        -2.0695e-03, -3.5156e-01,  2.8809e-01, -2.0264e-02, -8.1238e-02,
        -2.9739e-02,  1.1401e-01, -2.1606e-02,  1.4795e-01,  8.8072e-04,
        -8.4381e-03,  2.5864e-02,  4.0771e-02, -1.0229e-01, -1.2225e-01,
        -6.2561e-02, -1.0016e-01, -2.7368e-01, -8.6060e-02, -3.1433e-02,
         3.0670e-02,  6.1798e-02,  1.9455e-02, -1.5533e-02,  2.2266e-01,
        -1.4453e-01,  1.1023e-01,  5.7007e-02, -5.2917e-02,  2.1851e-02,
        -2.9083e-02, -2.3743e-02,  1.5674e-01, -1.9775e-01, -2.3056e-02,
        -9.3460e-03,  1.0193e-01, -1.8726e-01, -6.4392e-02, -6.2805e-02,
         7.5562e-02, -1.6760e-01, -1.1688e-01,  2.8015e-02, -1.0919e-01,
        -1.1879e-02, -5.0049e-02,  1.8845e-02, -7.2205e-02,  2.4365e-01,
         2.6855e-02, -1.5388e-02,  6.9763e-02,  5.0232e-02,  1.3477e-01,
        -9.5459e-02, -7.9224e-02,  2.4048e-01, -8.8501e-02, -1.0388e-01,
        -7.4463e-02,  8.4229e-02, -5.0476e-02,  2.0111e-02, -1.8762e-01,
         7.7209e-02,  1.3318e-01,  1.2168e+00, -1.1208e-02,  6.1646e-02,
        -5.8319e-02, -2.2003e-02,  1.2573e-01, -7.0679e-02, -1.4001e-01,
        -1.2064e-03,  2.1530e-02, -2.8473e-02,  2.4551e-02,  3.5461e-02,
         5.5206e-02,  5.6122e-02, -1.0205e-01,  3.4088e-02, -1.1230e-01,
        -2.6147e-01, -4.8431e-02,  1.2146e-01, -6.3721e-02, -4.0527e-02,
         2.0642e-01,  9.1553e-02, -2.6779e-02,  1.3525e-01, -7.9712e-02,
         1.5480e-02,  9.6680e-02, -6.1005e-02, -5.5237e-02,  1.9275e-01,
        -1.2561e-01, -7.4707e-02,  2.5269e-01,  8.6792e-02, -1.7395e-02,
        -5.9387e-02,  6.9580e-02, -7.6782e-02,  2.5879e-02,  3.7842e-02,
        -1.1726e-02,  6.5063e-02,  1.3573e-02,  7.0129e-02,  9.0714e-03,
         4.5776e-02, -6.6040e-02,  6.6650e-02, -4.7852e-02, -1.3025e-01,
         1.4587e-01,  2.0972e-01,  7.6599e-02, -1.8701e-01,  8.8928e-02,
         1.6003e-01, -7.3814e-03, -1.5283e-01, -2.7252e-02,  4.1168e-02,
         3.6621e-02, -6.9092e-02,  3.0716e-02, -6.1432e-02, -3.8330e-02,
        -1.2793e-01, -1.3354e-01,  5.9082e-02, -4.7180e-02, -8.1360e-02,
        -7.0572e-03, -3.7537e-02, -1.4343e-01,  3.2959e-02, -2.9236e-02,
         1.8384e-01,  1.3403e-01, -2.1606e-01,  5.8136e-02, -1.0608e-01,
         2.0752e-01,  1.9458e-01, -2.6337e-02,  2.3779e-01,  2.1301e-02,
        -3.8422e-02,  2.4426e-01, -6.0394e-02, -6.2561e-02, -8.0750e-02,
         4.0680e-02, -7.0810e-04, -1.9019e-01, -1.3342e-01,  4.2023e-02,
         3.3245e-03,  1.8262e-01, -3.8971e-02, -7.0007e-02,  7.7896e-03,
        -1.2781e-01,  2.5903e-01,  7.0557e-02,  1.2024e-01,  1.7273e-01,
         8.8562e-02,  2.0825e-01, -9.3231e-03, -9.8389e-02, -6.7017e-02,
        -8.6426e-02, -1.3452e-01,  2.9614e-01,  1.4542e-02, -1.7590e-01,
        -2.1387e-01,  1.6223e-01, -1.8713e-01, -8.6243e-02, -7.2510e-02,
        -4.6120e-03, -2.8290e-02,  3.9307e-02,  3.2471e-01, -6.1157e-02,
         3.3646e-03, -4.1382e-02, -1.2463e-01,  1.7383e-01, -5.4131e-03,
        -1.9263e-01, -1.0979e-02, -2.4951e-01,  1.4343e-01,  1.7053e-01,
         6.4697e-02,  1.6575e-03, -1.6586e-02,  1.5839e-02,  3.9246e-02,
        -1.6687e-01,  1.7468e-01,  1.8457e-01, -1.1737e-01,  2.4216e-02,
         1.3708e-01,  1.4595e-02,  2.1631e-01, -2.5742e-02,  1.6064e-01,
        -3.2129e-01,  5.4016e-02, -1.0266e-01,  4.7264e-03, -1.2238e-01,
         3.3765e-01, -9.9640e-03, -1.0114e-01,  5.9326e-02, -8.7036e-02,
        -1.0773e-01, -3.5187e-02, -3.4332e-02,  4.6295e-02,  5.4688e-02,
         8.1177e-02,  4.6448e-02,  5.7190e-02, -1.2164e-01, -2.7481e-02,
         1.1151e-01, -2.0630e-01,  1.9028e-02,  1.1517e-01, -3.5431e-02,
        -4.4769e-02, -1.0028e-01,  2.7954e-01,  1.3000e-01,  2.0691e-01,
        -3.9124e-02, -1.0669e-01, -4.2480e-02,  1.7346e-01, -3.8528e-03,
        -1.4026e-01, -1.1115e-01,  5.2338e-03,  7.7087e-02, -3.5645e-02,
         2.6489e-02,  1.3062e-02,  5.5756e-02,  1.2769e-01,  4.7455e-02,
        -8.1726e-02,  3.9520e-02, -2.2339e-02,  1.7590e-01, -4.4365e-03,
        -3.5919e-02,  8.0322e-02,  2.7393e-01, -7.9895e-02, -1.2390e-01,
        -6.1310e-02, -1.6711e-01, -1.7410e-02,  2.0178e-01, -8.9417e-02,
         8.0017e-02, -1.4490e-01,  1.3135e-01, -1.1688e-01, -7.2693e-02,
         4.4037e-02, -2.5314e-02, -1.9446e-01,  9.2590e-02,  9.1858e-03,
        -2.0493e-02,  1.7712e-01,  5.5511e-02, -1.1908e-01,  9.3872e-02,
         2.5749e-03, -7.6721e-02,  3.7098e-03, -9.8648e-03,  9.6680e-02,
        -6.0059e-02, -1.2744e-01,  2.9354e-03, -4.1656e-02,  3.8544e-02,
        -8.4106e-02,  1.1084e-01, -8.6182e-02,  7.4219e-02,  2.6749e-02,
         5.2795e-02, -2.4509e-03, -1.1700e-01, -3.4393e-02,  2.2781e-02,
        -2.9037e-02, -1.1212e-01,  3.5522e-02,  7.7332e-02,  5.1392e-02,
        -3.5065e-02,  5.5115e-02,  6.6452e-03,  8.0078e-02, -1.5393e-01,
        -2.7930e-01,  9.9640e-03, -4.9438e-02, -3.7628e-02, -1.9458e-01,
         5.6793e-02, -1.0315e-01,  4.5074e-02,  1.7365e-02,  2.3651e-02,
         2.1408e-02,  8.1787e-03, -1.7480e-01,  6.2927e-02,  8.3801e-02,
         5.3558e-02,  7.6180e-03, -1.1334e-01, -7.8247e-02,  2.8667e-03,
        -6.3416e-02,  6.2988e-02,  2.1643e-01,  6.2561e-03, -2.1362e-01,
        -5.3986e-02,  1.7725e-01,  1.1670e-01, -1.8994e-01,  1.9653e-01,
         1.9019e-01, -9.4652e-04,  3.6835e-02, -4.3762e-02,  3.2684e-02,
        -1.6992e-01, -1.6174e-01, -1.2720e-01,  4.3610e-02, -1.9141e-01,
         1.6809e-01, -3.8696e-02, -1.2769e-01, -1.7896e-01, -7.1228e-02,
        -1.0086e-02,  1.3794e-01, -1.3940e-01, -2.0279e-02, -8.5999e-02,
         1.5839e-02,  1.9910e-01, -1.0004e-01,  7.7438e-03, -2.9102e+00,
         2.0081e-01,  3.2074e-02, -1.2115e-01,  4.0863e-02, -3.6865e-02,
         2.2018e-02,  6.1249e-02, -9.7168e-02,  5.5695e-02,  1.5930e-01,
        -1.0162e-01,  7.9712e-02,  3.5217e-02,  5.8472e-02,  8.0444e-02,
         6.3599e-02, -1.4801e-02,  1.7357e-03,  3.2153e-01, -5.4321e-02,
         6.0883e-02, -2.1484e-01,  1.0399e-02, -7.3181e-02,  7.9468e-02,
        -6.1401e-02, -3.4302e-01,  4.6875e-02,  7.9468e-02,  8.6914e-02,
         3.0899e-02, -1.9702e-01,  2.4084e-01,  1.0040e-01,  1.2244e-01,
        -1.7847e-01, -1.3257e-01,  9.2712e-02, -6.2286e-02, -8.8318e-02,
         1.8164e-01, -4.6082e-02, -8.9233e-02,  2.8412e-02, -9.6893e-03,
        -1.6504e-01,  6.8970e-02,  3.8116e-02,  2.6367e-01, -2.3645e-01,
         3.8208e-02, -2.3556e-03, -5.4932e-03, -3.3539e-02,  7.7307e-05,
        -1.7676e-01, -1.7505e-01,  2.1347e-02, -1.4832e-02, -1.3879e-01,
        -7.8796e-02, -7.0862e-02, -6.1096e-02, -1.3818e-01,  6.1859e-02,
         3.8086e-02, -1.9897e-01,  1.4941e-01,  1.1169e-01,  1.1578e-01,
        -6.3110e-02,  1.6968e-01,  8.6594e-03, -4.9194e-02, -2.4329e-01,
         1.7261e-01,  4.1504e-02,  1.1957e-01, -1.5649e-01, -1.4307e-01,
        -5.1544e-02,  6.3660e-02,  1.9409e-02,  1.9608e-02,  1.0785e-01,
        -1.7151e-01, -5.6610e-02, -1.0071e-03, -2.2839e-01, -1.1896e-01,
         1.8127e-01, -1.7071e-03, -7.4158e-02, -1.1340e-01,  1.3538e-01,
        -1.1639e-01, -1.1559e-02,  6.2500e-02, -4.6631e-02, -6.3171e-02,
         4.8599e-03, -1.6675e-01, -3.7994e-02,  3.4637e-02,  1.6577e-01,
         9.7595e-02, -2.4826e-02, -3.0766e-03,  1.1060e-01,  1.0114e-01,
         1.2231e-01,  1.2878e-02, -2.4268e-01,  6.8045e-04,  4.6021e-02,
         1.0437e-02, -1.8762e-01, -1.3147e-01,  1.5393e-01,  2.9388e-02,
         2.2205e-01,  2.8610e-02,  3.0365e-02, -3.2257e-02,  8.6731e-02,
        -7.7820e-02, -6.4392e-02, -1.0144e-01, -4.7302e-02, -7.9529e-02,
        -6.4468e-03,  2.5317e-01,  1.1414e-01,  1.2793e-01,  9.6893e-03,
        -1.3931e-02,  4.8950e-02,  6.9275e-02, -4.1779e-02,  1.1096e-01,
         6.3057e-03,  1.3818e-01, -1.8625e-03, -1.6809e-01,  2.0935e-01,
         1.3208e-01, -1.3184e-01, -7.0984e-02,  1.1829e-01,  1.5942e-01,
        -1.6711e-01,  5.1697e-02, -2.3438e-02, -8.4106e-02,  7.5867e-02,
         6.1127e-02,  1.7639e-01,  7.8630e-04, -1.8591e-01, -5.0842e-02,
        -2.4402e-01, -4.5258e-02, -8.4000e-03,  5.9204e-02, -1.2611e-02,
        -2.6184e-02,  5.6488e-02,  1.5857e-01,  3.0322e-01, -1.6931e-01,
         6.1432e-02,  2.2424e-01, -3.1952e-02, -1.4270e-01, -1.4087e-01,
        -1.9455e-02,  9.7778e-02, -2.0703e-01, -2.0337e-01, -1.9699e-02,
        -2.3120e-01, -3.6133e-02,  6.3538e-02,  3.3569e-02,  3.3203e-02,
         4.7455e-02, -7.8613e-02,  2.0178e-01, -2.6016e-02,  1.5186e-01,
        -7.0740e-02,  2.3071e-02,  6.3171e-02,  7.5302e-03, -5.4550e-03,
         1.4429e-03, -1.9873e-01,  8.9844e-02,  3.3966e-02,  1.4978e-01,
        -1.0706e-01, -9.5642e-02, -1.9055e-01, -7.9651e-02, -2.3010e-01,
        -4.8218e-02,  1.1768e-01, -2.6993e-02,  2.4216e-02,  1.2805e-01,
         1.0254e-01, -6.7017e-02,  2.0532e-01, -1.9141e-01, -2.3706e-01,
        -1.5503e-01, -6.0272e-02,  7.2083e-02,  3.3169e-03, -2.0618e-01,
         1.2245e-02, -7.1289e-02,  8.6792e-02,  5.0232e-02, -3.9307e-02,
        -2.7512e-02,  8.1848e-02,  3.9368e-02,  9.4788e-02, -1.5373e-02,
         1.3367e-01, -4.4785e-03,  9.5398e-02, -2.1851e-02,  1.7017e-01,
         6.8481e-02, -3.4454e-02, -8.7036e-02, -1.4661e-01, -8.5938e-02,
        -2.7563e-01,  5.7190e-02, -1.0846e-01, -1.9312e-01, -1.0974e-01,
        -1.2012e-01,  3.4973e-02, -6.6650e-02,  1.8726e-01,  1.7651e-01,
        -4.8431e-02,  7.8247e-02,  3.9001e-02, -9.8755e-02,  9.0149e-02,
         2.0471e-01, -2.8613e-01,  1.0962e-01, -6.2256e-02,  2.3242e-01,
         4.7760e-02, -4.9957e-02, -1.1151e-01,  9.3384e-02,  6.3354e-02,
         1.0455e-01,  1.4001e-01,  1.1639e-01,  3.6774e-02, -1.3770e-01,
        -3.3325e-01, -1.2366e-01, -1.2402e-01,  2.1672e-04, -5.4688e-02,
        -3.9948e-02,  1.3342e-01, -1.2469e-01, -6.0196e-03,  7.2571e-02,
         8.7769e-02, -7.4463e-02, -2.3181e-01, -1.7639e-01, -1.3623e-01,
         8.8013e-02, -1.0242e-01,  5.3772e-02,  3.7933e-02, -1.6785e-01,
         4.8492e-02, -8.1421e-02, -1.2634e-01], device='cuda:0',
       requires_grad=True)]
2024-05-15 09:31:10.415 | INFO     | __main__:train:38 - [Parameter containing:
tensor([[ 0.0140,  0.0086, -0.0101,  ..., -0.0117, -0.0243,  0.0044],
        [-0.0255,  0.0537,  0.0109,  ...,  0.0122, -0.0141, -0.0050],
        [ 0.0052,  0.0088, -0.0233,  ..., -0.0006, -0.0374, -0.0270],
        ...,
        [-0.0082,  0.0195,  0.0208,  ...,  0.0177,  0.0125,  0.0179],
        [ 0.0060,  0.0263,  0.0043,  ...,  0.0071,  0.0258,  0.0259],
        [ 0.0220,  0.0209,  0.0165,  ..., -0.0156,  0.0295, -0.0047]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0138, -0.0109, -0.0047,  ...,  0.0097, -0.0180,  0.0049],
        [-0.0048,  0.0361, -0.0159,  ..., -0.0111,  0.0125,  0.0179],
        [-0.0059,  0.0111, -0.0034,  ..., -0.0119,  0.0029,  0.0209],
        ...,
        [-0.0116,  0.0152, -0.0191,  ...,  0.0415,  0.0013,  0.0277],
        [ 0.0257, -0.0091, -0.0276,  ...,  0.0088,  0.0040,  0.0081],
        [-0.0384,  0.0135, -0.0108,  ..., -0.0070, -0.0051,  0.0077]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0228, -0.0118, -0.0041,  ..., -0.0367,  0.0029,  0.0461],
        [-0.0385,  0.0101,  0.0005,  ..., -0.0171, -0.0031, -0.0187],
        [ 0.0123, -0.0247, -0.0075,  ...,  0.0023,  0.0374, -0.0162],
        ...,
        [ 0.0168, -0.0095, -0.0204,  ...,  0.0013,  0.0051,  0.0040],
        [ 0.0025, -0.0468, -0.0601,  ...,  0.0210, -0.0471,  0.0403],
        [-0.0046,  0.0167, -0.0252,  ...,  0.0092, -0.0288, -0.0052]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0270, -0.0419, -0.0426,  ...,  0.0132, -0.0078,  0.0168],
        [ 0.0090,  0.0196, -0.0313,  ..., -0.0302,  0.0011, -0.0198],
        [ 0.0317, -0.0116, -0.0089,  ...,  0.0257,  0.0005, -0.0264],
        ...,
        [-0.0410,  0.0411,  0.0237,  ..., -0.0377,  0.0200, -0.0133],
        [ 0.0059,  0.0018,  0.0257,  ...,  0.0165, -0.0149, -0.0335],
        [-0.0208, -0.0046, -0.0012,  ...,  0.0415, -0.0019,  0.0380]],
       device='cuda:0', requires_grad=True)]
2024-05-15 09:31:19.916 | INFO     | __main__:train:123 - Epoch: [0][0/390]	 loss 9.02754	 cls_loss: 4.6247 cluster_loss: 4.5810 sup_con_loss: 2.7763 contrastive_loss: 5.3224 
2024-05-15 09:31:35.504 | INFO     | __main__:train:123 - Epoch: [0][20/390]	 loss 7.19419	 cls_loss: 2.6880 cluster_loss: 3.8162 sup_con_loss: 1.9332 contrastive_loss: 4.7634 
2024-05-15 09:31:47.991 | INFO     | __main__:train:123 - Epoch: [0][40/390]	 loss 5.68419	 cls_loss: 1.4107 cluster_loss: 2.2066 sup_con_loss: 2.0271 contrastive_loss: 4.6872 
2024-05-15 09:32:00.787 | INFO     | __main__:train:123 - Epoch: [0][60/390]	 loss 5.36236	 cls_loss: 1.2083 cluster_loss: 1.9825 sup_con_loss: 1.7634 contrastive_loss: 4.6671 
2024-05-15 09:32:14.017 | INFO     | __main__:train:123 - Epoch: [0][80/390]	 loss 5.35211	 cls_loss: 1.1002 cluster_loss: 1.8447 sup_con_loss: 2.1278 contrastive_loss: 4.6511 
2024-05-15 09:32:26.851 | INFO     | __main__:train:123 - Epoch: [0][100/390]	 loss 5.37948	 cls_loss: 1.0614 cluster_loss: 1.8075 sup_con_loss: 2.3086 contrastive_loss: 4.6539 
2024-05-15 09:32:39.795 | INFO     | __main__:train:123 - Epoch: [0][120/390]	 loss 5.19104	 cls_loss: 0.8901 cluster_loss: 1.8341 sup_con_loss: 1.8000 contrastive_loss: 4.7036 
2024-05-15 09:32:52.733 | INFO     | __main__:train:123 - Epoch: [0][140/390]	 loss 4.87577	 cls_loss: 0.8386 cluster_loss: 1.6588 sup_con_loss: 1.3653 contrastive_loss: 4.6556 
2024-05-15 09:33:05.451 | INFO     | __main__:train:123 - Epoch: [0][160/390]	 loss 4.85235	 cls_loss: 0.9185 cluster_loss: 1.6292 sup_con_loss: 1.2977 contrastive_loss: 4.6426 
2024-05-15 09:33:18.328 | INFO     | __main__:train:123 - Epoch: [0][180/390]	 loss 5.15163	 cls_loss: 1.0559 cluster_loss: 1.5718 sup_con_loss: 2.1008 contrastive_loss: 4.6541 
2024-05-15 09:33:31.039 | INFO     | __main__:train:123 - Epoch: [0][200/390]	 loss 4.86390	 cls_loss: 0.9971 cluster_loss: 1.6043 sup_con_loss: 1.2812 contrastive_loss: 4.6518 
2024-05-15 09:33:44.019 | INFO     | __main__:train:123 - Epoch: [0][220/390]	 loss 4.85465	 cls_loss: 0.7547 cluster_loss: 1.6522 sup_con_loss: 1.4239 contrastive_loss: 4.6434 
2024-05-15 09:33:56.868 | INFO     | __main__:train:123 - Epoch: [0][240/390]	 loss 4.69834	 cls_loss: 0.7175 cluster_loss: 1.5245 sup_con_loss: 1.2390 contrastive_loss: 4.6503 
2024-05-15 09:34:09.814 | INFO     | __main__:train:123 - Epoch: [0][260/390]	 loss 4.85103	 cls_loss: 0.7294 cluster_loss: 1.6302 sup_con_loss: 1.4334 contrastive_loss: 4.6683 
2024-05-15 09:34:22.734 | INFO     | __main__:train:123 - Epoch: [0][280/390]	 loss 5.08801	 cls_loss: 0.9938 cluster_loss: 1.5286 sup_con_loss: 2.0990 contrastive_loss: 4.6338 
2024-05-15 09:34:35.738 | INFO     | __main__:train:123 - Epoch: [0][300/390]	 loss 4.84120	 cls_loss: 0.8488 cluster_loss: 1.6046 sup_con_loss: 1.3433 contrastive_loss: 4.6630 
2024-05-15 09:34:48.614 | INFO     | __main__:train:123 - Epoch: [0][320/390]	 loss 4.76295	 cls_loss: 0.7015 cluster_loss: 1.5555 sup_con_loss: 1.3956 contrastive_loss: 4.6430 
2024-05-15 09:35:01.443 | INFO     | __main__:train:123 - Epoch: [0][340/390]	 loss 4.71777	 cls_loss: 0.6985 cluster_loss: 1.5440 sup_con_loss: 1.3147 contrastive_loss: 4.6301 
2024-05-15 09:35:14.241 | INFO     | __main__:train:123 - Epoch: [0][360/390]	 loss 4.74949	 cls_loss: 0.9574 cluster_loss: 1.5237 sup_con_loss: 1.1460 contrastive_loss: 4.6506 
2024-05-15 09:35:26.682 | INFO     | __main__:train:123 - Epoch: [0][380/390]	 loss 5.05744	 cls_loss: 0.9901 cluster_loss: 1.5470 sup_con_loss: 1.9422 contrastive_loss: 4.6548 
2024-05-15 09:35:32.406 | INFO     | __main__:train:126 - Train Epoch: 0 Avg Loss: 5.2863 
2024-05-15 09:35:32.408 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:35:58.835 | INFO     | __main__:train:135 - Train Accuracies: All 0.7473 | Old 0.7876 | New 0.5860
2024-05-15 09:36:03.667 | INFO     | __main__:train:123 - Epoch: [1][0/390]	 loss 4.75290	 cls_loss: 0.7485 cluster_loss: 1.4104 sup_con_loss: 1.6067 contrastive_loss: 4.6336 
2024-05-15 09:36:16.693 | INFO     | __main__:train:123 - Epoch: [1][20/390]	 loss 4.78949	 cls_loss: 0.9697 cluster_loss: 1.4247 sup_con_loss: 1.4368 contrastive_loss: 4.6479 
2024-05-15 09:36:29.638 | INFO     | __main__:train:123 - Epoch: [1][40/390]	 loss 4.78723	 cls_loss: 0.7571 cluster_loss: 1.4145 sup_con_loss: 1.6842 contrastive_loss: 4.6360 
2024-05-15 09:36:42.702 | INFO     | __main__:train:123 - Epoch: [1][60/390]	 loss 4.91055	 cls_loss: 0.9071 cluster_loss: 1.6206 sup_con_loss: 1.4756 contrastive_loss: 4.6512 
2024-05-15 09:36:55.524 | INFO     | __main__:train:123 - Epoch: [1][80/390]	 loss 4.63003	 cls_loss: 0.7319 cluster_loss: 1.3759 sup_con_loss: 1.3582 contrastive_loss: 4.6218 
2024-05-15 09:37:08.597 | INFO     | __main__:train:123 - Epoch: [1][100/390]	 loss 4.58058	 cls_loss: 0.6574 cluster_loss: 1.3395 sup_con_loss: 1.3269 contrastive_loss: 4.6391 
2024-05-15 09:37:21.603 | INFO     | __main__:train:123 - Epoch: [1][120/390]	 loss 4.99887	 cls_loss: 0.7873 cluster_loss: 1.5217 sup_con_loss: 2.0378 contrastive_loss: 4.6477 
2024-05-15 09:37:34.630 | INFO     | __main__:train:123 - Epoch: [1][140/390]	 loss 4.59418	 cls_loss: 0.6832 cluster_loss: 1.3659 sup_con_loss: 1.2865 contrastive_loss: 4.6415 
2024-05-15 09:37:47.465 | INFO     | __main__:train:123 - Epoch: [1][160/390]	 loss 5.01579	 cls_loss: 1.0401 cluster_loss: 1.4638 sup_con_loss: 1.9160 contrastive_loss: 4.6611 
2024-05-15 09:38:00.382 | INFO     | __main__:train:123 - Epoch: [1][180/390]	 loss 4.75134	 cls_loss: 0.8330 cluster_loss: 1.4210 sup_con_loss: 1.4883 contrastive_loss: 4.6388 
2024-05-15 09:38:13.118 | INFO     | __main__:train:123 - Epoch: [1][200/390]	 loss 4.58385	 cls_loss: 0.7288 cluster_loss: 1.4006 sup_con_loss: 1.1683 contrastive_loss: 4.6300 
2024-05-15 09:38:26.029 | INFO     | __main__:train:123 - Epoch: [1][220/390]	 loss 4.66630	 cls_loss: 0.7009 cluster_loss: 1.3889 sup_con_loss: 1.4384 contrastive_loss: 4.6380 
2024-05-15 09:38:38.682 | INFO     | __main__:train:123 - Epoch: [1][240/390]	 loss 4.67172	 cls_loss: 0.8347 cluster_loss: 1.3513 sup_con_loss: 1.4137 contrastive_loss: 4.6253 
2024-05-15 09:38:51.740 | INFO     | __main__:train:123 - Epoch: [1][260/390]	 loss 5.04532	 cls_loss: 0.7848 cluster_loss: 1.6310 sup_con_loss: 1.9955 contrastive_loss: 4.6339 
2024-05-15 09:39:04.799 | INFO     | __main__:train:123 - Epoch: [1][280/390]	 loss 4.85244	 cls_loss: 0.9735 cluster_loss: 1.4997 sup_con_loss: 1.4689 contrastive_loss: 4.6505 
2024-05-15 09:39:17.490 | INFO     | __main__:train:123 - Epoch: [1][300/390]	 loss 4.55237	 cls_loss: 0.7717 cluster_loss: 1.4564 sup_con_loss: 0.9155 contrastive_loss: 4.6388 
2024-05-15 09:39:30.529 | INFO     | __main__:train:123 - Epoch: [1][320/390]	 loss 4.85225	 cls_loss: 0.8356 cluster_loss: 1.4359 sup_con_loss: 1.7545 contrastive_loss: 4.6344 
2024-05-15 09:39:43.753 | INFO     | __main__:train:123 - Epoch: [1][340/390]	 loss 4.94512	 cls_loss: 0.8590 cluster_loss: 1.4786 sup_con_loss: 1.8866 contrastive_loss: 4.6509 
2024-05-15 09:39:56.507 | INFO     | __main__:train:123 - Epoch: [1][360/390]	 loss 4.72769	 cls_loss: 0.7173 cluster_loss: 1.4793 sup_con_loss: 1.4406 contrastive_loss: 4.6321 
2024-05-15 09:40:08.878 | INFO     | __main__:train:123 - Epoch: [1][380/390]	 loss 4.67647	 cls_loss: 0.9336 cluster_loss: 1.3640 sup_con_loss: 1.2848 contrastive_loss: 4.6360 
2024-05-15 09:40:14.545 | INFO     | __main__:train:126 - Train Epoch: 1 Avg Loss: 4.7557 
2024-05-15 09:40:14.546 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:40:41.184 | INFO     | __main__:train:135 - Train Accuracies: All 0.7575 | Old 0.7920 | New 0.6195
2024-05-15 09:40:45.561 | INFO     | __main__:train:123 - Epoch: [2][0/390]	 loss 4.97298	 cls_loss: 1.0169 cluster_loss: 1.4352 sup_con_loss: 1.9077 contrastive_loss: 4.6408 
2024-05-15 09:40:59.563 | INFO     | __main__:train:123 - Epoch: [2][20/390]	 loss 4.70085	 cls_loss: 0.8376 cluster_loss: 1.4151 sup_con_loss: 1.3624 contrastive_loss: 4.6324 
2024-05-15 09:41:12.182 | INFO     | __main__:train:123 - Epoch: [2][40/390]	 loss 4.75114	 cls_loss: 0.8443 cluster_loss: 1.3619 sup_con_loss: 1.5822 contrastive_loss: 4.6410 
2024-05-15 09:41:25.101 | INFO     | __main__:train:123 - Epoch: [2][60/390]	 loss 4.81181	 cls_loss: 0.7550 cluster_loss: 1.4437 sup_con_loss: 1.6889 contrastive_loss: 4.6432 
2024-05-15 09:41:38.212 | INFO     | __main__:train:123 - Epoch: [2][80/390]	 loss 4.80839	 cls_loss: 0.7692 cluster_loss: 1.5293 sup_con_loss: 1.5027 contrastive_loss: 4.6449 
2024-05-15 09:41:51.107 | INFO     | __main__:train:123 - Epoch: [2][100/390]	 loss 4.69688	 cls_loss: 0.6421 cluster_loss: 1.3558 sup_con_loss: 1.6783 contrastive_loss: 4.6207 
2024-05-15 09:42:03.849 | INFO     | __main__:train:123 - Epoch: [2][120/390]	 loss 4.75120	 cls_loss: 0.7638 cluster_loss: 1.4316 sup_con_loss: 1.5239 contrastive_loss: 4.6461 
2024-05-15 09:42:16.874 | INFO     | __main__:train:123 - Epoch: [2][140/390]	 loss 4.44503	 cls_loss: 0.6941 cluster_loss: 1.2357 sup_con_loss: 1.1322 contrastive_loss: 4.6194 
2024-05-15 09:42:29.841 | INFO     | __main__:train:123 - Epoch: [2][160/390]	 loss 4.83132	 cls_loss: 0.8354 cluster_loss: 1.4173 sup_con_loss: 1.7115 contrastive_loss: 4.6441 
2024-05-15 09:42:43.167 | INFO     | __main__:train:123 - Epoch: [2][180/390]	 loss 4.57163	 cls_loss: 0.6387 cluster_loss: 1.3360 sup_con_loss: 1.3329 contrastive_loss: 4.6357 
2024-05-15 09:42:56.383 | INFO     | __main__:train:123 - Epoch: [2][200/390]	 loss 4.73290	 cls_loss: 0.8543 cluster_loss: 1.3648 sup_con_loss: 1.5538 contrastive_loss: 4.6199 
2024-05-15 09:43:09.273 | INFO     | __main__:train:123 - Epoch: [2][220/390]	 loss 4.81435	 cls_loss: 0.8434 cluster_loss: 1.4673 sup_con_loss: 1.5764 contrastive_loss: 4.6364 
2024-05-15 09:43:22.358 | INFO     | __main__:train:123 - Epoch: [2][240/390]	 loss 4.39911	 cls_loss: 0.6368 cluster_loss: 1.2534 sup_con_loss: 1.0378 contrastive_loss: 4.6127 
2024-05-15 09:43:35.247 | INFO     | __main__:train:123 - Epoch: [2][260/390]	 loss 4.55369	 cls_loss: 0.6437 cluster_loss: 1.3518 sup_con_loss: 1.2612 contrastive_loss: 4.6282 
2024-05-15 09:43:48.408 | INFO     | __main__:train:123 - Epoch: [2][280/390]	 loss 4.55543	 cls_loss: 0.7812 cluster_loss: 1.4904 sup_con_loss: 0.8543 contrastive_loss: 4.6372 
2024-05-15 09:44:01.545 | INFO     | __main__:train:123 - Epoch: [2][300/390]	 loss 4.61109	 cls_loss: 0.8013 cluster_loss: 1.3035 sup_con_loss: 1.3688 contrastive_loss: 4.6219 
2024-05-15 09:44:14.537 | INFO     | __main__:train:123 - Epoch: [2][320/390]	 loss 4.75508	 cls_loss: 0.8180 cluster_loss: 1.3774 sup_con_loss: 1.5771 contrastive_loss: 4.6484 
2024-05-15 09:44:27.391 | INFO     | __main__:train:123 - Epoch: [2][340/390]	 loss 4.84885	 cls_loss: 0.7632 cluster_loss: 1.4787 sup_con_loss: 1.7575 contrastive_loss: 4.6238 
2024-05-15 09:44:40.189 | INFO     | __main__:train:123 - Epoch: [2][360/390]	 loss 4.78046	 cls_loss: 0.8080 cluster_loss: 1.4961 sup_con_loss: 1.4399 contrastive_loss: 4.6480 
2024-05-15 09:44:52.480 | INFO     | __main__:train:123 - Epoch: [2][380/390]	 loss 4.59983	 cls_loss: 0.7444 cluster_loss: 1.3362 sup_con_loss: 1.3144 contrastive_loss: 4.6319 
2024-05-15 09:44:58.376 | INFO     | __main__:train:126 - Train Epoch: 2 Avg Loss: 4.6660 
2024-05-15 09:44:58.376 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:45:24.698 | INFO     | __main__:train:135 - Train Accuracies: All 0.7721 | Old 0.8007 | New 0.6575
2024-05-15 09:45:28.935 | INFO     | __main__:train:123 - Epoch: [3][0/390]	 loss 4.75576	 cls_loss: 0.9203 cluster_loss: 1.2856 sup_con_loss: 1.6530 contrastive_loss: 4.6453 
2024-05-15 09:45:42.784 | INFO     | __main__:train:123 - Epoch: [3][20/390]	 loss 4.67423	 cls_loss: 0.7960 cluster_loss: 1.2945 sup_con_loss: 1.5647 contrastive_loss: 4.6254 
2024-05-15 09:45:55.600 | INFO     | __main__:train:123 - Epoch: [3][40/390]	 loss 4.81682	 cls_loss: 0.6890 cluster_loss: 1.4466 sup_con_loss: 1.7882 contrastive_loss: 4.6300 
2024-05-15 09:46:08.538 | INFO     | __main__:train:123 - Epoch: [3][60/390]	 loss 4.46654	 cls_loss: 0.5399 cluster_loss: 1.3482 sup_con_loss: 1.1349 contrastive_loss: 4.6216 
2024-05-15 09:46:21.358 | INFO     | __main__:train:123 - Epoch: [3][80/390]	 loss 4.82176	 cls_loss: 0.6939 cluster_loss: 1.4815 sup_con_loss: 1.7123 contrastive_loss: 4.6410 
2024-05-15 09:46:34.184 | INFO     | __main__:train:123 - Epoch: [3][100/390]	 loss 4.79302	 cls_loss: 0.9379 cluster_loss: 1.4523 sup_con_loss: 1.4292 contrastive_loss: 4.6470 
2024-05-15 09:46:47.021 | INFO     | __main__:train:123 - Epoch: [3][120/390]	 loss 4.34380	 cls_loss: 0.5429 cluster_loss: 1.2973 sup_con_loss: 0.8705 contrastive_loss: 4.6244 
2024-05-15 09:46:59.708 | INFO     | __main__:train:123 - Epoch: [3][140/390]	 loss 4.86541	 cls_loss: 0.7546 cluster_loss: 1.3577 sup_con_loss: 2.0293 contrastive_loss: 4.6285 
2024-05-15 09:47:12.556 | INFO     | __main__:train:123 - Epoch: [3][160/390]	 loss 4.55304	 cls_loss: 0.7170 cluster_loss: 1.2551 sup_con_loss: 1.3728 contrastive_loss: 4.6243 
2024-05-15 09:47:25.379 | INFO     | __main__:train:123 - Epoch: [3][180/390]	 loss 4.61248	 cls_loss: 0.8598 cluster_loss: 1.3198 sup_con_loss: 1.2607 contrastive_loss: 4.6345 
2024-05-15 09:47:38.508 | INFO     | __main__:train:123 - Epoch: [3][200/390]	 loss 4.66835	 cls_loss: 0.8380 cluster_loss: 1.3035 sup_con_loss: 1.5054 contrastive_loss: 4.6168 
2024-05-15 09:47:51.475 | INFO     | __main__:train:123 - Epoch: [3][220/390]	 loss 4.53642	 cls_loss: 0.7503 cluster_loss: 1.2830 sup_con_loss: 1.2339 contrastive_loss: 4.6277 
2024-05-15 09:48:04.585 | INFO     | __main__:train:123 - Epoch: [3][240/390]	 loss 4.65175	 cls_loss: 0.6616 cluster_loss: 1.3080 sup_con_loss: 1.5934 contrastive_loss: 4.6343 
2024-05-15 09:48:17.287 | INFO     | __main__:train:123 - Epoch: [3][260/390]	 loss 4.49777	 cls_loss: 0.6433 cluster_loss: 1.2180 sup_con_loss: 1.3573 contrastive_loss: 4.6244 
2024-05-15 09:48:30.405 | INFO     | __main__:train:123 - Epoch: [3][280/390]	 loss 4.51113	 cls_loss: 0.6135 cluster_loss: 1.3092 sup_con_loss: 1.2694 contrastive_loss: 4.6171 
2024-05-15 09:48:43.633 | INFO     | __main__:train:123 - Epoch: [3][300/390]	 loss 4.44319	 cls_loss: 0.5804 cluster_loss: 1.3044 sup_con_loss: 1.0760 contrastive_loss: 4.6393 
2024-05-15 09:48:56.403 | INFO     | __main__:train:123 - Epoch: [3][320/390]	 loss 4.83301	 cls_loss: 0.9031 cluster_loss: 1.3978 sup_con_loss: 1.7269 contrastive_loss: 4.6215 
2024-05-15 09:49:09.282 | INFO     | __main__:train:123 - Epoch: [3][340/390]	 loss 4.51400	 cls_loss: 0.8332 cluster_loss: 1.2257 sup_con_loss: 1.2090 contrastive_loss: 4.6193 
2024-05-15 09:49:22.466 | INFO     | __main__:train:123 - Epoch: [3][360/390]	 loss 4.29509	 cls_loss: 0.4694 cluster_loss: 1.2655 sup_con_loss: 0.8814 contrastive_loss: 4.6150 
2024-05-15 09:49:35.061 | INFO     | __main__:train:123 - Epoch: [3][380/390]	 loss 4.64100	 cls_loss: 0.7251 cluster_loss: 1.3622 sup_con_loss: 1.4203 contrastive_loss: 4.6226 
2024-05-15 09:49:40.675 | INFO     | __main__:train:126 - Train Epoch: 3 Avg Loss: 4.6077 
2024-05-15 09:49:40.676 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:50:07.645 | INFO     | __main__:train:135 - Train Accuracies: All 0.7674 | Old 0.8067 | New 0.6100
2024-05-15 09:50:13.507 | INFO     | __main__:train:123 - Epoch: [4][0/390]	 loss 4.61258	 cls_loss: 0.6552 cluster_loss: 1.2913 sup_con_loss: 1.5345 contrastive_loss: 4.6259 
2024-05-15 09:50:26.461 | INFO     | __main__:train:123 - Epoch: [4][20/390]	 loss 4.38036	 cls_loss: 0.7534 cluster_loss: 1.1895 sup_con_loss: 0.9947 contrastive_loss: 4.6082 
2024-05-15 09:50:39.644 | INFO     | __main__:train:123 - Epoch: [4][40/390]	 loss 4.64167	 cls_loss: 0.7145 cluster_loss: 1.2800 sup_con_loss: 1.5765 contrastive_loss: 4.6274 
2024-05-15 09:50:52.546 | INFO     | __main__:train:123 - Epoch: [4][60/390]	 loss 4.47685	 cls_loss: 0.7080 cluster_loss: 1.2678 sup_con_loss: 1.1391 contrastive_loss: 4.6251 
2024-05-15 09:51:05.332 | INFO     | __main__:train:123 - Epoch: [4][80/390]	 loss 4.37118	 cls_loss: 0.5969 cluster_loss: 1.1870 sup_con_loss: 1.1113 contrastive_loss: 4.6181 
2024-05-15 09:51:18.305 | INFO     | __main__:train:123 - Epoch: [4][100/390]	 loss 4.74780	 cls_loss: 0.8524 cluster_loss: 1.2431 sup_con_loss: 1.8308 contrastive_loss: 4.6164 
2024-05-15 09:51:31.225 | INFO     | __main__:train:123 - Epoch: [4][120/390]	 loss 4.67276	 cls_loss: 0.7047 cluster_loss: 1.3213 sup_con_loss: 1.5963 contrastive_loss: 4.6286 
2024-05-15 09:51:44.342 | INFO     | __main__:train:123 - Epoch: [4][140/390]	 loss 4.71807	 cls_loss: 0.6832 cluster_loss: 1.3526 sup_con_loss: 1.6964 contrastive_loss: 4.6246 
2024-05-15 09:51:57.454 | INFO     | __main__:train:123 - Epoch: [4][160/390]	 loss 4.60533	 cls_loss: 0.6067 cluster_loss: 1.3871 sup_con_loss: 1.3846 contrastive_loss: 4.6258 
2024-05-15 09:52:10.577 | INFO     | __main__:train:123 - Epoch: [4][180/390]	 loss 4.29582	 cls_loss: 0.5445 cluster_loss: 1.1697 sup_con_loss: 0.9582 contrastive_loss: 4.6301 
2024-05-15 09:52:23.687 | INFO     | __main__:train:123 - Epoch: [4][200/390]	 loss 4.84925	 cls_loss: 0.9201 cluster_loss: 1.4548 sup_con_loss: 1.6040 contrastive_loss: 4.6464 
2024-05-15 09:52:36.666 | INFO     | __main__:train:123 - Epoch: [4][220/390]	 loss 4.67280	 cls_loss: 0.8029 cluster_loss: 1.3300 sup_con_loss: 1.4698 contrastive_loss: 4.6351 
2024-05-15 09:52:49.709 | INFO     | __main__:train:123 - Epoch: [4][240/390]	 loss 4.44288	 cls_loss: 0.6816 cluster_loss: 1.2063 sup_con_loss: 1.1749 contrastive_loss: 4.6292 
2024-05-15 09:53:02.674 | INFO     | __main__:train:123 - Epoch: [4][260/390]	 loss 4.51300	 cls_loss: 0.6690 cluster_loss: 1.2558 sup_con_loss: 1.3213 contrastive_loss: 4.6155 
2024-05-15 09:53:15.452 | INFO     | __main__:train:123 - Epoch: [4][280/390]	 loss 4.41997	 cls_loss: 0.5728 cluster_loss: 1.2193 sup_con_loss: 1.2245 contrastive_loss: 4.6129 
2024-05-15 09:53:28.734 | INFO     | __main__:train:123 - Epoch: [4][300/390]	 loss 4.52897	 cls_loss: 0.7141 cluster_loss: 1.2792 sup_con_loss: 1.2688 contrastive_loss: 4.6207 
2024-05-15 09:53:41.590 | INFO     | __main__:train:123 - Epoch: [4][320/390]	 loss 4.58517	 cls_loss: 0.6388 cluster_loss: 1.3653 sup_con_loss: 1.3229 contrastive_loss: 4.6325 
2024-05-15 09:53:54.752 | INFO     | __main__:train:123 - Epoch: [4][340/390]	 loss 4.62302	 cls_loss: 0.6613 cluster_loss: 1.3587 sup_con_loss: 1.4284 contrastive_loss: 4.6284 
2024-05-15 09:54:07.532 | INFO     | __main__:train:123 - Epoch: [4][360/390]	 loss 4.65488	 cls_loss: 0.7036 cluster_loss: 1.3872 sup_con_loss: 1.4154 contrastive_loss: 4.6332 
2024-05-15 09:54:20.027 | INFO     | __main__:train:123 - Epoch: [4][380/390]	 loss 4.52876	 cls_loss: 0.6541 cluster_loss: 1.2785 sup_con_loss: 1.3216 contrastive_loss: 4.6250 
2024-05-15 09:54:25.841 | INFO     | __main__:train:126 - Train Epoch: 4 Avg Loss: 4.5515 
2024-05-15 09:54:25.841 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:54:52.397 | INFO     | __main__:train:135 - Train Accuracies: All 0.7729 | Old 0.8079 | New 0.6330
2024-05-15 09:54:58.052 | INFO     | __main__:train:123 - Epoch: [5][0/390]	 loss 4.42122	 cls_loss: 0.7281 cluster_loss: 1.2836 sup_con_loss: 0.9262 contrastive_loss: 4.6275 
2024-05-15 09:55:10.842 | INFO     | __main__:train:123 - Epoch: [5][20/390]	 loss 4.28398	 cls_loss: 0.5105 cluster_loss: 1.1737 sup_con_loss: 0.9621 contrastive_loss: 4.6240 
2024-05-15 09:55:23.756 | INFO     | __main__:train:123 - Epoch: [5][40/390]	 loss 4.47713	 cls_loss: 0.5502 cluster_loss: 1.2632 sup_con_loss: 1.3228 contrastive_loss: 4.6161 
2024-05-15 09:55:36.647 | INFO     | __main__:train:123 - Epoch: [5][60/390]	 loss 4.38335	 cls_loss: 0.5838 cluster_loss: 1.1912 sup_con_loss: 1.1502 contrastive_loss: 4.6187 
2024-05-15 09:55:49.228 | INFO     | __main__:train:123 - Epoch: [5][80/390]	 loss 4.73446	 cls_loss: 0.8079 cluster_loss: 1.3239 sup_con_loss: 1.6623 contrastive_loss: 4.6298 
2024-05-15 09:56:02.017 | INFO     | __main__:train:123 - Epoch: [5][100/390]	 loss 4.76469	 cls_loss: 0.8990 cluster_loss: 1.4067 sup_con_loss: 1.4969 contrastive_loss: 4.6335 
2024-05-15 09:56:14.974 | INFO     | __main__:train:123 - Epoch: [5][120/390]	 loss 4.27810	 cls_loss: 0.4377 cluster_loss: 1.1559 sup_con_loss: 1.0658 contrastive_loss: 4.6162 
2024-05-15 09:56:27.930 | INFO     | __main__:train:123 - Epoch: [5][140/390]	 loss 4.23995	 cls_loss: 0.5481 cluster_loss: 1.1601 sup_con_loss: 0.8337 contrastive_loss: 4.6189 
2024-05-15 09:56:40.897 | INFO     | __main__:train:123 - Epoch: [5][160/390]	 loss 4.22564	 cls_loss: 0.5755 cluster_loss: 1.1855 sup_con_loss: 0.7078 contrastive_loss: 4.6245 
2024-05-15 09:56:53.920 | INFO     | __main__:train:123 - Epoch: [5][180/390]	 loss 4.40508	 cls_loss: 0.5848 cluster_loss: 1.2091 sup_con_loss: 1.1727 contrastive_loss: 4.6216 
2024-05-15 09:57:06.837 | INFO     | __main__:train:123 - Epoch: [5][200/390]	 loss 4.47207	 cls_loss: 0.5522 cluster_loss: 1.2720 sup_con_loss: 1.2936 contrastive_loss: 4.6142 
2024-05-15 09:57:19.980 | INFO     | __main__:train:123 - Epoch: [5][220/390]	 loss 4.20726	 cls_loss: 0.3917 cluster_loss: 1.1103 sup_con_loss: 1.0020 contrastive_loss: 4.6119 
2024-05-15 09:57:33.072 | INFO     | __main__:train:123 - Epoch: [5][240/390]	 loss 4.39691	 cls_loss: 0.6714 cluster_loss: 1.3000 sup_con_loss: 0.9071 contrastive_loss: 4.6145 
2024-05-15 09:57:45.945 | INFO     | __main__:train:123 - Epoch: [5][260/390]	 loss 4.57092	 cls_loss: 0.7855 cluster_loss: 1.2168 sup_con_loss: 1.4047 contrastive_loss: 4.6360 
2024-05-15 09:57:58.714 | INFO     | __main__:train:123 - Epoch: [5][280/390]	 loss 4.28846	 cls_loss: 0.5611 cluster_loss: 1.2047 sup_con_loss: 0.8770 contrastive_loss: 4.6186 
2024-05-15 09:58:11.721 | INFO     | __main__:train:123 - Epoch: [5][300/390]	 loss 4.46617	 cls_loss: 0.5941 cluster_loss: 1.2291 sup_con_loss: 1.2936 contrastive_loss: 4.6255 
2024-05-15 09:58:24.539 | INFO     | __main__:train:123 - Epoch: [5][320/390]	 loss 4.25430	 cls_loss: 0.3900 cluster_loss: 1.1884 sup_con_loss: 0.9820 contrastive_loss: 4.6179 
2024-05-15 09:58:37.027 | INFO     | __main__:train:123 - Epoch: [5][340/390]	 loss 4.40644	 cls_loss: 0.5456 cluster_loss: 1.2546 sup_con_loss: 1.1114 contrastive_loss: 4.6323 
2024-05-15 09:58:49.883 | INFO     | __main__:train:123 - Epoch: [5][360/390]	 loss 4.64107	 cls_loss: 0.7521 cluster_loss: 1.3217 sup_con_loss: 1.4559 contrastive_loss: 4.6295 
2024-05-15 09:59:02.286 | INFO     | __main__:train:123 - Epoch: [5][380/390]	 loss 4.49369	 cls_loss: 0.6259 cluster_loss: 1.2176 sup_con_loss: 1.3810 contrastive_loss: 4.6151 
2024-05-15 09:59:08.097 | INFO     | __main__:train:126 - Train Epoch: 5 Avg Loss: 4.5105 
2024-05-15 09:59:08.098 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:59:34.336 | INFO     | __main__:train:135 - Train Accuracies: All 0.7795 | Old 0.8130 | New 0.6455
2024-05-15 09:59:37.592 | INFO     | __main__:train:123 - Epoch: [6][0/390]	 loss 4.56879	 cls_loss: 0.7038 cluster_loss: 1.2679 sup_con_loss: 1.4045 contrastive_loss: 4.6258 
2024-05-15 09:59:52.222 | INFO     | __main__:train:123 - Epoch: [6][20/390]	 loss 4.50510	 cls_loss: 0.6427 cluster_loss: 1.3556 sup_con_loss: 1.1066 contrastive_loss: 4.6334 
2024-05-15 10:00:05.165 | INFO     | __main__:train:123 - Epoch: [6][40/390]	 loss 4.36699	 cls_loss: 0.5829 cluster_loss: 1.2442 sup_con_loss: 0.9859 contrastive_loss: 4.6295 
2024-05-15 10:00:18.088 | INFO     | __main__:train:123 - Epoch: [6][60/390]	 loss 4.29949	 cls_loss: 0.4456 cluster_loss: 1.2392 sup_con_loss: 0.9514 contrastive_loss: 4.6231 
2024-05-15 10:00:30.714 | INFO     | __main__:train:123 - Epoch: [6][80/390]	 loss 4.55992	 cls_loss: 0.6180 cluster_loss: 1.2868 sup_con_loss: 1.4084 contrastive_loss: 4.6373 
2024-05-15 10:00:43.539 | INFO     | __main__:train:123 - Epoch: [6][100/390]	 loss 4.29256	 cls_loss: 0.5248 cluster_loss: 1.2089 sup_con_loss: 0.9009 contrastive_loss: 4.6273 
2024-05-15 10:00:56.524 | INFO     | __main__:train:123 - Epoch: [6][120/390]	 loss 4.27946	 cls_loss: 0.6698 cluster_loss: 1.1432 sup_con_loss: 0.8258 contrastive_loss: 4.6353 
2024-05-15 10:01:09.520 | INFO     | __main__:train:123 - Epoch: [6][140/390]	 loss 4.63804	 cls_loss: 0.7193 cluster_loss: 1.2439 sup_con_loss: 1.6352 contrastive_loss: 4.6238 
2024-05-15 10:01:22.463 | INFO     | __main__:train:123 - Epoch: [6][160/390]	 loss 4.49633	 cls_loss: 0.6340 cluster_loss: 1.1830 sup_con_loss: 1.4393 contrastive_loss: 4.6180 
2024-05-15 10:01:35.141 | INFO     | __main__:train:123 - Epoch: [6][180/390]	 loss 4.36490	 cls_loss: 0.5820 cluster_loss: 1.2476 sup_con_loss: 0.9765 contrastive_loss: 4.6284 
2024-05-15 10:01:48.032 | INFO     | __main__:train:123 - Epoch: [6][200/390]	 loss 4.72345	 cls_loss: 0.8630 cluster_loss: 1.2838 sup_con_loss: 1.6391 contrastive_loss: 4.6357 
2024-05-15 10:02:01.147 | INFO     | __main__:train:123 - Epoch: [6][220/390]	 loss 4.59358	 cls_loss: 0.8397 cluster_loss: 1.2685 sup_con_loss: 1.3473 contrastive_loss: 4.6209 
2024-05-15 10:02:14.080 | INFO     | __main__:train:123 - Epoch: [6][240/390]	 loss 4.72196	 cls_loss: 0.7712 cluster_loss: 1.3585 sup_con_loss: 1.5840 contrastive_loss: 4.6378 
2024-05-15 10:02:26.983 | INFO     | __main__:train:123 - Epoch: [6][260/390]	 loss 4.59770	 cls_loss: 0.8768 cluster_loss: 1.1750 sup_con_loss: 1.4745 contrastive_loss: 4.6323 
2024-05-15 10:02:39.884 | INFO     | __main__:train:123 - Epoch: [6][280/390]	 loss 4.37358	 cls_loss: 0.6440 cluster_loss: 1.1448 sup_con_loss: 1.1545 contrastive_loss: 4.6154 
2024-05-15 10:02:52.810 | INFO     | __main__:train:123 - Epoch: [6][300/390]	 loss 4.67561	 cls_loss: 0.8291 cluster_loss: 1.3657 sup_con_loss: 1.3901 contrastive_loss: 4.6326 
2024-05-15 10:03:05.824 | INFO     | __main__:train:123 - Epoch: [6][320/390]	 loss 4.58419	 cls_loss: 0.7127 cluster_loss: 1.2047 sup_con_loss: 1.5561 contrastive_loss: 4.6263 
2024-05-15 10:03:18.859 | INFO     | __main__:train:123 - Epoch: [6][340/390]	 loss 4.31811	 cls_loss: 0.5817 cluster_loss: 1.2261 sup_con_loss: 0.9004 contrastive_loss: 4.6191 
2024-05-15 10:03:32.178 | INFO     | __main__:train:123 - Epoch: [6][360/390]	 loss 4.61421	 cls_loss: 0.6364 cluster_loss: 1.2703 sup_con_loss: 1.5895 contrastive_loss: 4.6299 
2024-05-15 10:03:44.631 | INFO     | __main__:train:123 - Epoch: [6][380/390]	 loss 4.30334	 cls_loss: 0.6260 cluster_loss: 1.1332 sup_con_loss: 0.9884 contrastive_loss: 4.6181 
2024-05-15 10:03:50.624 | INFO     | __main__:train:126 - Train Epoch: 6 Avg Loss: 4.4920 
2024-05-15 10:03:50.625 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:04:16.677 | INFO     | __main__:train:135 - Train Accuracies: All 0.7788 | Old 0.8145 | New 0.6360
2024-05-15 10:04:21.844 | INFO     | __main__:train:123 - Epoch: [7][0/390]	 loss 4.28486	 cls_loss: 0.5033 cluster_loss: 1.1394 sup_con_loss: 1.0661 contrastive_loss: 4.6076 
2024-05-15 10:04:34.674 | INFO     | __main__:train:123 - Epoch: [7][20/390]	 loss 4.49715	 cls_loss: 0.5402 cluster_loss: 1.2467 sup_con_loss: 1.4183 contrastive_loss: 4.6174 
2024-05-15 10:04:47.632 | INFO     | __main__:train:123 - Epoch: [7][40/390]	 loss 4.37283	 cls_loss: 0.6894 cluster_loss: 1.1781 sup_con_loss: 1.0314 contrastive_loss: 4.6227 
2024-05-15 10:05:00.460 | INFO     | __main__:train:123 - Epoch: [7][60/390]	 loss 4.51769	 cls_loss: 0.6123 cluster_loss: 1.3188 sup_con_loss: 1.2455 contrastive_loss: 4.6311 
2024-05-15 10:05:13.479 | INFO     | __main__:train:123 - Epoch: [7][80/390]	 loss 4.59087	 cls_loss: 0.6581 cluster_loss: 1.2401 sup_con_loss: 1.5847 contrastive_loss: 4.6151 
2024-05-15 10:05:26.189 | INFO     | __main__:train:123 - Epoch: [7][100/390]	 loss 4.36700	 cls_loss: 0.5666 cluster_loss: 1.2090 sup_con_loss: 1.0888 contrastive_loss: 4.6181 
2024-05-15 10:05:39.162 | INFO     | __main__:train:123 - Epoch: [7][120/390]	 loss 4.60384	 cls_loss: 0.7535 cluster_loss: 1.2794 sup_con_loss: 1.4416 contrastive_loss: 4.6215 
2024-05-15 10:05:52.252 | INFO     | __main__:train:123 - Epoch: [7][140/390]	 loss 4.47614	 cls_loss: 0.6731 cluster_loss: 1.1061 sup_con_loss: 1.4938 contrastive_loss: 4.6135 
2024-05-15 10:06:05.340 | INFO     | __main__:train:123 - Epoch: [7][160/390]	 loss 4.67102	 cls_loss: 0.7711 cluster_loss: 1.2090 sup_con_loss: 1.7463 contrastive_loss: 4.6217 
2024-05-15 10:06:18.206 | INFO     | __main__:train:123 - Epoch: [7][180/390]	 loss 4.45268	 cls_loss: 0.5727 cluster_loss: 1.2370 sup_con_loss: 1.2622 contrastive_loss: 4.6252 
2024-05-15 10:06:31.244 | INFO     | __main__:train:123 - Epoch: [7][200/390]	 loss 4.48589	 cls_loss: 0.5715 cluster_loss: 1.2268 sup_con_loss: 1.3731 contrastive_loss: 4.6274 
2024-05-15 10:06:44.461 | INFO     | __main__:train:123 - Epoch: [7][220/390]	 loss 4.41802	 cls_loss: 0.5367 cluster_loss: 1.2345 sup_con_loss: 1.1909 contrastive_loss: 4.6322 
2024-05-15 10:06:57.604 | INFO     | __main__:train:123 - Epoch: [7][240/390]	 loss 4.43527	 cls_loss: 0.6678 cluster_loss: 1.3149 sup_con_loss: 0.9799 contrastive_loss: 4.6214 
2024-05-15 10:07:10.585 | INFO     | __main__:train:123 - Epoch: [7][260/390]	 loss 4.49333	 cls_loss: 0.6873 cluster_loss: 1.1949 sup_con_loss: 1.3455 contrastive_loss: 4.6234 
2024-05-15 10:07:23.757 | INFO     | __main__:train:123 - Epoch: [7][280/390]	 loss 4.41698	 cls_loss: 0.7129 cluster_loss: 1.2464 sup_con_loss: 1.0141 contrastive_loss: 4.6190 
2024-05-15 10:07:36.659 | INFO     | __main__:train:123 - Epoch: [7][300/390]	 loss 4.29285	 cls_loss: 0.4895 cluster_loss: 1.1712 sup_con_loss: 1.0382 contrastive_loss: 4.6105 
2024-05-15 10:07:49.334 | INFO     | __main__:train:123 - Epoch: [7][320/390]	 loss 4.42730	 cls_loss: 0.6116 cluster_loss: 1.1689 sup_con_loss: 1.2976 contrastive_loss: 4.6143 
2024-05-15 10:08:02.203 | INFO     | __main__:train:123 - Epoch: [7][340/390]	 loss 4.32312	 cls_loss: 0.4909 cluster_loss: 1.1328 sup_con_loss: 1.2017 contrastive_loss: 4.6067 
2024-05-15 10:08:14.952 | INFO     | __main__:train:123 - Epoch: [7][360/390]	 loss 4.30324	 cls_loss: 0.5234 cluster_loss: 1.1391 sup_con_loss: 1.0935 contrastive_loss: 4.6106 
2024-05-15 10:08:27.465 | INFO     | __main__:train:123 - Epoch: [7][380/390]	 loss 4.46675	 cls_loss: 0.6680 cluster_loss: 1.2221 sup_con_loss: 1.2499 contrastive_loss: 4.6171 
2024-05-15 10:08:33.502 | INFO     | __main__:train:126 - Train Epoch: 7 Avg Loss: 4.4537 
2024-05-15 10:08:33.503 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:09:00.809 | INFO     | __main__:train:135 - Train Accuracies: All 0.7881 | Old 0.8227 | New 0.6495
2024-05-15 10:09:05.230 | INFO     | __main__:train:123 - Epoch: [8][0/390]	 loss 4.48657	 cls_loss: 0.5371 cluster_loss: 1.2689 sup_con_loss: 1.3313 contrastive_loss: 4.6275 
2024-05-15 10:09:18.370 | INFO     | __main__:train:123 - Epoch: [8][20/390]	 loss 4.44070	 cls_loss: 0.7408 cluster_loss: 1.2191 sup_con_loss: 1.1225 contrastive_loss: 4.6094 
2024-05-15 10:09:31.334 | INFO     | __main__:train:123 - Epoch: [8][40/390]	 loss 4.67423	 cls_loss: 0.7016 cluster_loss: 1.2334 sup_con_loss: 1.7663 contrastive_loss: 4.6288 
2024-05-15 10:09:44.218 | INFO     | __main__:train:123 - Epoch: [8][60/390]	 loss 4.46884	 cls_loss: 0.6754 cluster_loss: 1.2981 sup_con_loss: 1.0706 contrastive_loss: 4.6369 
2024-05-15 10:09:57.037 | INFO     | __main__:train:123 - Epoch: [8][80/390]	 loss 4.49776	 cls_loss: 0.6245 cluster_loss: 1.2129 sup_con_loss: 1.3743 contrastive_loss: 4.6305 
2024-05-15 10:10:09.935 | INFO     | __main__:train:123 - Epoch: [8][100/390]	 loss 4.36927	 cls_loss: 0.6033 cluster_loss: 1.0714 sup_con_loss: 1.3311 contrastive_loss: 4.6090 
2024-05-15 10:10:22.685 | INFO     | __main__:train:123 - Epoch: [8][120/390]	 loss 4.32346	 cls_loss: 0.5661 cluster_loss: 1.2170 sup_con_loss: 0.9553 contrastive_loss: 4.6152 
2024-05-15 10:10:35.816 | INFO     | __main__:train:123 - Epoch: [8][140/390]	 loss 4.54194	 cls_loss: 0.6316 cluster_loss: 1.2974 sup_con_loss: 1.3491 contrastive_loss: 4.6237 
2024-05-15 10:10:48.774 | INFO     | __main__:train:123 - Epoch: [8][160/390]	 loss 4.45057	 cls_loss: 0.5686 cluster_loss: 1.3217 sup_con_loss: 1.0936 contrastive_loss: 4.6303 
2024-05-15 10:11:01.459 | INFO     | __main__:train:123 - Epoch: [8][180/390]	 loss 4.36410	 cls_loss: 0.5822 cluster_loss: 1.1861 sup_con_loss: 1.0964 contrastive_loss: 4.6241 
2024-05-15 10:11:14.357 | INFO     | __main__:train:123 - Epoch: [8][200/390]	 loss 4.37713	 cls_loss: 0.5852 cluster_loss: 1.1567 sup_con_loss: 1.1837 contrastive_loss: 4.6248 
2024-05-15 10:11:27.123 | INFO     | __main__:train:123 - Epoch: [8][220/390]	 loss 4.36118	 cls_loss: 0.4984 cluster_loss: 1.1509 sup_con_loss: 1.2233 contrastive_loss: 4.6315 
2024-05-15 10:11:40.078 | INFO     | __main__:train:123 - Epoch: [8][240/390]	 loss 4.39333	 cls_loss: 0.8138 cluster_loss: 1.1763 sup_con_loss: 0.9733 contrastive_loss: 4.6204 
2024-05-15 10:11:53.159 | INFO     | __main__:train:123 - Epoch: [8][260/390]	 loss 4.35901	 cls_loss: 0.5471 cluster_loss: 1.1908 sup_con_loss: 1.1119 contrastive_loss: 4.6221 
2024-05-15 10:12:06.099 | INFO     | __main__:train:123 - Epoch: [8][280/390]	 loss 4.56397	 cls_loss: 0.5434 cluster_loss: 1.3827 sup_con_loss: 1.3058 contrastive_loss: 4.6431 
2024-05-15 10:12:19.256 | INFO     | __main__:train:123 - Epoch: [8][300/390]	 loss 4.45231	 cls_loss: 0.6189 cluster_loss: 1.2913 sup_con_loss: 1.1196 contrastive_loss: 4.6223 
2024-05-15 10:12:32.143 | INFO     | __main__:train:123 - Epoch: [8][320/390]	 loss 4.38236	 cls_loss: 0.4833 cluster_loss: 1.1638 sup_con_loss: 1.3080 contrastive_loss: 4.6138 
2024-05-15 10:12:44.946 | INFO     | __main__:train:123 - Epoch: [8][340/390]	 loss 4.42149	 cls_loss: 0.5529 cluster_loss: 1.1854 sup_con_loss: 1.2906 contrastive_loss: 4.6243 
2024-05-15 10:12:57.942 | INFO     | __main__:train:123 - Epoch: [8][360/390]	 loss 4.54918	 cls_loss: 0.6359 cluster_loss: 1.3188 sup_con_loss: 1.3146 contrastive_loss: 4.6296 
2024-05-15 10:13:10.602 | INFO     | __main__:train:123 - Epoch: [8][380/390]	 loss 4.48124	 cls_loss: 0.6746 cluster_loss: 1.1704 sup_con_loss: 1.3926 contrastive_loss: 4.6108 
2024-05-15 10:13:16.433 | INFO     | __main__:train:126 - Train Epoch: 8 Avg Loss: 4.4229 
2024-05-15 10:13:16.434 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:13:42.710 | INFO     | __main__:train:135 - Train Accuracies: All 0.7802 | Old 0.8179 | New 0.6295
2024-05-15 10:13:48.039 | INFO     | __main__:train:123 - Epoch: [9][0/390]	 loss 4.44276	 cls_loss: 0.6284 cluster_loss: 1.3376 sup_con_loss: 0.9688 contrastive_loss: 4.6374 
2024-05-15 10:14:01.480 | INFO     | __main__:train:123 - Epoch: [9][20/390]	 loss 4.56033	 cls_loss: 0.6213 cluster_loss: 1.3168 sup_con_loss: 1.3908 contrastive_loss: 4.6157 
2024-05-15 10:14:14.109 | INFO     | __main__:train:123 - Epoch: [9][40/390]	 loss 4.44629	 cls_loss: 0.7076 cluster_loss: 1.1757 sup_con_loss: 1.2411 contrastive_loss: 4.6155 
2024-05-15 10:14:27.104 | INFO     | __main__:train:123 - Epoch: [9][60/390]	 loss 4.55467	 cls_loss: 0.6900 cluster_loss: 1.2151 sup_con_loss: 1.4866 contrastive_loss: 4.6201 
2024-05-15 10:14:40.091 | INFO     | __main__:train:123 - Epoch: [9][80/390]	 loss 4.49856	 cls_loss: 0.7029 cluster_loss: 1.2686 sup_con_loss: 1.2037 contrastive_loss: 4.6257 
2024-05-15 10:14:53.249 | INFO     | __main__:train:123 - Epoch: [9][100/390]	 loss 4.50219	 cls_loss: 0.7241 cluster_loss: 1.1796 sup_con_loss: 1.3675 contrastive_loss: 4.6206 
2024-05-15 10:15:06.130 | INFO     | __main__:train:123 - Epoch: [9][120/390]	 loss 4.31565	 cls_loss: 0.4784 cluster_loss: 1.1654 sup_con_loss: 1.1302 contrastive_loss: 4.6079 
2024-05-15 10:15:18.898 | INFO     | __main__:train:123 - Epoch: [9][140/390]	 loss 4.35555	 cls_loss: 0.6095 cluster_loss: 1.1825 sup_con_loss: 1.0663 contrastive_loss: 4.6160 
2024-05-15 10:15:31.846 | INFO     | __main__:train:123 - Epoch: [9][160/390]	 loss 4.55040	 cls_loss: 0.6607 cluster_loss: 1.2252 sup_con_loss: 1.4859 contrastive_loss: 4.6196 
2024-05-15 10:15:44.617 | INFO     | __main__:train:123 - Epoch: [9][180/390]	 loss 4.42887	 cls_loss: 0.6001 cluster_loss: 1.1214 sup_con_loss: 1.4177 contrastive_loss: 4.6058 
2024-05-15 10:15:57.549 | INFO     | __main__:train:123 - Epoch: [9][200/390]	 loss 4.67265	 cls_loss: 0.6722 cluster_loss: 1.3640 sup_con_loss: 1.5618 contrastive_loss: 4.6218 
2024-05-15 10:16:10.565 | INFO     | __main__:train:123 - Epoch: [9][220/390]	 loss 4.37964	 cls_loss: 0.5882 cluster_loss: 1.1591 sup_con_loss: 1.1931 contrastive_loss: 4.6196 
2024-05-15 10:16:23.559 | INFO     | __main__:train:123 - Epoch: [9][240/390]	 loss 4.44877	 cls_loss: 0.6029 cluster_loss: 1.1799 sup_con_loss: 1.3464 contrastive_loss: 4.6148 
2024-05-15 10:16:36.283 | INFO     | __main__:train:123 - Epoch: [9][260/390]	 loss 4.34782	 cls_loss: 0.6422 cluster_loss: 1.1898 sup_con_loss: 0.9878 contrastive_loss: 4.6215 
2024-05-15 10:16:49.185 | INFO     | __main__:train:123 - Epoch: [9][280/390]	 loss 4.56067	 cls_loss: 0.6909 cluster_loss: 1.2130 sup_con_loss: 1.4937 contrastive_loss: 4.6271 
2024-05-15 10:17:02.316 | INFO     | __main__:train:123 - Epoch: [9][300/390]	 loss 4.42457	 cls_loss: 0.6592 cluster_loss: 1.2314 sup_con_loss: 1.1087 contrastive_loss: 4.6237 
2024-05-15 10:17:15.374 | INFO     | __main__:train:123 - Epoch: [9][320/390]	 loss 4.36389	 cls_loss: 0.4908 cluster_loss: 1.2123 sup_con_loss: 1.1448 contrastive_loss: 4.6207 
2024-05-15 10:17:28.493 | INFO     | __main__:train:123 - Epoch: [9][340/390]	 loss 4.51034	 cls_loss: 0.7143 cluster_loss: 1.2484 sup_con_loss: 1.2722 contrastive_loss: 4.6209 
2024-05-15 10:17:41.475 | INFO     | __main__:train:123 - Epoch: [9][360/390]	 loss 4.24468	 cls_loss: 0.6162 cluster_loss: 1.0459 sup_con_loss: 1.0193 contrastive_loss: 4.6037 
2024-05-15 10:17:54.353 | INFO     | __main__:train:123 - Epoch: [9][380/390]	 loss 4.31946	 cls_loss: 0.5637 cluster_loss: 1.1649 sup_con_loss: 1.0392 contrastive_loss: 4.6173 
2024-05-15 10:18:00.244 | INFO     | __main__:train:126 - Train Epoch: 9 Avg Loss: 4.4062 
2024-05-15 10:18:00.245 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:18:27.604 | INFO     | __main__:train:135 - Train Accuracies: All 0.7861 | Old 0.8180 | New 0.6585
2024-05-15 10:18:33.126 | INFO     | __main__:train:123 - Epoch: [10][0/390]	 loss 4.31496	 cls_loss: 0.5334 cluster_loss: 1.1599 sup_con_loss: 1.0779 contrastive_loss: 4.6109 
2024-05-15 10:18:46.203 | INFO     | __main__:train:123 - Epoch: [10][20/390]	 loss 4.39930	 cls_loss: 0.5573 cluster_loss: 1.1974 sup_con_loss: 1.2142 contrastive_loss: 4.6168 
2024-05-15 10:18:58.982 | INFO     | __main__:train:123 - Epoch: [10][40/390]	 loss 4.29688	 cls_loss: 0.5643 cluster_loss: 1.1591 sup_con_loss: 0.9728 contrastive_loss: 4.6238 
2024-05-15 10:19:12.022 | INFO     | __main__:train:123 - Epoch: [10][60/390]	 loss 4.36977	 cls_loss: 0.4671 cluster_loss: 1.2159 sup_con_loss: 1.1730 contrastive_loss: 4.6238 
2024-05-15 10:19:24.948 | INFO     | __main__:train:123 - Epoch: [10][80/390]	 loss 4.19480	 cls_loss: 0.5164 cluster_loss: 1.0872 sup_con_loss: 0.8790 contrastive_loss: 4.6150 
2024-05-15 10:19:38.024 | INFO     | __main__:train:123 - Epoch: [10][100/390]	 loss 4.30194	 cls_loss: 0.4751 cluster_loss: 1.1456 sup_con_loss: 1.1277 contrastive_loss: 4.6097 
2024-05-15 10:19:51.376 | INFO     | __main__:train:123 - Epoch: [10][120/390]	 loss 4.29111	 cls_loss: 0.5587 cluster_loss: 1.1958 sup_con_loss: 0.9107 contrastive_loss: 4.6147 
2024-05-15 10:20:04.157 | INFO     | __main__:train:123 - Epoch: [10][140/390]	 loss 4.37239	 cls_loss: 0.4895 cluster_loss: 1.2472 sup_con_loss: 1.0910 contrastive_loss: 4.6285 
2024-05-15 10:20:17.363 | INFO     | __main__:train:123 - Epoch: [10][160/390]	 loss 4.29748	 cls_loss: 0.4871 cluster_loss: 1.1942 sup_con_loss: 0.9822 contrastive_loss: 4.6261 
2024-05-15 10:20:30.502 | INFO     | __main__:train:123 - Epoch: [10][180/390]	 loss 4.40802	 cls_loss: 0.6146 cluster_loss: 1.1979 sup_con_loss: 1.1700 contrastive_loss: 4.6227 
2024-05-15 10:20:43.591 | INFO     | __main__:train:123 - Epoch: [10][200/390]	 loss 4.58107	 cls_loss: 0.6741 cluster_loss: 1.2465 sup_con_loss: 1.4825 contrastive_loss: 4.6401 
2024-05-15 10:20:56.824 | INFO     | __main__:train:123 - Epoch: [10][220/390]	 loss 4.28091	 cls_loss: 0.5240 cluster_loss: 1.0616 sup_con_loss: 1.1767 contrastive_loss: 4.6087 
2024-05-15 10:21:10.006 | INFO     | __main__:train:123 - Epoch: [10][240/390]	 loss 4.43735	 cls_loss: 0.4469 cluster_loss: 1.2660 sup_con_loss: 1.3091 contrastive_loss: 4.6151 
2024-05-15 10:21:22.685 | INFO     | __main__:train:123 - Epoch: [10][260/390]	 loss 4.38108	 cls_loss: 0.7260 cluster_loss: 1.1087 sup_con_loss: 1.1635 contrastive_loss: 4.6140 
2024-05-15 10:21:35.614 | INFO     | __main__:train:123 - Epoch: [10][280/390]	 loss 4.25413	 cls_loss: 0.5850 cluster_loss: 1.1211 sup_con_loss: 0.9245 contrastive_loss: 4.6109 
2024-05-15 10:21:48.218 | INFO     | __main__:train:123 - Epoch: [10][300/390]	 loss 4.24966	 cls_loss: 0.4642 cluster_loss: 1.1322 sup_con_loss: 1.0090 contrastive_loss: 4.6125 
2024-05-15 10:22:01.333 | INFO     | __main__:train:123 - Epoch: [10][320/390]	 loss 4.39839	 cls_loss: 0.5868 cluster_loss: 1.2782 sup_con_loss: 1.0190 contrastive_loss: 4.6239 
2024-05-15 10:22:14.485 | INFO     | __main__:train:123 - Epoch: [10][340/390]	 loss 4.54070	 cls_loss: 0.5655 cluster_loss: 1.2276 sup_con_loss: 1.5400 contrastive_loss: 4.6243 
2024-05-15 10:22:27.343 | INFO     | __main__:train:123 - Epoch: [10][360/390]	 loss 4.39866	 cls_loss: 0.5481 cluster_loss: 1.2655 sup_con_loss: 1.0959 contrastive_loss: 4.6164 
2024-05-15 10:22:40.143 | INFO     | __main__:train:123 - Epoch: [10][380/390]	 loss 4.13806	 cls_loss: 0.4699 cluster_loss: 1.0356 sup_con_loss: 0.8717 contrastive_loss: 4.6083 
2024-05-15 10:22:45.954 | INFO     | __main__:train:126 - Train Epoch: 10 Avg Loss: 4.3761 
2024-05-15 10:22:45.955 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:23:12.403 | INFO     | __main__:train:135 - Train Accuracies: All 0.7893 | Old 0.8206 | New 0.6640
2024-05-15 10:23:18.074 | INFO     | __main__:train:123 - Epoch: [11][0/390]	 loss 4.26001	 cls_loss: 0.6736 cluster_loss: 1.0648 sup_con_loss: 0.9527 contrastive_loss: 4.6134 
2024-05-15 10:23:31.165 | INFO     | __main__:train:123 - Epoch: [11][20/390]	 loss 4.13035	 cls_loss: 0.3905 cluster_loss: 1.0077 sup_con_loss: 0.9503 contrastive_loss: 4.6247 
2024-05-15 10:23:44.116 | INFO     | __main__:train:123 - Epoch: [11][40/390]	 loss 4.35895	 cls_loss: 0.5795 cluster_loss: 1.1962 sup_con_loss: 1.0850 contrastive_loss: 4.6136 
2024-05-15 10:23:57.061 | INFO     | __main__:train:123 - Epoch: [11][60/390]	 loss 4.34046	 cls_loss: 0.5338 cluster_loss: 1.1168 sup_con_loss: 1.2359 contrastive_loss: 4.6080 
2024-05-15 10:24:10.310 | INFO     | __main__:train:123 - Epoch: [11][80/390]	 loss 4.28372	 cls_loss: 0.4657 cluster_loss: 1.1334 sup_con_loss: 1.1105 contrastive_loss: 4.6082 
2024-05-15 10:24:23.253 | INFO     | __main__:train:123 - Epoch: [11][100/390]	 loss 4.33986	 cls_loss: 0.5166 cluster_loss: 1.0777 sup_con_loss: 1.3012 contrastive_loss: 4.6202 
2024-05-15 10:24:36.356 | INFO     | __main__:train:123 - Epoch: [11][120/390]	 loss 4.18043	 cls_loss: 0.5399 cluster_loss: 1.0363 sup_con_loss: 0.9217 contrastive_loss: 4.6081 
2024-05-15 10:24:49.310 | INFO     | __main__:train:123 - Epoch: [11][140/390]	 loss 4.33898	 cls_loss: 0.5825 cluster_loss: 1.1591 sup_con_loss: 1.1147 contrastive_loss: 4.6023 
2024-05-15 10:25:02.312 | INFO     | __main__:train:123 - Epoch: [11][160/390]	 loss 4.34991	 cls_loss: 0.5032 cluster_loss: 1.1597 sup_con_loss: 1.2048 contrastive_loss: 4.6128 
2024-05-15 10:25:15.281 | INFO     | __main__:train:123 - Epoch: [11][180/390]	 loss 4.35867	 cls_loss: 0.6087 cluster_loss: 1.2666 sup_con_loss: 0.8936 contrastive_loss: 4.6301 
2024-05-15 10:25:28.464 | INFO     | __main__:train:123 - Epoch: [11][200/390]	 loss 4.54767	 cls_loss: 0.6983 cluster_loss: 1.1948 sup_con_loss: 1.4862 contrastive_loss: 4.6254 
2024-05-15 10:25:41.370 | INFO     | __main__:train:123 - Epoch: [11][220/390]	 loss 4.51042	 cls_loss: 0.5421 cluster_loss: 1.2736 sup_con_loss: 1.4153 contrastive_loss: 4.6115 
2024-05-15 10:25:54.318 | INFO     | __main__:train:123 - Epoch: [11][240/390]	 loss 4.48631	 cls_loss: 0.6185 cluster_loss: 1.0798 sup_con_loss: 1.6093 contrastive_loss: 4.6226 
2024-05-15 10:26:07.394 | INFO     | __main__:train:123 - Epoch: [11][260/390]	 loss 4.21006	 cls_loss: 0.5560 cluster_loss: 1.1527 sup_con_loss: 0.7539 contrastive_loss: 4.6190 
2024-05-15 10:26:20.413 | INFO     | __main__:train:123 - Epoch: [11][280/390]	 loss 4.35824	 cls_loss: 0.6318 cluster_loss: 1.1659 sup_con_loss: 1.0629 contrastive_loss: 4.6266 
2024-05-15 10:26:33.350 | INFO     | __main__:train:123 - Epoch: [11][300/390]	 loss 4.35576	 cls_loss: 0.5629 cluster_loss: 1.1880 sup_con_loss: 1.0818 contrastive_loss: 4.6275 
2024-05-15 10:26:46.488 | INFO     | __main__:train:123 - Epoch: [11][320/390]	 loss 4.41265	 cls_loss: 0.5784 cluster_loss: 1.1474 sup_con_loss: 1.3281 contrastive_loss: 4.6148 
2024-05-15 10:26:59.665 | INFO     | __main__:train:123 - Epoch: [11][340/390]	 loss 4.28826	 cls_loss: 0.4612 cluster_loss: 1.1354 sup_con_loss: 1.1074 contrastive_loss: 4.6172 
2024-05-15 10:27:12.517 | INFO     | __main__:train:123 - Epoch: [11][360/390]	 loss 4.38420	 cls_loss: 0.6129 cluster_loss: 1.2621 sup_con_loss: 0.9716 contrastive_loss: 4.6297 
2024-05-15 10:27:25.299 | INFO     | __main__:train:123 - Epoch: [11][380/390]	 loss 4.28921	 cls_loss: 0.4665 cluster_loss: 1.1736 sup_con_loss: 1.0406 contrastive_loss: 4.6137 
2024-05-15 10:27:31.388 | INFO     | __main__:train:126 - Train Epoch: 11 Avg Loss: 4.3427 
2024-05-15 10:27:31.390 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:27:57.887 | INFO     | __main__:train:135 - Train Accuracies: All 0.7894 | Old 0.8210 | New 0.6630
2024-05-15 10:28:01.571 | INFO     | __main__:train:123 - Epoch: [12][0/390]	 loss 4.41900	 cls_loss: 0.5589 cluster_loss: 1.2184 sup_con_loss: 1.2319 contrastive_loss: 4.6158 
2024-05-15 10:28:16.205 | INFO     | __main__:train:123 - Epoch: [12][20/390]	 loss 4.30898	 cls_loss: 0.5034 cluster_loss: 1.0752 sup_con_loss: 1.2693 contrastive_loss: 4.5995 
2024-05-15 10:28:29.211 | INFO     | __main__:train:123 - Epoch: [12][40/390]	 loss 4.23001	 cls_loss: 0.6065 cluster_loss: 1.1194 sup_con_loss: 0.8445 contrastive_loss: 4.6070 
2024-05-15 10:28:42.154 | INFO     | __main__:train:123 - Epoch: [12][60/390]	 loss 4.43695	 cls_loss: 0.6462 cluster_loss: 1.2445 sup_con_loss: 1.1336 contrastive_loss: 4.6233 
2024-05-15 10:28:55.145 | INFO     | __main__:train:123 - Epoch: [12][80/390]	 loss 4.50768	 cls_loss: 0.8031 cluster_loss: 1.1763 sup_con_loss: 1.3072 contrastive_loss: 4.6224 
2024-05-15 10:29:08.183 | INFO     | __main__:train:123 - Epoch: [12][100/390]	 loss 4.36892	 cls_loss: 0.5584 cluster_loss: 1.0674 sup_con_loss: 1.3894 contrastive_loss: 4.6052 
2024-05-15 10:29:21.211 | INFO     | __main__:train:123 - Epoch: [12][120/390]	 loss 4.27488	 cls_loss: 0.6681 cluster_loss: 1.1419 sup_con_loss: 0.8640 contrastive_loss: 4.6098 
2024-05-15 10:29:34.477 | INFO     | __main__:train:123 - Epoch: [12][140/390]	 loss 4.15132	 cls_loss: 0.4419 cluster_loss: 1.0157 sup_con_loss: 0.9796 contrastive_loss: 4.6055 
2024-05-15 10:29:47.435 | INFO     | __main__:train:123 - Epoch: [12][160/390]	 loss 4.35122	 cls_loss: 0.6082 cluster_loss: 1.1725 sup_con_loss: 1.0742 contrastive_loss: 4.6158 
2024-05-15 10:30:00.246 | INFO     | __main__:train:123 - Epoch: [12][180/390]	 loss 4.19146	 cls_loss: 0.4112 cluster_loss: 1.0699 sup_con_loss: 1.0319 contrastive_loss: 4.6014 
2024-05-15 10:30:13.424 | INFO     | __main__:train:123 - Epoch: [12][200/390]	 loss 4.19490	 cls_loss: 0.5362 cluster_loss: 1.0379 sup_con_loss: 0.9574 contrastive_loss: 4.6116 
2024-05-15 10:30:26.457 | INFO     | __main__:train:123 - Epoch: [12][220/390]	 loss 4.37627	 cls_loss: 0.5686 cluster_loss: 1.1809 sup_con_loss: 1.1748 contrastive_loss: 4.6131 
2024-05-15 10:30:39.196 | INFO     | __main__:train:123 - Epoch: [12][240/390]	 loss 4.29262	 cls_loss: 0.5383 cluster_loss: 1.1579 sup_con_loss: 0.9987 contrastive_loss: 4.6186 
2024-05-15 10:30:52.291 | INFO     | __main__:train:123 - Epoch: [12][260/390]	 loss 4.37681	 cls_loss: 0.5784 cluster_loss: 1.1159 sup_con_loss: 1.2996 contrastive_loss: 4.6064 
2024-05-15 10:31:05.352 | INFO     | __main__:train:123 - Epoch: [12][280/390]	 loss 4.38091	 cls_loss: 0.6154 cluster_loss: 1.0656 sup_con_loss: 1.3662 contrastive_loss: 4.6073 
2024-05-15 10:31:18.357 | INFO     | __main__:train:123 - Epoch: [12][300/390]	 loss 4.10201	 cls_loss: 0.4537 cluster_loss: 1.0326 sup_con_loss: 0.7766 contrastive_loss: 4.6157 
2024-05-15 10:31:31.263 | INFO     | __main__:train:123 - Epoch: [12][320/390]	 loss 4.30544	 cls_loss: 0.4967 cluster_loss: 1.1020 sup_con_loss: 1.2032 contrastive_loss: 4.6064 
2024-05-15 10:31:44.146 | INFO     | __main__:train:123 - Epoch: [12][340/390]	 loss 4.22409	 cls_loss: 0.4784 cluster_loss: 1.1283 sup_con_loss: 0.9274 contrastive_loss: 4.6133 
2024-05-15 10:31:57.180 | INFO     | __main__:train:123 - Epoch: [12][360/390]	 loss 4.34366	 cls_loss: 0.5376 cluster_loss: 1.1226 sup_con_loss: 1.2199 contrastive_loss: 4.6136 
2024-05-15 10:32:09.577 | INFO     | __main__:train:123 - Epoch: [12][380/390]	 loss 4.43424	 cls_loss: 0.6366 cluster_loss: 1.0880 sup_con_loss: 1.4552 contrastive_loss: 4.6075 
2024-05-15 10:32:15.654 | INFO     | __main__:train:126 - Train Epoch: 12 Avg Loss: 4.3067 
2024-05-15 10:32:15.654 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:32:41.910 | INFO     | __main__:train:135 - Train Accuracies: All 0.7918 | Old 0.8213 | New 0.6740
2024-05-15 10:32:47.083 | INFO     | __main__:train:123 - Epoch: [13][0/390]	 loss 4.30359	 cls_loss: 0.5534 cluster_loss: 1.1843 sup_con_loss: 0.9835 contrastive_loss: 4.6091 
2024-05-15 10:33:00.276 | INFO     | __main__:train:123 - Epoch: [13][20/390]	 loss 4.51346	 cls_loss: 0.5805 cluster_loss: 1.2539 sup_con_loss: 1.4025 contrastive_loss: 4.6222 
2024-05-15 10:33:13.002 | INFO     | __main__:train:123 - Epoch: [13][40/390]	 loss 4.30479	 cls_loss: 0.6041 cluster_loss: 1.0617 sup_con_loss: 1.1667 contrastive_loss: 4.6076 
2024-05-15 10:33:25.870 | INFO     | __main__:train:123 - Epoch: [13][60/390]	 loss 4.21623	 cls_loss: 0.5239 cluster_loss: 1.0944 sup_con_loss: 0.9224 contrastive_loss: 4.6134 
2024-05-15 10:33:39.001 | INFO     | __main__:train:123 - Epoch: [13][80/390]	 loss 4.35832	 cls_loss: 0.5653 cluster_loss: 1.1549 sup_con_loss: 1.1434 contrastive_loss: 4.6302 
2024-05-15 10:33:51.943 | INFO     | __main__:train:123 - Epoch: [13][100/390]	 loss 4.40599	 cls_loss: 0.6501 cluster_loss: 1.1089 sup_con_loss: 1.3032 contrastive_loss: 4.6177 
2024-05-15 10:34:05.137 | INFO     | __main__:train:123 - Epoch: [13][120/390]	 loss 4.29179	 cls_loss: 0.5580 cluster_loss: 1.0636 sup_con_loss: 1.1453 contrastive_loss: 4.6220 
2024-05-15 10:34:17.643 | INFO     | __main__:train:123 - Epoch: [13][140/390]	 loss 4.34370	 cls_loss: 0.6023 cluster_loss: 1.1019 sup_con_loss: 1.1815 contrastive_loss: 4.6202 
2024-05-15 10:34:30.740 | INFO     | __main__:train:123 - Epoch: [13][160/390]	 loss 4.28557	 cls_loss: 0.6128 cluster_loss: 1.0764 sup_con_loss: 1.0529 contrastive_loss: 4.6198 
2024-05-15 10:34:43.562 | INFO     | __main__:train:123 - Epoch: [13][180/390]	 loss 4.49889	 cls_loss: 0.7704 cluster_loss: 1.1229 sup_con_loss: 1.4447 contrastive_loss: 4.6058 
2024-05-15 10:34:56.875 | INFO     | __main__:train:123 - Epoch: [13][200/390]	 loss 4.29096	 cls_loss: 0.4397 cluster_loss: 1.1460 sup_con_loss: 1.1299 contrastive_loss: 4.6103 
2024-05-15 10:35:09.825 | INFO     | __main__:train:123 - Epoch: [13][220/390]	 loss 4.58675	 cls_loss: 0.6436 cluster_loss: 1.1604 sup_con_loss: 1.6969 contrastive_loss: 4.6359 
2024-05-15 10:35:22.738 | INFO     | __main__:train:123 - Epoch: [13][240/390]	 loss 4.29060	 cls_loss: 0.4973 cluster_loss: 1.1437 sup_con_loss: 1.0703 contrastive_loss: 4.6131 
2024-05-15 10:35:35.537 | INFO     | __main__:train:123 - Epoch: [13][260/390]	 loss 4.42192	 cls_loss: 0.6361 cluster_loss: 1.2467 sup_con_loss: 1.1147 contrastive_loss: 4.6135 
2024-05-15 10:35:48.739 | INFO     | __main__:train:123 - Epoch: [13][280/390]	 loss 4.28682	 cls_loss: 0.4529 cluster_loss: 1.1967 sup_con_loss: 0.9944 contrastive_loss: 4.6191 
2024-05-15 10:36:01.623 | INFO     | __main__:train:123 - Epoch: [13][300/390]	 loss 4.25961	 cls_loss: 0.5274 cluster_loss: 1.1045 sup_con_loss: 1.0202 contrastive_loss: 4.6154 
2024-05-15 10:36:14.844 | INFO     | __main__:train:123 - Epoch: [13][320/390]	 loss 4.27507	 cls_loss: 0.6235 cluster_loss: 1.1363 sup_con_loss: 0.9136 contrastive_loss: 4.6131 
2024-05-15 10:36:27.599 | INFO     | __main__:train:123 - Epoch: [13][340/390]	 loss 4.12798	 cls_loss: 0.4634 cluster_loss: 1.0301 sup_con_loss: 0.8718 contrastive_loss: 4.6017 
2024-05-15 10:36:40.819 | INFO     | __main__:train:123 - Epoch: [13][360/390]	 loss 4.23781	 cls_loss: 0.5722 cluster_loss: 1.1373 sup_con_loss: 0.8447 contrastive_loss: 4.6195 
2024-05-15 10:36:53.118 | INFO     | __main__:train:123 - Epoch: [13][380/390]	 loss 4.36698	 cls_loss: 0.5143 cluster_loss: 1.1003 sup_con_loss: 1.3341 contrastive_loss: 4.6229 
2024-05-15 10:36:59.226 | INFO     | __main__:train:126 - Train Epoch: 13 Avg Loss: 4.3039 
2024-05-15 10:36:59.228 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:37:25.764 | INFO     | __main__:train:135 - Train Accuracies: All 0.7938 | Old 0.8251 | New 0.6685
2024-05-15 10:37:29.406 | INFO     | __main__:train:123 - Epoch: [14][0/390]	 loss 4.42180	 cls_loss: 0.6791 cluster_loss: 1.0782 sup_con_loss: 1.3910 contrastive_loss: 4.6099 
2024-05-15 10:37:43.603 | INFO     | __main__:train:123 - Epoch: [14][20/390]	 loss 4.22523	 cls_loss: 0.5050 cluster_loss: 1.1180 sup_con_loss: 0.9187 contrastive_loss: 4.6157 
2024-05-15 10:37:56.601 | INFO     | __main__:train:123 - Epoch: [14][40/390]	 loss 4.07973	 cls_loss: 0.4518 cluster_loss: 1.0080 sup_con_loss: 0.7683 contrastive_loss: 4.6116 
2024-05-15 10:38:09.712 | INFO     | __main__:train:123 - Epoch: [14][60/390]	 loss 4.29689	 cls_loss: 0.5123 cluster_loss: 1.0858 sup_con_loss: 1.1734 contrastive_loss: 4.6171 
2024-05-15 10:38:22.791 | INFO     | __main__:train:123 - Epoch: [14][80/390]	 loss 4.12918	 cls_loss: 0.5233 cluster_loss: 1.0433 sup_con_loss: 0.7601 contrastive_loss: 4.6182 
2024-05-15 10:38:35.966 | INFO     | __main__:train:123 - Epoch: [14][100/390]	 loss 4.01728	 cls_loss: 0.4618 cluster_loss: 0.9973 sup_con_loss: 0.6128 contrastive_loss: 4.6045 
2024-05-15 10:38:49.351 | INFO     | __main__:train:123 - Epoch: [14][120/390]	 loss 4.32608	 cls_loss: 0.4708 cluster_loss: 1.0874 sup_con_loss: 1.3082 contrastive_loss: 4.6102 
2024-05-15 10:39:01.937 | INFO     | __main__:train:123 - Epoch: [14][140/390]	 loss 4.13825	 cls_loss: 0.4907 cluster_loss: 0.9351 sup_con_loss: 1.0331 contrastive_loss: 4.6109 
2024-05-15 10:39:14.893 | INFO     | __main__:train:123 - Epoch: [14][160/390]	 loss 4.23769	 cls_loss: 0.5107 cluster_loss: 1.0737 sup_con_loss: 1.0317 contrastive_loss: 4.6154 
2024-05-15 10:39:27.730 | INFO     | __main__:train:123 - Epoch: [14][180/390]	 loss 4.28337	 cls_loss: 0.5478 cluster_loss: 1.1242 sup_con_loss: 1.0553 contrastive_loss: 4.6024 
2024-05-15 10:39:40.871 | INFO     | __main__:train:123 - Epoch: [14][200/390]	 loss 3.98652	 cls_loss: 0.4237 cluster_loss: 1.0116 sup_con_loss: 0.5378 contrastive_loss: 4.6038 
2024-05-15 10:39:54.020 | INFO     | __main__:train:123 - Epoch: [14][220/390]	 loss 4.24824	 cls_loss: 0.4861 cluster_loss: 1.0149 sup_con_loss: 1.2269 contrastive_loss: 4.5984 
2024-05-15 10:40:07.129 | INFO     | __main__:train:123 - Epoch: [14][240/390]	 loss 4.51309	 cls_loss: 0.7506 cluster_loss: 1.1633 sup_con_loss: 1.4012 contrastive_loss: 4.6212 
2024-05-15 10:40:20.077 | INFO     | __main__:train:123 - Epoch: [14][260/390]	 loss 4.34576	 cls_loss: 0.5026 cluster_loss: 1.0475 sup_con_loss: 1.3984 contrastive_loss: 4.6146 
2024-05-15 10:40:33.296 | INFO     | __main__:train:123 - Epoch: [14][280/390]	 loss 4.24643	 cls_loss: 0.5073 cluster_loss: 1.1391 sup_con_loss: 0.9563 contrastive_loss: 4.6058 
2024-05-15 10:40:46.176 | INFO     | __main__:train:123 - Epoch: [14][300/390]	 loss 4.31685	 cls_loss: 0.5564 cluster_loss: 1.1610 sup_con_loss: 1.0376 contrastive_loss: 4.6221 
2024-05-15 10:40:58.929 | INFO     | __main__:train:123 - Epoch: [14][320/390]	 loss 4.37478	 cls_loss: 0.5482 cluster_loss: 1.0278 sup_con_loss: 1.4835 contrastive_loss: 4.6087 
2024-05-15 10:41:12.005 | INFO     | __main__:train:123 - Epoch: [14][340/390]	 loss 4.33784	 cls_loss: 0.4989 cluster_loss: 1.1691 sup_con_loss: 1.1414 contrastive_loss: 4.6213 
2024-05-15 10:41:24.906 | INFO     | __main__:train:123 - Epoch: [14][360/390]	 loss 4.16641	 cls_loss: 0.4271 cluster_loss: 1.0648 sup_con_loss: 0.9460 contrastive_loss: 4.6057 
2024-05-15 10:41:37.535 | INFO     | __main__:train:123 - Epoch: [14][380/390]	 loss 4.16385	 cls_loss: 0.4690 cluster_loss: 1.0821 sup_con_loss: 0.8657 contrastive_loss: 4.6052 
2024-05-15 10:41:43.241 | INFO     | __main__:train:126 - Train Epoch: 14 Avg Loss: 4.2866 
2024-05-15 10:41:43.241 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:42:10.293 | INFO     | __main__:train:135 - Train Accuracies: All 0.7970 | Old 0.8275 | New 0.6750
2024-05-15 10:42:15.834 | INFO     | __main__:train:123 - Epoch: [15][0/390]	 loss 4.37704	 cls_loss: 0.6044 cluster_loss: 1.1361 sup_con_loss: 1.2293 contrastive_loss: 4.6105 
2024-05-15 10:42:28.948 | INFO     | __main__:train:123 - Epoch: [15][20/390]	 loss 4.15029	 cls_loss: 0.5362 cluster_loss: 1.0912 sup_con_loss: 0.7146 contrastive_loss: 4.6204 
2024-05-15 10:42:41.680 | INFO     | __main__:train:123 - Epoch: [15][40/390]	 loss 4.23781	 cls_loss: 0.5198 cluster_loss: 1.0393 sup_con_loss: 1.0992 contrastive_loss: 4.6087 
2024-05-15 10:42:54.430 | INFO     | __main__:train:123 - Epoch: [15][60/390]	 loss 4.24787	 cls_loss: 0.5212 cluster_loss: 1.0142 sup_con_loss: 1.1735 contrastive_loss: 4.6085 
2024-05-15 10:43:07.404 | INFO     | __main__:train:123 - Epoch: [15][80/390]	 loss 4.28440	 cls_loss: 0.5036 cluster_loss: 1.1555 sup_con_loss: 1.0336 contrastive_loss: 4.6081 
2024-05-15 10:43:20.804 | INFO     | __main__:train:123 - Epoch: [15][100/390]	 loss 4.25505	 cls_loss: 0.4625 cluster_loss: 1.1152 sup_con_loss: 1.0577 contrastive_loss: 4.6125 
2024-05-15 10:43:33.962 | INFO     | __main__:train:123 - Epoch: [15][120/390]	 loss 4.16794	 cls_loss: 0.4682 cluster_loss: 1.0257 sup_con_loss: 0.9735 contrastive_loss: 4.6102 
2024-05-15 10:43:47.004 | INFO     | __main__:train:123 - Epoch: [15][140/390]	 loss 4.43544	 cls_loss: 0.6358 cluster_loss: 1.1635 sup_con_loss: 1.3159 contrastive_loss: 4.6094 
2024-05-15 10:43:59.992 | INFO     | __main__:train:123 - Epoch: [15][160/390]	 loss 4.31713	 cls_loss: 0.4874 cluster_loss: 1.1216 sup_con_loss: 1.1746 contrastive_loss: 4.6252 
2024-05-15 10:44:12.793 | INFO     | __main__:train:123 - Epoch: [15][180/390]	 loss 4.22378	 cls_loss: 0.4207 cluster_loss: 1.0332 sup_con_loss: 1.1709 contrastive_loss: 4.6080 
2024-05-15 10:44:25.871 | INFO     | __main__:train:123 - Epoch: [15][200/390]	 loss 4.49042	 cls_loss: 0.5334 cluster_loss: 1.2390 sup_con_loss: 1.4141 contrastive_loss: 4.6207 
2024-05-15 10:44:38.875 | INFO     | __main__:train:123 - Epoch: [15][220/390]	 loss 4.37743	 cls_loss: 0.5623 cluster_loss: 1.1094 sup_con_loss: 1.2995 contrastive_loss: 4.6225 
2024-05-15 10:44:51.738 | INFO     | __main__:train:123 - Epoch: [15][240/390]	 loss 4.18647	 cls_loss: 0.4779 cluster_loss: 1.0463 sup_con_loss: 0.9778 contrastive_loss: 4.6106 
2024-05-15 10:45:04.674 | INFO     | __main__:train:123 - Epoch: [15][260/390]	 loss 4.57394	 cls_loss: 0.6238 cluster_loss: 1.2645 sup_con_loss: 1.5149 contrastive_loss: 4.6207 
2024-05-15 10:45:17.422 | INFO     | __main__:train:123 - Epoch: [15][280/390]	 loss 4.10537	 cls_loss: 0.3973 cluster_loss: 1.0275 sup_con_loss: 0.8837 contrastive_loss: 4.5987 
2024-05-15 10:45:30.440 | INFO     | __main__:train:123 - Epoch: [15][300/390]	 loss 4.12198	 cls_loss: 0.4346 cluster_loss: 1.0351 sup_con_loss: 0.8739 contrastive_loss: 4.6019 
2024-05-15 10:45:43.433 | INFO     | __main__:train:123 - Epoch: [15][320/390]	 loss 4.27642	 cls_loss: 0.6563 cluster_loss: 1.0191 sup_con_loss: 1.0897 contrastive_loss: 4.6198 
2024-05-15 10:45:56.762 | INFO     | __main__:train:123 - Epoch: [15][340/390]	 loss 4.36542	 cls_loss: 0.4260 cluster_loss: 1.1107 sup_con_loss: 1.4218 contrastive_loss: 4.6103 
2024-05-15 10:46:09.859 | INFO     | __main__:train:123 - Epoch: [15][360/390]	 loss 4.21786	 cls_loss: 0.4499 cluster_loss: 1.0996 sup_con_loss: 1.0126 contrastive_loss: 4.6019 
2024-05-15 10:46:22.629 | INFO     | __main__:train:123 - Epoch: [15][380/390]	 loss 4.34017	 cls_loss: 0.5877 cluster_loss: 1.1733 sup_con_loss: 1.0841 contrastive_loss: 4.6037 
2024-05-15 10:46:28.614 | INFO     | __main__:train:126 - Train Epoch: 15 Avg Loss: 4.2591 
2024-05-15 10:46:28.614 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:46:55.553 | INFO     | __main__:train:135 - Train Accuracies: All 0.7991 | Old 0.8264 | New 0.6900
2024-05-15 10:47:00.617 | INFO     | __main__:train:123 - Epoch: [16][0/390]	 loss 4.15705	 cls_loss: 0.4787 cluster_loss: 1.0478 sup_con_loss: 0.8933 contrastive_loss: 4.6089 
2024-05-15 10:47:14.259 | INFO     | __main__:train:123 - Epoch: [16][20/390]	 loss 4.27214	 cls_loss: 0.6248 cluster_loss: 1.0242 sup_con_loss: 1.1235 contrastive_loss: 4.6070 
2024-05-15 10:47:27.325 | INFO     | __main__:train:123 - Epoch: [16][40/390]	 loss 4.16908	 cls_loss: 0.5404 cluster_loss: 0.9821 sup_con_loss: 0.9999 contrastive_loss: 4.6025 
2024-05-15 10:47:40.445 | INFO     | __main__:train:123 - Epoch: [16][60/390]	 loss 4.29866	 cls_loss: 0.5569 cluster_loss: 1.0561 sup_con_loss: 1.2197 contrastive_loss: 4.6005 
2024-05-15 10:47:53.361 | INFO     | __main__:train:123 - Epoch: [16][80/390]	 loss 4.02547	 cls_loss: 0.4112 cluster_loss: 1.0230 sup_con_loss: 0.6352 contrastive_loss: 4.6066 
2024-05-15 10:48:06.366 | INFO     | __main__:train:123 - Epoch: [16][100/390]	 loss 4.37389	 cls_loss: 0.5528 cluster_loss: 1.1804 sup_con_loss: 1.1598 contrastive_loss: 4.6265 
2024-05-15 10:48:19.700 | INFO     | __main__:train:123 - Epoch: [16][120/390]	 loss 4.27331	 cls_loss: 0.4430 cluster_loss: 1.1234 sup_con_loss: 1.1114 contrastive_loss: 4.6140 
2024-05-15 10:48:32.535 | INFO     | __main__:train:123 - Epoch: [16][140/390]	 loss 4.15451	 cls_loss: 0.4367 cluster_loss: 0.9830 sup_con_loss: 1.0562 contrastive_loss: 4.6046 
2024-05-15 10:48:45.583 | INFO     | __main__:train:123 - Epoch: [16][160/390]	 loss 4.21244	 cls_loss: 0.5132 cluster_loss: 1.0817 sup_con_loss: 0.9605 contrastive_loss: 4.6055 
2024-05-15 10:48:58.697 | INFO     | __main__:train:123 - Epoch: [16][180/390]	 loss 4.19860	 cls_loss: 0.4190 cluster_loss: 1.1387 sup_con_loss: 0.8749 contrastive_loss: 4.6240 
2024-05-15 10:49:12.041 | INFO     | __main__:train:123 - Epoch: [16][200/390]	 loss 4.09593	 cls_loss: 0.4065 cluster_loss: 1.0203 sup_con_loss: 0.8412 contrastive_loss: 4.6092 
2024-05-15 10:49:24.951 | INFO     | __main__:train:123 - Epoch: [16][220/390]	 loss 4.17601	 cls_loss: 0.4533 cluster_loss: 1.0430 sup_con_loss: 0.9971 contrastive_loss: 4.6006 
2024-05-15 10:49:38.060 | INFO     | __main__:train:123 - Epoch: [16][240/390]	 loss 4.19838	 cls_loss: 0.6158 cluster_loss: 0.9555 sup_con_loss: 1.0474 contrastive_loss: 4.6080 
2024-05-15 10:49:51.015 | INFO     | __main__:train:123 - Epoch: [16][260/390]	 loss 4.13943	 cls_loss: 0.3818 cluster_loss: 1.0790 sup_con_loss: 0.8711 contrastive_loss: 4.6147 
2024-05-15 10:50:04.226 | INFO     | __main__:train:123 - Epoch: [16][280/390]	 loss 4.35255	 cls_loss: 0.4741 cluster_loss: 1.1780 sup_con_loss: 1.1494 contrastive_loss: 4.6440 
2024-05-15 10:50:17.286 | INFO     | __main__:train:123 - Epoch: [16][300/390]	 loss 4.14167	 cls_loss: 0.4181 cluster_loss: 0.9489 sup_con_loss: 1.1061 contrastive_loss: 4.6022 
2024-05-15 10:50:30.646 | INFO     | __main__:train:123 - Epoch: [16][320/390]	 loss 4.22984	 cls_loss: 0.5604 cluster_loss: 1.1170 sup_con_loss: 0.8758 contrastive_loss: 4.6171 
2024-05-15 10:50:43.561 | INFO     | __main__:train:123 - Epoch: [16][340/390]	 loss 4.15324	 cls_loss: 0.4542 cluster_loss: 1.0371 sup_con_loss: 0.9075 contrastive_loss: 4.6192 
2024-05-15 10:50:56.535 | INFO     | __main__:train:123 - Epoch: [16][360/390]	 loss 4.22857	 cls_loss: 0.5104 cluster_loss: 1.1063 sup_con_loss: 0.9302 contrastive_loss: 4.6235 
2024-05-15 10:51:09.484 | INFO     | __main__:train:123 - Epoch: [16][380/390]	 loss 4.26927	 cls_loss: 0.4665 cluster_loss: 1.1304 sup_con_loss: 1.0736 contrastive_loss: 4.6084 
2024-05-15 10:51:15.488 | INFO     | __main__:train:126 - Train Epoch: 16 Avg Loss: 4.2462 
2024-05-15 10:51:15.489 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:51:41.598 | INFO     | __main__:train:135 - Train Accuracies: All 0.7967 | Old 0.8263 | New 0.6785
2024-05-15 10:51:47.211 | INFO     | __main__:train:123 - Epoch: [17][0/390]	 loss 4.06647	 cls_loss: 0.4582 cluster_loss: 0.9977 sup_con_loss: 0.7451 contrastive_loss: 4.6104 
2024-05-15 10:51:59.903 | INFO     | __main__:train:123 - Epoch: [17][20/390]	 loss 4.33837	 cls_loss: 0.4333 cluster_loss: 1.1550 sup_con_loss: 1.2591 contrastive_loss: 4.6081 
2024-05-15 10:52:12.925 | INFO     | __main__:train:123 - Epoch: [17][40/390]	 loss 4.18069	 cls_loss: 0.5005 cluster_loss: 1.1284 sup_con_loss: 0.7745 contrastive_loss: 4.6169 
2024-05-15 10:52:25.869 | INFO     | __main__:train:123 - Epoch: [17][60/390]	 loss 4.20903	 cls_loss: 0.4886 cluster_loss: 1.0170 sup_con_loss: 1.0989 contrastive_loss: 4.6036 
2024-05-15 10:52:38.667 | INFO     | __main__:train:123 - Epoch: [17][80/390]	 loss 4.37479	 cls_loss: 0.5249 cluster_loss: 1.1610 sup_con_loss: 1.2527 contrastive_loss: 4.6122 
2024-05-15 10:52:51.644 | INFO     | __main__:train:123 - Epoch: [17][100/390]	 loss 4.13990	 cls_loss: 0.4504 cluster_loss: 1.0394 sup_con_loss: 0.8955 contrastive_loss: 4.6050 
2024-05-15 10:53:04.579 | INFO     | __main__:train:123 - Epoch: [17][120/390]	 loss 4.10445	 cls_loss: 0.4482 cluster_loss: 1.0075 sup_con_loss: 0.8515 contrastive_loss: 4.6072 
2024-05-15 10:53:17.512 | INFO     | __main__:train:123 - Epoch: [17][140/390]	 loss 4.31775	 cls_loss: 0.6762 cluster_loss: 1.0039 sup_con_loss: 1.2513 contrastive_loss: 4.6009 
2024-05-15 10:53:30.563 | INFO     | __main__:train:123 - Epoch: [17][160/390]	 loss 4.29906	 cls_loss: 0.5137 cluster_loss: 1.1843 sup_con_loss: 1.0142 contrastive_loss: 4.6070 
2024-05-15 10:53:43.204 | INFO     | __main__:train:123 - Epoch: [17][180/390]	 loss 4.33460	 cls_loss: 0.5760 cluster_loss: 1.0955 sup_con_loss: 1.1766 contrastive_loss: 4.6294 
2024-05-15 10:53:56.232 | INFO     | __main__:train:123 - Epoch: [17][200/390]	 loss 4.27453	 cls_loss: 0.5445 cluster_loss: 1.1281 sup_con_loss: 1.0083 contrastive_loss: 4.6120 
2024-05-15 10:54:08.971 | INFO     | __main__:train:123 - Epoch: [17][220/390]	 loss 4.07452	 cls_loss: 0.4149 cluster_loss: 1.0772 sup_con_loss: 0.6745 contrastive_loss: 4.6047 
2024-05-15 10:54:21.979 | INFO     | __main__:train:123 - Epoch: [17][240/390]	 loss 4.15537	 cls_loss: 0.4297 cluster_loss: 1.0936 sup_con_loss: 0.8606 contrastive_loss: 4.6045 
2024-05-15 10:54:34.966 | INFO     | __main__:train:123 - Epoch: [17][260/390]	 loss 4.18820	 cls_loss: 0.4938 cluster_loss: 0.9796 sup_con_loss: 1.1047 contrastive_loss: 4.6031 
2024-05-15 10:54:47.569 | INFO     | __main__:train:123 - Epoch: [17][280/390]	 loss 4.13292	 cls_loss: 0.4605 cluster_loss: 1.0226 sup_con_loss: 0.8843 contrastive_loss: 4.6117 
2024-05-15 10:55:00.520 | INFO     | __main__:train:123 - Epoch: [17][300/390]	 loss 4.32988	 cls_loss: 0.5989 cluster_loss: 1.0600 sup_con_loss: 1.2418 contrastive_loss: 4.6103 
2024-05-15 10:55:13.442 | INFO     | __main__:train:123 - Epoch: [17][320/390]	 loss 4.00311	 cls_loss: 0.3595 cluster_loss: 0.9663 sup_con_loss: 0.7365 contrastive_loss: 4.6022 
2024-05-15 10:55:26.310 | INFO     | __main__:train:123 - Epoch: [17][340/390]	 loss 4.10809	 cls_loss: 0.4811 cluster_loss: 0.9979 sup_con_loss: 0.8416 contrastive_loss: 4.6100 
2024-05-15 10:55:39.158 | INFO     | __main__:train:123 - Epoch: [17][360/390]	 loss 4.32087	 cls_loss: 0.5267 cluster_loss: 1.1181 sup_con_loss: 1.1771 contrastive_loss: 4.6120 
2024-05-15 10:55:51.668 | INFO     | __main__:train:123 - Epoch: [17][380/390]	 loss 4.25959	 cls_loss: 0.4515 cluster_loss: 1.0340 sup_con_loss: 1.2540 contrastive_loss: 4.6009 
2024-05-15 10:55:57.345 | INFO     | __main__:train:126 - Train Epoch: 17 Avg Loss: 4.2294 
2024-05-15 10:55:57.345 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:56:23.441 | INFO     | __main__:train:135 - Train Accuracies: All 0.7991 | Old 0.8274 | New 0.6860
2024-05-15 10:56:27.248 | INFO     | __main__:train:123 - Epoch: [18][0/390]	 loss 4.09874	 cls_loss: 0.4277 cluster_loss: 0.9585 sup_con_loss: 0.9623 contrastive_loss: 4.5988 
2024-05-15 10:56:41.732 | INFO     | __main__:train:123 - Epoch: [18][20/390]	 loss 4.28150	 cls_loss: 0.5440 cluster_loss: 1.1198 sup_con_loss: 1.0452 contrastive_loss: 4.6114 
2024-05-15 10:56:54.621 | INFO     | __main__:train:123 - Epoch: [18][40/390]	 loss 4.52252	 cls_loss: 0.6653 cluster_loss: 1.2246 sup_con_loss: 1.3929 contrastive_loss: 4.6248 
2024-05-15 10:57:07.540 | INFO     | __main__:train:123 - Epoch: [18][60/390]	 loss 4.20282	 cls_loss: 0.4820 cluster_loss: 1.0293 sup_con_loss: 1.0661 contrastive_loss: 4.6029 
2024-05-15 10:57:20.461 | INFO     | __main__:train:123 - Epoch: [18][80/390]	 loss 4.05719	 cls_loss: 0.3338 cluster_loss: 0.9878 sup_con_loss: 0.8919 contrastive_loss: 4.5940 
2024-05-15 10:57:33.361 | INFO     | __main__:train:123 - Epoch: [18][100/390]	 loss 4.20990	 cls_loss: 0.4654 cluster_loss: 1.0526 sup_con_loss: 1.0424 contrastive_loss: 4.6124 
2024-05-15 10:57:46.290 | INFO     | __main__:train:123 - Epoch: [18][120/390]	 loss 4.25073	 cls_loss: 0.6653 cluster_loss: 0.9833 sup_con_loss: 1.0952 contrastive_loss: 4.6083 
2024-05-15 10:57:59.280 | INFO     | __main__:train:123 - Epoch: [18][140/390]	 loss 4.31119	 cls_loss: 0.5143 cluster_loss: 1.0937 sup_con_loss: 1.2233 contrastive_loss: 4.6033 
2024-05-15 10:58:12.444 | INFO     | __main__:train:123 - Epoch: [18][160/390]	 loss 4.32410	 cls_loss: 0.5498 cluster_loss: 1.0475 sup_con_loss: 1.2805 contrastive_loss: 4.6194 
2024-05-15 10:58:25.487 | INFO     | __main__:train:123 - Epoch: [18][180/390]	 loss 4.10309	 cls_loss: 0.4083 cluster_loss: 0.9048 sup_con_loss: 1.0828 contrastive_loss: 4.6048 
2024-05-15 10:58:38.625 | INFO     | __main__:train:123 - Epoch: [18][200/390]	 loss 4.24076	 cls_loss: 0.4598 cluster_loss: 1.1297 sup_con_loss: 0.9883 contrastive_loss: 4.6148 
2024-05-15 10:58:51.511 | INFO     | __main__:train:123 - Epoch: [18][220/390]	 loss 4.23434	 cls_loss: 0.4755 cluster_loss: 1.1147 sup_con_loss: 0.9818 contrastive_loss: 4.6150 
2024-05-15 10:59:04.453 | INFO     | __main__:train:123 - Epoch: [18][240/390]	 loss 4.02110	 cls_loss: 0.3749 cluster_loss: 0.9712 sup_con_loss: 0.7620 contrastive_loss: 4.6029 
2024-05-15 10:59:17.311 | INFO     | __main__:train:123 - Epoch: [18][260/390]	 loss 4.10537	 cls_loss: 0.4429 cluster_loss: 1.0087 sup_con_loss: 0.8570 contrastive_loss: 4.6073 
2024-05-15 10:59:30.384 | INFO     | __main__:train:123 - Epoch: [18][280/390]	 loss 4.29256	 cls_loss: 0.4614 cluster_loss: 1.1729 sup_con_loss: 1.0552 contrastive_loss: 4.6143 
2024-05-15 10:59:43.396 | INFO     | __main__:train:123 - Epoch: [18][300/390]	 loss 4.38829	 cls_loss: 0.6194 cluster_loss: 1.0825 sup_con_loss: 1.3597 contrastive_loss: 4.6030 
2024-05-15 10:59:56.378 | INFO     | __main__:train:123 - Epoch: [18][320/390]	 loss 4.30296	 cls_loss: 0.5445 cluster_loss: 1.0661 sup_con_loss: 1.2153 contrastive_loss: 4.6062 
2024-05-15 11:00:09.502 | INFO     | __main__:train:123 - Epoch: [18][340/390]	 loss 4.33417	 cls_loss: 0.4652 cluster_loss: 1.1826 sup_con_loss: 1.1818 contrastive_loss: 4.5986 
2024-05-15 11:00:22.311 | INFO     | __main__:train:123 - Epoch: [18][360/390]	 loss 4.27781	 cls_loss: 0.4696 cluster_loss: 1.0434 sup_con_loss: 1.2323 contrastive_loss: 4.6214 
2024-05-15 11:00:34.806 | INFO     | __main__:train:123 - Epoch: [18][380/390]	 loss 4.24029	 cls_loss: 0.5018 cluster_loss: 1.0794 sup_con_loss: 1.0441 contrastive_loss: 4.6117 
2024-05-15 11:00:40.834 | INFO     | __main__:train:126 - Train Epoch: 18 Avg Loss: 4.2031 
2024-05-15 11:00:40.835 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:01:07.337 | INFO     | __main__:train:135 - Train Accuracies: All 0.8025 | Old 0.8305 | New 0.6905
2024-05-15 11:01:13.019 | INFO     | __main__:train:123 - Epoch: [19][0/390]	 loss 4.07808	 cls_loss: 0.3833 cluster_loss: 0.9278 sup_con_loss: 1.0073 contrastive_loss: 4.5974 
2024-05-15 11:01:25.951 | INFO     | __main__:train:123 - Epoch: [19][20/390]	 loss 4.18671	 cls_loss: 0.5235 cluster_loss: 1.0366 sup_con_loss: 0.9585 contrastive_loss: 4.6064 
2024-05-15 11:01:38.735 | INFO     | __main__:train:123 - Epoch: [19][40/390]	 loss 4.30947	 cls_loss: 0.5186 cluster_loss: 1.0803 sup_con_loss: 1.2192 contrastive_loss: 4.6139 
2024-05-15 11:01:51.579 | INFO     | __main__:train:123 - Epoch: [19][60/390]	 loss 4.00035	 cls_loss: 0.4309 cluster_loss: 0.9206 sup_con_loss: 0.7428 contrastive_loss: 4.6018 
2024-05-15 11:02:04.395 | INFO     | __main__:train:123 - Epoch: [19][80/390]	 loss 4.22771	 cls_loss: 0.5619 cluster_loss: 1.0079 sup_con_loss: 1.0918 contrastive_loss: 4.6058 
2024-05-15 11:02:16.923 | INFO     | __main__:train:123 - Epoch: [19][100/390]	 loss 4.11154	 cls_loss: 0.3679 cluster_loss: 1.0596 sup_con_loss: 0.8659 contrastive_loss: 4.6015 
2024-05-15 11:02:29.967 | INFO     | __main__:train:123 - Epoch: [19][120/390]	 loss 4.19778	 cls_loss: 0.4895 cluster_loss: 1.1572 sup_con_loss: 0.7749 contrastive_loss: 4.6201 
2024-05-15 11:02:42.789 | INFO     | __main__:train:123 - Epoch: [19][140/390]	 loss 4.01872	 cls_loss: 0.3813 cluster_loss: 1.0255 sup_con_loss: 0.6452 contrastive_loss: 4.6044 
2024-05-15 11:02:55.924 | INFO     | __main__:train:123 - Epoch: [19][160/390]	 loss 4.35398	 cls_loss: 0.4362 cluster_loss: 1.0658 sup_con_loss: 1.4429 contrastive_loss: 4.6208 
2024-05-15 11:03:09.013 | INFO     | __main__:train:123 - Epoch: [19][180/390]	 loss 4.23506	 cls_loss: 0.4498 cluster_loss: 1.0638 sup_con_loss: 1.1132 contrastive_loss: 4.6101 
2024-05-15 11:03:22.059 | INFO     | __main__:train:123 - Epoch: [19][200/390]	 loss 4.06666	 cls_loss: 0.4145 cluster_loss: 1.0043 sup_con_loss: 0.7997 contrastive_loss: 4.5984 
2024-05-15 11:03:34.854 | INFO     | __main__:train:123 - Epoch: [19][220/390]	 loss 4.26348	 cls_loss: 0.5847 cluster_loss: 1.0219 sup_con_loss: 1.1551 contrastive_loss: 4.6006 
2024-05-15 11:03:48.185 | INFO     | __main__:train:123 - Epoch: [19][240/390]	 loss 4.26061	 cls_loss: 0.4583 cluster_loss: 1.1252 sup_con_loss: 1.0588 contrastive_loss: 4.6127 
2024-05-15 11:04:01.433 | INFO     | __main__:train:123 - Epoch: [19][260/390]	 loss 4.28950	 cls_loss: 0.4998 cluster_loss: 1.1976 sup_con_loss: 0.9555 contrastive_loss: 4.6180 
2024-05-15 11:04:14.272 | INFO     | __main__:train:123 - Epoch: [19][280/390]	 loss 4.00995	 cls_loss: 0.3654 cluster_loss: 0.9470 sup_con_loss: 0.7867 contrastive_loss: 4.6018 
2024-05-15 11:04:27.281 | INFO     | __main__:train:123 - Epoch: [19][300/390]	 loss 4.20587	 cls_loss: 0.4906 cluster_loss: 1.0343 sup_con_loss: 1.0356 contrastive_loss: 4.6144 
2024-05-15 11:04:40.048 | INFO     | __main__:train:123 - Epoch: [19][320/390]	 loss 3.99285	 cls_loss: 0.3625 cluster_loss: 0.8635 sup_con_loss: 0.9057 contrastive_loss: 4.5965 
2024-05-15 11:04:52.910 | INFO     | __main__:train:123 - Epoch: [19][340/390]	 loss 4.41764	 cls_loss: 0.6484 cluster_loss: 1.1469 sup_con_loss: 1.2635 contrastive_loss: 4.6200 
2024-05-15 11:05:05.703 | INFO     | __main__:train:123 - Epoch: [19][360/390]	 loss 4.38623	 cls_loss: 0.6744 cluster_loss: 1.0469 sup_con_loss: 1.3383 contrastive_loss: 4.6174 
2024-05-15 11:05:18.198 | INFO     | __main__:train:123 - Epoch: [19][380/390]	 loss 4.16440	 cls_loss: 0.4899 cluster_loss: 0.9806 sup_con_loss: 1.0329 contrastive_loss: 4.6062 
2024-05-15 11:05:23.820 | INFO     | __main__:train:126 - Train Epoch: 19 Avg Loss: 4.1850 
2024-05-15 11:05:23.821 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:05:50.235 | INFO     | __main__:train:135 - Train Accuracies: All 0.8019 | Old 0.8285 | New 0.6955
2024-05-15 11:05:54.205 | INFO     | __main__:train:123 - Epoch: [20][0/390]	 loss 4.14186	 cls_loss: 0.3894 cluster_loss: 1.0644 sup_con_loss: 0.9077 contrastive_loss: 4.6092 
2024-05-15 11:06:07.793 | INFO     | __main__:train:123 - Epoch: [20][20/390]	 loss 4.17902	 cls_loss: 0.5420 cluster_loss: 1.0498 sup_con_loss: 0.9004 contrastive_loss: 4.6028 
2024-05-15 11:06:20.512 | INFO     | __main__:train:123 - Epoch: [20][40/390]	 loss 4.49356	 cls_loss: 0.5974 cluster_loss: 1.1560 sup_con_loss: 1.5083 contrastive_loss: 4.6233 
2024-05-15 11:06:33.519 | INFO     | __main__:train:123 - Epoch: [20][60/390]	 loss 4.18730	 cls_loss: 0.4168 cluster_loss: 1.0707 sup_con_loss: 1.0076 contrastive_loss: 4.6043 
2024-05-15 11:06:46.358 | INFO     | __main__:train:123 - Epoch: [20][80/390]	 loss 4.20530	 cls_loss: 0.4574 cluster_loss: 1.0248 sup_con_loss: 1.1037 contrastive_loss: 4.6043 
2024-05-15 11:06:59.121 | INFO     | __main__:train:123 - Epoch: [20][100/390]	 loss 4.07009	 cls_loss: 0.4553 cluster_loss: 0.9547 sup_con_loss: 0.8640 contrastive_loss: 4.5965 
2024-05-15 11:07:12.137 | INFO     | __main__:train:123 - Epoch: [20][120/390]	 loss 3.96470	 cls_loss: 0.3897 cluster_loss: 0.9787 sup_con_loss: 0.5681 contrastive_loss: 4.6051 
2024-05-15 11:07:25.141 | INFO     | __main__:train:123 - Epoch: [20][140/390]	 loss 4.18348	 cls_loss: 0.3946 cluster_loss: 1.0493 sup_con_loss: 1.0490 contrastive_loss: 4.6095 
2024-05-15 11:07:38.206 | INFO     | __main__:train:123 - Epoch: [20][160/390]	 loss 4.16362	 cls_loss: 0.4790 cluster_loss: 1.0297 sup_con_loss: 0.9395 contrastive_loss: 4.6121 
2024-05-15 11:07:51.009 | INFO     | __main__:train:123 - Epoch: [20][180/390]	 loss 4.32178	 cls_loss: 0.5837 cluster_loss: 1.0609 sup_con_loss: 1.2282 contrastive_loss: 4.6123 
2024-05-15 11:08:04.020 | INFO     | __main__:train:123 - Epoch: [20][200/390]	 loss 3.95861	 cls_loss: 0.3312 cluster_loss: 0.9680 sup_con_loss: 0.6438 contrastive_loss: 4.5972 
2024-05-15 11:08:16.853 | INFO     | __main__:train:123 - Epoch: [20][220/390]	 loss 4.11238	 cls_loss: 0.4625 cluster_loss: 0.9783 sup_con_loss: 0.9185 contrastive_loss: 4.6048 
2024-05-15 11:08:29.833 | INFO     | __main__:train:123 - Epoch: [20][240/390]	 loss 4.14876	 cls_loss: 0.3916 cluster_loss: 0.9676 sup_con_loss: 1.1157 contrastive_loss: 4.6035 
2024-05-15 11:08:42.690 | INFO     | __main__:train:123 - Epoch: [20][260/390]	 loss 3.98919	 cls_loss: 0.3539 cluster_loss: 0.9560 sup_con_loss: 0.7139 contrastive_loss: 4.6062 
2024-05-15 11:08:55.536 | INFO     | __main__:train:123 - Epoch: [20][280/390]	 loss 4.31115	 cls_loss: 0.6396 cluster_loss: 1.0703 sup_con_loss: 1.1403 contrastive_loss: 4.6038 
2024-05-15 11:09:08.627 | INFO     | __main__:train:123 - Epoch: [20][300/390]	 loss 4.16886	 cls_loss: 0.4224 cluster_loss: 1.0086 sup_con_loss: 1.0653 contrastive_loss: 4.6040 
2024-05-15 11:09:21.771 | INFO     | __main__:train:123 - Epoch: [20][320/390]	 loss 4.19485	 cls_loss: 0.4328 cluster_loss: 1.0675 sup_con_loss: 1.0126 contrastive_loss: 4.6078 
2024-05-15 11:09:34.674 | INFO     | __main__:train:123 - Epoch: [20][340/390]	 loss 4.17720	 cls_loss: 0.5879 cluster_loss: 1.0372 sup_con_loss: 0.8762 contrastive_loss: 4.6009 
2024-05-15 11:09:47.506 | INFO     | __main__:train:123 - Epoch: [20][360/390]	 loss 4.11916	 cls_loss: 0.3642 cluster_loss: 0.9983 sup_con_loss: 1.0082 contrastive_loss: 4.5999 
2024-05-15 11:09:59.998 | INFO     | __main__:train:123 - Epoch: [20][380/390]	 loss 4.30202	 cls_loss: 0.5794 cluster_loss: 1.0934 sup_con_loss: 1.1044 contrastive_loss: 4.6184 
2024-05-15 11:10:05.953 | INFO     | __main__:train:126 - Train Epoch: 20 Avg Loss: 4.1688 
2024-05-15 11:10:05.954 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:10:32.441 | INFO     | __main__:train:135 - Train Accuracies: All 0.7990 | Old 0.8254 | New 0.6935
2024-05-15 11:10:37.946 | INFO     | __main__:train:123 - Epoch: [21][0/390]	 loss 4.18379	 cls_loss: 0.3707 cluster_loss: 1.0718 sup_con_loss: 1.0284 contrastive_loss: 4.6115 
2024-05-15 11:10:51.035 | INFO     | __main__:train:123 - Epoch: [21][20/390]	 loss 4.26526	 cls_loss: 0.6648 cluster_loss: 0.9585 sup_con_loss: 1.1782 contrastive_loss: 4.6110 
2024-05-15 11:11:04.197 | INFO     | __main__:train:123 - Epoch: [21][40/390]	 loss 4.12900	 cls_loss: 0.4625 cluster_loss: 0.9814 sup_con_loss: 0.9556 contrastive_loss: 4.6073 
2024-05-15 11:11:17.116 | INFO     | __main__:train:123 - Epoch: [21][60/390]	 loss 4.14649	 cls_loss: 0.3986 cluster_loss: 1.0002 sup_con_loss: 1.0415 contrastive_loss: 4.6036 
2024-05-15 11:11:30.074 | INFO     | __main__:train:123 - Epoch: [21][80/390]	 loss 4.23883	 cls_loss: 0.4741 cluster_loss: 0.9935 sup_con_loss: 1.2293 contrastive_loss: 4.6106 
2024-05-15 11:11:43.151 | INFO     | __main__:train:123 - Epoch: [21][100/390]	 loss 4.01986	 cls_loss: 0.3528 cluster_loss: 0.8775 sup_con_loss: 0.9734 contrastive_loss: 4.5928 
2024-05-15 11:11:56.106 | INFO     | __main__:train:123 - Epoch: [21][120/390]	 loss 4.05112	 cls_loss: 0.4411 cluster_loss: 0.9076 sup_con_loss: 0.8986 contrastive_loss: 4.6035 
2024-05-15 11:12:08.987 | INFO     | __main__:train:123 - Epoch: [21][140/390]	 loss 4.20617	 cls_loss: 0.4338 cluster_loss: 1.0249 sup_con_loss: 1.1304 contrastive_loss: 4.6039 
2024-05-15 11:12:22.272 | INFO     | __main__:train:123 - Epoch: [21][160/390]	 loss 4.18289	 cls_loss: 0.4216 cluster_loss: 1.0565 sup_con_loss: 1.0095 contrastive_loss: 4.6082 
2024-05-15 11:12:35.195 | INFO     | __main__:train:123 - Epoch: [21][180/390]	 loss 4.30593	 cls_loss: 0.5309 cluster_loss: 1.0981 sup_con_loss: 1.1590 contrastive_loss: 4.6164 
2024-05-15 11:12:47.891 | INFO     | __main__:train:123 - Epoch: [21][200/390]	 loss 4.27166	 cls_loss: 0.4687 cluster_loss: 1.1657 sup_con_loss: 0.9982 contrastive_loss: 4.6162 
2024-05-15 11:13:00.781 | INFO     | __main__:train:123 - Epoch: [21][220/390]	 loss 4.13631	 cls_loss: 0.3459 cluster_loss: 1.0302 sup_con_loss: 0.9779 contrastive_loss: 4.6205 
2024-05-15 11:13:13.529 | INFO     | __main__:train:123 - Epoch: [21][240/390]	 loss 4.50293	 cls_loss: 0.3927 cluster_loss: 1.2223 sup_con_loss: 1.6407 contrastive_loss: 4.6105 
2024-05-15 11:13:26.241 | INFO     | __main__:train:123 - Epoch: [21][260/390]	 loss 4.11335	 cls_loss: 0.3427 cluster_loss: 1.0448 sup_con_loss: 0.9172 contrastive_loss: 4.6050 
2024-05-15 11:13:39.115 | INFO     | __main__:train:123 - Epoch: [21][280/390]	 loss 4.27972	 cls_loss: 0.5426 cluster_loss: 1.0356 sup_con_loss: 1.2077 contrastive_loss: 4.6062 
2024-05-15 11:13:52.011 | INFO     | __main__:train:123 - Epoch: [21][300/390]	 loss 4.19809	 cls_loss: 0.3988 cluster_loss: 1.1558 sup_con_loss: 0.8779 contrastive_loss: 4.6154 
2024-05-15 11:14:04.805 | INFO     | __main__:train:123 - Epoch: [21][320/390]	 loss 4.15660	 cls_loss: 0.4876 cluster_loss: 1.0649 sup_con_loss: 0.8572 contrastive_loss: 4.6057 
2024-05-15 11:14:17.647 | INFO     | __main__:train:123 - Epoch: [21][340/390]	 loss 4.03498	 cls_loss: 0.4702 cluster_loss: 0.9021 sup_con_loss: 0.8415 contrastive_loss: 4.5992 
2024-05-15 11:14:30.667 | INFO     | __main__:train:123 - Epoch: [21][360/390]	 loss 4.22271	 cls_loss: 0.3975 cluster_loss: 1.0660 sup_con_loss: 1.1179 contrastive_loss: 4.6145 
2024-05-15 11:14:43.522 | INFO     | __main__:train:123 - Epoch: [21][380/390]	 loss 4.07053	 cls_loss: 0.4126 cluster_loss: 0.9358 sup_con_loss: 0.9501 contrastive_loss: 4.5928 
2024-05-15 11:14:49.600 | INFO     | __main__:train:126 - Train Epoch: 21 Avg Loss: 4.1579 
2024-05-15 11:14:49.600 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:15:15.516 | INFO     | __main__:train:135 - Train Accuracies: All 0.8024 | Old 0.8287 | New 0.6970
2024-05-15 11:15:20.375 | INFO     | __main__:train:123 - Epoch: [22][0/390]	 loss 4.05457	 cls_loss: 0.3632 cluster_loss: 1.0283 sup_con_loss: 0.7497 contrastive_loss: 4.6102 
2024-05-15 11:15:33.706 | INFO     | __main__:train:123 - Epoch: [22][20/390]	 loss 4.09785	 cls_loss: 0.5874 cluster_loss: 0.9878 sup_con_loss: 0.7261 contrastive_loss: 4.6093 
2024-05-15 11:15:46.589 | INFO     | __main__:train:123 - Epoch: [22][40/390]	 loss 4.11909	 cls_loss: 0.4701 cluster_loss: 0.9340 sup_con_loss: 1.0304 contrastive_loss: 4.5951 
2024-05-15 11:15:59.499 | INFO     | __main__:train:123 - Epoch: [22][60/390]	 loss 3.99822	 cls_loss: 0.4300 cluster_loss: 0.9691 sup_con_loss: 0.6418 contrastive_loss: 4.6049 
2024-05-15 11:16:12.558 | INFO     | __main__:train:123 - Epoch: [22][80/390]	 loss 4.15120	 cls_loss: 0.3836 cluster_loss: 1.0682 sup_con_loss: 0.9362 contrastive_loss: 4.6076 
2024-05-15 11:16:25.549 | INFO     | __main__:train:123 - Epoch: [22][100/390]	 loss 4.21083	 cls_loss: 0.4238 cluster_loss: 1.1439 sup_con_loss: 0.8969 contrastive_loss: 4.6232 
2024-05-15 11:16:38.323 | INFO     | __main__:train:123 - Epoch: [22][120/390]	 loss 4.01138	 cls_loss: 0.3779 cluster_loss: 0.9389 sup_con_loss: 0.8081 contrastive_loss: 4.5938 
2024-05-15 11:16:51.450 | INFO     | __main__:train:123 - Epoch: [22][140/390]	 loss 4.45641	 cls_loss: 0.5948 cluster_loss: 1.1688 sup_con_loss: 1.4020 contrastive_loss: 4.6121 
2024-05-15 11:17:04.416 | INFO     | __main__:train:123 - Epoch: [22][160/390]	 loss 4.19379	 cls_loss: 0.3998 cluster_loss: 1.0967 sup_con_loss: 0.9931 contrastive_loss: 4.6053 
2024-05-15 11:17:17.723 | INFO     | __main__:train:123 - Epoch: [22][180/390]	 loss 4.15467	 cls_loss: 0.4118 cluster_loss: 1.0956 sup_con_loss: 0.8844 contrastive_loss: 4.5982 
2024-05-15 11:17:30.691 | INFO     | __main__:train:123 - Epoch: [22][200/390]	 loss 4.19634	 cls_loss: 0.5758 cluster_loss: 1.0353 sup_con_loss: 0.9207 contrastive_loss: 4.6148 
2024-05-15 11:17:43.708 | INFO     | __main__:train:123 - Epoch: [22][220/390]	 loss 4.20758	 cls_loss: 0.4785 cluster_loss: 1.0063 sup_con_loss: 1.1377 contrastive_loss: 4.5966 
2024-05-15 11:17:56.726 | INFO     | __main__:train:123 - Epoch: [22][240/390]	 loss 4.08262	 cls_loss: 0.3318 cluster_loss: 1.0387 sup_con_loss: 0.8384 contrastive_loss: 4.6121 
2024-05-15 11:18:09.742 | INFO     | __main__:train:123 - Epoch: [22][260/390]	 loss 4.08402	 cls_loss: 0.3704 cluster_loss: 1.0072 sup_con_loss: 0.8669 contrastive_loss: 4.6097 
2024-05-15 11:18:22.715 | INFO     | __main__:train:123 - Epoch: [22][280/390]	 loss 4.04806	 cls_loss: 0.3934 cluster_loss: 1.0051 sup_con_loss: 0.7661 contrastive_loss: 4.5983 
2024-05-15 11:18:35.802 | INFO     | __main__:train:123 - Epoch: [22][300/390]	 loss 4.07784	 cls_loss: 0.4373 cluster_loss: 0.9648 sup_con_loss: 0.8778 contrastive_loss: 4.6007 
2024-05-15 11:18:48.829 | INFO     | __main__:train:123 - Epoch: [22][320/390]	 loss 4.04755	 cls_loss: 0.3352 cluster_loss: 0.9984 sup_con_loss: 0.8275 contrastive_loss: 4.6025 
2024-05-15 11:19:02.073 | INFO     | __main__:train:123 - Epoch: [22][340/390]	 loss 4.16271	 cls_loss: 0.3602 cluster_loss: 1.1267 sup_con_loss: 0.8688 contrastive_loss: 4.6156 
2024-05-15 11:19:15.198 | INFO     | __main__:train:123 - Epoch: [22][360/390]	 loss 4.15986	 cls_loss: 0.4920 cluster_loss: 1.0913 sup_con_loss: 0.8037 contrastive_loss: 4.6107 
2024-05-15 11:19:27.788 | INFO     | __main__:train:123 - Epoch: [22][380/390]	 loss 4.07655	 cls_loss: 0.3641 cluster_loss: 1.0439 sup_con_loss: 0.7968 contrastive_loss: 4.6026 
2024-05-15 11:19:33.890 | INFO     | __main__:train:126 - Train Epoch: 22 Avg Loss: 4.1359 
2024-05-15 11:19:33.890 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:20:01.089 | INFO     | __main__:train:135 - Train Accuracies: All 0.8016 | Old 0.8295 | New 0.6900
2024-05-15 11:20:06.801 | INFO     | __main__:train:123 - Epoch: [23][0/390]	 loss 4.17498	 cls_loss: 0.4933 cluster_loss: 0.9820 sup_con_loss: 1.0650 contrastive_loss: 4.6019 
2024-05-15 11:20:19.873 | INFO     | __main__:train:123 - Epoch: [23][20/390]	 loss 4.18969	 cls_loss: 0.4218 cluster_loss: 1.0623 sup_con_loss: 1.0332 contrastive_loss: 4.5999 
2024-05-15 11:20:32.934 | INFO     | __main__:train:123 - Epoch: [23][40/390]	 loss 4.24321	 cls_loss: 0.4631 cluster_loss: 1.0295 sup_con_loss: 1.2094 contrastive_loss: 4.5979 
2024-05-15 11:20:45.929 | INFO     | __main__:train:123 - Epoch: [23][60/390]	 loss 3.97145	 cls_loss: 0.3515 cluster_loss: 0.9965 sup_con_loss: 0.5820 contrastive_loss: 4.6107 
2024-05-15 11:20:59.009 | INFO     | __main__:train:123 - Epoch: [23][80/390]	 loss 4.19638	 cls_loss: 0.4316 cluster_loss: 1.0921 sup_con_loss: 0.9824 contrastive_loss: 4.6025 
2024-05-15 11:21:11.988 | INFO     | __main__:train:123 - Epoch: [23][100/390]	 loss 4.11548	 cls_loss: 0.4133 cluster_loss: 1.0176 sup_con_loss: 0.9116 contrastive_loss: 4.6005 
2024-05-15 11:21:24.866 | INFO     | __main__:train:123 - Epoch: [23][120/390]	 loss 4.16814	 cls_loss: 0.3314 cluster_loss: 1.0602 sup_con_loss: 1.0591 contrastive_loss: 4.6036 
2024-05-15 11:21:37.951 | INFO     | __main__:train:123 - Epoch: [23][140/390]	 loss 4.08786	 cls_loss: 0.4181 cluster_loss: 0.9796 sup_con_loss: 0.8803 contrastive_loss: 4.6103 
2024-05-15 11:21:51.136 | INFO     | __main__:train:123 - Epoch: [23][160/390]	 loss 4.14992	 cls_loss: 0.4502 cluster_loss: 0.9549 sup_con_loss: 1.1060 contrastive_loss: 4.5917 
2024-05-15 11:22:04.346 | INFO     | __main__:train:123 - Epoch: [23][180/390]	 loss 4.16840	 cls_loss: 0.3979 cluster_loss: 1.0806 sup_con_loss: 0.9567 contrastive_loss: 4.6029 
2024-05-15 11:22:17.056 | INFO     | __main__:train:123 - Epoch: [23][200/390]	 loss 4.24234	 cls_loss: 0.5264 cluster_loss: 1.0808 sup_con_loss: 1.0327 contrastive_loss: 4.6063 
2024-05-15 11:22:29.767 | INFO     | __main__:train:123 - Epoch: [23][220/390]	 loss 4.14709	 cls_loss: 0.3870 cluster_loss: 1.0147 sup_con_loss: 1.0248 contrastive_loss: 4.6052 
2024-05-15 11:22:42.850 | INFO     | __main__:train:123 - Epoch: [23][240/390]	 loss 4.03418	 cls_loss: 0.3983 cluster_loss: 0.9503 sup_con_loss: 0.8067 contrastive_loss: 4.6073 
2024-05-15 11:22:55.855 | INFO     | __main__:train:123 - Epoch: [23][260/390]	 loss 4.25040	 cls_loss: 0.4223 cluster_loss: 1.0355 sup_con_loss: 1.2412 contrastive_loss: 4.6078 
2024-05-15 11:23:09.045 | INFO     | __main__:train:123 - Epoch: [23][280/390]	 loss 4.18338	 cls_loss: 0.5234 cluster_loss: 1.0936 sup_con_loss: 0.8303 contrastive_loss: 4.6134 
2024-05-15 11:23:22.111 | INFO     | __main__:train:123 - Epoch: [23][300/390]	 loss 4.23515	 cls_loss: 0.4122 cluster_loss: 1.1263 sup_con_loss: 1.0504 contrastive_loss: 4.6018 
2024-05-15 11:23:35.191 | INFO     | __main__:train:123 - Epoch: [23][320/390]	 loss 4.01937	 cls_loss: 0.3592 cluster_loss: 0.9364 sup_con_loss: 0.8556 contrastive_loss: 4.5931 
2024-05-15 11:23:48.443 | INFO     | __main__:train:123 - Epoch: [23][340/390]	 loss 4.11689	 cls_loss: 0.3550 cluster_loss: 1.0628 sup_con_loss: 0.8703 contrastive_loss: 4.6112 
2024-05-15 11:24:01.559 | INFO     | __main__:train:123 - Epoch: [23][360/390]	 loss 4.20031	 cls_loss: 0.3270 cluster_loss: 1.0596 sup_con_loss: 1.1676 contrastive_loss: 4.5976 
2024-05-15 11:24:14.189 | INFO     | __main__:train:123 - Epoch: [23][380/390]	 loss 3.99001	 cls_loss: 0.2848 cluster_loss: 0.9503 sup_con_loss: 0.8127 contrastive_loss: 4.5972 
2024-05-15 11:24:20.106 | INFO     | __main__:train:126 - Train Epoch: 23 Avg Loss: 4.1161 
2024-05-15 11:24:20.106 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:24:46.767 | INFO     | __main__:train:135 - Train Accuracies: All 0.8024 | Old 0.8310 | New 0.6880
2024-05-15 11:24:52.485 | INFO     | __main__:train:123 - Epoch: [24][0/390]	 loss 4.05596	 cls_loss: 0.4960 cluster_loss: 0.9741 sup_con_loss: 0.7391 contrastive_loss: 4.6008 
2024-05-15 11:25:05.417 | INFO     | __main__:train:123 - Epoch: [24][20/390]	 loss 3.99434	 cls_loss: 0.3008 cluster_loss: 0.9729 sup_con_loss: 0.7542 contrastive_loss: 4.6041 
2024-05-15 11:25:18.034 | INFO     | __main__:train:123 - Epoch: [24][40/390]	 loss 4.07142	 cls_loss: 0.4156 cluster_loss: 0.9577 sup_con_loss: 0.9043 contrastive_loss: 4.5953 
2024-05-15 11:25:30.787 | INFO     | __main__:train:123 - Epoch: [24][60/390]	 loss 4.23199	 cls_loss: 0.4065 cluster_loss: 1.1148 sup_con_loss: 1.0544 contrastive_loss: 4.6092 
2024-05-15 11:25:43.645 | INFO     | __main__:train:123 - Epoch: [24][80/390]	 loss 4.09409	 cls_loss: 0.3877 cluster_loss: 0.9592 sup_con_loss: 0.9888 contrastive_loss: 4.5982 
2024-05-15 11:25:56.393 | INFO     | __main__:train:123 - Epoch: [24][100/390]	 loss 4.28103	 cls_loss: 0.4496 cluster_loss: 1.0427 sup_con_loss: 1.2770 contrastive_loss: 4.6138 
2024-05-15 11:26:09.412 | INFO     | __main__:train:123 - Epoch: [24][120/390]	 loss 4.04114	 cls_loss: 0.3802 cluster_loss: 0.9848 sup_con_loss: 0.8035 contrastive_loss: 4.5950 
2024-05-15 11:26:22.397 | INFO     | __main__:train:123 - Epoch: [24][140/390]	 loss 4.03271	 cls_loss: 0.3942 cluster_loss: 0.8722 sup_con_loss: 0.9594 contrastive_loss: 4.6031 
2024-05-15 11:26:35.407 | INFO     | __main__:train:123 - Epoch: [24][160/390]	 loss 3.94481	 cls_loss: 0.3991 cluster_loss: 0.8482 sup_con_loss: 0.7708 contrastive_loss: 4.5907 
2024-05-15 11:26:48.383 | INFO     | __main__:train:123 - Epoch: [24][180/390]	 loss 4.09982	 cls_loss: 0.4259 cluster_loss: 0.9823 sup_con_loss: 0.9180 contrastive_loss: 4.6015 
2024-05-15 11:27:01.397 | INFO     | __main__:train:123 - Epoch: [24][200/390]	 loss 4.20948	 cls_loss: 0.5651 cluster_loss: 1.0347 sup_con_loss: 0.9866 contrastive_loss: 4.6058 
2024-05-15 11:27:14.385 | INFO     | __main__:train:123 - Epoch: [24][220/390]	 loss 4.08389	 cls_loss: 0.3394 cluster_loss: 1.0137 sup_con_loss: 0.9004 contrastive_loss: 4.6016 
2024-05-15 11:27:27.417 | INFO     | __main__:train:123 - Epoch: [24][240/390]	 loss 4.16631	 cls_loss: 0.4382 cluster_loss: 0.9829 sup_con_loss: 1.0711 contrastive_loss: 4.6141 
2024-05-15 11:27:40.381 | INFO     | __main__:train:123 - Epoch: [24][260/390]	 loss 4.02075	 cls_loss: 0.3699 cluster_loss: 0.8275 sup_con_loss: 1.0593 contrastive_loss: 4.5887 
2024-05-15 11:27:53.484 | INFO     | __main__:train:123 - Epoch: [24][280/390]	 loss 4.12661	 cls_loss: 0.4529 cluster_loss: 1.0272 sup_con_loss: 0.8880 contrastive_loss: 4.5994 
2024-05-15 11:28:06.516 | INFO     | __main__:train:123 - Epoch: [24][300/390]	 loss 4.26302	 cls_loss: 0.4119 cluster_loss: 1.1206 sup_con_loss: 1.1448 contrastive_loss: 4.5997 
2024-05-15 11:28:19.346 | INFO     | __main__:train:123 - Epoch: [24][320/390]	 loss 3.99965	 cls_loss: 0.3840 cluster_loss: 0.8913 sup_con_loss: 0.8473 contrastive_loss: 4.5990 
2024-05-15 11:28:32.430 | INFO     | __main__:train:123 - Epoch: [24][340/390]	 loss 4.03310	 cls_loss: 0.3968 cluster_loss: 0.9292 sup_con_loss: 0.8536 contrastive_loss: 4.6023 
2024-05-15 11:28:45.482 | INFO     | __main__:train:123 - Epoch: [24][360/390]	 loss 4.15048	 cls_loss: 0.3932 cluster_loss: 0.9731 sup_con_loss: 1.1195 contrastive_loss: 4.5977 
2024-05-15 11:28:58.077 | INFO     | __main__:train:123 - Epoch: [24][380/390]	 loss 3.96518	 cls_loss: 0.3774 cluster_loss: 0.8666 sup_con_loss: 0.8074 contrastive_loss: 4.5956 
2024-05-15 11:29:03.973 | INFO     | __main__:train:126 - Train Epoch: 24 Avg Loss: 4.1122 
2024-05-15 11:29:03.973 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:29:30.069 | INFO     | __main__:train:135 - Train Accuracies: All 0.8038 | Old 0.8306 | New 0.6965
2024-05-15 11:29:35.622 | INFO     | __main__:train:123 - Epoch: [25][0/390]	 loss 4.08198	 cls_loss: 0.4075 cluster_loss: 1.0138 sup_con_loss: 0.8356 contrastive_loss: 4.5969 
2024-05-15 11:29:48.775 | INFO     | __main__:train:123 - Epoch: [25][20/390]	 loss 4.10656	 cls_loss: 0.3930 cluster_loss: 0.9878 sup_con_loss: 0.9549 contrastive_loss: 4.6042 
2024-05-15 11:30:01.687 | INFO     | __main__:train:123 - Epoch: [25][40/390]	 loss 4.32032	 cls_loss: 0.6099 cluster_loss: 1.0956 sup_con_loss: 1.1390 contrastive_loss: 4.6093 
2024-05-15 11:30:14.517 | INFO     | __main__:train:123 - Epoch: [25][60/390]	 loss 4.25568	 cls_loss: 0.3635 cluster_loss: 1.1130 sup_con_loss: 1.1692 contrastive_loss: 4.6089 
2024-05-15 11:30:27.274 | INFO     | __main__:train:123 - Epoch: [25][80/390]	 loss 4.11627	 cls_loss: 0.5097 cluster_loss: 0.9654 sup_con_loss: 0.8973 contrastive_loss: 4.6098 
2024-05-15 11:30:40.049 | INFO     | __main__:train:123 - Epoch: [25][100/390]	 loss 4.18792	 cls_loss: 0.4535 cluster_loss: 1.0302 sup_con_loss: 1.0519 contrastive_loss: 4.6022 
2024-05-15 11:30:53.027 | INFO     | __main__:train:123 - Epoch: [25][120/390]	 loss 4.14059	 cls_loss: 0.3147 cluster_loss: 1.0155 sup_con_loss: 1.0823 contrastive_loss: 4.6024 
2024-05-15 11:31:05.937 | INFO     | __main__:train:123 - Epoch: [25][140/390]	 loss 4.21825	 cls_loss: 0.4173 cluster_loss: 1.0669 sup_con_loss: 1.0909 contrastive_loss: 4.6106 
2024-05-15 11:31:18.754 | INFO     | __main__:train:123 - Epoch: [25][160/390]	 loss 4.14634	 cls_loss: 0.4740 cluster_loss: 0.8558 sup_con_loss: 1.2515 contrastive_loss: 4.5941 
2024-05-15 11:31:31.623 | INFO     | __main__:train:123 - Epoch: [25][180/390]	 loss 4.16572	 cls_loss: 0.4277 cluster_loss: 1.1026 sup_con_loss: 0.8654 contrastive_loss: 4.6099 
2024-05-15 11:31:44.607 | INFO     | __main__:train:123 - Epoch: [25][200/390]	 loss 4.21790	 cls_loss: 0.4280 cluster_loss: 1.0672 sup_con_loss: 1.0799 contrastive_loss: 4.6099 
2024-05-15 11:31:57.509 | INFO     | __main__:train:123 - Epoch: [25][220/390]	 loss 4.18890	 cls_loss: 0.4574 cluster_loss: 1.0240 sup_con_loss: 1.0534 contrastive_loss: 4.6069 
2024-05-15 11:32:10.681 | INFO     | __main__:train:123 - Epoch: [25][240/390]	 loss 4.20952	 cls_loss: 0.3907 cluster_loss: 1.1695 sup_con_loss: 0.9026 contrastive_loss: 4.6103 
2024-05-15 11:32:23.752 | INFO     | __main__:train:123 - Epoch: [25][260/390]	 loss 4.08337	 cls_loss: 0.3055 cluster_loss: 1.0759 sup_con_loss: 0.8040 contrastive_loss: 4.6088 
2024-05-15 11:32:36.745 | INFO     | __main__:train:123 - Epoch: [25][280/390]	 loss 4.09849	 cls_loss: 0.3777 cluster_loss: 0.9993 sup_con_loss: 0.9391 contrastive_loss: 4.5970 
2024-05-15 11:32:49.753 | INFO     | __main__:train:123 - Epoch: [25][300/390]	 loss 3.97360	 cls_loss: 0.3146 cluster_loss: 0.9037 sup_con_loss: 0.8102 contrastive_loss: 4.6039 
2024-05-15 11:33:03.148 | INFO     | __main__:train:123 - Epoch: [25][320/390]	 loss 4.08555	 cls_loss: 0.3518 cluster_loss: 0.9711 sup_con_loss: 0.9670 contrastive_loss: 4.6042 
2024-05-15 11:33:15.976 | INFO     | __main__:train:123 - Epoch: [25][340/390]	 loss 4.08028	 cls_loss: 0.2418 cluster_loss: 1.0676 sup_con_loss: 0.8835 contrastive_loss: 4.6038 
2024-05-15 11:33:28.836 | INFO     | __main__:train:123 - Epoch: [25][360/390]	 loss 3.95763	 cls_loss: 0.5095 cluster_loss: 0.8560 sup_con_loss: 0.6783 contrastive_loss: 4.5931 
2024-05-15 11:33:41.515 | INFO     | __main__:train:123 - Epoch: [25][380/390]	 loss 4.10943	 cls_loss: 0.4552 cluster_loss: 0.9984 sup_con_loss: 0.8802 contrastive_loss: 4.6047 
2024-05-15 11:33:47.393 | INFO     | __main__:train:126 - Train Epoch: 25 Avg Loss: 4.0956 
2024-05-15 11:33:47.394 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:34:13.766 | INFO     | __main__:train:135 - Train Accuracies: All 0.8014 | Old 0.8289 | New 0.6915
2024-05-15 11:34:17.521 | INFO     | __main__:train:123 - Epoch: [26][0/390]	 loss 4.03094	 cls_loss: 0.3223 cluster_loss: 0.9757 sup_con_loss: 0.8508 contrastive_loss: 4.5940 
2024-05-15 11:34:31.378 | INFO     | __main__:train:123 - Epoch: [26][20/390]	 loss 4.03386	 cls_loss: 0.3391 cluster_loss: 0.9792 sup_con_loss: 0.8355 contrastive_loss: 4.5943 
2024-05-15 11:34:44.395 | INFO     | __main__:train:123 - Epoch: [26][40/390]	 loss 4.23521	 cls_loss: 0.3387 cluster_loss: 1.2033 sup_con_loss: 0.9812 contrastive_loss: 4.6017 
2024-05-15 11:34:57.417 | INFO     | __main__:train:123 - Epoch: [26][60/390]	 loss 4.11411	 cls_loss: 0.3888 cluster_loss: 0.9981 sup_con_loss: 0.9577 contrastive_loss: 4.6062 
2024-05-15 11:35:10.537 | INFO     | __main__:train:123 - Epoch: [26][80/390]	 loss 4.18563	 cls_loss: 0.4263 cluster_loss: 0.9929 sup_con_loss: 1.1380 contrastive_loss: 4.6043 
2024-05-15 11:35:23.246 | INFO     | __main__:train:123 - Epoch: [26][100/390]	 loss 3.99760	 cls_loss: 0.4291 cluster_loss: 0.9759 sup_con_loss: 0.6253 contrastive_loss: 4.6064 
2024-05-15 11:35:36.602 | INFO     | __main__:train:123 - Epoch: [26][120/390]	 loss 4.19457	 cls_loss: 0.3869 cluster_loss: 1.0609 sup_con_loss: 1.0781 contrastive_loss: 4.6034 
2024-05-15 11:35:49.701 | INFO     | __main__:train:123 - Epoch: [26][140/390]	 loss 3.95469	 cls_loss: 0.3607 cluster_loss: 0.9041 sup_con_loss: 0.7312 contrastive_loss: 4.5921 
2024-05-15 11:36:02.718 | INFO     | __main__:train:123 - Epoch: [26][160/390]	 loss 3.91989	 cls_loss: 0.3191 cluster_loss: 0.8564 sup_con_loss: 0.7588 contrastive_loss: 4.5938 
2024-05-15 11:36:16.086 | INFO     | __main__:train:123 - Epoch: [26][180/390]	 loss 4.21049	 cls_loss: 0.4528 cluster_loss: 1.0188 sup_con_loss: 1.1513 contrastive_loss: 4.5951 
2024-05-15 11:36:29.199 | INFO     | __main__:train:123 - Epoch: [26][200/390]	 loss 4.08022	 cls_loss: 0.3812 cluster_loss: 0.9995 sup_con_loss: 0.8591 contrastive_loss: 4.6099 
2024-05-15 11:36:42.106 | INFO     | __main__:train:123 - Epoch: [26][220/390]	 loss 4.17258	 cls_loss: 0.4172 cluster_loss: 1.0432 sup_con_loss: 1.0290 contrastive_loss: 4.5974 
2024-05-15 11:36:55.355 | INFO     | __main__:train:123 - Epoch: [26][240/390]	 loss 4.10593	 cls_loss: 0.4039 cluster_loss: 0.9613 sup_con_loss: 0.9927 contrastive_loss: 4.6035 
2024-05-15 11:37:08.351 | INFO     | __main__:train:123 - Epoch: [26][260/390]	 loss 4.02792	 cls_loss: 0.3128 cluster_loss: 0.9148 sup_con_loss: 0.9575 contrastive_loss: 4.5980 
2024-05-15 11:37:21.401 | INFO     | __main__:train:123 - Epoch: [26][280/390]	 loss 4.17061	 cls_loss: 0.4467 cluster_loss: 1.0450 sup_con_loss: 0.9607 contrastive_loss: 4.6135 
2024-05-15 11:37:34.241 | INFO     | __main__:train:123 - Epoch: [26][300/390]	 loss 4.12472	 cls_loss: 0.4832 cluster_loss: 0.9800 sup_con_loss: 0.9418 contrastive_loss: 4.5984 
2024-05-15 11:37:47.476 | INFO     | __main__:train:123 - Epoch: [26][320/390]	 loss 4.20862	 cls_loss: 0.4953 cluster_loss: 0.9366 sup_con_loss: 1.2334 contrastive_loss: 4.6074 
2024-05-15 11:38:00.409 | INFO     | __main__:train:123 - Epoch: [26][340/390]	 loss 3.98425	 cls_loss: 0.2671 cluster_loss: 0.9086 sup_con_loss: 0.8984 contrastive_loss: 4.5935 
2024-05-15 11:38:13.747 | INFO     | __main__:train:123 - Epoch: [26][360/390]	 loss 4.20775	 cls_loss: 0.3734 cluster_loss: 1.0563 sup_con_loss: 1.1368 contrastive_loss: 4.6040 
2024-05-15 11:38:26.347 | INFO     | __main__:train:123 - Epoch: [26][380/390]	 loss 3.97755	 cls_loss: 0.3569 cluster_loss: 0.8187 sup_con_loss: 0.9667 contrastive_loss: 4.5879 
2024-05-15 11:38:31.997 | INFO     | __main__:train:126 - Train Epoch: 26 Avg Loss: 4.0717 
2024-05-15 11:38:31.997 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:38:58.998 | INFO     | __main__:train:135 - Train Accuracies: All 0.8051 | Old 0.8317 | New 0.6985
2024-05-15 11:39:03.173 | INFO     | __main__:train:123 - Epoch: [27][0/390]	 loss 4.01370	 cls_loss: 0.2767 cluster_loss: 0.9129 sup_con_loss: 0.9591 contrastive_loss: 4.5965 
2024-05-15 11:39:16.823 | INFO     | __main__:train:123 - Epoch: [27][20/390]	 loss 4.15407	 cls_loss: 0.3374 cluster_loss: 1.0385 sup_con_loss: 1.0380 contrastive_loss: 4.6117 
2024-05-15 11:39:29.950 | INFO     | __main__:train:123 - Epoch: [27][40/390]	 loss 4.08752	 cls_loss: 0.3147 cluster_loss: 0.9902 sup_con_loss: 0.9756 contrastive_loss: 4.6035 
2024-05-15 11:39:42.878 | INFO     | __main__:train:123 - Epoch: [27][60/390]	 loss 4.05765	 cls_loss: 0.2888 cluster_loss: 1.0065 sup_con_loss: 0.8982 contrastive_loss: 4.5969 
2024-05-15 11:39:55.646 | INFO     | __main__:train:123 - Epoch: [27][80/390]	 loss 4.10075	 cls_loss: 0.4681 cluster_loss: 0.9722 sup_con_loss: 0.8912 contrastive_loss: 4.6048 
2024-05-15 11:40:08.630 | INFO     | __main__:train:123 - Epoch: [27][100/390]	 loss 4.13893	 cls_loss: 0.5025 cluster_loss: 0.9523 sup_con_loss: 1.0217 contrastive_loss: 4.5945 
2024-05-15 11:40:21.505 | INFO     | __main__:train:123 - Epoch: [27][120/390]	 loss 4.19288	 cls_loss: 0.4142 cluster_loss: 1.0719 sup_con_loss: 1.0277 contrastive_loss: 4.6023 
2024-05-15 11:40:34.714 | INFO     | __main__:train:123 - Epoch: [27][140/390]	 loss 3.94376	 cls_loss: 0.2973 cluster_loss: 0.9016 sup_con_loss: 0.7644 contrastive_loss: 4.5941 
2024-05-15 11:40:47.836 | INFO     | __main__:train:123 - Epoch: [27][160/390]	 loss 4.12621	 cls_loss: 0.3570 cluster_loss: 1.0466 sup_con_loss: 0.9376 contrastive_loss: 4.6043 
2024-05-15 11:41:00.533 | INFO     | __main__:train:123 - Epoch: [27][180/390]	 loss 4.13838	 cls_loss: 0.3178 cluster_loss: 1.0873 sup_con_loss: 0.9219 contrastive_loss: 4.6119 
2024-05-15 11:41:13.613 | INFO     | __main__:train:123 - Epoch: [27][200/390]	 loss 4.24419	 cls_loss: 0.5366 cluster_loss: 1.0631 sup_con_loss: 1.0328 contrastive_loss: 4.6214 
2024-05-15 11:41:26.611 | INFO     | __main__:train:123 - Epoch: [27][220/390]	 loss 4.01572	 cls_loss: 0.3270 cluster_loss: 0.9410 sup_con_loss: 0.8414 contrastive_loss: 4.6078 
2024-05-15 11:41:39.578 | INFO     | __main__:train:123 - Epoch: [27][240/390]	 loss 4.07129	 cls_loss: 0.3744 cluster_loss: 0.9598 sup_con_loss: 0.9212 contrastive_loss: 4.6061 
2024-05-15 11:41:52.801 | INFO     | __main__:train:123 - Epoch: [27][260/390]	 loss 4.06281	 cls_loss: 0.4480 cluster_loss: 0.9685 sup_con_loss: 0.8058 contrastive_loss: 4.6069 
2024-05-15 11:42:05.901 | INFO     | __main__:train:123 - Epoch: [27][280/390]	 loss 4.11833	 cls_loss: 0.3174 cluster_loss: 0.9793 sup_con_loss: 1.0801 contrastive_loss: 4.6041 
2024-05-15 11:42:18.753 | INFO     | __main__:train:123 - Epoch: [27][300/390]	 loss 4.04188	 cls_loss: 0.3712 cluster_loss: 0.9833 sup_con_loss: 0.8076 contrastive_loss: 4.6003 
2024-05-15 11:42:31.733 | INFO     | __main__:train:123 - Epoch: [27][320/390]	 loss 4.16447	 cls_loss: 0.3843 cluster_loss: 0.9604 sup_con_loss: 1.2040 contrastive_loss: 4.5912 
2024-05-15 11:42:44.790 | INFO     | __main__:train:123 - Epoch: [27][340/390]	 loss 3.91113	 cls_loss: 0.3704 cluster_loss: 0.9050 sup_con_loss: 0.6039 contrastive_loss: 4.5875 
2024-05-15 11:42:57.556 | INFO     | __main__:train:123 - Epoch: [27][360/390]	 loss 4.10599	 cls_loss: 0.3333 cluster_loss: 1.0586 sup_con_loss: 0.8806 contrastive_loss: 4.6046 
2024-05-15 11:43:09.891 | INFO     | __main__:train:123 - Epoch: [27][380/390]	 loss 4.03317	 cls_loss: 0.3394 cluster_loss: 1.0352 sup_con_loss: 0.7294 contrastive_loss: 4.5941 
2024-05-15 11:43:15.675 | INFO     | __main__:train:126 - Train Epoch: 27 Avg Loss: 4.0642 
2024-05-15 11:43:15.676 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:43:42.397 | INFO     | __main__:train:135 - Train Accuracies: All 0.8046 | Old 0.8326 | New 0.6925
2024-05-15 11:43:48.058 | INFO     | __main__:train:123 - Epoch: [28][0/390]	 loss 4.07350	 cls_loss: 0.3227 cluster_loss: 0.9785 sup_con_loss: 0.9697 contrastive_loss: 4.5925 
2024-05-15 11:44:00.995 | INFO     | __main__:train:123 - Epoch: [28][20/390]	 loss 4.06026	 cls_loss: 0.3354 cluster_loss: 0.9235 sup_con_loss: 1.0103 contrastive_loss: 4.5984 
2024-05-15 11:44:13.758 | INFO     | __main__:train:123 - Epoch: [28][40/390]	 loss 3.97603	 cls_loss: 0.4333 cluster_loss: 0.9108 sup_con_loss: 0.7088 contrastive_loss: 4.5911 
2024-05-15 11:44:26.808 | INFO     | __main__:train:123 - Epoch: [28][60/390]	 loss 3.98510	 cls_loss: 0.2832 cluster_loss: 0.9714 sup_con_loss: 0.7611 contrastive_loss: 4.5972 
2024-05-15 11:44:39.669 | INFO     | __main__:train:123 - Epoch: [28][80/390]	 loss 4.14069	 cls_loss: 0.3364 cluster_loss: 1.0875 sup_con_loss: 0.9155 contrastive_loss: 4.6087 
2024-05-15 11:44:52.647 | INFO     | __main__:train:123 - Epoch: [28][100/390]	 loss 4.07708	 cls_loss: 0.4693 cluster_loss: 0.9245 sup_con_loss: 0.9094 contrastive_loss: 4.6056 
2024-05-15 11:45:05.567 | INFO     | __main__:train:123 - Epoch: [28][120/390]	 loss 4.08789	 cls_loss: 0.4233 cluster_loss: 1.0389 sup_con_loss: 0.7698 contrastive_loss: 4.6078 
2024-05-15 11:45:18.855 | INFO     | __main__:train:123 - Epoch: [28][140/390]	 loss 3.95758	 cls_loss: 0.4062 cluster_loss: 0.8652 sup_con_loss: 0.7632 contrastive_loss: 4.5938 
2024-05-15 11:45:31.756 | INFO     | __main__:train:123 - Epoch: [28][160/390]	 loss 4.12858	 cls_loss: 0.3298 cluster_loss: 1.0991 sup_con_loss: 0.8745 contrastive_loss: 4.6041 
2024-05-15 11:45:44.940 | INFO     | __main__:train:123 - Epoch: [28][180/390]	 loss 4.04013	 cls_loss: 0.2914 cluster_loss: 0.9988 sup_con_loss: 0.8504 contrastive_loss: 4.6019 
2024-05-15 11:45:58.011 | INFO     | __main__:train:123 - Epoch: [28][200/390]	 loss 4.15677	 cls_loss: 0.3122 cluster_loss: 1.0528 sup_con_loss: 1.0548 contrastive_loss: 4.6062 
2024-05-15 11:46:10.870 | INFO     | __main__:train:123 - Epoch: [28][220/390]	 loss 3.97153	 cls_loss: 0.3037 cluster_loss: 0.9908 sup_con_loss: 0.6665 contrastive_loss: 4.5968 
2024-05-15 11:46:24.166 | INFO     | __main__:train:123 - Epoch: [28][240/390]	 loss 3.99997	 cls_loss: 0.3145 cluster_loss: 0.9302 sup_con_loss: 0.8482 contrastive_loss: 4.5976 
2024-05-15 11:46:37.101 | INFO     | __main__:train:123 - Epoch: [28][260/390]	 loss 3.95560	 cls_loss: 0.3333 cluster_loss: 0.9709 sup_con_loss: 0.6253 contrastive_loss: 4.5985 
2024-05-15 11:46:50.190 | INFO     | __main__:train:123 - Epoch: [28][280/390]	 loss 4.07275	 cls_loss: 0.3492 cluster_loss: 0.9414 sup_con_loss: 0.9876 contrastive_loss: 4.6045 
2024-05-15 11:47:03.081 | INFO     | __main__:train:123 - Epoch: [28][300/390]	 loss 4.09798	 cls_loss: 0.3892 cluster_loss: 0.9953 sup_con_loss: 0.9228 contrastive_loss: 4.6028 
2024-05-15 11:47:16.006 | INFO     | __main__:train:123 - Epoch: [28][320/390]	 loss 3.90493	 cls_loss: 0.3219 cluster_loss: 0.9155 sup_con_loss: 0.6040 contrastive_loss: 4.5936 
2024-05-15 11:47:29.061 | INFO     | __main__:train:123 - Epoch: [28][340/390]	 loss 4.14532	 cls_loss: 0.3680 cluster_loss: 1.0349 sup_con_loss: 1.0030 contrastive_loss: 4.6044 
2024-05-15 11:47:41.981 | INFO     | __main__:train:123 - Epoch: [28][360/390]	 loss 4.04082	 cls_loss: 0.4364 cluster_loss: 0.9647 sup_con_loss: 0.7815 contrastive_loss: 4.5962 
2024-05-15 11:47:54.556 | INFO     | __main__:train:123 - Epoch: [28][380/390]	 loss 4.03337	 cls_loss: 0.3559 cluster_loss: 1.0273 sup_con_loss: 0.7165 contrastive_loss: 4.6004 
2024-05-15 11:48:00.239 | INFO     | __main__:train:126 - Train Epoch: 28 Avg Loss: 4.0514 
2024-05-15 11:48:00.239 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:48:27.115 | INFO     | __main__:train:135 - Train Accuracies: All 0.8043 | Old 0.8311 | New 0.6970
2024-05-15 11:48:32.039 | INFO     | __main__:train:123 - Epoch: [29][0/390]	 loss 4.15530	 cls_loss: 0.2767 cluster_loss: 1.0794 sup_con_loss: 1.0591 contrastive_loss: 4.5941 
2024-05-15 11:48:45.447 | INFO     | __main__:train:123 - Epoch: [29][20/390]	 loss 4.16207	 cls_loss: 0.3160 cluster_loss: 1.0130 sup_con_loss: 1.1634 contrastive_loss: 4.5936 
2024-05-15 11:48:58.193 | INFO     | __main__:train:123 - Epoch: [29][40/390]	 loss 3.98783	 cls_loss: 0.3280 cluster_loss: 0.8817 sup_con_loss: 0.8955 contrastive_loss: 4.5946 
2024-05-15 11:49:10.791 | INFO     | __main__:train:123 - Epoch: [29][60/390]	 loss 3.91914	 cls_loss: 0.3254 cluster_loss: 0.9376 sup_con_loss: 0.5936 contrastive_loss: 4.5970 
2024-05-15 11:49:23.638 | INFO     | __main__:train:123 - Epoch: [29][80/390]	 loss 4.03356	 cls_loss: 0.2511 cluster_loss: 0.9895 sup_con_loss: 0.9107 contrastive_loss: 4.5905 
2024-05-15 11:49:36.560 | INFO     | __main__:train:123 - Epoch: [29][100/390]	 loss 3.98755	 cls_loss: 0.3416 cluster_loss: 0.9687 sup_con_loss: 0.6937 contrastive_loss: 4.6085 
2024-05-15 11:49:49.368 | INFO     | __main__:train:123 - Epoch: [29][120/390]	 loss 4.16581	 cls_loss: 0.4198 cluster_loss: 1.0400 sup_con_loss: 0.9977 contrastive_loss: 4.6057 
2024-05-15 11:50:02.411 | INFO     | __main__:train:123 - Epoch: [29][140/390]	 loss 4.12759	 cls_loss: 0.4400 cluster_loss: 1.0133 sup_con_loss: 0.9208 contrastive_loss: 4.6041 
2024-05-15 11:50:15.396 | INFO     | __main__:train:123 - Epoch: [29][160/390]	 loss 4.05593	 cls_loss: 0.4182 cluster_loss: 0.9331 sup_con_loss: 0.9035 contrastive_loss: 4.5952 
2024-05-15 11:50:28.273 | INFO     | __main__:train:123 - Epoch: [29][180/390]	 loss 3.92936	 cls_loss: 0.2520 cluster_loss: 0.9569 sup_con_loss: 0.6693 contrastive_loss: 4.5922 
2024-05-15 11:50:41.408 | INFO     | __main__:train:123 - Epoch: [29][200/390]	 loss 3.91494	 cls_loss: 0.2361 cluster_loss: 0.9464 sup_con_loss: 0.6554 contrastive_loss: 4.5966 
2024-05-15 11:50:54.679 | INFO     | __main__:train:123 - Epoch: [29][220/390]	 loss 4.11890	 cls_loss: 0.3692 cluster_loss: 0.9702 sup_con_loss: 1.0703 contrastive_loss: 4.5914 
2024-05-15 11:51:07.825 | INFO     | __main__:train:123 - Epoch: [29][240/390]	 loss 4.16306	 cls_loss: 0.4938 cluster_loss: 0.9964 sup_con_loss: 1.0131 contrastive_loss: 4.5969 
2024-05-15 11:51:20.914 | INFO     | __main__:train:123 - Epoch: [29][260/390]	 loss 4.10437	 cls_loss: 0.3111 cluster_loss: 1.0563 sup_con_loss: 0.9089 contrastive_loss: 4.6012 
2024-05-15 11:51:33.768 | INFO     | __main__:train:123 - Epoch: [29][280/390]	 loss 3.99733	 cls_loss: 0.2576 cluster_loss: 0.9959 sup_con_loss: 0.7763 contrastive_loss: 4.5971 
2024-05-15 11:51:46.767 | INFO     | __main__:train:123 - Epoch: [29][300/390]	 loss 4.05841	 cls_loss: 0.2759 cluster_loss: 0.9480 sup_con_loss: 1.0298 contrastive_loss: 4.5926 
2024-05-15 11:52:00.105 | INFO     | __main__:train:123 - Epoch: [29][320/390]	 loss 4.07599	 cls_loss: 0.3907 cluster_loss: 0.9814 sup_con_loss: 0.8931 contrastive_loss: 4.5981 
2024-05-15 11:52:12.996 | INFO     | __main__:train:123 - Epoch: [29][340/390]	 loss 4.16215	 cls_loss: 0.3538 cluster_loss: 1.0561 sup_con_loss: 1.0357 contrastive_loss: 4.5990 
2024-05-15 11:52:26.290 | INFO     | __main__:train:123 - Epoch: [29][360/390]	 loss 3.97972	 cls_loss: 0.3102 cluster_loss: 0.9484 sup_con_loss: 0.7689 contrastive_loss: 4.5933 
2024-05-15 11:52:39.062 | INFO     | __main__:train:123 - Epoch: [29][380/390]	 loss 3.97258	 cls_loss: 0.3031 cluster_loss: 0.9127 sup_con_loss: 0.8139 contrastive_loss: 4.5975 
2024-05-15 11:52:44.946 | INFO     | __main__:train:126 - Train Epoch: 29 Avg Loss: 4.0430 
2024-05-15 11:52:44.947 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:53:11.865 | INFO     | __main__:train:135 - Train Accuracies: All 0.8054 | Old 0.8327 | New 0.6960
2024-05-15 11:53:15.774 | INFO     | __main__:train:123 - Epoch: [30][0/390]	 loss 3.97830	 cls_loss: 0.3711 cluster_loss: 0.8649 sup_con_loss: 0.8724 contrastive_loss: 4.5860 
2024-05-15 11:53:29.592 | INFO     | __main__:train:123 - Epoch: [30][20/390]	 loss 4.07853	 cls_loss: 0.3480 cluster_loss: 0.9930 sup_con_loss: 0.9075 contrastive_loss: 4.6056 
2024-05-15 11:53:42.208 | INFO     | __main__:train:123 - Epoch: [30][40/390]	 loss 3.99475	 cls_loss: 0.2851 cluster_loss: 0.9201 sup_con_loss: 0.8838 contrastive_loss: 4.5962 
2024-05-15 11:53:55.058 | INFO     | __main__:train:123 - Epoch: [30][60/390]	 loss 4.13714	 cls_loss: 0.4332 cluster_loss: 1.0015 sup_con_loss: 0.9799 contrastive_loss: 4.6025 
2024-05-15 11:54:08.133 | INFO     | __main__:train:123 - Epoch: [30][80/390]	 loss 4.00796	 cls_loss: 0.3484 cluster_loss: 0.9052 sup_con_loss: 0.8940 contrastive_loss: 4.5919 
2024-05-15 11:54:21.081 | INFO     | __main__:train:123 - Epoch: [30][100/390]	 loss 3.97633	 cls_loss: 0.3172 cluster_loss: 0.8885 sup_con_loss: 0.8569 contrastive_loss: 4.5967 
2024-05-15 11:54:33.880 | INFO     | __main__:train:123 - Epoch: [30][120/390]	 loss 4.06848	 cls_loss: 0.3309 cluster_loss: 1.0509 sup_con_loss: 0.8027 contrastive_loss: 4.5979 
2024-05-15 11:54:47.270 | INFO     | __main__:train:123 - Epoch: [30][140/390]	 loss 4.08248	 cls_loss: 0.3326 cluster_loss: 0.9792 sup_con_loss: 0.9607 contrastive_loss: 4.6051 
2024-05-15 11:55:00.411 | INFO     | __main__:train:123 - Epoch: [30][160/390]	 loss 4.05401	 cls_loss: 0.4609 cluster_loss: 0.9705 sup_con_loss: 0.7638 contrastive_loss: 4.6070 
2024-05-15 11:55:13.109 | INFO     | __main__:train:123 - Epoch: [30][180/390]	 loss 4.08443	 cls_loss: 0.4529 cluster_loss: 0.8870 sup_con_loss: 1.0390 contrastive_loss: 4.5934 
2024-05-15 11:55:26.244 | INFO     | __main__:train:123 - Epoch: [30][200/390]	 loss 4.05087	 cls_loss: 0.3654 cluster_loss: 0.9360 sup_con_loss: 0.9298 contrastive_loss: 4.5987 
2024-05-15 11:55:39.084 | INFO     | __main__:train:123 - Epoch: [30][220/390]	 loss 4.00849	 cls_loss: 0.3447 cluster_loss: 0.9125 sup_con_loss: 0.8762 contrastive_loss: 4.5970 
2024-05-15 11:55:52.123 | INFO     | __main__:train:123 - Epoch: [30][240/390]	 loss 3.82101	 cls_loss: 0.2596 cluster_loss: 0.8128 sup_con_loss: 0.6209 contrastive_loss: 4.5916 
2024-05-15 11:56:05.352 | INFO     | __main__:train:123 - Epoch: [30][260/390]	 loss 4.03950	 cls_loss: 0.2813 cluster_loss: 0.9907 sup_con_loss: 0.8797 contrastive_loss: 4.5988 
2024-05-15 11:56:18.392 | INFO     | __main__:train:123 - Epoch: [30][280/390]	 loss 4.04157	 cls_loss: 0.2645 cluster_loss: 1.0313 sup_con_loss: 0.8181 contrastive_loss: 4.6036 
2024-05-15 11:56:31.374 | INFO     | __main__:train:123 - Epoch: [30][300/390]	 loss 3.92184	 cls_loss: 0.2880 cluster_loss: 0.9230 sup_con_loss: 0.6629 contrastive_loss: 4.5986 
2024-05-15 11:56:44.376 | INFO     | __main__:train:123 - Epoch: [30][320/390]	 loss 4.11266	 cls_loss: 0.3112 cluster_loss: 1.0339 sup_con_loss: 0.9797 contrastive_loss: 4.5981 
2024-05-15 11:56:57.246 | INFO     | __main__:train:123 - Epoch: [30][340/390]	 loss 4.05732	 cls_loss: 0.4182 cluster_loss: 0.9761 sup_con_loss: 0.8160 contrastive_loss: 4.6013 
2024-05-15 11:57:10.014 | INFO     | __main__:train:123 - Epoch: [30][360/390]	 loss 4.05669	 cls_loss: 0.2870 cluster_loss: 1.0127 sup_con_loss: 0.8830 contrastive_loss: 4.5983 
2024-05-15 11:57:22.445 | INFO     | __main__:train:123 - Epoch: [30][380/390]	 loss 4.02481	 cls_loss: 0.3765 cluster_loss: 0.9585 sup_con_loss: 0.8073 contrastive_loss: 4.5961 
2024-05-15 11:57:28.181 | INFO     | __main__:train:126 - Train Epoch: 30 Avg Loss: 4.0255 
2024-05-15 11:57:28.182 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:57:55.216 | INFO     | __main__:train:135 - Train Accuracies: All 0.8086 | Old 0.8331 | New 0.7105
2024-05-15 11:58:00.725 | INFO     | __main__:train:123 - Epoch: [31][0/390]	 loss 4.05030	 cls_loss: 0.3305 cluster_loss: 0.9448 sup_con_loss: 0.9658 contrastive_loss: 4.5885 
2024-05-15 11:58:13.526 | INFO     | __main__:train:123 - Epoch: [31][20/390]	 loss 3.95307	 cls_loss: 0.3054 cluster_loss: 0.9546 sup_con_loss: 0.6955 contrastive_loss: 4.5881 
2024-05-15 11:58:26.650 | INFO     | __main__:train:123 - Epoch: [31][40/390]	 loss 4.23437	 cls_loss: 0.5062 cluster_loss: 1.1218 sup_con_loss: 0.9361 contrastive_loss: 4.6160 
2024-05-15 11:58:39.439 | INFO     | __main__:train:123 - Epoch: [31][60/390]	 loss 4.02642	 cls_loss: 0.4626 cluster_loss: 0.9788 sup_con_loss: 0.6778 contrastive_loss: 4.6016 
2024-05-15 11:58:52.637 | INFO     | __main__:train:123 - Epoch: [31][80/390]	 loss 3.95787	 cls_loss: 0.3273 cluster_loss: 0.9079 sup_con_loss: 0.7709 contrastive_loss: 4.5898 
2024-05-15 11:59:05.414 | INFO     | __main__:train:123 - Epoch: [31][100/390]	 loss 4.08968	 cls_loss: 0.3103 cluster_loss: 1.0292 sup_con_loss: 0.9289 contrastive_loss: 4.5953 
2024-05-15 11:59:18.287 | INFO     | __main__:train:123 - Epoch: [31][120/390]	 loss 4.16425	 cls_loss: 0.3877 cluster_loss: 1.0174 sup_con_loss: 1.0926 contrastive_loss: 4.5920 
2024-05-15 11:59:31.229 | INFO     | __main__:train:123 - Epoch: [31][140/390]	 loss 4.15797	 cls_loss: 0.3270 cluster_loss: 1.0630 sup_con_loss: 1.0281 contrastive_loss: 4.6042 
2024-05-15 11:59:44.079 | INFO     | __main__:train:123 - Epoch: [31][160/390]	 loss 3.98520	 cls_loss: 0.2854 cluster_loss: 0.9567 sup_con_loss: 0.7763 contrastive_loss: 4.6027 
2024-05-15 11:59:56.916 | INFO     | __main__:train:123 - Epoch: [31][180/390]	 loss 4.02669	 cls_loss: 0.2806 cluster_loss: 0.9832 sup_con_loss: 0.8651 contrastive_loss: 4.5948 
2024-05-15 12:00:09.818 | INFO     | __main__:train:123 - Epoch: [31][200/390]	 loss 4.21460	 cls_loss: 0.3548 cluster_loss: 1.0123 sup_con_loss: 1.2746 contrastive_loss: 4.5944 
2024-05-15 12:00:22.970 | INFO     | __main__:train:123 - Epoch: [31][220/390]	 loss 4.02657	 cls_loss: 0.2743 cluster_loss: 0.9860 sup_con_loss: 0.8541 contrastive_loss: 4.6011 
2024-05-15 12:00:36.075 | INFO     | __main__:train:123 - Epoch: [31][240/390]	 loss 4.02525	 cls_loss: 0.3129 cluster_loss: 0.9702 sup_con_loss: 0.8452 contrastive_loss: 4.5989 
2024-05-15 12:00:49.036 | INFO     | __main__:train:123 - Epoch: [31][260/390]	 loss 4.03768	 cls_loss: 0.4193 cluster_loss: 0.9063 sup_con_loss: 0.8947 contrastive_loss: 4.5980 
2024-05-15 12:01:01.919 | INFO     | __main__:train:123 - Epoch: [31][280/390]	 loss 4.21502	 cls_loss: 0.4693 cluster_loss: 1.0562 sup_con_loss: 1.0637 contrastive_loss: 4.6030 
2024-05-15 12:01:15.115 | INFO     | __main__:train:123 - Epoch: [31][300/390]	 loss 3.92241	 cls_loss: 0.3097 cluster_loss: 0.8771 sup_con_loss: 0.7447 contrastive_loss: 4.5897 
2024-05-15 12:01:28.128 | INFO     | __main__:train:123 - Epoch: [31][320/390]	 loss 3.99809	 cls_loss: 0.3014 cluster_loss: 0.9494 sup_con_loss: 0.8229 contrastive_loss: 4.5961 
2024-05-15 12:01:41.312 | INFO     | __main__:train:123 - Epoch: [31][340/390]	 loss 4.06768	 cls_loss: 0.2999 cluster_loss: 1.0570 sup_con_loss: 0.8114 contrastive_loss: 4.6026 
2024-05-15 12:01:54.360 | INFO     | __main__:train:123 - Epoch: [31][360/390]	 loss 4.02950	 cls_loss: 0.4272 cluster_loss: 0.9751 sup_con_loss: 0.7341 contrastive_loss: 4.5989 
2024-05-15 12:02:06.835 | INFO     | __main__:train:123 - Epoch: [31][380/390]	 loss 4.10498	 cls_loss: 0.3218 cluster_loss: 1.0177 sup_con_loss: 0.9804 contrastive_loss: 4.5965 
2024-05-15 12:02:12.629 | INFO     | __main__:train:126 - Train Epoch: 31 Avg Loss: 4.0243 
2024-05-15 12:02:12.630 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:02:39.353 | INFO     | __main__:train:135 - Train Accuracies: All 0.8079 | Old 0.8365 | New 0.6935
2024-05-15 12:02:44.910 | INFO     | __main__:train:123 - Epoch: [32][0/390]	 loss 3.92273	 cls_loss: 0.3107 cluster_loss: 0.8964 sup_con_loss: 0.7040 contrastive_loss: 4.5922 
2024-05-15 12:02:58.085 | INFO     | __main__:train:123 - Epoch: [32][20/390]	 loss 3.96421	 cls_loss: 0.2750 cluster_loss: 0.9189 sup_con_loss: 0.8193 contrastive_loss: 4.5906 
2024-05-15 12:03:10.745 | INFO     | __main__:train:123 - Epoch: [32][40/390]	 loss 4.07787	 cls_loss: 0.3441 cluster_loss: 0.9987 sup_con_loss: 0.8920 contrastive_loss: 4.6093 
2024-05-15 12:03:23.862 | INFO     | __main__:train:123 - Epoch: [32][60/390]	 loss 4.01331	 cls_loss: 0.3679 cluster_loss: 0.9179 sup_con_loss: 0.8551 contrastive_loss: 4.5979 
2024-05-15 12:03:36.902 | INFO     | __main__:train:123 - Epoch: [32][80/390]	 loss 4.15049	 cls_loss: 0.2977 cluster_loss: 1.1106 sup_con_loss: 0.9452 contrastive_loss: 4.6055 
2024-05-15 12:03:49.654 | INFO     | __main__:train:123 - Epoch: [32][100/390]	 loss 4.10544	 cls_loss: 0.3517 cluster_loss: 1.0392 sup_con_loss: 0.8964 contrastive_loss: 4.6048 
2024-05-15 12:04:02.765 | INFO     | __main__:train:123 - Epoch: [32][120/390]	 loss 4.00637	 cls_loss: 0.3174 cluster_loss: 0.9284 sup_con_loss: 0.8714 contrastive_loss: 4.5951 
2024-05-15 12:04:15.910 | INFO     | __main__:train:123 - Epoch: [32][140/390]	 loss 4.03527	 cls_loss: 0.3015 cluster_loss: 0.9643 sup_con_loss: 0.8990 contrastive_loss: 4.5974 
2024-05-15 12:04:28.840 | INFO     | __main__:train:123 - Epoch: [32][160/390]	 loss 4.09116	 cls_loss: 0.3783 cluster_loss: 0.9950 sup_con_loss: 0.9235 contrastive_loss: 4.5981 
2024-05-15 12:04:41.789 | INFO     | __main__:train:123 - Epoch: [32][180/390]	 loss 4.01550	 cls_loss: 0.2696 cluster_loss: 0.9639 sup_con_loss: 0.8854 contrastive_loss: 4.5919 
2024-05-15 12:04:54.863 | INFO     | __main__:train:123 - Epoch: [32][200/390]	 loss 4.03896	 cls_loss: 0.3024 cluster_loss: 0.9722 sup_con_loss: 0.9010 contrastive_loss: 4.5936 
2024-05-15 12:05:07.902 | INFO     | __main__:train:123 - Epoch: [32][220/390]	 loss 3.97155	 cls_loss: 0.3752 cluster_loss: 0.8568 sup_con_loss: 0.8470 contrastive_loss: 4.5952 
2024-05-15 12:05:20.643 | INFO     | __main__:train:123 - Epoch: [32][240/390]	 loss 4.10786	 cls_loss: 0.3430 cluster_loss: 1.0768 sup_con_loss: 0.8383 contrastive_loss: 4.6069 
2024-05-15 12:05:33.660 | INFO     | __main__:train:123 - Epoch: [32][260/390]	 loss 4.02173	 cls_loss: 0.3320 cluster_loss: 1.0046 sup_con_loss: 0.7524 contrastive_loss: 4.5987 
2024-05-15 12:05:46.741 | INFO     | __main__:train:123 - Epoch: [32][280/390]	 loss 4.07466	 cls_loss: 0.4598 cluster_loss: 0.9035 sup_con_loss: 0.9677 contrastive_loss: 4.5966 
2024-05-15 12:05:59.810 | INFO     | __main__:train:123 - Epoch: [32][300/390]	 loss 4.02787	 cls_loss: 0.3855 cluster_loss: 0.9956 sup_con_loss: 0.7308 contrastive_loss: 4.6000 
2024-05-15 12:06:12.961 | INFO     | __main__:train:123 - Epoch: [32][320/390]	 loss 4.10403	 cls_loss: 0.4419 cluster_loss: 0.9351 sup_con_loss: 1.0117 contrastive_loss: 4.5961 
2024-05-15 12:06:26.039 | INFO     | __main__:train:123 - Epoch: [32][340/390]	 loss 4.09142	 cls_loss: 0.3658 cluster_loss: 1.0006 sup_con_loss: 0.9398 contrastive_loss: 4.5909 
2024-05-15 12:06:38.859 | INFO     | __main__:train:123 - Epoch: [32][360/390]	 loss 4.00268	 cls_loss: 0.3383 cluster_loss: 0.9277 sup_con_loss: 0.8376 contrastive_loss: 4.5971 
2024-05-15 12:06:51.362 | INFO     | __main__:train:123 - Epoch: [32][380/390]	 loss 3.89266	 cls_loss: 0.3132 cluster_loss: 0.8856 sup_con_loss: 0.6454 contrastive_loss: 4.5870 
2024-05-15 12:06:57.232 | INFO     | __main__:train:126 - Train Epoch: 32 Avg Loss: 4.0177 
2024-05-15 12:06:57.233 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:07:23.540 | INFO     | __main__:train:135 - Train Accuracies: All 0.8097 | Old 0.8349 | New 0.7090
2024-05-15 12:07:27.741 | INFO     | __main__:train:123 - Epoch: [33][0/390]	 loss 4.02649	 cls_loss: 0.3949 cluster_loss: 0.9564 sup_con_loss: 0.8143 contrastive_loss: 4.5871 
2024-05-15 12:07:41.664 | INFO     | __main__:train:123 - Epoch: [33][20/390]	 loss 3.91228	 cls_loss: 0.2443 cluster_loss: 0.9368 sup_con_loss: 0.6696 contrastive_loss: 4.5900 
2024-05-15 12:07:54.385 | INFO     | __main__:train:123 - Epoch: [33][40/390]	 loss 3.99106	 cls_loss: 0.2505 cluster_loss: 0.9864 sup_con_loss: 0.7865 contrastive_loss: 4.5953 
2024-05-15 12:08:06.967 | INFO     | __main__:train:123 - Epoch: [33][60/390]	 loss 4.09069	 cls_loss: 0.3784 cluster_loss: 0.9761 sup_con_loss: 0.9455 contrastive_loss: 4.6044 
2024-05-15 12:08:20.150 | INFO     | __main__:train:123 - Epoch: [33][80/390]	 loss 4.04224	 cls_loss: 0.3555 cluster_loss: 0.9084 sup_con_loss: 0.9790 contrastive_loss: 4.5918 
2024-05-15 12:08:33.284 | INFO     | __main__:train:123 - Epoch: [33][100/390]	 loss 3.87180	 cls_loss: 0.3183 cluster_loss: 0.9210 sup_con_loss: 0.4976 contrastive_loss: 4.5963 
2024-05-15 12:08:46.564 | INFO     | __main__:train:123 - Epoch: [33][120/390]	 loss 3.99491	 cls_loss: 0.3840 cluster_loss: 0.8795 sup_con_loss: 0.8791 contrastive_loss: 4.5864 
2024-05-15 12:08:59.341 | INFO     | __main__:train:123 - Epoch: [33][140/390]	 loss 3.92548	 cls_loss: 0.3402 cluster_loss: 0.9416 sup_con_loss: 0.6070 contrastive_loss: 4.5876 
2024-05-15 12:09:12.315 | INFO     | __main__:train:123 - Epoch: [33][160/390]	 loss 3.95876	 cls_loss: 0.3726 cluster_loss: 0.8898 sup_con_loss: 0.7653 contrastive_loss: 4.5879 
2024-05-15 12:09:25.190 | INFO     | __main__:train:123 - Epoch: [33][180/390]	 loss 3.93971	 cls_loss: 0.3301 cluster_loss: 0.8483 sup_con_loss: 0.8353 contrastive_loss: 4.5853 
2024-05-15 12:09:37.861 | INFO     | __main__:train:123 - Epoch: [33][200/390]	 loss 3.98008	 cls_loss: 0.3087 cluster_loss: 0.9010 sup_con_loss: 0.8599 contrastive_loss: 4.5930 
2024-05-15 12:09:51.074 | INFO     | __main__:train:123 - Epoch: [33][220/390]	 loss 4.06069	 cls_loss: 0.3036 cluster_loss: 0.9668 sup_con_loss: 0.9478 contrastive_loss: 4.6066 
2024-05-15 12:10:03.909 | INFO     | __main__:train:123 - Epoch: [33][240/390]	 loss 3.95152	 cls_loss: 0.2513 cluster_loss: 0.9059 sup_con_loss: 0.8439 contrastive_loss: 4.5836 
2024-05-15 12:10:16.804 | INFO     | __main__:train:123 - Epoch: [33][260/390]	 loss 4.03392	 cls_loss: 0.3320 cluster_loss: 1.0712 sup_con_loss: 0.6240 contrastive_loss: 4.6200 
2024-05-15 12:10:29.921 | INFO     | __main__:train:123 - Epoch: [33][280/390]	 loss 4.21808	 cls_loss: 0.3269 cluster_loss: 1.0831 sup_con_loss: 1.1472 contrastive_loss: 4.6125 
2024-05-15 12:10:42.986 | INFO     | __main__:train:123 - Epoch: [33][300/390]	 loss 4.08567	 cls_loss: 0.2738 cluster_loss: 1.0201 sup_con_loss: 0.9720 contrastive_loss: 4.5948 
2024-05-15 12:10:55.987 | INFO     | __main__:train:123 - Epoch: [33][320/390]	 loss 4.07635	 cls_loss: 0.3417 cluster_loss: 0.9536 sup_con_loss: 0.9969 contrastive_loss: 4.5969 
2024-05-15 12:11:08.927 | INFO     | __main__:train:123 - Epoch: [33][340/390]	 loss 3.95827	 cls_loss: 0.3109 cluster_loss: 0.9119 sup_con_loss: 0.7837 contrastive_loss: 4.5883 
2024-05-15 12:11:22.091 | INFO     | __main__:train:123 - Epoch: [33][360/390]	 loss 3.93959	 cls_loss: 0.2962 cluster_loss: 0.8508 sup_con_loss: 0.8438 contrastive_loss: 4.5963 
2024-05-15 12:11:34.667 | INFO     | __main__:train:123 - Epoch: [33][380/390]	 loss 4.12711	 cls_loss: 0.3493 cluster_loss: 1.0455 sup_con_loss: 0.9610 contrastive_loss: 4.5983 
2024-05-15 12:11:40.366 | INFO     | __main__:train:126 - Train Epoch: 33 Avg Loss: 4.0012 
2024-05-15 12:11:40.367 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:12:07.609 | INFO     | __main__:train:135 - Train Accuracies: All 0.8064 | Old 0.8356 | New 0.6895
2024-05-15 12:12:13.083 | INFO     | __main__:train:123 - Epoch: [34][0/390]	 loss 3.84077	 cls_loss: 0.2917 cluster_loss: 0.8081 sup_con_loss: 0.6586 contrastive_loss: 4.5891 
2024-05-15 12:12:25.885 | INFO     | __main__:train:123 - Epoch: [34][20/390]	 loss 4.18267	 cls_loss: 0.4037 cluster_loss: 1.0288 sup_con_loss: 1.0939 contrastive_loss: 4.5997 
2024-05-15 12:12:38.837 | INFO     | __main__:train:123 - Epoch: [34][40/390]	 loss 3.98726	 cls_loss: 0.4152 cluster_loss: 0.8307 sup_con_loss: 0.9118 contrastive_loss: 4.5889 
2024-05-15 12:12:51.776 | INFO     | __main__:train:123 - Epoch: [34][60/390]	 loss 4.02538	 cls_loss: 0.4287 cluster_loss: 0.9423 sup_con_loss: 0.8024 contrastive_loss: 4.5877 
2024-05-15 12:13:04.861 | INFO     | __main__:train:123 - Epoch: [34][80/390]	 loss 3.98277	 cls_loss: 0.3222 cluster_loss: 0.9040 sup_con_loss: 0.8452 contrastive_loss: 4.5948 
2024-05-15 12:13:17.916 | INFO     | __main__:train:123 - Epoch: [34][100/390]	 loss 4.02223	 cls_loss: 0.2624 cluster_loss: 0.9693 sup_con_loss: 0.8862 contrastive_loss: 4.6003 
2024-05-15 12:13:30.848 | INFO     | __main__:train:123 - Epoch: [34][120/390]	 loss 3.93069	 cls_loss: 0.3020 cluster_loss: 0.9343 sup_con_loss: 0.6627 contrastive_loss: 4.5935 
2024-05-15 12:13:43.772 | INFO     | __main__:train:123 - Epoch: [34][140/390]	 loss 4.11530	 cls_loss: 0.3948 cluster_loss: 1.0399 sup_con_loss: 0.8583 contrastive_loss: 4.6166 
2024-05-15 12:13:57.011 | INFO     | __main__:train:123 - Epoch: [34][160/390]	 loss 4.02468	 cls_loss: 0.3389 cluster_loss: 0.9282 sup_con_loss: 0.8930 contrastive_loss: 4.6003 
2024-05-15 12:14:10.153 | INFO     | __main__:train:123 - Epoch: [34][180/390]	 loss 3.95999	 cls_loss: 0.3816 cluster_loss: 0.8616 sup_con_loss: 0.8047 contrastive_loss: 4.5919 
2024-05-15 12:14:23.355 | INFO     | __main__:train:123 - Epoch: [34][200/390]	 loss 3.99726	 cls_loss: 0.2850 cluster_loss: 0.9420 sup_con_loss: 0.8664 contrastive_loss: 4.5877 
2024-05-15 12:14:36.314 | INFO     | __main__:train:123 - Epoch: [34][220/390]	 loss 3.98194	 cls_loss: 0.3643 cluster_loss: 0.8779 sup_con_loss: 0.8489 contrastive_loss: 4.5948 
2024-05-15 12:14:49.294 | INFO     | __main__:train:123 - Epoch: [34][240/390]	 loss 3.95295	 cls_loss: 0.2329 cluster_loss: 0.8424 sup_con_loss: 0.9771 contrastive_loss: 4.5875 
2024-05-15 12:15:02.223 | INFO     | __main__:train:123 - Epoch: [34][260/390]	 loss 4.04584	 cls_loss: 0.3136 cluster_loss: 0.9384 sup_con_loss: 0.9839 contrastive_loss: 4.5873 
2024-05-15 12:15:15.202 | INFO     | __main__:train:123 - Epoch: [34][280/390]	 loss 4.01430	 cls_loss: 0.2610 cluster_loss: 0.9408 sup_con_loss: 0.9337 contrastive_loss: 4.5917 
2024-05-15 12:15:27.993 | INFO     | __main__:train:123 - Epoch: [34][300/390]	 loss 4.06899	 cls_loss: 0.3353 cluster_loss: 0.9348 sup_con_loss: 1.0269 contrastive_loss: 4.5917 
2024-05-15 12:15:41.393 | INFO     | __main__:train:123 - Epoch: [34][320/390]	 loss 3.92705	 cls_loss: 0.2765 cluster_loss: 0.9208 sup_con_loss: 0.6984 contrastive_loss: 4.5958 
2024-05-15 12:15:54.532 | INFO     | __main__:train:123 - Epoch: [34][340/390]	 loss 3.99245	 cls_loss: 0.2353 cluster_loss: 1.0221 sup_con_loss: 0.7329 contrastive_loss: 4.5987 
2024-05-15 12:16:07.905 | INFO     | __main__:train:123 - Epoch: [34][360/390]	 loss 3.91854	 cls_loss: 0.2971 cluster_loss: 0.9491 sup_con_loss: 0.6066 contrastive_loss: 4.5928 
2024-05-15 12:16:20.702 | INFO     | __main__:train:123 - Epoch: [34][380/390]	 loss 3.93363	 cls_loss: 0.3389 cluster_loss: 0.9107 sup_con_loss: 0.6626 contrastive_loss: 4.6018 
2024-05-15 12:16:26.597 | INFO     | __main__:train:126 - Train Epoch: 34 Avg Loss: 4.0043 
2024-05-15 12:16:26.598 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:16:53.298 | INFO     | __main__:train:135 - Train Accuracies: All 0.8105 | Old 0.8333 | New 0.7195
2024-05-15 12:16:57.242 | INFO     | __main__:train:123 - Epoch: [35][0/390]	 loss 3.93485	 cls_loss: 0.3017 cluster_loss: 0.8479 sup_con_loss: 0.8282 contrastive_loss: 4.5973 
2024-05-15 12:17:11.138 | INFO     | __main__:train:123 - Epoch: [35][20/390]	 loss 4.00999	 cls_loss: 0.3212 cluster_loss: 0.9802 sup_con_loss: 0.7655 contrastive_loss: 4.6039 
2024-05-15 12:17:24.102 | INFO     | __main__:train:123 - Epoch: [35][40/390]	 loss 3.81167	 cls_loss: 0.2730 cluster_loss: 0.7788 sup_con_loss: 0.6538 contrastive_loss: 4.5863 
2024-05-15 12:17:37.306 | INFO     | __main__:train:123 - Epoch: [35][60/390]	 loss 4.07304	 cls_loss: 0.3801 cluster_loss: 1.0332 sup_con_loss: 0.7927 contrastive_loss: 4.6015 
2024-05-15 12:17:50.276 | INFO     | __main__:train:123 - Epoch: [35][80/390]	 loss 4.00401	 cls_loss: 0.2832 cluster_loss: 1.0377 sup_con_loss: 0.6797 contrastive_loss: 4.6038 
2024-05-15 12:18:03.123 | INFO     | __main__:train:123 - Epoch: [35][100/390]	 loss 4.00144	 cls_loss: 0.2755 cluster_loss: 1.0457 sup_con_loss: 0.6685 contrastive_loss: 4.6020 
2024-05-15 12:18:15.984 | INFO     | __main__:train:123 - Epoch: [35][120/390]	 loss 3.88329	 cls_loss: 0.2861 cluster_loss: 0.8375 sup_con_loss: 0.7354 contrastive_loss: 4.5867 
2024-05-15 12:18:29.028 | INFO     | __main__:train:123 - Epoch: [35][140/390]	 loss 3.91858	 cls_loss: 0.2792 cluster_loss: 0.9490 sup_con_loss: 0.6109 contrastive_loss: 4.6003 
2024-05-15 12:18:41.963 | INFO     | __main__:train:123 - Epoch: [35][160/390]	 loss 3.96139	 cls_loss: 0.3256 cluster_loss: 0.9211 sup_con_loss: 0.7447 contrastive_loss: 4.5970 
2024-05-15 12:18:54.882 | INFO     | __main__:train:123 - Epoch: [35][180/390]	 loss 3.86255	 cls_loss: 0.2816 cluster_loss: 0.8570 sup_con_loss: 0.6466 contrastive_loss: 4.5856 
2024-05-15 12:19:08.049 | INFO     | __main__:train:123 - Epoch: [35][200/390]	 loss 3.99377	 cls_loss: 0.3478 cluster_loss: 0.9252 sup_con_loss: 0.8090 contrastive_loss: 4.5962 
2024-05-15 12:19:21.195 | INFO     | __main__:train:123 - Epoch: [35][220/390]	 loss 4.02396	 cls_loss: 0.3924 cluster_loss: 0.8747 sup_con_loss: 0.9542 contrastive_loss: 4.5909 
2024-05-15 12:19:34.183 | INFO     | __main__:train:123 - Epoch: [35][240/390]	 loss 4.03082	 cls_loss: 0.3920 cluster_loss: 0.9037 sup_con_loss: 0.9220 contrastive_loss: 4.5900 
2024-05-15 12:19:47.193 | INFO     | __main__:train:123 - Epoch: [35][260/390]	 loss 3.98837	 cls_loss: 0.3159 cluster_loss: 0.9090 sup_con_loss: 0.8674 contrastive_loss: 4.5898 
2024-05-15 12:20:00.422 | INFO     | __main__:train:123 - Epoch: [35][280/390]	 loss 4.00884	 cls_loss: 0.3452 cluster_loss: 0.8926 sup_con_loss: 0.9203 contrastive_loss: 4.5934 
2024-05-15 12:20:13.597 | INFO     | __main__:train:123 - Epoch: [35][300/390]	 loss 3.94722	 cls_loss: 0.2874 cluster_loss: 0.9770 sup_con_loss: 0.6298 contrastive_loss: 4.6018 
2024-05-15 12:20:26.684 | INFO     | __main__:train:123 - Epoch: [35][320/390]	 loss 4.04457	 cls_loss: 0.4304 cluster_loss: 0.9372 sup_con_loss: 0.8351 contrastive_loss: 4.6038 
2024-05-15 12:20:39.605 | INFO     | __main__:train:123 - Epoch: [35][340/390]	 loss 4.05864	 cls_loss: 0.2544 cluster_loss: 1.0081 sup_con_loss: 0.9342 contrastive_loss: 4.5960 
2024-05-15 12:20:52.813 | INFO     | __main__:train:123 - Epoch: [35][360/390]	 loss 4.00902	 cls_loss: 0.3084 cluster_loss: 0.9432 sup_con_loss: 0.8590 contrastive_loss: 4.5959 
2024-05-15 12:21:05.590 | INFO     | __main__:train:123 - Epoch: [35][380/390]	 loss 4.00677	 cls_loss: 0.3105 cluster_loss: 0.9018 sup_con_loss: 0.9540 contrastive_loss: 4.5815 
2024-05-15 12:21:11.391 | INFO     | __main__:train:126 - Train Epoch: 35 Avg Loss: 3.9941 
2024-05-15 12:21:11.392 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:21:38.095 | INFO     | __main__:train:135 - Train Accuracies: All 0.8105 | Old 0.8345 | New 0.7145
2024-05-15 12:21:43.779 | INFO     | __main__:train:123 - Epoch: [36][0/390]	 loss 3.94029	 cls_loss: 0.3043 cluster_loss: 0.9985 sup_con_loss: 0.5584 contrastive_loss: 4.5990 
2024-05-15 12:21:56.927 | INFO     | __main__:train:123 - Epoch: [36][20/390]	 loss 3.97198	 cls_loss: 0.2587 cluster_loss: 0.9096 sup_con_loss: 0.8759 contrastive_loss: 4.5901 
2024-05-15 12:22:09.753 | INFO     | __main__:train:123 - Epoch: [36][40/390]	 loss 3.88395	 cls_loss: 0.3118 cluster_loss: 0.8287 sup_con_loss: 0.7230 contrastive_loss: 4.5894 
2024-05-15 12:22:22.880 | INFO     | __main__:train:123 - Epoch: [36][60/390]	 loss 4.13050	 cls_loss: 0.2826 cluster_loss: 0.9752 sup_con_loss: 1.1723 contrastive_loss: 4.5960 
2024-05-15 12:22:35.753 | INFO     | __main__:train:123 - Epoch: [36][80/390]	 loss 4.04027	 cls_loss: 0.2866 cluster_loss: 0.9966 sup_con_loss: 0.8598 contrastive_loss: 4.6019 
2024-05-15 12:22:48.721 | INFO     | __main__:train:123 - Epoch: [36][100/390]	 loss 3.94364	 cls_loss: 0.2863 cluster_loss: 0.9380 sup_con_loss: 0.6948 contrastive_loss: 4.6008 
2024-05-15 12:23:01.777 | INFO     | __main__:train:123 - Epoch: [36][120/390]	 loss 4.05915	 cls_loss: 0.3504 cluster_loss: 0.9627 sup_con_loss: 0.9179 contrastive_loss: 4.5992 
2024-05-15 12:23:14.630 | INFO     | __main__:train:123 - Epoch: [36][140/390]	 loss 3.95101	 cls_loss: 0.2944 cluster_loss: 0.9312 sup_con_loss: 0.7270 contrastive_loss: 4.5973 
2024-05-15 12:23:27.749 | INFO     | __main__:train:123 - Epoch: [36][160/390]	 loss 4.11137	 cls_loss: 0.3542 cluster_loss: 1.0816 sup_con_loss: 0.8277 contrastive_loss: 4.6072 
2024-05-15 12:23:40.730 | INFO     | __main__:train:123 - Epoch: [36][180/390]	 loss 4.03085	 cls_loss: 0.3092 cluster_loss: 0.9261 sup_con_loss: 0.9519 contrastive_loss: 4.5961 
2024-05-15 12:23:53.884 | INFO     | __main__:train:123 - Epoch: [36][200/390]	 loss 3.99780	 cls_loss: 0.2245 cluster_loss: 0.9532 sup_con_loss: 0.8805 contrastive_loss: 4.6022 
2024-05-15 12:24:06.909 | INFO     | __main__:train:123 - Epoch: [36][220/390]	 loss 4.10635	 cls_loss: 0.3308 cluster_loss: 0.9790 sup_con_loss: 1.0481 contrastive_loss: 4.5960 
2024-05-15 12:24:19.768 | INFO     | __main__:train:123 - Epoch: [36][240/390]	 loss 3.82038	 cls_loss: 0.2250 cluster_loss: 0.8797 sup_con_loss: 0.5289 contrastive_loss: 4.5919 
2024-05-15 12:24:32.718 | INFO     | __main__:train:123 - Epoch: [36][260/390]	 loss 4.14000	 cls_loss: 0.3143 cluster_loss: 1.0419 sup_con_loss: 1.0500 contrastive_loss: 4.5927 
2024-05-15 12:24:45.559 | INFO     | __main__:train:123 - Epoch: [36][280/390]	 loss 3.94872	 cls_loss: 0.2682 cluster_loss: 0.9063 sup_con_loss: 0.8000 contrastive_loss: 4.5935 
2024-05-15 12:24:58.604 | INFO     | __main__:train:123 - Epoch: [36][300/390]	 loss 3.92920	 cls_loss: 0.2702 cluster_loss: 0.8251 sup_con_loss: 0.9154 contrastive_loss: 4.5814 
2024-05-15 12:25:11.488 | INFO     | __main__:train:123 - Epoch: [36][320/390]	 loss 3.96350	 cls_loss: 0.2703 cluster_loss: 0.9627 sup_con_loss: 0.7326 contrastive_loss: 4.5950 
2024-05-15 12:25:24.460 | INFO     | __main__:train:123 - Epoch: [36][340/390]	 loss 4.06589	 cls_loss: 0.3043 cluster_loss: 0.9992 sup_con_loss: 0.9231 contrastive_loss: 4.5951 
2024-05-15 12:25:37.250 | INFO     | __main__:train:123 - Epoch: [36][360/390]	 loss 3.89808	 cls_loss: 0.2963 cluster_loss: 0.8861 sup_con_loss: 0.6565 contrastive_loss: 4.5979 
2024-05-15 12:25:49.991 | INFO     | __main__:train:123 - Epoch: [36][380/390]	 loss 3.92565	 cls_loss: 0.2909 cluster_loss: 0.9294 sup_con_loss: 0.6727 contrastive_loss: 4.5912 
2024-05-15 12:25:55.857 | INFO     | __main__:train:126 - Train Epoch: 36 Avg Loss: 3.9837 
2024-05-15 12:25:55.858 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:26:23.252 | INFO     | __main__:train:135 - Train Accuracies: All 0.8103 | Old 0.8353 | New 0.7105
2024-05-15 12:26:28.991 | INFO     | __main__:train:123 - Epoch: [37][0/390]	 loss 3.94698	 cls_loss: 0.2466 cluster_loss: 0.9223 sup_con_loss: 0.7893 contrastive_loss: 4.5922 
2024-05-15 12:26:42.288 | INFO     | __main__:train:123 - Epoch: [37][20/390]	 loss 3.96981	 cls_loss: 0.2957 cluster_loss: 0.9540 sup_con_loss: 0.7472 contrastive_loss: 4.5919 
2024-05-15 12:26:55.483 | INFO     | __main__:train:123 - Epoch: [37][40/390]	 loss 3.89075	 cls_loss: 0.2471 cluster_loss: 0.8664 sup_con_loss: 0.7390 contrastive_loss: 4.5884 
2024-05-15 12:27:08.149 | INFO     | __main__:train:123 - Epoch: [37][60/390]	 loss 3.88321	 cls_loss: 0.2429 cluster_loss: 0.8870 sup_con_loss: 0.6705 contrastive_loss: 4.5953 
2024-05-15 12:27:21.066 | INFO     | __main__:train:123 - Epoch: [37][80/390]	 loss 4.08301	 cls_loss: 0.3021 cluster_loss: 1.0148 sup_con_loss: 0.9109 contrastive_loss: 4.6136 
2024-05-15 12:27:33.845 | INFO     | __main__:train:123 - Epoch: [37][100/390]	 loss 3.98717	 cls_loss: 0.2160 cluster_loss: 1.0483 sup_con_loss: 0.7066 contrastive_loss: 4.5890 
2024-05-15 12:27:47.097 | INFO     | __main__:train:123 - Epoch: [37][120/390]	 loss 3.94984	 cls_loss: 0.2722 cluster_loss: 0.9143 sup_con_loss: 0.7728 contrastive_loss: 4.5997 
2024-05-15 12:28:00.022 | INFO     | __main__:train:123 - Epoch: [37][140/390]	 loss 3.95239	 cls_loss: 0.2995 cluster_loss: 0.9009 sup_con_loss: 0.7946 contrastive_loss: 4.5906 
2024-05-15 12:28:13.120 | INFO     | __main__:train:123 - Epoch: [37][160/390]	 loss 3.95826	 cls_loss: 0.3541 cluster_loss: 0.8533 sup_con_loss: 0.8584 contrastive_loss: 4.5834 
2024-05-15 12:28:25.970 | INFO     | __main__:train:123 - Epoch: [37][180/390]	 loss 3.99305	 cls_loss: 0.2284 cluster_loss: 0.9408 sup_con_loss: 0.9101 contrastive_loss: 4.5893 
2024-05-15 12:28:39.318 | INFO     | __main__:train:123 - Epoch: [37][200/390]	 loss 3.88377	 cls_loss: 0.2558 cluster_loss: 0.9219 sup_con_loss: 0.6101 contrastive_loss: 4.5868 
2024-05-15 12:28:52.433 | INFO     | __main__:train:123 - Epoch: [37][220/390]	 loss 3.99702	 cls_loss: 0.3012 cluster_loss: 0.9288 sup_con_loss: 0.8605 contrastive_loss: 4.5949 
2024-05-15 12:29:05.172 | INFO     | __main__:train:123 - Epoch: [37][240/390]	 loss 3.90293	 cls_loss: 0.2414 cluster_loss: 0.9391 sup_con_loss: 0.6398 contrastive_loss: 4.5909 
2024-05-15 12:29:17.901 | INFO     | __main__:train:123 - Epoch: [37][260/390]	 loss 3.93700	 cls_loss: 0.2779 cluster_loss: 0.9248 sup_con_loss: 0.7387 contrastive_loss: 4.5847 
2024-05-15 12:29:30.797 | INFO     | __main__:train:123 - Epoch: [37][280/390]	 loss 4.05712	 cls_loss: 0.2424 cluster_loss: 0.9825 sup_con_loss: 0.9976 contrastive_loss: 4.5915 
2024-05-15 12:29:43.779 | INFO     | __main__:train:123 - Epoch: [37][300/390]	 loss 3.94824	 cls_loss: 0.3007 cluster_loss: 0.9329 sup_con_loss: 0.7299 contrastive_loss: 4.5864 
2024-05-15 12:29:56.684 | INFO     | __main__:train:123 - Epoch: [37][320/390]	 loss 3.89166	 cls_loss: 0.2460 cluster_loss: 0.8909 sup_con_loss: 0.6875 contrastive_loss: 4.5937 
2024-05-15 12:30:09.769 | INFO     | __main__:train:123 - Epoch: [37][340/390]	 loss 3.91798	 cls_loss: 0.2345 cluster_loss: 0.8406 sup_con_loss: 0.8714 contrastive_loss: 4.5915 
2024-05-15 12:30:22.650 | INFO     | __main__:train:123 - Epoch: [37][360/390]	 loss 3.91390	 cls_loss: 0.2479 cluster_loss: 0.9006 sup_con_loss: 0.7528 contrastive_loss: 4.5819 
2024-05-15 12:30:35.235 | INFO     | __main__:train:123 - Epoch: [37][380/390]	 loss 4.02227	 cls_loss: 0.2269 cluster_loss: 0.9726 sup_con_loss: 0.9351 contrastive_loss: 4.5898 
2024-05-15 12:30:40.913 | INFO     | __main__:train:126 - Train Epoch: 37 Avg Loss: 3.9655 
2024-05-15 12:30:40.915 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:31:07.846 | INFO     | __main__:train:135 - Train Accuracies: All 0.8121 | Old 0.8361 | New 0.7160
2024-05-15 12:31:11.599 | INFO     | __main__:train:123 - Epoch: [38][0/390]	 loss 3.85879	 cls_loss: 0.2682 cluster_loss: 0.8853 sup_con_loss: 0.5754 contrastive_loss: 4.5971 
2024-05-15 12:31:25.629 | INFO     | __main__:train:123 - Epoch: [38][20/390]	 loss 4.02529	 cls_loss: 0.3011 cluster_loss: 0.9468 sup_con_loss: 0.9200 contrastive_loss: 4.5884 
2024-05-15 12:31:38.406 | INFO     | __main__:train:123 - Epoch: [38][40/390]	 loss 3.83792	 cls_loss: 0.3146 cluster_loss: 0.8330 sup_con_loss: 0.5861 contrastive_loss: 4.5865 
2024-05-15 12:31:51.555 | INFO     | __main__:train:123 - Epoch: [38][60/390]	 loss 3.87822	 cls_loss: 0.2249 cluster_loss: 0.8441 sup_con_loss: 0.7717 contrastive_loss: 4.5858 
2024-05-15 12:32:04.990 | INFO     | __main__:train:123 - Epoch: [38][80/390]	 loss 3.92312	 cls_loss: 0.3222 cluster_loss: 0.8815 sup_con_loss: 0.7200 contrastive_loss: 4.5928 
2024-05-15 12:32:18.185 | INFO     | __main__:train:123 - Epoch: [38][100/390]	 loss 3.98304	 cls_loss: 0.2539 cluster_loss: 0.9401 sup_con_loss: 0.8505 contrastive_loss: 4.5930 
2024-05-15 12:32:30.948 | INFO     | __main__:train:123 - Epoch: [38][120/390]	 loss 3.97583	 cls_loss: 0.2742 cluster_loss: 0.9452 sup_con_loss: 0.8085 contrastive_loss: 4.5885 
2024-05-15 12:32:43.828 | INFO     | __main__:train:123 - Epoch: [38][140/390]	 loss 4.10165	 cls_loss: 0.2308 cluster_loss: 1.0118 sup_con_loss: 1.0621 contrastive_loss: 4.6022 
2024-05-15 12:32:56.906 | INFO     | __main__:train:123 - Epoch: [38][160/390]	 loss 3.96838	 cls_loss: 0.2824 cluster_loss: 0.9232 sup_con_loss: 0.8028 contrastive_loss: 4.5977 
2024-05-15 12:33:09.871 | INFO     | __main__:train:123 - Epoch: [38][180/390]	 loss 3.88521	 cls_loss: 0.2433 cluster_loss: 0.8581 sup_con_loss: 0.7439 contrastive_loss: 4.5875 
2024-05-15 12:33:22.754 | INFO     | __main__:train:123 - Epoch: [38][200/390]	 loss 3.96522	 cls_loss: 0.3171 cluster_loss: 0.9226 sup_con_loss: 0.7750 contrastive_loss: 4.5896 
2024-05-15 12:33:35.616 | INFO     | __main__:train:123 - Epoch: [38][220/390]	 loss 4.04908	 cls_loss: 0.3015 cluster_loss: 0.9278 sup_con_loss: 1.0164 contrastive_loss: 4.5919 
2024-05-15 12:33:48.560 | INFO     | __main__:train:123 - Epoch: [38][240/390]	 loss 4.16197	 cls_loss: 0.3390 cluster_loss: 1.0898 sup_con_loss: 0.9979 contrastive_loss: 4.5933 
2024-05-15 12:34:01.397 | INFO     | __main__:train:123 - Epoch: [38][260/390]	 loss 4.11516	 cls_loss: 0.3913 cluster_loss: 1.0868 sup_con_loss: 0.8125 contrastive_loss: 4.5960 
2024-05-15 12:34:13.972 | INFO     | __main__:train:123 - Epoch: [38][280/390]	 loss 3.85323	 cls_loss: 0.1795 cluster_loss: 0.8505 sup_con_loss: 0.7261 contrastive_loss: 4.5899 
2024-05-15 12:34:26.736 | INFO     | __main__:train:123 - Epoch: [38][300/390]	 loss 4.02900	 cls_loss: 0.3908 cluster_loss: 1.0173 sup_con_loss: 0.6902 contrastive_loss: 4.5991 
2024-05-15 12:34:39.805 | INFO     | __main__:train:123 - Epoch: [38][320/390]	 loss 3.92253	 cls_loss: 0.2644 cluster_loss: 0.9153 sup_con_loss: 0.7179 contrastive_loss: 4.5905 
2024-05-15 12:34:52.891 | INFO     | __main__:train:123 - Epoch: [38][340/390]	 loss 3.85345	 cls_loss: 0.2802 cluster_loss: 0.8024 sup_con_loss: 0.7067 contrastive_loss: 4.5946 
2024-05-15 12:35:05.764 | INFO     | __main__:train:123 - Epoch: [38][360/390]	 loss 3.99279	 cls_loss: 0.2594 cluster_loss: 0.9632 sup_con_loss: 0.8122 contrastive_loss: 4.6026 
2024-05-15 12:35:18.189 | INFO     | __main__:train:123 - Epoch: [38][380/390]	 loss 4.05223	 cls_loss: 0.2783 cluster_loss: 1.0286 sup_con_loss: 0.8450 contrastive_loss: 4.6007 
2024-05-15 12:35:23.893 | INFO     | __main__:train:126 - Train Epoch: 38 Avg Loss: 3.9642 
2024-05-15 12:35:23.894 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:35:50.399 | INFO     | __main__:train:135 - Train Accuracies: All 0.8099 | Old 0.8374 | New 0.7000
2024-05-15 12:35:55.900 | INFO     | __main__:train:123 - Epoch: [39][0/390]	 loss 4.01170	 cls_loss: 0.3138 cluster_loss: 0.8852 sup_con_loss: 0.9905 contrastive_loss: 4.5843 
2024-05-15 12:36:08.721 | INFO     | __main__:train:123 - Epoch: [39][20/390]	 loss 3.97357	 cls_loss: 0.2213 cluster_loss: 0.9806 sup_con_loss: 0.7854 contrastive_loss: 4.5906 
2024-05-15 12:36:21.602 | INFO     | __main__:train:123 - Epoch: [39][40/390]	 loss 4.02128	 cls_loss: 0.2290 cluster_loss: 1.0424 sup_con_loss: 0.7841 contrastive_loss: 4.5987 
2024-05-15 12:36:34.473 | INFO     | __main__:train:123 - Epoch: [39][60/390]	 loss 4.11418	 cls_loss: 0.3336 cluster_loss: 1.0135 sup_con_loss: 0.9881 contrastive_loss: 4.6044 
2024-05-15 12:36:47.225 | INFO     | __main__:train:123 - Epoch: [39][80/390]	 loss 4.03336	 cls_loss: 0.2724 cluster_loss: 0.9887 sup_con_loss: 0.8834 contrastive_loss: 4.5941 
2024-05-15 12:37:00.153 | INFO     | __main__:train:123 - Epoch: [39][100/390]	 loss 3.97833	 cls_loss: 0.2811 cluster_loss: 0.9855 sup_con_loss: 0.7268 contrastive_loss: 4.5923 
2024-05-15 12:37:13.171 | INFO     | __main__:train:123 - Epoch: [39][120/390]	 loss 3.84175	 cls_loss: 0.2687 cluster_loss: 0.8513 sup_con_loss: 0.6076 contrastive_loss: 4.5872 
2024-05-15 12:37:26.110 | INFO     | __main__:train:123 - Epoch: [39][140/390]	 loss 3.99223	 cls_loss: 0.2791 cluster_loss: 0.9478 sup_con_loss: 0.8482 contrastive_loss: 4.5872 
2024-05-15 12:37:39.251 | INFO     | __main__:train:123 - Epoch: [39][160/390]	 loss 4.00714	 cls_loss: 0.3352 cluster_loss: 0.8926 sup_con_loss: 0.9318 contrastive_loss: 4.5900 
2024-05-15 12:37:52.395 | INFO     | __main__:train:123 - Epoch: [39][180/390]	 loss 4.04360	 cls_loss: 0.3123 cluster_loss: 0.9826 sup_con_loss: 0.8918 contrastive_loss: 4.5900 
2024-05-15 12:38:05.338 | INFO     | __main__:train:123 - Epoch: [39][200/390]	 loss 3.82563	 cls_loss: 0.2540 cluster_loss: 0.8742 sup_con_loss: 0.5352 contrastive_loss: 4.5864 
2024-05-15 12:38:18.238 | INFO     | __main__:train:123 - Epoch: [39][220/390]	 loss 3.93115	 cls_loss: 0.2811 cluster_loss: 0.8709 sup_con_loss: 0.8161 contrastive_loss: 4.5862 
2024-05-15 12:38:30.795 | INFO     | __main__:train:123 - Epoch: [39][240/390]	 loss 3.93276	 cls_loss: 0.3276 cluster_loss: 0.9045 sup_con_loss: 0.7138 contrastive_loss: 4.5851 
2024-05-15 12:38:43.587 | INFO     | __main__:train:123 - Epoch: [39][260/390]	 loss 4.10869	 cls_loss: 0.2963 cluster_loss: 1.0000 sup_con_loss: 1.0412 contrastive_loss: 4.6009 
2024-05-15 12:38:56.723 | INFO     | __main__:train:123 - Epoch: [39][280/390]	 loss 4.08277	 cls_loss: 0.2935 cluster_loss: 0.9451 sup_con_loss: 1.0893 contrastive_loss: 4.5915 
2024-05-15 12:39:09.727 | INFO     | __main__:train:123 - Epoch: [39][300/390]	 loss 3.89327	 cls_loss: 0.3123 cluster_loss: 0.8631 sup_con_loss: 0.6979 contrastive_loss: 4.5826 
2024-05-15 12:39:22.800 | INFO     | __main__:train:123 - Epoch: [39][320/390]	 loss 3.81225	 cls_loss: 0.2486 cluster_loss: 0.8020 sup_con_loss: 0.6453 contrastive_loss: 4.5816 
2024-05-15 12:39:35.574 | INFO     | __main__:train:123 - Epoch: [39][340/390]	 loss 4.03394	 cls_loss: 0.2821 cluster_loss: 1.0086 sup_con_loss: 0.8284 contrastive_loss: 4.5995 
2024-05-15 12:39:48.822 | INFO     | __main__:train:123 - Epoch: [39][360/390]	 loss 3.94678	 cls_loss: 0.2787 cluster_loss: 0.9146 sup_con_loss: 0.7827 contrastive_loss: 4.5859 
2024-05-15 12:40:01.307 | INFO     | __main__:train:123 - Epoch: [39][380/390]	 loss 3.94480	 cls_loss: 0.2691 cluster_loss: 0.9181 sup_con_loss: 0.7674 contrastive_loss: 4.5927 
2024-05-15 12:40:07.216 | INFO     | __main__:train:126 - Train Epoch: 39 Avg Loss: 3.9544 
2024-05-15 12:40:07.218 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:40:34.088 | INFO     | __main__:train:135 - Train Accuracies: All 0.8127 | Old 0.8380 | New 0.7115
2024-05-15 12:40:37.416 | INFO     | __main__:train:123 - Epoch: [40][0/390]	 loss 3.94985	 cls_loss: 0.1836 cluster_loss: 0.9283 sup_con_loss: 0.8486 contrastive_loss: 4.5926 
2024-05-15 12:40:52.237 | INFO     | __main__:train:123 - Epoch: [40][20/390]	 loss 3.96950	 cls_loss: 0.2826 cluster_loss: 0.9254 sup_con_loss: 0.8202 contrastive_loss: 4.5877 
2024-05-15 12:41:04.903 | INFO     | __main__:train:123 - Epoch: [40][40/390]	 loss 4.02715	 cls_loss: 0.2576 cluster_loss: 1.0051 sup_con_loss: 0.8333 contrastive_loss: 4.6031 
2024-05-15 12:41:17.469 | INFO     | __main__:train:123 - Epoch: [40][60/390]	 loss 4.08169	 cls_loss: 0.3791 cluster_loss: 0.9711 sup_con_loss: 0.9574 contrastive_loss: 4.5887 
2024-05-15 12:41:30.432 | INFO     | __main__:train:123 - Epoch: [40][80/390]	 loss 3.90232	 cls_loss: 0.2163 cluster_loss: 0.9503 sup_con_loss: 0.6369 contrastive_loss: 4.5939 
2024-05-15 12:41:43.407 | INFO     | __main__:train:123 - Epoch: [40][100/390]	 loss 4.21007	 cls_loss: 0.3640 cluster_loss: 1.1006 sup_con_loss: 1.0643 contrastive_loss: 4.6073 
2024-05-15 12:41:56.226 | INFO     | __main__:train:123 - Epoch: [40][120/390]	 loss 3.90040	 cls_loss: 0.3173 cluster_loss: 0.8684 sup_con_loss: 0.6975 contrastive_loss: 4.5857 
2024-05-15 12:42:09.501 | INFO     | __main__:train:123 - Epoch: [40][140/390]	 loss 3.96013	 cls_loss: 0.2156 cluster_loss: 0.9473 sup_con_loss: 0.8238 contrastive_loss: 4.5855 
2024-05-15 12:42:22.408 | INFO     | __main__:train:123 - Epoch: [40][160/390]	 loss 3.87409	 cls_loss: 0.2035 cluster_loss: 0.9352 sup_con_loss: 0.6011 contrastive_loss: 4.5917 
2024-05-15 12:42:35.574 | INFO     | __main__:train:123 - Epoch: [40][180/390]	 loss 3.94460	 cls_loss: 0.2391 cluster_loss: 0.9499 sup_con_loss: 0.7474 contrastive_loss: 4.5875 
2024-05-15 12:42:48.664 | INFO     | __main__:train:123 - Epoch: [40][200/390]	 loss 3.99982	 cls_loss: 0.1526 cluster_loss: 1.0618 sup_con_loss: 0.7834 contrastive_loss: 4.5878 
2024-05-15 12:43:01.713 | INFO     | __main__:train:123 - Epoch: [40][220/390]	 loss 4.01458	 cls_loss: 0.2428 cluster_loss: 0.9213 sup_con_loss: 1.0058 contrastive_loss: 4.5827 
2024-05-15 12:43:14.847 | INFO     | __main__:train:123 - Epoch: [40][240/390]	 loss 4.03706	 cls_loss: 0.3540 cluster_loss: 0.9941 sup_con_loss: 0.7979 contrastive_loss: 4.5965 
2024-05-15 12:43:27.901 | INFO     | __main__:train:123 - Epoch: [40][260/390]	 loss 3.99156	 cls_loss: 0.3791 cluster_loss: 0.9122 sup_con_loss: 0.7970 contrastive_loss: 4.5953 
2024-05-15 12:43:41.096 | INFO     | __main__:train:123 - Epoch: [40][280/390]	 loss 3.93233	 cls_loss: 0.2628 cluster_loss: 0.9378 sup_con_loss: 0.7017 contrastive_loss: 4.5925 
2024-05-15 12:43:54.104 | INFO     | __main__:train:123 - Epoch: [40][300/390]	 loss 3.93411	 cls_loss: 0.2951 cluster_loss: 0.8994 sup_con_loss: 0.7520 contrastive_loss: 4.5893 
2024-05-15 12:44:06.966 | INFO     | __main__:train:123 - Epoch: [40][320/390]	 loss 3.76071	 cls_loss: 0.1974 cluster_loss: 0.8313 sup_con_loss: 0.4916 contrastive_loss: 4.5835 
2024-05-15 12:44:19.916 | INFO     | __main__:train:123 - Epoch: [40][340/390]	 loss 3.86820	 cls_loss: 0.2532 cluster_loss: 0.8422 sup_con_loss: 0.7208 contrastive_loss: 4.5844 
2024-05-15 12:44:32.720 | INFO     | __main__:train:123 - Epoch: [40][360/390]	 loss 3.96390	 cls_loss: 0.3238 cluster_loss: 0.9537 sup_con_loss: 0.6908 contrastive_loss: 4.5983 
2024-05-15 12:44:45.439 | INFO     | __main__:train:123 - Epoch: [40][380/390]	 loss 4.01558	 cls_loss: 0.2517 cluster_loss: 0.9620 sup_con_loss: 0.9053 contrastive_loss: 4.5929 
2024-05-15 12:44:51.329 | INFO     | __main__:train:126 - Train Epoch: 40 Avg Loss: 3.9595 
2024-05-15 12:44:51.330 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:45:17.870 | INFO     | __main__:train:135 - Train Accuracies: All 0.8109 | Old 0.8377 | New 0.7035
2024-05-15 12:45:23.976 | INFO     | __main__:train:123 - Epoch: [41][0/390]	 loss 3.97366	 cls_loss: 0.2919 cluster_loss: 0.9257 sup_con_loss: 0.7958 contrastive_loss: 4.6020 
2024-05-15 12:45:37.182 | INFO     | __main__:train:123 - Epoch: [41][20/390]	 loss 3.95591	 cls_loss: 0.2780 cluster_loss: 0.8751 sup_con_loss: 0.8867 contrastive_loss: 4.5837 
2024-05-15 12:45:49.986 | INFO     | __main__:train:123 - Epoch: [41][40/390]	 loss 4.00363	 cls_loss: 0.2573 cluster_loss: 0.9694 sup_con_loss: 0.8523 contrastive_loss: 4.5926 
2024-05-15 12:46:02.954 | INFO     | __main__:train:123 - Epoch: [41][60/390]	 loss 3.93772	 cls_loss: 0.2325 cluster_loss: 0.9169 sup_con_loss: 0.7903 contrastive_loss: 4.5904 
2024-05-15 12:46:15.721 | INFO     | __main__:train:123 - Epoch: [41][80/390]	 loss 3.88373	 cls_loss: 0.2648 cluster_loss: 0.8534 sup_con_loss: 0.7249 contrastive_loss: 4.5887 
2024-05-15 12:46:28.866 | INFO     | __main__:train:123 - Epoch: [41][100/390]	 loss 4.07196	 cls_loss: 0.2622 cluster_loss: 0.9325 sup_con_loss: 1.1149 contrastive_loss: 4.5905 
2024-05-15 12:46:42.211 | INFO     | __main__:train:123 - Epoch: [41][120/390]	 loss 3.88659	 cls_loss: 0.2678 cluster_loss: 0.8666 sup_con_loss: 0.7101 contrastive_loss: 4.5862 
2024-05-15 12:46:55.501 | INFO     | __main__:train:123 - Epoch: [41][140/390]	 loss 3.88455	 cls_loss: 0.2480 cluster_loss: 0.8626 sup_con_loss: 0.7298 contrastive_loss: 4.5870 
2024-05-15 12:47:08.465 | INFO     | __main__:train:123 - Epoch: [41][160/390]	 loss 3.92808	 cls_loss: 0.2394 cluster_loss: 0.9357 sup_con_loss: 0.7179 contrastive_loss: 4.5920 
2024-05-15 12:47:21.331 | INFO     | __main__:train:123 - Epoch: [41][180/390]	 loss 3.99528	 cls_loss: 0.2516 cluster_loss: 0.9724 sup_con_loss: 0.8284 contrastive_loss: 4.5926 
2024-05-15 12:47:34.421 | INFO     | __main__:train:123 - Epoch: [41][200/390]	 loss 3.88961	 cls_loss: 0.3170 cluster_loss: 0.8544 sup_con_loss: 0.6910 contrastive_loss: 4.5868 
2024-05-15 12:47:47.383 | INFO     | __main__:train:123 - Epoch: [41][220/390]	 loss 3.86922	 cls_loss: 0.2264 cluster_loss: 0.8972 sup_con_loss: 0.6256 contrastive_loss: 4.5966 
2024-05-15 12:48:00.462 | INFO     | __main__:train:123 - Epoch: [41][240/390]	 loss 3.91297	 cls_loss: 0.2462 cluster_loss: 0.8750 sup_con_loss: 0.7956 contrastive_loss: 4.5840 
2024-05-15 12:48:13.395 | INFO     | __main__:train:123 - Epoch: [41][260/390]	 loss 3.97857	 cls_loss: 0.3189 cluster_loss: 0.9314 sup_con_loss: 0.7814 contrastive_loss: 4.5971 
2024-05-15 12:48:26.716 | INFO     | __main__:train:123 - Epoch: [41][280/390]	 loss 3.90807	 cls_loss: 0.1678 cluster_loss: 0.9107 sup_con_loss: 0.7876 contrastive_loss: 4.5873 
2024-05-15 12:48:39.918 | INFO     | __main__:train:123 - Epoch: [41][300/390]	 loss 3.80751	 cls_loss: 0.2328 cluster_loss: 0.8515 sup_con_loss: 0.5365 contrastive_loss: 4.5919 
2024-05-15 12:48:52.852 | INFO     | __main__:train:123 - Epoch: [41][320/390]	 loss 3.84771	 cls_loss: 0.2559 cluster_loss: 0.7794 sup_con_loss: 0.7880 contrastive_loss: 4.5781 
2024-05-15 12:49:05.515 | INFO     | __main__:train:123 - Epoch: [41][340/390]	 loss 4.02997	 cls_loss: 0.2477 cluster_loss: 0.9993 sup_con_loss: 0.8701 contrastive_loss: 4.5987 
2024-05-15 12:49:18.500 | INFO     | __main__:train:123 - Epoch: [41][360/390]	 loss 3.95394	 cls_loss: 0.2046 cluster_loss: 0.8801 sup_con_loss: 0.9354 contrastive_loss: 4.5890 
2024-05-15 12:49:31.189 | INFO     | __main__:train:123 - Epoch: [41][380/390]	 loss 3.89813	 cls_loss: 0.2044 cluster_loss: 0.8649 sup_con_loss: 0.8122 contrastive_loss: 4.5848 
2024-05-15 12:49:37.130 | INFO     | __main__:train:126 - Train Epoch: 41 Avg Loss: 3.9418 
2024-05-15 12:49:37.131 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:50:03.733 | INFO     | __main__:train:135 - Train Accuracies: All 0.8143 | Old 0.8387 | New 0.7165
2024-05-15 12:50:08.394 | INFO     | __main__:train:123 - Epoch: [42][0/390]	 loss 4.03034	 cls_loss: 0.1777 cluster_loss: 1.0420 sup_con_loss: 0.8761 contrastive_loss: 4.5911 
2024-05-15 12:50:22.112 | INFO     | __main__:train:123 - Epoch: [42][20/390]	 loss 3.90095	 cls_loss: 0.2798 cluster_loss: 0.8556 sup_con_loss: 0.7555 contrastive_loss: 4.5884 
2024-05-15 12:50:35.334 | INFO     | __main__:train:123 - Epoch: [42][40/390]	 loss 3.98231	 cls_loss: 0.2473 cluster_loss: 0.9459 sup_con_loss: 0.8554 contrastive_loss: 4.5869 
2024-05-15 12:50:48.258 | INFO     | __main__:train:123 - Epoch: [42][60/390]	 loss 3.97746	 cls_loss: 0.3302 cluster_loss: 0.9197 sup_con_loss: 0.8148 contrastive_loss: 4.5830 
2024-05-15 12:51:01.237 | INFO     | __main__:train:123 - Epoch: [42][80/390]	 loss 3.85004	 cls_loss: 0.2570 cluster_loss: 0.8123 sup_con_loss: 0.7217 contrastive_loss: 4.5839 
2024-05-15 12:51:14.064 | INFO     | __main__:train:123 - Epoch: [42][100/390]	 loss 3.98938	 cls_loss: 0.3253 cluster_loss: 0.9331 sup_con_loss: 0.8146 contrastive_loss: 4.5906 
2024-05-15 12:51:27.258 | INFO     | __main__:train:123 - Epoch: [42][120/390]	 loss 3.93862	 cls_loss: 0.2825 cluster_loss: 0.7958 sup_con_loss: 0.9893 contrastive_loss: 4.5788 
2024-05-15 12:51:40.095 | INFO     | __main__:train:123 - Epoch: [42][140/390]	 loss 3.93671	 cls_loss: 0.2637 cluster_loss: 0.9215 sup_con_loss: 0.7480 contrastive_loss: 4.5902 
2024-05-15 12:51:53.123 | INFO     | __main__:train:123 - Epoch: [42][160/390]	 loss 3.88100	 cls_loss: 0.2267 cluster_loss: 0.9041 sup_con_loss: 0.6559 contrastive_loss: 4.5914 
2024-05-15 12:52:06.031 | INFO     | __main__:train:123 - Epoch: [42][180/390]	 loss 3.76751	 cls_loss: 0.1866 cluster_loss: 0.7664 sup_con_loss: 0.6373 contrastive_loss: 4.5861 
2024-05-15 12:52:18.994 | INFO     | __main__:train:123 - Epoch: [42][200/390]	 loss 4.05441	 cls_loss: 0.2262 cluster_loss: 0.9692 sup_con_loss: 1.0145 contrastive_loss: 4.6003 
2024-05-15 12:52:31.886 | INFO     | __main__:train:123 - Epoch: [42][220/390]	 loss 3.96917	 cls_loss: 0.2621 cluster_loss: 0.8674 sup_con_loss: 0.9504 contrastive_loss: 4.5862 
2024-05-15 12:52:45.054 | INFO     | __main__:train:123 - Epoch: [42][240/390]	 loss 3.87739	 cls_loss: 0.2054 cluster_loss: 0.8816 sup_con_loss: 0.7151 contrastive_loss: 4.5880 
2024-05-15 12:52:58.287 | INFO     | __main__:train:123 - Epoch: [42][260/390]	 loss 3.98926	 cls_loss: 0.2398 cluster_loss: 1.0159 sup_con_loss: 0.7336 contrastive_loss: 4.5973 
2024-05-15 12:53:11.215 | INFO     | __main__:train:123 - Epoch: [42][280/390]	 loss 3.92544	 cls_loss: 0.2279 cluster_loss: 0.8989 sup_con_loss: 0.8028 contrastive_loss: 4.5853 
2024-05-15 12:53:24.104 | INFO     | __main__:train:123 - Epoch: [42][300/390]	 loss 3.83369	 cls_loss: 0.2736 cluster_loss: 0.8108 sup_con_loss: 0.6403 contrastive_loss: 4.5951 
2024-05-15 12:53:37.312 | INFO     | __main__:train:123 - Epoch: [42][320/390]	 loss 4.02655	 cls_loss: 0.2509 cluster_loss: 1.0119 sup_con_loss: 0.8324 contrastive_loss: 4.5995 
2024-05-15 12:53:50.012 | INFO     | __main__:train:123 - Epoch: [42][340/390]	 loss 3.94509	 cls_loss: 0.2512 cluster_loss: 0.9788 sup_con_loss: 0.6670 contrastive_loss: 4.5961 
2024-05-15 12:54:02.959 | INFO     | __main__:train:123 - Epoch: [42][360/390]	 loss 4.08329	 cls_loss: 0.2561 cluster_loss: 0.9867 sup_con_loss: 1.0357 contrastive_loss: 4.5997 
2024-05-15 12:54:15.490 | INFO     | __main__:train:123 - Epoch: [42][380/390]	 loss 3.90175	 cls_loss: 0.1985 cluster_loss: 0.8519 sup_con_loss: 0.8588 contrastive_loss: 4.5815 
2024-05-15 12:54:21.665 | INFO     | __main__:train:126 - Train Epoch: 42 Avg Loss: 3.9442 
2024-05-15 12:54:21.666 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:54:48.867 | INFO     | __main__:train:135 - Train Accuracies: All 0.8096 | Old 0.8360 | New 0.7040
2024-05-15 12:54:54.500 | INFO     | __main__:train:123 - Epoch: [43][0/390]	 loss 4.02601	 cls_loss: 0.2062 cluster_loss: 0.9505 sup_con_loss: 1.0003 contrastive_loss: 4.5937 
2024-05-15 12:55:07.475 | INFO     | __main__:train:123 - Epoch: [43][20/390]	 loss 3.86737	 cls_loss: 0.2440 cluster_loss: 0.8463 sup_con_loss: 0.7296 contrastive_loss: 4.5793 
2024-05-15 12:55:20.616 | INFO     | __main__:train:123 - Epoch: [43][40/390]	 loss 3.82343	 cls_loss: 0.2154 cluster_loss: 0.8672 sup_con_loss: 0.5677 contrastive_loss: 4.5933 
2024-05-15 12:55:33.775 | INFO     | __main__:train:123 - Epoch: [43][60/390]	 loss 3.89493	 cls_loss: 0.2417 cluster_loss: 0.8540 sup_con_loss: 0.7819 contrastive_loss: 4.5870 
2024-05-15 12:55:46.963 | INFO     | __main__:train:123 - Epoch: [43][80/390]	 loss 3.93196	 cls_loss: 0.2977 cluster_loss: 0.8325 sup_con_loss: 0.8736 contrastive_loss: 4.5859 
2024-05-15 12:56:00.142 | INFO     | __main__:train:123 - Epoch: [43][100/390]	 loss 3.94897	 cls_loss: 0.2730 cluster_loss: 0.9011 sup_con_loss: 0.7872 contrastive_loss: 4.6033 
2024-05-15 12:56:13.149 | INFO     | __main__:train:123 - Epoch: [43][120/390]	 loss 3.86409	 cls_loss: 0.1669 cluster_loss: 0.8809 sup_con_loss: 0.7303 contrastive_loss: 4.5808 
2024-05-15 12:56:25.985 | INFO     | __main__:train:123 - Epoch: [43][140/390]	 loss 3.91633	 cls_loss: 0.1944 cluster_loss: 0.9642 sup_con_loss: 0.6742 contrastive_loss: 4.5932 
2024-05-15 12:56:38.957 | INFO     | __main__:train:123 - Epoch: [43][160/390]	 loss 3.90348	 cls_loss: 0.2225 cluster_loss: 0.9006 sup_con_loss: 0.7372 contrastive_loss: 4.5880 
2024-05-15 12:56:51.904 | INFO     | __main__:train:123 - Epoch: [43][180/390]	 loss 3.89162	 cls_loss: 0.1999 cluster_loss: 0.8743 sup_con_loss: 0.7873 contrastive_loss: 4.5813 
2024-05-15 12:57:04.906 | INFO     | __main__:train:123 - Epoch: [43][200/390]	 loss 3.80006	 cls_loss: 0.1856 cluster_loss: 0.8241 sup_con_loss: 0.6305 contrastive_loss: 4.5828 
2024-05-15 12:57:17.956 | INFO     | __main__:train:123 - Epoch: [43][220/390]	 loss 3.95467	 cls_loss: 0.2667 cluster_loss: 0.9330 sup_con_loss: 0.7886 contrastive_loss: 4.5828 
2024-05-15 12:57:30.922 | INFO     | __main__:train:123 - Epoch: [43][240/390]	 loss 3.99985	 cls_loss: 0.3281 cluster_loss: 0.9531 sup_con_loss: 0.8096 contrastive_loss: 4.5879 
2024-05-15 12:57:43.880 | INFO     | __main__:train:123 - Epoch: [43][260/390]	 loss 4.06513	 cls_loss: 0.2192 cluster_loss: 1.0263 sup_con_loss: 0.9517 contrastive_loss: 4.5973 
2024-05-15 12:57:56.828 | INFO     | __main__:train:123 - Epoch: [43][280/390]	 loss 3.83365	 cls_loss: 0.2531 cluster_loss: 0.8688 sup_con_loss: 0.5760 contrastive_loss: 4.5827 
2024-05-15 12:58:09.893 | INFO     | __main__:train:123 - Epoch: [43][300/390]	 loss 3.88113	 cls_loss: 0.3062 cluster_loss: 0.8082 sup_con_loss: 0.7583 contrastive_loss: 4.5895 
2024-05-15 12:58:22.885 | INFO     | __main__:train:123 - Epoch: [43][320/390]	 loss 3.89127	 cls_loss: 0.2748 cluster_loss: 0.8928 sup_con_loss: 0.6511 contrastive_loss: 4.5951 
2024-05-15 12:58:35.971 | INFO     | __main__:train:123 - Epoch: [43][340/390]	 loss 3.89530	 cls_loss: 0.2546 cluster_loss: 0.8834 sup_con_loss: 0.7156 contrastive_loss: 4.5869 
2024-05-15 12:58:48.932 | INFO     | __main__:train:123 - Epoch: [43][360/390]	 loss 3.96281	 cls_loss: 0.2421 cluster_loss: 1.0073 sup_con_loss: 0.6852 contrastive_loss: 4.5900 
2024-05-15 12:59:01.507 | INFO     | __main__:train:123 - Epoch: [43][380/390]	 loss 3.93987	 cls_loss: 0.2650 cluster_loss: 0.9383 sup_con_loss: 0.7210 contrastive_loss: 4.5921 
2024-05-15 12:59:07.163 | INFO     | __main__:train:126 - Train Epoch: 43 Avg Loss: 3.9362 
2024-05-15 12:59:07.164 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:59:33.715 | INFO     | __main__:train:135 - Train Accuracies: All 0.8115 | Old 0.8347 | New 0.7185
2024-05-15 12:59:38.431 | INFO     | __main__:train:123 - Epoch: [44][0/390]	 loss 4.01502	 cls_loss: 0.2925 cluster_loss: 0.9884 sup_con_loss: 0.8092 contrastive_loss: 4.5954 
2024-05-15 12:59:51.794 | INFO     | __main__:train:123 - Epoch: [44][20/390]	 loss 4.03180	 cls_loss: 0.2584 cluster_loss: 0.9666 sup_con_loss: 0.9351 contrastive_loss: 4.5935 
2024-05-15 13:00:04.780 | INFO     | __main__:train:123 - Epoch: [44][40/390]	 loss 3.94182	 cls_loss: 0.2371 cluster_loss: 0.9294 sup_con_loss: 0.7502 contrastive_loss: 4.6033 
2024-05-15 13:00:17.646 | INFO     | __main__:train:123 - Epoch: [44][60/390]	 loss 3.97652	 cls_loss: 0.2745 cluster_loss: 0.9447 sup_con_loss: 0.7956 contrastive_loss: 4.5969 
2024-05-15 13:00:30.942 | INFO     | __main__:train:123 - Epoch: [44][80/390]	 loss 3.99869	 cls_loss: 0.2598 cluster_loss: 0.9643 sup_con_loss: 0.8583 contrastive_loss: 4.5855 
2024-05-15 13:00:43.642 | INFO     | __main__:train:123 - Epoch: [44][100/390]	 loss 4.03041	 cls_loss: 0.3080 cluster_loss: 1.0508 sup_con_loss: 0.7090 contrastive_loss: 4.6022 
2024-05-15 13:00:56.625 | INFO     | __main__:train:123 - Epoch: [44][120/390]	 loss 3.85726	 cls_loss: 0.2142 cluster_loss: 0.8629 sup_con_loss: 0.7024 contrastive_loss: 4.5778 
2024-05-15 13:01:09.798 | INFO     | __main__:train:123 - Epoch: [44][140/390]	 loss 3.99118	 cls_loss: 0.2862 cluster_loss: 0.9358 sup_con_loss: 0.8463 contrastive_loss: 4.5946 
2024-05-15 13:01:22.739 | INFO     | __main__:train:123 - Epoch: [44][160/390]	 loss 3.79790	 cls_loss: 0.2763 cluster_loss: 0.8459 sup_con_loss: 0.4637 contrastive_loss: 4.5986 
2024-05-15 13:01:35.416 | INFO     | __main__:train:123 - Epoch: [44][180/390]	 loss 3.99010	 cls_loss: 0.2086 cluster_loss: 0.9149 sup_con_loss: 0.9889 contrastive_loss: 4.5789 
2024-05-15 13:01:48.378 | INFO     | __main__:train:123 - Epoch: [44][200/390]	 loss 4.02487	 cls_loss: 0.2695 cluster_loss: 0.9251 sup_con_loss: 0.9631 contrastive_loss: 4.6033 
2024-05-15 13:02:01.321 | INFO     | __main__:train:123 - Epoch: [44][220/390]	 loss 3.99373	 cls_loss: 0.2432 cluster_loss: 0.9181 sup_con_loss: 0.9458 contrastive_loss: 4.5859 
2024-05-15 13:02:14.410 | INFO     | __main__:train:123 - Epoch: [44][240/390]	 loss 3.81137	 cls_loss: 0.3086 cluster_loss: 0.8260 sup_con_loss: 0.5089 contrastive_loss: 4.5974 
2024-05-15 13:02:27.309 | INFO     | __main__:train:123 - Epoch: [44][260/390]	 loss 3.94024	 cls_loss: 0.2419 cluster_loss: 0.8582 sup_con_loss: 0.9142 contrastive_loss: 4.5812 
2024-05-15 13:02:40.228 | INFO     | __main__:train:123 - Epoch: [44][280/390]	 loss 3.89210	 cls_loss: 0.2303 cluster_loss: 0.8697 sup_con_loss: 0.7609 contrastive_loss: 4.5844 
2024-05-15 13:02:53.176 | INFO     | __main__:train:123 - Epoch: [44][300/390]	 loss 3.94309	 cls_loss: 0.2653 cluster_loss: 0.9357 sup_con_loss: 0.7240 contrastive_loss: 4.5978 
2024-05-15 13:03:06.077 | INFO     | __main__:train:123 - Epoch: [44][320/390]	 loss 3.82948	 cls_loss: 0.2703 cluster_loss: 0.8169 sup_con_loss: 0.6473 contrastive_loss: 4.5805 
2024-05-15 13:03:19.073 | INFO     | __main__:train:123 - Epoch: [44][340/390]	 loss 3.89576	 cls_loss: 0.2029 cluster_loss: 0.8797 sup_con_loss: 0.7717 contrastive_loss: 4.5890 
2024-05-15 13:03:31.980 | INFO     | __main__:train:123 - Epoch: [44][360/390]	 loss 3.85308	 cls_loss: 0.2573 cluster_loss: 0.8577 sup_con_loss: 0.6366 contrastive_loss: 4.5888 
2024-05-15 13:03:44.952 | INFO     | __main__:train:123 - Epoch: [44][380/390]	 loss 3.85563	 cls_loss: 0.2689 cluster_loss: 0.8570 sup_con_loss: 0.6455 contrastive_loss: 4.5824 
2024-05-15 13:03:50.951 | INFO     | __main__:train:126 - Train Epoch: 44 Avg Loss: 3.9321 
2024-05-15 13:03:50.952 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:04:17.560 | INFO     | __main__:train:135 - Train Accuracies: All 0.8143 | Old 0.8401 | New 0.7110
2024-05-15 13:04:22.546 | INFO     | __main__:train:123 - Epoch: [45][0/390]	 loss 3.98705	 cls_loss: 0.2247 cluster_loss: 0.9879 sup_con_loss: 0.8006 contrastive_loss: 4.5939 
2024-05-15 13:04:35.951 | INFO     | __main__:train:123 - Epoch: [45][20/390]	 loss 3.85299	 cls_loss: 0.2190 cluster_loss: 0.8218 sup_con_loss: 0.7537 contrastive_loss: 4.5821 
2024-05-15 13:04:49.006 | INFO     | __main__:train:123 - Epoch: [45][40/390]	 loss 3.97368	 cls_loss: 0.2642 cluster_loss: 0.9035 sup_con_loss: 0.8830 contrastive_loss: 4.5921 
2024-05-15 13:05:01.938 | INFO     | __main__:train:123 - Epoch: [45][60/390]	 loss 3.90421	 cls_loss: 0.2474 cluster_loss: 0.9550 sup_con_loss: 0.5779 contrastive_loss: 4.6071 
2024-05-15 13:05:15.140 | INFO     | __main__:train:123 - Epoch: [45][80/390]	 loss 3.92962	 cls_loss: 0.2126 cluster_loss: 0.9280 sup_con_loss: 0.7603 contrastive_loss: 4.5936 
2024-05-15 13:05:28.439 | INFO     | __main__:train:123 - Epoch: [45][100/390]	 loss 3.74500	 cls_loss: 0.2401 cluster_loss: 0.7324 sup_con_loss: 0.5935 contrastive_loss: 4.5803 
2024-05-15 13:05:41.335 | INFO     | __main__:train:123 - Epoch: [45][120/390]	 loss 3.99682	 cls_loss: 0.2531 cluster_loss: 0.9816 sup_con_loss: 0.8022 contrastive_loss: 4.5991 
2024-05-15 13:05:54.168 | INFO     | __main__:train:123 - Epoch: [45][140/390]	 loss 3.83669	 cls_loss: 0.2230 cluster_loss: 0.8478 sup_con_loss: 0.6320 contrastive_loss: 4.5944 
2024-05-15 13:06:07.374 | INFO     | __main__:train:123 - Epoch: [45][160/390]	 loss 4.02411	 cls_loss: 0.2614 cluster_loss: 1.0533 sup_con_loss: 0.7267 contrastive_loss: 4.6055 
2024-05-15 13:06:20.351 | INFO     | __main__:train:123 - Epoch: [45][180/390]	 loss 4.00172	 cls_loss: 0.2855 cluster_loss: 1.0299 sup_con_loss: 0.7022 contrastive_loss: 4.5948 
2024-05-15 13:06:33.423 | INFO     | __main__:train:123 - Epoch: [45][200/390]	 loss 3.89997	 cls_loss: 0.1919 cluster_loss: 0.9236 sup_con_loss: 0.7017 contrastive_loss: 4.5952 
2024-05-15 13:06:46.494 | INFO     | __main__:train:123 - Epoch: [45][220/390]	 loss 3.91930	 cls_loss: 0.2829 cluster_loss: 0.9553 sup_con_loss: 0.6310 contrastive_loss: 4.5824 
2024-05-15 13:06:59.423 | INFO     | __main__:train:123 - Epoch: [45][240/390]	 loss 3.95393	 cls_loss: 0.2409 cluster_loss: 0.8892 sup_con_loss: 0.8977 contrastive_loss: 4.5807 
2024-05-15 13:07:12.459 | INFO     | __main__:train:123 - Epoch: [45][260/390]	 loss 4.06897	 cls_loss: 0.2366 cluster_loss: 1.0303 sup_con_loss: 0.9275 contrastive_loss: 4.6028 
2024-05-15 13:07:25.570 | INFO     | __main__:train:123 - Epoch: [45][280/390]	 loss 3.88168	 cls_loss: 0.2165 cluster_loss: 0.9017 sup_con_loss: 0.6746 contrastive_loss: 4.5902 
2024-05-15 13:07:38.307 | INFO     | __main__:train:123 - Epoch: [45][300/390]	 loss 3.95282	 cls_loss: 0.2015 cluster_loss: 0.9928 sup_con_loss: 0.7243 contrastive_loss: 4.5899 
2024-05-15 13:07:51.200 | INFO     | __main__:train:123 - Epoch: [45][320/390]	 loss 3.81449	 cls_loss: 0.1995 cluster_loss: 0.8525 sup_con_loss: 0.6092 contrastive_loss: 4.5806 
2024-05-15 13:08:04.390 | INFO     | __main__:train:123 - Epoch: [45][340/390]	 loss 3.96643	 cls_loss: 0.1821 cluster_loss: 0.9395 sup_con_loss: 0.8938 contrastive_loss: 4.5834 
2024-05-15 13:08:17.278 | INFO     | __main__:train:123 - Epoch: [45][360/390]	 loss 3.86802	 cls_loss: 0.2109 cluster_loss: 0.9021 sup_con_loss: 0.6591 contrastive_loss: 4.5803 
2024-05-15 13:08:29.801 | INFO     | __main__:train:123 - Epoch: [45][380/390]	 loss 4.02696	 cls_loss: 0.2708 cluster_loss: 0.9497 sup_con_loss: 0.9641 contrastive_loss: 4.5807 
2024-05-15 13:08:35.773 | INFO     | __main__:train:126 - Train Epoch: 45 Avg Loss: 3.9305 
2024-05-15 13:08:35.774 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:09:02.021 | INFO     | __main__:train:135 - Train Accuracies: All 0.8178 | Old 0.8404 | New 0.7275
2024-05-15 13:09:06.761 | INFO     | __main__:train:123 - Epoch: [46][0/390]	 loss 4.00696	 cls_loss: 0.3337 cluster_loss: 0.9799 sup_con_loss: 0.7712 contrastive_loss: 4.5897 
2024-05-15 13:09:19.784 | INFO     | __main__:train:123 - Epoch: [46][20/390]	 loss 3.92490	 cls_loss: 0.1992 cluster_loss: 0.9199 sup_con_loss: 0.7846 contrastive_loss: 4.5887 
2024-05-15 13:09:32.904 | INFO     | __main__:train:123 - Epoch: [46][40/390]	 loss 3.93528	 cls_loss: 0.1922 cluster_loss: 0.8855 sup_con_loss: 0.8936 contrastive_loss: 4.5841 
2024-05-15 13:09:45.990 | INFO     | __main__:train:123 - Epoch: [46][60/390]	 loss 3.95964	 cls_loss: 0.1987 cluster_loss: 0.9724 sup_con_loss: 0.7866 contrastive_loss: 4.5888 
2024-05-15 13:09:59.349 | INFO     | __main__:train:123 - Epoch: [46][80/390]	 loss 4.05342	 cls_loss: 0.2136 cluster_loss: 1.0186 sup_con_loss: 0.9523 contrastive_loss: 4.5896 
2024-05-15 13:10:12.270 | INFO     | __main__:train:123 - Epoch: [46][100/390]	 loss 4.08733	 cls_loss: 0.1907 cluster_loss: 1.0717 sup_con_loss: 0.9681 contrastive_loss: 4.5925 
2024-05-15 13:10:25.484 | INFO     | __main__:train:123 - Epoch: [46][120/390]	 loss 3.88183	 cls_loss: 0.2477 cluster_loss: 0.8557 sup_con_loss: 0.7312 contrastive_loss: 4.5893 
2024-05-15 13:10:38.529 | INFO     | __main__:train:123 - Epoch: [46][140/390]	 loss 3.94225	 cls_loss: 0.2811 cluster_loss: 0.9224 sup_con_loss: 0.7456 contrastive_loss: 4.5898 
2024-05-15 13:10:51.777 | INFO     | __main__:train:123 - Epoch: [46][160/390]	 loss 3.92508	 cls_loss: 0.3123 cluster_loss: 0.8376 sup_con_loss: 0.8294 contrastive_loss: 4.5862 
2024-05-15 13:11:04.720 | INFO     | __main__:train:123 - Epoch: [46][180/390]	 loss 3.99902	 cls_loss: 0.2260 cluster_loss: 0.9634 sup_con_loss: 0.8938 contrastive_loss: 4.5860 
2024-05-15 13:11:17.641 | INFO     | __main__:train:123 - Epoch: [46][200/390]	 loss 3.96918	 cls_loss: 0.3201 cluster_loss: 0.8749 sup_con_loss: 0.8907 contrastive_loss: 4.5795 
2024-05-15 13:11:30.657 | INFO     | __main__:train:123 - Epoch: [46][220/390]	 loss 3.80792	 cls_loss: 0.1924 cluster_loss: 0.8054 sup_con_loss: 0.6787 contrastive_loss: 4.5839 
2024-05-15 13:11:43.627 | INFO     | __main__:train:123 - Epoch: [46][240/390]	 loss 4.03832	 cls_loss: 0.2344 cluster_loss: 0.9713 sup_con_loss: 0.9678 contrastive_loss: 4.5942 
2024-05-15 13:11:56.644 | INFO     | __main__:train:123 - Epoch: [46][260/390]	 loss 4.07213	 cls_loss: 0.1711 cluster_loss: 1.0326 sup_con_loss: 1.0344 contrastive_loss: 4.5831 
2024-05-15 13:12:09.624 | INFO     | __main__:train:123 - Epoch: [46][280/390]	 loss 3.88468	 cls_loss: 0.2220 cluster_loss: 0.9110 sup_con_loss: 0.6533 contrastive_loss: 4.5940 
2024-05-15 13:12:22.653 | INFO     | __main__:train:123 - Epoch: [46][300/390]	 loss 3.94120	 cls_loss: 0.2053 cluster_loss: 0.9949 sup_con_loss: 0.6647 contrastive_loss: 4.6000 
2024-05-15 13:12:35.840 | INFO     | __main__:train:123 - Epoch: [46][320/390]	 loss 3.88900	 cls_loss: 0.2147 cluster_loss: 0.8633 sup_con_loss: 0.7748 contrastive_loss: 4.5869 
2024-05-15 13:12:48.756 | INFO     | __main__:train:123 - Epoch: [46][340/390]	 loss 3.98684	 cls_loss: 0.2707 cluster_loss: 0.9994 sup_con_loss: 0.7320 contrastive_loss: 4.5943 
2024-05-15 13:13:01.794 | INFO     | __main__:train:123 - Epoch: [46][360/390]	 loss 3.94792	 cls_loss: 0.2409 cluster_loss: 0.9575 sup_con_loss: 0.7377 contrastive_loss: 4.5893 
2024-05-15 13:13:14.533 | INFO     | __main__:train:123 - Epoch: [46][380/390]	 loss 3.94407	 cls_loss: 0.1958 cluster_loss: 0.9611 sup_con_loss: 0.7513 contrastive_loss: 4.5968 
2024-05-15 13:13:20.523 | INFO     | __main__:train:126 - Train Epoch: 46 Avg Loss: 3.9262 
2024-05-15 13:13:20.524 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:13:47.265 | INFO     | __main__:train:135 - Train Accuracies: All 0.8129 | Old 0.8359 | New 0.7210
2024-05-15 13:13:50.388 | INFO     | __main__:train:123 - Epoch: [47][0/390]	 loss 3.89351	 cls_loss: 0.2237 cluster_loss: 0.9276 sup_con_loss: 0.6597 contrastive_loss: 4.5867 
2024-05-15 13:14:05.661 | INFO     | __main__:train:123 - Epoch: [47][20/390]	 loss 3.74865	 cls_loss: 0.1840 cluster_loss: 0.8360 sup_con_loss: 0.4712 contrastive_loss: 4.5783 
2024-05-15 13:14:18.351 | INFO     | __main__:train:123 - Epoch: [47][40/390]	 loss 3.96992	 cls_loss: 0.1974 cluster_loss: 0.9241 sup_con_loss: 0.9185 contrastive_loss: 4.5826 
2024-05-15 13:14:31.160 | INFO     | __main__:train:123 - Epoch: [47][60/390]	 loss 4.02646	 cls_loss: 0.2366 cluster_loss: 0.9875 sup_con_loss: 0.9013 contrastive_loss: 4.5944 
2024-05-15 13:14:44.295 | INFO     | __main__:train:123 - Epoch: [47][80/390]	 loss 3.99678	 cls_loss: 0.1681 cluster_loss: 1.0096 sup_con_loss: 0.8538 contrastive_loss: 4.5890 
2024-05-15 13:14:57.378 | INFO     | __main__:train:123 - Epoch: [47][100/390]	 loss 4.02826	 cls_loss: 0.2072 cluster_loss: 1.0044 sup_con_loss: 0.9104 contrastive_loss: 4.5911 
2024-05-15 13:15:10.583 | INFO     | __main__:train:123 - Epoch: [47][120/390]	 loss 3.83464	 cls_loss: 0.2032 cluster_loss: 0.8606 sup_con_loss: 0.6326 contrastive_loss: 4.5888 
2024-05-15 13:15:23.660 | INFO     | __main__:train:123 - Epoch: [47][140/390]	 loss 3.81395	 cls_loss: 0.1881 cluster_loss: 0.8215 sup_con_loss: 0.6720 contrastive_loss: 4.5830 
2024-05-15 13:15:36.834 | INFO     | __main__:train:123 - Epoch: [47][160/390]	 loss 3.89406	 cls_loss: 0.2396 cluster_loss: 0.8194 sup_con_loss: 0.8562 contrastive_loss: 4.5814 
2024-05-15 13:15:49.816 | INFO     | __main__:train:123 - Epoch: [47][180/390]	 loss 3.87854	 cls_loss: 0.2211 cluster_loss: 0.8715 sup_con_loss: 0.7156 contrastive_loss: 4.5911 
2024-05-15 13:16:02.769 | INFO     | __main__:train:123 - Epoch: [47][200/390]	 loss 3.77221	 cls_loss: 0.2609 cluster_loss: 0.8052 sup_con_loss: 0.4871 contrastive_loss: 4.5954 
2024-05-15 13:16:15.677 | INFO     | __main__:train:123 - Epoch: [47][220/390]	 loss 3.85259	 cls_loss: 0.1623 cluster_loss: 0.8867 sup_con_loss: 0.6726 contrastive_loss: 4.5909 
2024-05-15 13:16:28.704 | INFO     | __main__:train:123 - Epoch: [47][240/390]	 loss 3.84492	 cls_loss: 0.2027 cluster_loss: 0.8799 sup_con_loss: 0.6434 contrastive_loss: 4.5798 
2024-05-15 13:16:41.661 | INFO     | __main__:train:123 - Epoch: [47][260/390]	 loss 4.05472	 cls_loss: 0.2575 cluster_loss: 1.0598 sup_con_loss: 0.8127 contrastive_loss: 4.6019 
2024-05-15 13:16:54.752 | INFO     | __main__:train:123 - Epoch: [47][280/390]	 loss 3.94177	 cls_loss: 0.2774 cluster_loss: 0.8996 sup_con_loss: 0.7954 contrastive_loss: 4.5870 
2024-05-15 13:17:08.037 | INFO     | __main__:train:123 - Epoch: [47][300/390]	 loss 3.76775	 cls_loss: 0.1919 cluster_loss: 0.7650 sup_con_loss: 0.6490 contrastive_loss: 4.5788 
2024-05-15 13:17:20.906 | INFO     | __main__:train:123 - Epoch: [47][320/390]	 loss 3.83543	 cls_loss: 0.2136 cluster_loss: 0.7734 sup_con_loss: 0.8007 contrastive_loss: 4.5811 
2024-05-15 13:17:33.889 | INFO     | __main__:train:123 - Epoch: [47][340/390]	 loss 3.92844	 cls_loss: 0.2118 cluster_loss: 0.8965 sup_con_loss: 0.8302 contrastive_loss: 4.5862 
2024-05-15 13:17:46.730 | INFO     | __main__:train:123 - Epoch: [47][360/390]	 loss 3.97725	 cls_loss: 0.1934 cluster_loss: 0.9935 sup_con_loss: 0.8171 contrastive_loss: 4.5811 
2024-05-15 13:17:59.392 | INFO     | __main__:train:123 - Epoch: [47][380/390]	 loss 3.91644	 cls_loss: 0.2407 cluster_loss: 0.9503 sup_con_loss: 0.6544 contrastive_loss: 4.5930 
2024-05-15 13:18:05.418 | INFO     | __main__:train:126 - Train Epoch: 47 Avg Loss: 3.9224 
2024-05-15 13:18:05.419 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:18:32.093 | INFO     | __main__:train:135 - Train Accuracies: All 0.8112 | Old 0.8346 | New 0.7175
2024-05-15 13:18:37.652 | INFO     | __main__:train:123 - Epoch: [48][0/390]	 loss 3.99417	 cls_loss: 0.2967 cluster_loss: 0.9225 sup_con_loss: 0.8878 contrastive_loss: 4.5846 
2024-05-15 13:18:50.746 | INFO     | __main__:train:123 - Epoch: [48][20/390]	 loss 3.86093	 cls_loss: 0.2876 cluster_loss: 0.8169 sup_con_loss: 0.7144 contrastive_loss: 4.5834 
2024-05-15 13:19:03.836 | INFO     | __main__:train:123 - Epoch: [48][40/390]	 loss 3.96857	 cls_loss: 0.2629 cluster_loss: 0.9423 sup_con_loss: 0.8074 contrastive_loss: 4.5869 
2024-05-15 13:19:17.072 | INFO     | __main__:train:123 - Epoch: [48][60/390]	 loss 4.00029	 cls_loss: 0.2524 cluster_loss: 0.9344 sup_con_loss: 0.9146 contrastive_loss: 4.5914 
2024-05-15 13:19:30.216 | INFO     | __main__:train:123 - Epoch: [48][80/390]	 loss 3.90086	 cls_loss: 0.2436 cluster_loss: 0.8279 sup_con_loss: 0.8412 contrastive_loss: 4.5894 
2024-05-15 13:19:43.455 | INFO     | __main__:train:123 - Epoch: [48][100/390]	 loss 3.84974	 cls_loss: 0.1780 cluster_loss: 0.8456 sup_con_loss: 0.7548 contrastive_loss: 4.5748 
2024-05-15 13:19:56.649 | INFO     | __main__:train:123 - Epoch: [48][120/390]	 loss 3.82713	 cls_loss: 0.1608 cluster_loss: 0.8307 sup_con_loss: 0.7204 contrastive_loss: 4.5827 
2024-05-15 13:20:09.709 | INFO     | __main__:train:123 - Epoch: [48][140/390]	 loss 3.87960	 cls_loss: 0.1644 cluster_loss: 0.9109 sup_con_loss: 0.6997 contrastive_loss: 4.5925 
2024-05-15 13:20:22.802 | INFO     | __main__:train:123 - Epoch: [48][160/390]	 loss 3.83575	 cls_loss: 0.1890 cluster_loss: 0.8592 sup_con_loss: 0.6566 contrastive_loss: 4.5866 
2024-05-15 13:20:35.828 | INFO     | __main__:train:123 - Epoch: [48][180/390]	 loss 3.95546	 cls_loss: 0.2099 cluster_loss: 0.9929 sup_con_loss: 0.7094 contrastive_loss: 4.5974 
2024-05-15 13:20:48.878 | INFO     | __main__:train:123 - Epoch: [48][200/390]	 loss 3.93536	 cls_loss: 0.1980 cluster_loss: 0.9353 sup_con_loss: 0.7980 contrastive_loss: 4.5828 
2024-05-15 13:21:02.023 | INFO     | __main__:train:123 - Epoch: [48][220/390]	 loss 3.89162	 cls_loss: 0.1850 cluster_loss: 0.8705 sup_con_loss: 0.7877 contrastive_loss: 4.5928 
2024-05-15 13:21:14.791 | INFO     | __main__:train:123 - Epoch: [48][240/390]	 loss 3.90182	 cls_loss: 0.1713 cluster_loss: 0.8805 sup_con_loss: 0.8316 contrastive_loss: 4.5822 
2024-05-15 13:21:27.596 | INFO     | __main__:train:123 - Epoch: [48][260/390]	 loss 3.90414	 cls_loss: 0.2059 cluster_loss: 0.9116 sup_con_loss: 0.7410 contrastive_loss: 4.5850 
2024-05-15 13:21:40.778 | INFO     | __main__:train:123 - Epoch: [48][280/390]	 loss 3.94461	 cls_loss: 0.2032 cluster_loss: 0.9657 sup_con_loss: 0.7429 contrastive_loss: 4.5936 
2024-05-15 13:21:53.979 | INFO     | __main__:train:123 - Epoch: [48][300/390]	 loss 3.91874	 cls_loss: 0.2290 cluster_loss: 0.9238 sup_con_loss: 0.7278 contrastive_loss: 4.5898 
2024-05-15 13:22:06.903 | INFO     | __main__:train:123 - Epoch: [48][320/390]	 loss 3.90749	 cls_loss: 0.1943 cluster_loss: 0.9134 sup_con_loss: 0.7465 contrastive_loss: 4.5916 
2024-05-15 13:22:20.019 | INFO     | __main__:train:123 - Epoch: [48][340/390]	 loss 4.04018	 cls_loss: 0.1904 cluster_loss: 0.9946 sup_con_loss: 0.9879 contrastive_loss: 4.5865 
2024-05-15 13:22:33.080 | INFO     | __main__:train:123 - Epoch: [48][360/390]	 loss 4.03300	 cls_loss: 0.2342 cluster_loss: 0.9435 sup_con_loss: 0.9949 contrastive_loss: 4.5993 
2024-05-15 13:22:45.507 | INFO     | __main__:train:123 - Epoch: [48][380/390]	 loss 3.86930	 cls_loss: 0.2318 cluster_loss: 0.8401 sup_con_loss: 0.7624 contrastive_loss: 4.5774 
2024-05-15 13:22:51.569 | INFO     | __main__:train:126 - Train Epoch: 48 Avg Loss: 3.9162 
2024-05-15 13:22:51.571 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:23:18.460 | INFO     | __main__:train:135 - Train Accuracies: All 0.8144 | Old 0.8376 | New 0.7215
2024-05-15 13:23:23.136 | INFO     | __main__:train:123 - Epoch: [49][0/390]	 loss 3.84317	 cls_loss: 0.1789 cluster_loss: 0.8802 sup_con_loss: 0.6526 contrastive_loss: 4.5847 
2024-05-15 13:23:36.539 | INFO     | __main__:train:123 - Epoch: [49][20/390]	 loss 3.88752	 cls_loss: 0.2289 cluster_loss: 0.8577 sup_con_loss: 0.7507 contrastive_loss: 4.5956 
2024-05-15 13:23:49.561 | INFO     | __main__:train:123 - Epoch: [49][40/390]	 loss 3.85193	 cls_loss: 0.1795 cluster_loss: 0.8551 sup_con_loss: 0.7196 contrastive_loss: 4.5869 
2024-05-15 13:24:02.797 | INFO     | __main__:train:123 - Epoch: [49][60/390]	 loss 3.96497	 cls_loss: 0.1856 cluster_loss: 1.0080 sup_con_loss: 0.7451 contrastive_loss: 4.5908 
2024-05-15 13:24:15.641 | INFO     | __main__:train:123 - Epoch: [49][80/390]	 loss 4.00721	 cls_loss: 0.2747 cluster_loss: 0.9357 sup_con_loss: 0.9144 contrastive_loss: 4.5890 
2024-05-15 13:24:28.675 | INFO     | __main__:train:123 - Epoch: [49][100/390]	 loss 3.97390	 cls_loss: 0.2571 cluster_loss: 0.8978 sup_con_loss: 0.9056 contrastive_loss: 4.5899 
2024-05-15 13:24:41.869 | INFO     | __main__:train:123 - Epoch: [49][120/390]	 loss 3.86471	 cls_loss: 0.1953 cluster_loss: 0.8653 sup_con_loss: 0.7286 contrastive_loss: 4.5829 
2024-05-15 13:24:54.803 | INFO     | __main__:train:123 - Epoch: [49][140/390]	 loss 3.95362	 cls_loss: 0.2008 cluster_loss: 0.9433 sup_con_loss: 0.8342 contrastive_loss: 4.5819 
2024-05-15 13:25:07.720 | INFO     | __main__:train:123 - Epoch: [49][160/390]	 loss 3.86305	 cls_loss: 0.2228 cluster_loss: 0.8448 sup_con_loss: 0.7242 contrastive_loss: 4.5884 
2024-05-15 13:25:20.548 | INFO     | __main__:train:123 - Epoch: [49][180/390]	 loss 3.84003	 cls_loss: 0.2279 cluster_loss: 0.8907 sup_con_loss: 0.5667 contrastive_loss: 4.5891 
2024-05-15 13:25:33.258 | INFO     | __main__:train:123 - Epoch: [49][200/390]	 loss 3.83257	 cls_loss: 0.1763 cluster_loss: 0.9360 sup_con_loss: 0.5112 contrastive_loss: 4.5900 
2024-05-15 13:25:46.110 | INFO     | __main__:train:123 - Epoch: [49][220/390]	 loss 3.85963	 cls_loss: 0.2251 cluster_loss: 0.8132 sup_con_loss: 0.7863 contrastive_loss: 4.5801 
2024-05-15 13:25:59.393 | INFO     | __main__:train:123 - Epoch: [49][240/390]	 loss 3.95143	 cls_loss: 0.1921 cluster_loss: 0.9228 sup_con_loss: 0.8688 contrastive_loss: 4.5851 
2024-05-15 13:26:12.441 | INFO     | __main__:train:123 - Epoch: [49][260/390]	 loss 3.95459	 cls_loss: 0.2727 cluster_loss: 0.9444 sup_con_loss: 0.7480 contrastive_loss: 4.5900 
2024-05-15 13:26:25.597 | INFO     | __main__:train:123 - Epoch: [49][280/390]	 loss 3.87464	 cls_loss: 0.2920 cluster_loss: 0.8944 sup_con_loss: 0.5935 contrastive_loss: 4.5898 
2024-05-15 13:26:38.457 | INFO     | __main__:train:123 - Epoch: [49][300/390]	 loss 3.88729	 cls_loss: 0.2760 cluster_loss: 0.8200 sup_con_loss: 0.7952 contrastive_loss: 4.5836 
2024-05-15 13:26:51.416 | INFO     | __main__:train:123 - Epoch: [49][320/390]	 loss 4.00660	 cls_loss: 0.2650 cluster_loss: 0.9592 sup_con_loss: 0.8683 contrastive_loss: 4.5946 
2024-05-15 13:27:04.423 | INFO     | __main__:train:123 - Epoch: [49][340/390]	 loss 3.89301	 cls_loss: 0.2029 cluster_loss: 0.8617 sup_con_loss: 0.8202 contrastive_loss: 4.5767 
2024-05-15 13:27:17.306 | INFO     | __main__:train:123 - Epoch: [49][360/390]	 loss 4.04956	 cls_loss: 0.1615 cluster_loss: 1.0716 sup_con_loss: 0.8820 contrastive_loss: 4.5966 
2024-05-15 13:27:30.002 | INFO     | __main__:train:123 - Epoch: [49][380/390]	 loss 3.91432	 cls_loss: 0.2089 cluster_loss: 0.9194 sup_con_loss: 0.7499 contrastive_loss: 4.5863 
2024-05-15 13:27:35.977 | INFO     | __main__:train:126 - Train Epoch: 49 Avg Loss: 3.9115 
2024-05-15 13:27:35.978 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:28:02.728 | INFO     | __main__:train:135 - Train Accuracies: All 0.8137 | Old 0.8377 | New 0.7175
2024-05-15 13:28:07.734 | INFO     | __main__:train:123 - Epoch: [50][0/390]	 loss 3.99305	 cls_loss: 0.1907 cluster_loss: 1.0142 sup_con_loss: 0.8138 contrastive_loss: 4.5881 
2024-05-15 13:28:21.289 | INFO     | __main__:train:123 - Epoch: [50][20/390]	 loss 3.92154	 cls_loss: 0.2018 cluster_loss: 0.9169 sup_con_loss: 0.7778 contrastive_loss: 4.5888 
2024-05-15 13:28:34.379 | INFO     | __main__:train:123 - Epoch: [50][40/390]	 loss 3.87520	 cls_loss: 0.2224 cluster_loss: 0.8396 sup_con_loss: 0.7768 contrastive_loss: 4.5843 
2024-05-15 13:28:47.324 | INFO     | __main__:train:123 - Epoch: [50][60/390]	 loss 3.96073	 cls_loss: 0.2382 cluster_loss: 0.9871 sup_con_loss: 0.7120 contrastive_loss: 4.5946 
2024-05-15 13:29:00.026 | INFO     | __main__:train:123 - Epoch: [50][80/390]	 loss 3.92991	 cls_loss: 0.2380 cluster_loss: 0.9693 sup_con_loss: 0.6759 contrastive_loss: 4.5846 
2024-05-15 13:29:13.211 | INFO     | __main__:train:123 - Epoch: [50][100/390]	 loss 3.93220	 cls_loss: 0.1849 cluster_loss: 0.9861 sup_con_loss: 0.7035 contrastive_loss: 4.5850 
2024-05-15 13:29:26.011 | INFO     | __main__:train:123 - Epoch: [50][120/390]	 loss 3.92623	 cls_loss: 0.1856 cluster_loss: 0.9631 sup_con_loss: 0.7306 contrastive_loss: 4.5839 
2024-05-15 13:29:38.996 | INFO     | __main__:train:123 - Epoch: [50][140/390]	 loss 3.89235	 cls_loss: 0.2103 cluster_loss: 0.8314 sup_con_loss: 0.8542 contrastive_loss: 4.5836 
2024-05-15 13:29:52.323 | INFO     | __main__:train:123 - Epoch: [50][160/390]	 loss 3.86016	 cls_loss: 0.2261 cluster_loss: 0.8879 sup_con_loss: 0.6502 contrastive_loss: 4.5789 
2024-05-15 13:30:05.227 | INFO     | __main__:train:123 - Epoch: [50][180/390]	 loss 3.80976	 cls_loss: 0.2050 cluster_loss: 0.8545 sup_con_loss: 0.5435 contrastive_loss: 4.6037 
2024-05-15 13:30:18.135 | INFO     | __main__:train:123 - Epoch: [50][200/390]	 loss 3.95650	 cls_loss: 0.2035 cluster_loss: 0.9651 sup_con_loss: 0.7909 contrastive_loss: 4.5864 
2024-05-15 13:30:31.172 | INFO     | __main__:train:123 - Epoch: [50][220/390]	 loss 3.97818	 cls_loss: 0.1667 cluster_loss: 1.0783 sup_con_loss: 0.6769 contrastive_loss: 4.5877 
2024-05-15 13:30:44.145 | INFO     | __main__:train:123 - Epoch: [50][240/390]	 loss 3.91570	 cls_loss: 0.2849 cluster_loss: 0.9519 sup_con_loss: 0.6163 contrastive_loss: 4.5870 
2024-05-15 13:30:57.234 | INFO     | __main__:train:123 - Epoch: [50][260/390]	 loss 3.88685	 cls_loss: 0.1929 cluster_loss: 0.8550 sup_con_loss: 0.8062 contrastive_loss: 4.5868 
2024-05-15 13:31:10.050 | INFO     | __main__:train:123 - Epoch: [50][280/390]	 loss 3.99630	 cls_loss: 0.2415 cluster_loss: 0.9175 sup_con_loss: 0.9408 contrastive_loss: 4.5940 
2024-05-15 13:31:23.031 | INFO     | __main__:train:123 - Epoch: [50][300/390]	 loss 3.81628	 cls_loss: 0.1869 cluster_loss: 0.8275 sup_con_loss: 0.6713 contrastive_loss: 4.5816 
2024-05-15 13:31:35.844 | INFO     | __main__:train:123 - Epoch: [50][320/390]	 loss 3.99005	 cls_loss: 0.2617 cluster_loss: 1.0012 sup_con_loss: 0.7589 contrastive_loss: 4.5877 
2024-05-15 13:31:48.657 | INFO     | __main__:train:123 - Epoch: [50][340/390]	 loss 3.78071	 cls_loss: 0.2066 cluster_loss: 0.7444 sup_con_loss: 0.7150 contrastive_loss: 4.5758 
2024-05-15 13:32:01.700 | INFO     | __main__:train:123 - Epoch: [50][360/390]	 loss 3.88535	 cls_loss: 0.2116 cluster_loss: 0.8476 sup_con_loss: 0.7980 contrastive_loss: 4.5862 
2024-05-15 13:32:14.519 | INFO     | __main__:train:123 - Epoch: [50][380/390]	 loss 3.98943	 cls_loss: 0.2633 cluster_loss: 0.9451 sup_con_loss: 0.8509 contrastive_loss: 4.5925 
2024-05-15 13:32:20.231 | INFO     | __main__:train:126 - Train Epoch: 50 Avg Loss: 3.9067 
2024-05-15 13:32:20.231 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:32:47.711 | INFO     | __main__:train:135 - Train Accuracies: All 0.8122 | Old 0.8386 | New 0.7065
2024-05-15 13:32:53.161 | INFO     | __main__:train:123 - Epoch: [51][0/390]	 loss 3.87887	 cls_loss: 0.2309 cluster_loss: 0.8918 sup_con_loss: 0.6749 contrastive_loss: 4.5880 
2024-05-15 13:33:06.360 | INFO     | __main__:train:123 - Epoch: [51][20/390]	 loss 3.94837	 cls_loss: 0.2152 cluster_loss: 1.0169 sup_con_loss: 0.6330 contrastive_loss: 4.6008 
2024-05-15 13:33:18.983 | INFO     | __main__:train:123 - Epoch: [51][40/390]	 loss 3.85452	 cls_loss: 0.2839 cluster_loss: 0.8167 sup_con_loss: 0.6970 contrastive_loss: 4.5852 
2024-05-15 13:33:32.057 | INFO     | __main__:train:123 - Epoch: [51][60/390]	 loss 3.83148	 cls_loss: 0.1924 cluster_loss: 0.8399 sup_con_loss: 0.6859 contrastive_loss: 4.5817 
2024-05-15 13:33:45.019 | INFO     | __main__:train:123 - Epoch: [51][80/390]	 loss 3.87928	 cls_loss: 0.1592 cluster_loss: 0.9172 sup_con_loss: 0.6984 contrastive_loss: 4.5891 
2024-05-15 13:33:57.905 | INFO     | __main__:train:123 - Epoch: [51][100/390]	 loss 3.85740	 cls_loss: 0.2219 cluster_loss: 0.8575 sup_con_loss: 0.6963 contrastive_loss: 4.5826 
2024-05-15 13:34:11.022 | INFO     | __main__:train:123 - Epoch: [51][120/390]	 loss 3.83407	 cls_loss: 0.2259 cluster_loss: 0.8200 sup_con_loss: 0.6733 contrastive_loss: 4.5944 
2024-05-15 13:34:23.773 | INFO     | __main__:train:123 - Epoch: [51][140/390]	 loss 3.80279	 cls_loss: 0.2076 cluster_loss: 0.8456 sup_con_loss: 0.5652 contrastive_loss: 4.5887 
2024-05-15 13:34:37.080 | INFO     | __main__:train:123 - Epoch: [51][160/390]	 loss 3.95494	 cls_loss: 0.2154 cluster_loss: 0.9214 sup_con_loss: 0.8617 contrastive_loss: 4.5832 
2024-05-15 13:34:49.893 | INFO     | __main__:train:123 - Epoch: [51][180/390]	 loss 4.00901	 cls_loss: 0.2378 cluster_loss: 0.9671 sup_con_loss: 0.8892 contrastive_loss: 4.5937 
2024-05-15 13:35:02.843 | INFO     | __main__:train:123 - Epoch: [51][200/390]	 loss 3.91060	 cls_loss: 0.2145 cluster_loss: 0.8949 sup_con_loss: 0.7759 contrastive_loss: 4.5881 
2024-05-15 13:35:15.879 | INFO     | __main__:train:123 - Epoch: [51][220/390]	 loss 3.93178	 cls_loss: 0.2351 cluster_loss: 0.9231 sup_con_loss: 0.7772 contrastive_loss: 4.5808 
2024-05-15 13:35:28.865 | INFO     | __main__:train:123 - Epoch: [51][240/390]	 loss 3.90899	 cls_loss: 0.1986 cluster_loss: 0.8870 sup_con_loss: 0.8075 contrastive_loss: 4.5851 
2024-05-15 13:35:41.800 | INFO     | __main__:train:123 - Epoch: [51][260/390]	 loss 3.98953	 cls_loss: 0.2165 cluster_loss: 0.9325 sup_con_loss: 0.9342 contrastive_loss: 4.5856 
2024-05-15 13:35:54.625 | INFO     | __main__:train:123 - Epoch: [51][280/390]	 loss 3.83658	 cls_loss: 0.1989 cluster_loss: 0.8919 sup_con_loss: 0.5947 contrastive_loss: 4.5832 
2024-05-15 13:36:07.792 | INFO     | __main__:train:123 - Epoch: [51][300/390]	 loss 3.97447	 cls_loss: 0.1746 cluster_loss: 0.9467 sup_con_loss: 0.9046 contrastive_loss: 4.5867 
2024-05-15 13:36:21.016 | INFO     | __main__:train:123 - Epoch: [51][320/390]	 loss 3.86237	 cls_loss: 0.1978 cluster_loss: 0.8918 sup_con_loss: 0.6606 contrastive_loss: 4.5881 
2024-05-15 13:36:34.100 | INFO     | __main__:train:123 - Epoch: [51][340/390]	 loss 3.97697	 cls_loss: 0.2195 cluster_loss: 0.9616 sup_con_loss: 0.8482 contrastive_loss: 4.5819 
2024-05-15 13:36:47.072 | INFO     | __main__:train:123 - Epoch: [51][360/390]	 loss 3.85333	 cls_loss: 0.1709 cluster_loss: 0.8677 sup_con_loss: 0.7101 contrastive_loss: 4.5861 
2024-05-15 13:36:59.512 | INFO     | __main__:train:123 - Epoch: [51][380/390]	 loss 3.93448	 cls_loss: 0.2075 cluster_loss: 0.9118 sup_con_loss: 0.8306 contrastive_loss: 4.5822 
2024-05-15 13:37:05.203 | INFO     | __main__:train:126 - Train Epoch: 51 Avg Loss: 3.9068 
2024-05-15 13:37:05.204 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:37:31.521 | INFO     | __main__:train:135 - Train Accuracies: All 0.8113 | Old 0.8361 | New 0.7120
2024-05-15 13:37:37.311 | INFO     | __main__:train:123 - Epoch: [52][0/390]	 loss 4.05488	 cls_loss: 0.2864 cluster_loss: 1.0446 sup_con_loss: 0.8289 contrastive_loss: 4.5932 
2024-05-15 13:37:50.270 | INFO     | __main__:train:123 - Epoch: [52][20/390]	 loss 3.95568	 cls_loss: 0.1649 cluster_loss: 0.9668 sup_con_loss: 0.8225 contrastive_loss: 4.5871 
2024-05-15 13:38:03.220 | INFO     | __main__:train:123 - Epoch: [52][40/390]	 loss 3.95895	 cls_loss: 0.1709 cluster_loss: 0.9530 sup_con_loss: 0.8486 contrastive_loss: 4.5887 
2024-05-15 13:38:16.268 | INFO     | __main__:train:123 - Epoch: [52][60/390]	 loss 3.83530	 cls_loss: 0.2089 cluster_loss: 0.8124 sup_con_loss: 0.7205 contrastive_loss: 4.5876 
2024-05-15 13:38:29.484 | INFO     | __main__:train:123 - Epoch: [52][80/390]	 loss 3.91064	 cls_loss: 0.2176 cluster_loss: 0.8034 sup_con_loss: 0.9622 contrastive_loss: 4.5777 
2024-05-15 13:38:42.473 | INFO     | __main__:train:123 - Epoch: [52][100/390]	 loss 3.89879	 cls_loss: 0.1790 cluster_loss: 0.8975 sup_con_loss: 0.7887 contrastive_loss: 4.5796 
2024-05-15 13:38:55.538 | INFO     | __main__:train:123 - Epoch: [52][120/390]	 loss 4.03160	 cls_loss: 0.2370 cluster_loss: 0.9563 sup_con_loss: 0.9903 contrastive_loss: 4.5853 
2024-05-15 13:39:08.557 | INFO     | __main__:train:123 - Epoch: [52][140/390]	 loss 3.78648	 cls_loss: 0.1901 cluster_loss: 0.8423 sup_con_loss: 0.5398 contrastive_loss: 4.5901 
2024-05-15 13:39:21.732 | INFO     | __main__:train:123 - Epoch: [52][160/390]	 loss 3.91634	 cls_loss: 0.2455 cluster_loss: 0.8870 sup_con_loss: 0.7869 contrastive_loss: 4.5822 
2024-05-15 13:39:34.672 | INFO     | __main__:train:123 - Epoch: [52][180/390]	 loss 3.89159	 cls_loss: 0.2334 cluster_loss: 0.8800 sup_con_loss: 0.7278 contrastive_loss: 4.5895 
2024-05-15 13:39:47.739 | INFO     | __main__:train:123 - Epoch: [52][200/390]	 loss 3.87218	 cls_loss: 0.2062 cluster_loss: 0.8842 sup_con_loss: 0.7047 contrastive_loss: 4.5825 
2024-05-15 13:40:00.834 | INFO     | __main__:train:123 - Epoch: [52][220/390]	 loss 3.85970	 cls_loss: 0.2113 cluster_loss: 0.8522 sup_con_loss: 0.7333 contrastive_loss: 4.5771 
2024-05-15 13:40:13.714 | INFO     | __main__:train:123 - Epoch: [52][240/390]	 loss 3.97012	 cls_loss: 0.2273 cluster_loss: 0.9834 sup_con_loss: 0.7552 contrastive_loss: 4.5954 
2024-05-15 13:40:26.798 | INFO     | __main__:train:123 - Epoch: [52][260/390]	 loss 3.80361	 cls_loss: 0.1927 cluster_loss: 0.7868 sup_con_loss: 0.7114 contrastive_loss: 4.5781 
2024-05-15 13:40:39.782 | INFO     | __main__:train:123 - Epoch: [52][280/390]	 loss 3.99237	 cls_loss: 0.2067 cluster_loss: 0.9990 sup_con_loss: 0.8179 contrastive_loss: 4.5914 
2024-05-15 13:40:52.596 | INFO     | __main__:train:123 - Epoch: [52][300/390]	 loss 3.93301	 cls_loss: 0.2163 cluster_loss: 0.9934 sup_con_loss: 0.6338 contrastive_loss: 4.5996 
2024-05-15 13:41:05.598 | INFO     | __main__:train:123 - Epoch: [52][320/390]	 loss 3.96368	 cls_loss: 0.2149 cluster_loss: 0.9608 sup_con_loss: 0.8071 contrastive_loss: 4.5868 
2024-05-15 13:41:18.503 | INFO     | __main__:train:123 - Epoch: [52][340/390]	 loss 4.00997	 cls_loss: 0.2344 cluster_loss: 0.9881 sup_con_loss: 0.8530 contrastive_loss: 4.5956 
2024-05-15 13:41:31.548 | INFO     | __main__:train:123 - Epoch: [52][360/390]	 loss 3.78606	 cls_loss: 0.1863 cluster_loss: 0.7580 sup_con_loss: 0.7266 contrastive_loss: 4.5752 
2024-05-15 13:41:43.905 | INFO     | __main__:train:123 - Epoch: [52][380/390]	 loss 4.02995	 cls_loss: 0.2363 cluster_loss: 0.9851 sup_con_loss: 0.9384 contrastive_loss: 4.5822 
2024-05-15 13:41:49.706 | INFO     | __main__:train:126 - Train Epoch: 52 Avg Loss: 3.9039 
2024-05-15 13:41:49.708 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:42:16.619 | INFO     | __main__:train:135 - Train Accuracies: All 0.8133 | Old 0.8393 | New 0.7095
2024-05-15 13:42:21.234 | INFO     | __main__:train:123 - Epoch: [53][0/390]	 loss 3.88469	 cls_loss: 0.1732 cluster_loss: 0.8490 sup_con_loss: 0.8398 contrastive_loss: 4.5820 
2024-05-15 13:42:34.512 | INFO     | __main__:train:123 - Epoch: [53][20/390]	 loss 3.88544	 cls_loss: 0.1729 cluster_loss: 0.9016 sup_con_loss: 0.7206 contrastive_loss: 4.5949 
2024-05-15 13:42:47.238 | INFO     | __main__:train:123 - Epoch: [53][40/390]	 loss 3.79546	 cls_loss: 0.1962 cluster_loss: 0.8646 sup_con_loss: 0.5379 contrastive_loss: 4.5792 
2024-05-15 13:43:00.120 | INFO     | __main__:train:123 - Epoch: [53][60/390]	 loss 3.73980	 cls_loss: 0.1610 cluster_loss: 0.7818 sup_con_loss: 0.5623 contrastive_loss: 4.5823 
2024-05-15 13:43:13.000 | INFO     | __main__:train:123 - Epoch: [53][80/390]	 loss 4.00413	 cls_loss: 0.1882 cluster_loss: 1.0306 sup_con_loss: 0.8223 contrastive_loss: 4.5855 
2024-05-15 13:43:26.250 | INFO     | __main__:train:123 - Epoch: [53][100/390]	 loss 3.99210	 cls_loss: 0.2091 cluster_loss: 0.9689 sup_con_loss: 0.8933 contrastive_loss: 4.5792 
2024-05-15 13:43:39.188 | INFO     | __main__:train:123 - Epoch: [53][120/390]	 loss 3.83005	 cls_loss: 0.2142 cluster_loss: 0.8521 sup_con_loss: 0.6361 contrastive_loss: 4.5825 
2024-05-15 13:43:52.180 | INFO     | __main__:train:123 - Epoch: [53][140/390]	 loss 3.80261	 cls_loss: 0.2124 cluster_loss: 0.8068 sup_con_loss: 0.6421 contrastive_loss: 4.5832 
2024-05-15 13:44:05.148 | INFO     | __main__:train:123 - Epoch: [53][160/390]	 loss 3.92302	 cls_loss: 0.2125 cluster_loss: 0.9032 sup_con_loss: 0.8027 contrastive_loss: 4.5855 
2024-05-15 13:44:18.607 | INFO     | __main__:train:123 - Epoch: [53][180/390]	 loss 3.80857	 cls_loss: 0.1908 cluster_loss: 0.8086 sup_con_loss: 0.6848 contrastive_loss: 4.5792 
2024-05-15 13:44:31.798 | INFO     | __main__:train:123 - Epoch: [53][200/390]	 loss 3.85510	 cls_loss: 0.2019 cluster_loss: 0.8651 sup_con_loss: 0.6717 contrastive_loss: 4.5955 
2024-05-15 13:44:45.162 | INFO     | __main__:train:123 - Epoch: [53][220/390]	 loss 3.75983	 cls_loss: 0.1868 cluster_loss: 0.8296 sup_con_loss: 0.5142 contrastive_loss: 4.5773 
2024-05-15 13:44:58.347 | INFO     | __main__:train:123 - Epoch: [53][240/390]	 loss 4.00737	 cls_loss: 0.2405 cluster_loss: 0.9048 sup_con_loss: 1.0087 contrastive_loss: 4.5877 
2024-05-15 13:45:11.444 | INFO     | __main__:train:123 - Epoch: [53][260/390]	 loss 3.94656	 cls_loss: 0.1922 cluster_loss: 0.9156 sup_con_loss: 0.8813 contrastive_loss: 4.5780 
2024-05-15 13:45:24.084 | INFO     | __main__:train:123 - Epoch: [53][280/390]	 loss 3.84245	 cls_loss: 0.1971 cluster_loss: 0.8957 sup_con_loss: 0.6043 contrastive_loss: 4.5843 
2024-05-15 13:45:37.093 | INFO     | __main__:train:123 - Epoch: [53][300/390]	 loss 3.77506	 cls_loss: 0.2205 cluster_loss: 0.8368 sup_con_loss: 0.4899 contrastive_loss: 4.5884 
2024-05-15 13:45:50.183 | INFO     | __main__:train:123 - Epoch: [53][320/390]	 loss 3.86829	 cls_loss: 0.2396 cluster_loss: 0.8731 sup_con_loss: 0.6875 contrastive_loss: 4.5789 
2024-05-15 13:46:03.408 | INFO     | __main__:train:123 - Epoch: [53][340/390]	 loss 3.87727	 cls_loss: 0.2490 cluster_loss: 0.8687 sup_con_loss: 0.7072 contrastive_loss: 4.5815 
2024-05-15 13:46:16.473 | INFO     | __main__:train:123 - Epoch: [53][360/390]	 loss 3.95501	 cls_loss: 0.1415 cluster_loss: 0.9302 sup_con_loss: 0.9253 contrastive_loss: 4.5800 
2024-05-15 13:46:28.848 | INFO     | __main__:train:123 - Epoch: [53][380/390]	 loss 3.87585	 cls_loss: 0.2467 cluster_loss: 0.8136 sup_con_loss: 0.7955 contrastive_loss: 4.5881 
2024-05-15 13:46:34.581 | INFO     | __main__:train:126 - Train Epoch: 53 Avg Loss: 3.8894 
2024-05-15 13:46:34.582 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:47:01.600 | INFO     | __main__:train:135 - Train Accuracies: All 0.8145 | Old 0.8383 | New 0.7195
2024-05-15 13:47:05.532 | INFO     | __main__:train:123 - Epoch: [54][0/390]	 loss 3.85995	 cls_loss: 0.1685 cluster_loss: 0.8553 sup_con_loss: 0.7501 contrastive_loss: 4.5884 
2024-05-15 13:47:20.323 | INFO     | __main__:train:123 - Epoch: [54][20/390]	 loss 3.74517	 cls_loss: 0.1805 cluster_loss: 0.7812 sup_con_loss: 0.5672 contrastive_loss: 4.5780 
2024-05-15 13:47:33.179 | INFO     | __main__:train:123 - Epoch: [54][40/390]	 loss 3.89109	 cls_loss: 0.1826 cluster_loss: 0.9094 sup_con_loss: 0.7187 contrastive_loss: 4.5915 
2024-05-15 13:47:46.211 | INFO     | __main__:train:123 - Epoch: [54][60/390]	 loss 4.00849	 cls_loss: 0.2037 cluster_loss: 0.9827 sup_con_loss: 0.8945 contrastive_loss: 4.5928 
2024-05-15 13:47:58.921 | INFO     | __main__:train:123 - Epoch: [54][80/390]	 loss 3.85875	 cls_loss: 0.1798 cluster_loss: 0.8968 sup_con_loss: 0.6779 contrastive_loss: 4.5779 
2024-05-15 13:48:11.862 | INFO     | __main__:train:123 - Epoch: [54][100/390]	 loss 3.96682	 cls_loss: 0.2381 cluster_loss: 0.9185 sup_con_loss: 0.8783 contrastive_loss: 4.5832 
2024-05-15 13:48:25.104 | INFO     | __main__:train:123 - Epoch: [54][120/390]	 loss 3.88575	 cls_loss: 0.1786 cluster_loss: 0.8671 sup_con_loss: 0.7880 contrastive_loss: 4.5905 
2024-05-15 13:48:37.874 | INFO     | __main__:train:123 - Epoch: [54][140/390]	 loss 3.91258	 cls_loss: 0.2163 cluster_loss: 0.8451 sup_con_loss: 0.8929 contrastive_loss: 4.5770 
2024-05-15 13:48:50.772 | INFO     | __main__:train:123 - Epoch: [54][160/390]	 loss 3.86595	 cls_loss: 0.1525 cluster_loss: 0.8888 sup_con_loss: 0.7302 contrastive_loss: 4.5835 
2024-05-15 13:49:03.935 | INFO     | __main__:train:123 - Epoch: [54][180/390]	 loss 3.81437	 cls_loss: 0.1893 cluster_loss: 0.8293 sup_con_loss: 0.6692 contrastive_loss: 4.5766 
2024-05-15 13:49:17.023 | INFO     | __main__:train:123 - Epoch: [54][200/390]	 loss 3.99886	 cls_loss: 0.2013 cluster_loss: 0.9490 sup_con_loss: 0.9594 contrastive_loss: 4.5781 
2024-05-15 13:49:30.353 | INFO     | __main__:train:123 - Epoch: [54][220/390]	 loss 3.91455	 cls_loss: 0.2551 cluster_loss: 0.8791 sup_con_loss: 0.7850 contrastive_loss: 4.5833 
2024-05-15 13:49:43.409 | INFO     | __main__:train:123 - Epoch: [54][240/390]	 loss 3.88058	 cls_loss: 0.2009 cluster_loss: 0.8175 sup_con_loss: 0.8588 contrastive_loss: 4.5820 
2024-05-15 13:49:56.553 | INFO     | __main__:train:123 - Epoch: [54][260/390]	 loss 3.84963	 cls_loss: 0.2195 cluster_loss: 0.8967 sup_con_loss: 0.5998 contrastive_loss: 4.5847 
2024-05-15 13:50:09.496 | INFO     | __main__:train:123 - Epoch: [54][280/390]	 loss 3.92063	 cls_loss: 0.2137 cluster_loss: 0.9408 sup_con_loss: 0.7201 contrastive_loss: 4.5882 
2024-05-15 13:50:22.608 | INFO     | __main__:train:123 - Epoch: [54][300/390]	 loss 3.93821	 cls_loss: 0.2329 cluster_loss: 0.9226 sup_con_loss: 0.7834 contrastive_loss: 4.5889 
2024-05-15 13:50:35.737 | INFO     | __main__:train:123 - Epoch: [54][320/390]	 loss 3.80129	 cls_loss: 0.2101 cluster_loss: 0.8400 sup_con_loss: 0.5667 contrastive_loss: 4.5898 
2024-05-15 13:50:48.778 | INFO     | __main__:train:123 - Epoch: [54][340/390]	 loss 3.83398	 cls_loss: 0.1718 cluster_loss: 0.8953 sup_con_loss: 0.6127 contrastive_loss: 4.5808 
2024-05-15 13:51:01.966 | INFO     | __main__:train:123 - Epoch: [54][360/390]	 loss 3.85217	 cls_loss: 0.1969 cluster_loss: 0.8304 sup_con_loss: 0.7695 contrastive_loss: 4.5757 
2024-05-15 13:51:14.514 | INFO     | __main__:train:123 - Epoch: [54][380/390]	 loss 3.90741	 cls_loss: 0.1847 cluster_loss: 0.9341 sup_con_loss: 0.7241 contrastive_loss: 4.5880 
2024-05-15 13:51:20.255 | INFO     | __main__:train:126 - Train Epoch: 54 Avg Loss: 3.8934 
2024-05-15 13:51:20.255 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:51:47.280 | INFO     | __main__:train:135 - Train Accuracies: All 0.8126 | Old 0.8367 | New 0.7160
2024-05-15 13:51:52.839 | INFO     | __main__:train:123 - Epoch: [55][0/390]	 loss 3.89742	 cls_loss: 0.2084 cluster_loss: 0.9062 sup_con_loss: 0.7318 contrastive_loss: 4.5836 
2024-05-15 13:52:05.784 | INFO     | __main__:train:123 - Epoch: [55][20/390]	 loss 3.92606	 cls_loss: 0.1870 cluster_loss: 0.8802 sup_con_loss: 0.8797 contrastive_loss: 4.5855 
2024-05-15 13:52:18.535 | INFO     | __main__:train:123 - Epoch: [55][40/390]	 loss 3.89144	 cls_loss: 0.1548 cluster_loss: 0.8943 sup_con_loss: 0.8000 contrastive_loss: 4.5784 
2024-05-15 13:52:31.185 | INFO     | __main__:train:123 - Epoch: [55][60/390]	 loss 3.97954	 cls_loss: 0.2044 cluster_loss: 0.9595 sup_con_loss: 0.8639 contrastive_loss: 4.5876 
2024-05-15 13:52:44.298 | INFO     | __main__:train:123 - Epoch: [55][80/390]	 loss 3.98083	 cls_loss: 0.1761 cluster_loss: 1.0050 sup_con_loss: 0.8154 contrastive_loss: 4.5854 
2024-05-15 13:52:57.357 | INFO     | __main__:train:123 - Epoch: [55][100/390]	 loss 3.97822	 cls_loss: 0.1638 cluster_loss: 1.0108 sup_con_loss: 0.8045 contrastive_loss: 4.5881 
2024-05-15 13:53:10.215 | INFO     | __main__:train:123 - Epoch: [55][120/390]	 loss 3.89120	 cls_loss: 0.1491 cluster_loss: 0.9147 sup_con_loss: 0.7608 contrastive_loss: 4.5818 
2024-05-15 13:53:23.208 | INFO     | __main__:train:123 - Epoch: [55][140/390]	 loss 4.03358	 cls_loss: 0.2670 cluster_loss: 1.0592 sup_con_loss: 0.7621 contrastive_loss: 4.5922 
2024-05-15 13:53:36.293 | INFO     | __main__:train:123 - Epoch: [55][160/390]	 loss 4.01302	 cls_loss: 0.1981 cluster_loss: 0.9891 sup_con_loss: 0.8998 contrastive_loss: 4.5937 
2024-05-15 13:53:49.527 | INFO     | __main__:train:123 - Epoch: [55][180/390]	 loss 3.83111	 cls_loss: 0.1344 cluster_loss: 0.8476 sup_con_loss: 0.7206 contrastive_loss: 4.5860 
2024-05-15 13:54:02.367 | INFO     | __main__:train:123 - Epoch: [55][200/390]	 loss 3.97515	 cls_loss: 0.1966 cluster_loss: 0.9608 sup_con_loss: 0.8706 contrastive_loss: 4.5802 
2024-05-15 13:54:15.213 | INFO     | __main__:train:123 - Epoch: [55][220/390]	 loss 3.87498	 cls_loss: 0.1723 cluster_loss: 0.9544 sup_con_loss: 0.6205 contrastive_loss: 4.5803 
2024-05-15 13:54:28.094 | INFO     | __main__:train:123 - Epoch: [55][240/390]	 loss 3.91990	 cls_loss: 0.1759 cluster_loss: 0.9564 sup_con_loss: 0.7113 contrastive_loss: 4.5964 
2024-05-15 13:54:41.020 | INFO     | __main__:train:123 - Epoch: [55][260/390]	 loss 3.94030	 cls_loss: 0.2224 cluster_loss: 0.9925 sup_con_loss: 0.6606 contrastive_loss: 4.5940 
2024-05-15 13:54:53.972 | INFO     | __main__:train:123 - Epoch: [55][280/390]	 loss 3.78681	 cls_loss: 0.1749 cluster_loss: 0.8389 sup_con_loss: 0.5898 contrastive_loss: 4.5752 
2024-05-15 13:55:06.940 | INFO     | __main__:train:123 - Epoch: [55][300/390]	 loss 3.85710	 cls_loss: 0.2245 cluster_loss: 0.8506 sup_con_loss: 0.7169 contrastive_loss: 4.5765 
2024-05-15 13:55:20.003 | INFO     | __main__:train:123 - Epoch: [55][320/390]	 loss 3.94198	 cls_loss: 0.2214 cluster_loss: 0.9056 sup_con_loss: 0.8523 contrastive_loss: 4.5809 
2024-05-15 13:55:33.043 | INFO     | __main__:train:123 - Epoch: [55][340/390]	 loss 3.88503	 cls_loss: 0.2245 cluster_loss: 0.9009 sup_con_loss: 0.6844 contrastive_loss: 4.5867 
2024-05-15 13:55:46.051 | INFO     | __main__:train:123 - Epoch: [55][360/390]	 loss 3.81804	 cls_loss: 0.1228 cluster_loss: 0.8834 sup_con_loss: 0.6325 contrastive_loss: 4.5838 
2024-05-15 13:55:58.808 | INFO     | __main__:train:123 - Epoch: [55][380/390]	 loss 3.86366	 cls_loss: 0.1512 cluster_loss: 0.9191 sup_con_loss: 0.6506 contrastive_loss: 4.5933 
2024-05-15 13:56:04.870 | INFO     | __main__:train:126 - Train Epoch: 55 Avg Loss: 3.8901 
2024-05-15 13:56:04.871 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:56:31.441 | INFO     | __main__:train:135 - Train Accuracies: All 0.8133 | Old 0.8354 | New 0.7250
2024-05-15 13:56:35.768 | INFO     | __main__:train:123 - Epoch: [56][0/390]	 loss 3.87140	 cls_loss: 0.2180 cluster_loss: 0.8471 sup_con_loss: 0.7548 contrastive_loss: 4.5851 
2024-05-15 13:56:49.671 | INFO     | __main__:train:123 - Epoch: [56][20/390]	 loss 3.88388	 cls_loss: 0.1354 cluster_loss: 0.9214 sup_con_loss: 0.7420 contrastive_loss: 4.5813 
2024-05-15 13:57:02.743 | INFO     | __main__:train:123 - Epoch: [56][40/390]	 loss 3.89089	 cls_loss: 0.1566 cluster_loss: 0.8709 sup_con_loss: 0.8494 contrastive_loss: 4.5734 
2024-05-15 13:57:15.333 | INFO     | __main__:train:123 - Epoch: [56][60/390]	 loss 3.90747	 cls_loss: 0.1908 cluster_loss: 0.9788 sup_con_loss: 0.6405 contrastive_loss: 4.5851 
2024-05-15 13:57:28.078 | INFO     | __main__:train:123 - Epoch: [56][80/390]	 loss 3.85155	 cls_loss: 0.1763 cluster_loss: 0.8628 sup_con_loss: 0.7122 contrastive_loss: 4.5843 
2024-05-15 13:57:41.308 | INFO     | __main__:train:123 - Epoch: [56][100/390]	 loss 3.97618	 cls_loss: 0.2326 cluster_loss: 0.9421 sup_con_loss: 0.8638 contrastive_loss: 4.5847 
2024-05-15 13:57:54.310 | INFO     | __main__:train:123 - Epoch: [56][120/390]	 loss 3.97457	 cls_loss: 0.1880 cluster_loss: 0.8970 sup_con_loss: 0.9843 contrastive_loss: 4.5865 
2024-05-15 13:58:07.383 | INFO     | __main__:train:123 - Epoch: [56][140/390]	 loss 3.93148	 cls_loss: 0.2031 cluster_loss: 0.9561 sup_con_loss: 0.7432 contrastive_loss: 4.5828 
2024-05-15 13:58:20.373 | INFO     | __main__:train:123 - Epoch: [56][160/390]	 loss 3.98330	 cls_loss: 0.2316 cluster_loss: 0.9509 sup_con_loss: 0.8652 contrastive_loss: 4.5867 
2024-05-15 13:58:33.309 | INFO     | __main__:train:123 - Epoch: [56][180/390]	 loss 3.85406	 cls_loss: 0.1556 cluster_loss: 0.8706 sup_con_loss: 0.7283 contrastive_loss: 4.5828 
2024-05-15 13:58:46.091 | INFO     | __main__:train:123 - Epoch: [56][200/390]	 loss 3.86991	 cls_loss: 0.2050 cluster_loss: 0.8895 sup_con_loss: 0.6917 contrastive_loss: 4.5813 
2024-05-15 13:58:59.141 | INFO     | __main__:train:123 - Epoch: [56][220/390]	 loss 3.83505	 cls_loss: 0.2032 cluster_loss: 0.8419 sup_con_loss: 0.6846 contrastive_loss: 4.5802 
2024-05-15 13:59:12.035 | INFO     | __main__:train:123 - Epoch: [56][240/390]	 loss 3.87681	 cls_loss: 0.1791 cluster_loss: 0.9335 sup_con_loss: 0.6493 contrastive_loss: 4.5848 
2024-05-15 13:59:25.002 | INFO     | __main__:train:123 - Epoch: [56][260/390]	 loss 3.77366	 cls_loss: 0.1857 cluster_loss: 0.7908 sup_con_loss: 0.6213 contrastive_loss: 4.5803 
2024-05-15 13:59:37.831 | INFO     | __main__:train:123 - Epoch: [56][280/390]	 loss 3.77953	 cls_loss: 0.1826 cluster_loss: 0.8663 sup_con_loss: 0.4983 contrastive_loss: 4.5817 
2024-05-15 13:59:50.694 | INFO     | __main__:train:123 - Epoch: [56][300/390]	 loss 3.82747	 cls_loss: 0.2261 cluster_loss: 0.8359 sup_con_loss: 0.6509 contrastive_loss: 4.5803 
2024-05-15 14:00:03.807 | INFO     | __main__:train:123 - Epoch: [56][320/390]	 loss 3.94143	 cls_loss: 0.1633 cluster_loss: 0.9682 sup_con_loss: 0.7721 contrastive_loss: 4.5919 
2024-05-15 14:00:16.735 | INFO     | __main__:train:123 - Epoch: [56][340/390]	 loss 3.74098	 cls_loss: 0.1764 cluster_loss: 0.7491 sup_con_loss: 0.6173 contrastive_loss: 4.5789 
2024-05-15 14:00:29.630 | INFO     | __main__:train:123 - Epoch: [56][360/390]	 loss 3.91401	 cls_loss: 0.1758 cluster_loss: 0.9463 sup_con_loss: 0.7266 contrastive_loss: 4.5893 
2024-05-15 14:00:42.259 | INFO     | __main__:train:123 - Epoch: [56][380/390]	 loss 3.82302	 cls_loss: 0.1617 cluster_loss: 0.8796 sup_con_loss: 0.6202 contrastive_loss: 4.5809 
2024-05-15 14:00:48.054 | INFO     | __main__:train:126 - Train Epoch: 56 Avg Loss: 3.8826 
2024-05-15 14:00:48.055 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:01:14.504 | INFO     | __main__:train:135 - Train Accuracies: All 0.8115 | Old 0.8366 | New 0.7110
2024-05-15 14:01:17.675 | INFO     | __main__:train:123 - Epoch: [57][0/390]	 loss 3.97583	 cls_loss: 0.1847 cluster_loss: 0.9664 sup_con_loss: 0.8464 contrastive_loss: 4.5951 
2024-05-15 14:01:32.732 | INFO     | __main__:train:123 - Epoch: [57][20/390]	 loss 3.87213	 cls_loss: 0.2072 cluster_loss: 0.8135 sup_con_loss: 0.8368 contrastive_loss: 4.5815 
2024-05-15 14:01:45.624 | INFO     | __main__:train:123 - Epoch: [57][40/390]	 loss 3.87630	 cls_loss: 0.2034 cluster_loss: 0.8476 sup_con_loss: 0.7826 contrastive_loss: 4.5850 
2024-05-15 14:01:58.631 | INFO     | __main__:train:123 - Epoch: [57][60/390]	 loss 3.82202	 cls_loss: 0.1979 cluster_loss: 0.8263 sup_con_loss: 0.6829 contrastive_loss: 4.5794 
2024-05-15 14:02:11.605 | INFO     | __main__:train:123 - Epoch: [57][80/390]	 loss 3.92582	 cls_loss: 0.1731 cluster_loss: 0.8911 sup_con_loss: 0.8832 contrastive_loss: 4.5798 
2024-05-15 14:02:24.306 | INFO     | __main__:train:123 - Epoch: [57][100/390]	 loss 3.86286	 cls_loss: 0.1651 cluster_loss: 0.8564 sup_con_loss: 0.7748 contrastive_loss: 4.5804 
2024-05-15 14:02:37.405 | INFO     | __main__:train:123 - Epoch: [57][120/390]	 loss 3.80544	 cls_loss: 0.1604 cluster_loss: 0.8010 sup_con_loss: 0.7272 contrastive_loss: 4.5755 
2024-05-15 14:02:50.491 | INFO     | __main__:train:123 - Epoch: [57][140/390]	 loss 3.89528	 cls_loss: 0.1863 cluster_loss: 0.8159 sup_con_loss: 0.9230 contrastive_loss: 4.5795 
2024-05-15 14:03:03.385 | INFO     | __main__:train:123 - Epoch: [57][160/390]	 loss 3.86527	 cls_loss: 0.1792 cluster_loss: 0.9365 sup_con_loss: 0.6197 contrastive_loss: 4.5799 
2024-05-15 14:03:16.293 | INFO     | __main__:train:123 - Epoch: [57][180/390]	 loss 3.92260	 cls_loss: 0.1789 cluster_loss: 0.9258 sup_con_loss: 0.7970 contrastive_loss: 4.5835 
2024-05-15 14:03:29.229 | INFO     | __main__:train:123 - Epoch: [57][200/390]	 loss 3.98038	 cls_loss: 0.2297 cluster_loss: 0.9457 sup_con_loss: 0.8650 contrastive_loss: 4.5886 
2024-05-15 14:03:42.168 | INFO     | __main__:train:123 - Epoch: [57][220/390]	 loss 3.86486	 cls_loss: 0.1427 cluster_loss: 0.8836 sup_con_loss: 0.7533 contrastive_loss: 4.5799 
2024-05-15 14:03:55.398 | INFO     | __main__:train:123 - Epoch: [57][240/390]	 loss 3.84469	 cls_loss: 0.2157 cluster_loss: 0.9006 sup_con_loss: 0.5811 contrastive_loss: 4.5853 
2024-05-15 14:04:08.287 | INFO     | __main__:train:123 - Epoch: [57][260/390]	 loss 3.88953	 cls_loss: 0.1792 cluster_loss: 0.9365 sup_con_loss: 0.6706 contrastive_loss: 4.5898 
2024-05-15 14:04:21.221 | INFO     | __main__:train:123 - Epoch: [57][280/390]	 loss 3.88905	 cls_loss: 0.1699 cluster_loss: 0.8776 sup_con_loss: 0.7939 contrastive_loss: 4.5866 
2024-05-15 14:04:34.309 | INFO     | __main__:train:123 - Epoch: [57][300/390]	 loss 3.99960	 cls_loss: 0.1847 cluster_loss: 0.9547 sup_con_loss: 0.9339 contrastive_loss: 4.5963 
2024-05-15 14:04:47.172 | INFO     | __main__:train:123 - Epoch: [57][320/390]	 loss 3.92956	 cls_loss: 0.2131 cluster_loss: 0.8984 sup_con_loss: 0.8295 contrastive_loss: 4.5857 
2024-05-15 14:05:00.040 | INFO     | __main__:train:123 - Epoch: [57][340/390]	 loss 3.98270	 cls_loss: 0.2031 cluster_loss: 0.9757 sup_con_loss: 0.8384 contrastive_loss: 4.5908 
2024-05-15 14:05:13.004 | INFO     | __main__:train:123 - Epoch: [57][360/390]	 loss 3.75620	 cls_loss: 0.1719 cluster_loss: 0.7244 sup_con_loss: 0.7237 contrastive_loss: 4.5722 
2024-05-15 14:05:25.450 | INFO     | __main__:train:123 - Epoch: [57][380/390]	 loss 3.84808	 cls_loss: 0.1501 cluster_loss: 0.8091 sup_con_loss: 0.8468 contrastive_loss: 4.5742 
2024-05-15 14:05:31.295 | INFO     | __main__:train:126 - Train Epoch: 57 Avg Loss: 3.8907 
2024-05-15 14:05:31.295 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:05:57.876 | INFO     | __main__:train:135 - Train Accuracies: All 0.8130 | Old 0.8331 | New 0.7325
2024-05-15 14:06:02.723 | INFO     | __main__:train:123 - Epoch: [58][0/390]	 loss 3.85673	 cls_loss: 0.2016 cluster_loss: 0.8657 sup_con_loss: 0.6958 contrastive_loss: 4.5845 
2024-05-15 14:06:15.866 | INFO     | __main__:train:123 - Epoch: [58][20/390]	 loss 3.68101	 cls_loss: 0.1564 cluster_loss: 0.7169 sup_con_loss: 0.5322 contrastive_loss: 4.5754 
2024-05-15 14:06:28.444 | INFO     | __main__:train:123 - Epoch: [58][40/390]	 loss 4.00164	 cls_loss: 0.1723 cluster_loss: 0.9991 sup_con_loss: 0.8902 contrastive_loss: 4.5852 
2024-05-15 14:06:41.349 | INFO     | __main__:train:123 - Epoch: [58][60/390]	 loss 3.85092	 cls_loss: 0.1556 cluster_loss: 0.8502 sup_con_loss: 0.7646 contrastive_loss: 4.5789 
2024-05-15 14:06:54.212 | INFO     | __main__:train:123 - Epoch: [58][80/390]	 loss 3.76844	 cls_loss: 0.1721 cluster_loss: 0.7935 sup_con_loss: 0.6108 contrastive_loss: 4.5825 
2024-05-15 14:07:07.007 | INFO     | __main__:train:123 - Epoch: [58][100/390]	 loss 3.83812	 cls_loss: 0.1784 cluster_loss: 0.8579 sup_con_loss: 0.6863 contrastive_loss: 4.5813 
2024-05-15 14:07:20.192 | INFO     | __main__:train:123 - Epoch: [58][120/390]	 loss 4.08588	 cls_loss: 0.1680 cluster_loss: 1.0654 sup_con_loss: 1.0009 contrastive_loss: 4.5911 
2024-05-15 14:07:32.719 | INFO     | __main__:train:123 - Epoch: [58][140/390]	 loss 3.88644	 cls_loss: 0.1711 cluster_loss: 0.8907 sup_con_loss: 0.7643 contrastive_loss: 4.5847 
2024-05-15 14:07:45.948 | INFO     | __main__:train:123 - Epoch: [58][160/390]	 loss 4.05016	 cls_loss: 0.2393 cluster_loss: 0.9543 sup_con_loss: 1.0512 contrastive_loss: 4.5819 
2024-05-15 14:07:58.995 | INFO     | __main__:train:123 - Epoch: [58][180/390]	 loss 3.87915	 cls_loss: 0.1901 cluster_loss: 0.8592 sup_con_loss: 0.7802 contrastive_loss: 4.5862 
2024-05-15 14:08:12.378 | INFO     | __main__:train:123 - Epoch: [58][200/390]	 loss 3.81891	 cls_loss: 0.1476 cluster_loss: 0.8609 sup_con_loss: 0.6644 contrastive_loss: 4.5772 
2024-05-15 14:08:25.346 | INFO     | __main__:train:123 - Epoch: [58][220/390]	 loss 3.90649	 cls_loss: 0.2041 cluster_loss: 0.9174 sup_con_loss: 0.7263 contrastive_loss: 4.5916 
2024-05-15 14:08:38.478 | INFO     | __main__:train:123 - Epoch: [58][240/390]	 loss 3.92954	 cls_loss: 0.2072 cluster_loss: 0.9135 sup_con_loss: 0.8111 contrastive_loss: 4.5837 
2024-05-15 14:08:51.340 | INFO     | __main__:train:123 - Epoch: [58][260/390]	 loss 3.82922	 cls_loss: 0.2336 cluster_loss: 0.8953 sup_con_loss: 0.5185 contrastive_loss: 4.5908 
2024-05-15 14:09:04.340 | INFO     | __main__:train:123 - Epoch: [58][280/390]	 loss 3.93086	 cls_loss: 0.1806 cluster_loss: 0.9391 sup_con_loss: 0.7929 contrastive_loss: 4.5842 
2024-05-15 14:09:17.194 | INFO     | __main__:train:123 - Epoch: [58][300/390]	 loss 4.05136	 cls_loss: 0.1946 cluster_loss: 1.0999 sup_con_loss: 0.7978 contrastive_loss: 4.5985 
2024-05-15 14:09:30.212 | INFO     | __main__:train:123 - Epoch: [58][320/390]	 loss 3.82594	 cls_loss: 0.1639 cluster_loss: 0.8381 sup_con_loss: 0.7058 contrastive_loss: 4.5797 
2024-05-15 14:09:43.218 | INFO     | __main__:train:123 - Epoch: [58][340/390]	 loss 3.72124	 cls_loss: 0.1920 cluster_loss: 0.7706 sup_con_loss: 0.5142 contrastive_loss: 4.5742 
2024-05-15 14:09:56.278 | INFO     | __main__:train:123 - Epoch: [58][360/390]	 loss 3.76484	 cls_loss: 0.1698 cluster_loss: 0.7186 sup_con_loss: 0.7530 contrastive_loss: 4.5766 
2024-05-15 14:10:08.833 | INFO     | __main__:train:123 - Epoch: [58][380/390]	 loss 3.91853	 cls_loss: 0.1997 cluster_loss: 0.8870 sup_con_loss: 0.8451 contrastive_loss: 4.5790 
2024-05-15 14:10:14.593 | INFO     | __main__:train:126 - Train Epoch: 58 Avg Loss: 3.8776 
2024-05-15 14:10:14.594 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:10:40.735 | INFO     | __main__:train:135 - Train Accuracies: All 0.8112 | Old 0.8346 | New 0.7175
2024-05-15 14:10:45.700 | INFO     | __main__:train:123 - Epoch: [59][0/390]	 loss 3.87718	 cls_loss: 0.1594 cluster_loss: 0.8668 sup_con_loss: 0.7811 contrastive_loss: 4.5917 
2024-05-15 14:10:58.990 | INFO     | __main__:train:123 - Epoch: [59][20/390]	 loss 3.72774	 cls_loss: 0.1486 cluster_loss: 0.7658 sup_con_loss: 0.5870 contrastive_loss: 4.5731 
2024-05-15 14:11:12.202 | INFO     | __main__:train:123 - Epoch: [59][40/390]	 loss 3.88312	 cls_loss: 0.1517 cluster_loss: 0.8843 sup_con_loss: 0.7738 contrastive_loss: 4.5914 
2024-05-15 14:11:25.475 | INFO     | __main__:train:123 - Epoch: [59][60/390]	 loss 3.84593	 cls_loss: 0.2047 cluster_loss: 0.8665 sup_con_loss: 0.6727 contrastive_loss: 4.5779 
2024-05-15 14:11:38.262 | INFO     | __main__:train:123 - Epoch: [59][80/390]	 loss 3.74166	 cls_loss: 0.1680 cluster_loss: 0.7989 sup_con_loss: 0.5254 contrastive_loss: 4.5841 
2024-05-15 14:11:51.340 | INFO     | __main__:train:123 - Epoch: [59][100/390]	 loss 3.95665	 cls_loss: 0.1638 cluster_loss: 0.9236 sup_con_loss: 0.9178 contrastive_loss: 4.5811 
2024-05-15 14:12:04.380 | INFO     | __main__:train:123 - Epoch: [59][120/390]	 loss 3.93417	 cls_loss: 0.1787 cluster_loss: 0.9810 sup_con_loss: 0.7268 contrastive_loss: 4.5840 
2024-05-15 14:12:17.248 | INFO     | __main__:train:123 - Epoch: [59][140/390]	 loss 3.71690	 cls_loss: 0.1934 cluster_loss: 0.7905 sup_con_loss: 0.4588 contrastive_loss: 4.5767 
2024-05-15 14:12:30.161 | INFO     | __main__:train:123 - Epoch: [59][160/390]	 loss 3.79530	 cls_loss: 0.1622 cluster_loss: 0.8546 sup_con_loss: 0.5817 contrastive_loss: 4.5838 
2024-05-15 14:12:43.413 | INFO     | __main__:train:123 - Epoch: [59][180/390]	 loss 3.88394	 cls_loss: 0.1512 cluster_loss: 0.9453 sup_con_loss: 0.6797 contrastive_loss: 4.5826 
2024-05-15 14:12:56.624 | INFO     | __main__:train:123 - Epoch: [59][200/390]	 loss 3.86082	 cls_loss: 0.2051 cluster_loss: 0.9035 sup_con_loss: 0.6275 contrastive_loss: 4.5879 
2024-05-15 14:13:09.481 | INFO     | __main__:train:123 - Epoch: [59][220/390]	 loss 3.79328	 cls_loss: 0.2014 cluster_loss: 0.8416 sup_con_loss: 0.5728 contrastive_loss: 4.5774 
2024-05-15 14:13:22.439 | INFO     | __main__:train:123 - Epoch: [59][240/390]	 loss 3.89733	 cls_loss: 0.2057 cluster_loss: 0.8524 sup_con_loss: 0.8365 contrastive_loss: 4.5824 
2024-05-15 14:13:35.208 | INFO     | __main__:train:123 - Epoch: [59][260/390]	 loss 3.84128	 cls_loss: 0.2009 cluster_loss: 0.8946 sup_con_loss: 0.6003 contrastive_loss: 4.5836 
2024-05-15 14:13:48.327 | INFO     | __main__:train:123 - Epoch: [59][280/390]	 loss 3.87146	 cls_loss: 0.1737 cluster_loss: 0.8811 sup_con_loss: 0.7426 contrastive_loss: 4.5816 
2024-05-15 14:14:01.195 | INFO     | __main__:train:123 - Epoch: [59][300/390]	 loss 3.65177	 cls_loss: 0.1588 cluster_loss: 0.7093 sup_con_loss: 0.4686 contrastive_loss: 4.5710 
2024-05-15 14:14:14.140 | INFO     | __main__:train:123 - Epoch: [59][320/390]	 loss 3.80135	 cls_loss: 0.1966 cluster_loss: 0.8839 sup_con_loss: 0.5142 contrastive_loss: 4.5816 
2024-05-15 14:14:27.031 | INFO     | __main__:train:123 - Epoch: [59][340/390]	 loss 4.00338	 cls_loss: 0.1762 cluster_loss: 1.0263 sup_con_loss: 0.8414 contrastive_loss: 4.5848 
2024-05-15 14:14:39.741 | INFO     | __main__:train:123 - Epoch: [59][360/390]	 loss 3.93724	 cls_loss: 0.2541 cluster_loss: 0.9113 sup_con_loss: 0.7931 contrastive_loss: 4.5822 
2024-05-15 14:14:52.680 | INFO     | __main__:train:123 - Epoch: [59][380/390]	 loss 4.02966	 cls_loss: 0.2294 cluster_loss: 1.0366 sup_con_loss: 0.8252 contrastive_loss: 4.5951 
2024-05-15 14:14:58.426 | INFO     | __main__:train:126 - Train Epoch: 59 Avg Loss: 3.8784 
2024-05-15 14:14:58.427 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:15:24.790 | INFO     | __main__:train:135 - Train Accuracies: All 0.8109 | Old 0.8335 | New 0.7205
2024-05-15 14:15:30.019 | INFO     | __main__:train:123 - Epoch: [60][0/390]	 loss 3.88342	 cls_loss: 0.1835 cluster_loss: 0.8901 sup_con_loss: 0.7455 contrastive_loss: 4.5842 
2024-05-15 14:15:43.060 | INFO     | __main__:train:123 - Epoch: [60][20/390]	 loss 3.92300	 cls_loss: 0.2096 cluster_loss: 0.8823 sup_con_loss: 0.8463 contrastive_loss: 4.5845 
2024-05-15 14:15:55.798 | INFO     | __main__:train:123 - Epoch: [60][40/390]	 loss 3.88592	 cls_loss: 0.2388 cluster_loss: 0.8728 sup_con_loss: 0.7320 contrastive_loss: 4.5827 
2024-05-15 14:16:08.280 | INFO     | __main__:train:123 - Epoch: [60][60/390]	 loss 3.84730	 cls_loss: 0.1827 cluster_loss: 0.8042 sup_con_loss: 0.8155 contrastive_loss: 4.5772 
2024-05-15 14:16:21.129 | INFO     | __main__:train:123 - Epoch: [60][80/390]	 loss 4.02681	 cls_loss: 0.2306 cluster_loss: 0.9668 sup_con_loss: 0.9518 contrastive_loss: 4.5916 
2024-05-15 14:16:34.204 | INFO     | __main__:train:123 - Epoch: [60][100/390]	 loss 3.89856	 cls_loss: 0.1757 cluster_loss: 0.8610 sup_con_loss: 0.8409 contrastive_loss: 4.5894 
2024-05-15 14:16:47.559 | INFO     | __main__:train:123 - Epoch: [60][120/390]	 loss 3.79484	 cls_loss: 0.1665 cluster_loss: 0.8527 sup_con_loss: 0.5875 contrastive_loss: 4.5795 
2024-05-15 14:17:00.471 | INFO     | __main__:train:123 - Epoch: [60][140/390]	 loss 3.94059	 cls_loss: 0.1986 cluster_loss: 0.9364 sup_con_loss: 0.8015 contrastive_loss: 4.5876 
2024-05-15 14:17:13.559 | INFO     | __main__:train:123 - Epoch: [60][160/390]	 loss 3.84475	 cls_loss: 0.1439 cluster_loss: 0.9175 sup_con_loss: 0.6193 contrastive_loss: 4.5865 
2024-05-15 14:17:26.514 | INFO     | __main__:train:123 - Epoch: [60][180/390]	 loss 3.86247	 cls_loss: 0.1868 cluster_loss: 0.8536 sup_con_loss: 0.7557 contrastive_loss: 4.5812 
2024-05-15 14:17:39.596 | INFO     | __main__:train:123 - Epoch: [60][200/390]	 loss 3.78293	 cls_loss: 0.1850 cluster_loss: 0.7817 sup_con_loss: 0.6749 contrastive_loss: 4.5752 
2024-05-15 14:17:52.501 | INFO     | __main__:train:123 - Epoch: [60][220/390]	 loss 3.90278	 cls_loss: 0.1527 cluster_loss: 0.9497 sup_con_loss: 0.7228 contrastive_loss: 4.5832 
2024-05-15 14:18:05.388 | INFO     | __main__:train:123 - Epoch: [60][240/390]	 loss 3.84934	 cls_loss: 0.1688 cluster_loss: 0.8847 sup_con_loss: 0.6689 contrastive_loss: 4.5863 
2024-05-15 14:18:18.252 | INFO     | __main__:train:123 - Epoch: [60][260/390]	 loss 3.91055	 cls_loss: 0.2121 cluster_loss: 0.8757 sup_con_loss: 0.8247 contrastive_loss: 4.5823 
2024-05-15 14:18:31.039 | INFO     | __main__:train:123 - Epoch: [60][280/390]	 loss 3.94176	 cls_loss: 0.1763 cluster_loss: 0.9072 sup_con_loss: 0.8972 contrastive_loss: 4.5790 
2024-05-15 14:18:44.106 | INFO     | __main__:train:123 - Epoch: [60][300/390]	 loss 3.86365	 cls_loss: 0.1714 cluster_loss: 0.9227 sup_con_loss: 0.6170 contrastive_loss: 4.5969 
2024-05-15 14:18:57.166 | INFO     | __main__:train:123 - Epoch: [60][320/390]	 loss 3.86948	 cls_loss: 0.1481 cluster_loss: 0.8857 sup_con_loss: 0.7514 contrastive_loss: 4.5830 
2024-05-15 14:19:10.092 | INFO     | __main__:train:123 - Epoch: [60][340/390]	 loss 3.78736	 cls_loss: 0.1567 cluster_loss: 0.8147 sup_con_loss: 0.6478 contrastive_loss: 4.5788 
2024-05-15 14:19:22.913 | INFO     | __main__:train:123 - Epoch: [60][360/390]	 loss 3.99373	 cls_loss: 0.2060 cluster_loss: 0.9441 sup_con_loss: 0.9385 contrastive_loss: 4.5839 
2024-05-15 14:19:35.562 | INFO     | __main__:train:123 - Epoch: [60][380/390]	 loss 3.94484	 cls_loss: 0.1782 cluster_loss: 0.9669 sup_con_loss: 0.7871 contrastive_loss: 4.5823 
2024-05-15 14:19:41.297 | INFO     | __main__:train:126 - Train Epoch: 60 Avg Loss: 3.8758 
2024-05-15 14:19:41.299 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:20:08.199 | INFO     | __main__:train:135 - Train Accuracies: All 0.8121 | Old 0.8356 | New 0.7180
2024-05-15 14:20:13.178 | INFO     | __main__:train:123 - Epoch: [61][0/390]	 loss 3.79547	 cls_loss: 0.1708 cluster_loss: 0.8364 sup_con_loss: 0.6084 contrastive_loss: 4.5833 
2024-05-15 14:20:26.201 | INFO     | __main__:train:123 - Epoch: [61][20/390]	 loss 3.88971	 cls_loss: 0.1901 cluster_loss: 0.9167 sup_con_loss: 0.7119 contrastive_loss: 4.5818 
2024-05-15 14:20:39.401 | INFO     | __main__:train:123 - Epoch: [61][40/390]	 loss 3.79597	 cls_loss: 0.1636 cluster_loss: 0.9332 sup_con_loss: 0.4310 contrastive_loss: 4.5865 
2024-05-15 14:20:52.358 | INFO     | __main__:train:123 - Epoch: [61][60/390]	 loss 3.86943	 cls_loss: 0.1876 cluster_loss: 0.9042 sup_con_loss: 0.6568 contrastive_loss: 4.5941 
2024-05-15 14:21:05.301 | INFO     | __main__:train:123 - Epoch: [61][80/390]	 loss 3.97106	 cls_loss: 0.1895 cluster_loss: 0.9311 sup_con_loss: 0.9047 contrastive_loss: 4.5890 
2024-05-15 14:21:18.595 | INFO     | __main__:train:123 - Epoch: [61][100/390]	 loss 3.98974	 cls_loss: 0.1913 cluster_loss: 0.9837 sup_con_loss: 0.8571 contrastive_loss: 4.5899 
2024-05-15 14:21:31.547 | INFO     | __main__:train:123 - Epoch: [61][120/390]	 loss 3.86962	 cls_loss: 0.1949 cluster_loss: 0.9219 sup_con_loss: 0.6216 contrastive_loss: 4.5917 
2024-05-15 14:21:44.250 | INFO     | __main__:train:123 - Epoch: [61][140/390]	 loss 3.96681	 cls_loss: 0.2008 cluster_loss: 0.9359 sup_con_loss: 0.8846 contrastive_loss: 4.5825 
2024-05-15 14:21:57.138 | INFO     | __main__:train:123 - Epoch: [61][160/390]	 loss 3.95440	 cls_loss: 0.1729 cluster_loss: 0.9155 sup_con_loss: 0.9182 contrastive_loss: 4.5806 
2024-05-15 14:22:10.298 | INFO     | __main__:train:123 - Epoch: [61][180/390]	 loss 3.90515	 cls_loss: 0.1696 cluster_loss: 0.8740 sup_con_loss: 0.8502 contrastive_loss: 4.5848 
2024-05-15 14:22:23.632 | INFO     | __main__:train:123 - Epoch: [61][200/390]	 loss 3.96435	 cls_loss: 0.1556 cluster_loss: 0.9844 sup_con_loss: 0.8315 contrastive_loss: 4.5832 
2024-05-15 14:22:36.563 | INFO     | __main__:train:123 - Epoch: [61][220/390]	 loss 3.85816	 cls_loss: 0.1940 cluster_loss: 0.8730 sup_con_loss: 0.6914 contrastive_loss: 4.5859 
2024-05-15 14:22:49.709 | INFO     | __main__:train:123 - Epoch: [61][240/390]	 loss 3.83203	 cls_loss: 0.1723 cluster_loss: 0.8511 sup_con_loss: 0.6958 contrastive_loss: 4.5769 
2024-05-15 14:23:02.753 | INFO     | __main__:train:123 - Epoch: [61][260/390]	 loss 3.96347	 cls_loss: 0.1614 cluster_loss: 0.9804 sup_con_loss: 0.8199 contrastive_loss: 4.5889 
2024-05-15 14:23:15.954 | INFO     | __main__:train:123 - Epoch: [61][280/390]	 loss 3.92397	 cls_loss: 0.1774 cluster_loss: 0.9345 sup_con_loss: 0.7788 contrastive_loss: 4.5875 
2024-05-15 14:23:28.833 | INFO     | __main__:train:123 - Epoch: [61][300/390]	 loss 4.01463	 cls_loss: 0.1919 cluster_loss: 0.9927 sup_con_loss: 0.9360 contrastive_loss: 4.5763 
2024-05-15 14:23:41.797 | INFO     | __main__:train:123 - Epoch: [61][320/390]	 loss 3.91604	 cls_loss: 0.1767 cluster_loss: 0.8757 sup_con_loss: 0.8781 contrastive_loss: 4.5810 
2024-05-15 14:23:54.969 | INFO     | __main__:train:123 - Epoch: [61][340/390]	 loss 3.79786	 cls_loss: 0.1468 cluster_loss: 0.7990 sup_con_loss: 0.7251 contrastive_loss: 4.5743 
2024-05-15 14:24:07.941 | INFO     | __main__:train:123 - Epoch: [61][360/390]	 loss 3.88081	 cls_loss: 0.1963 cluster_loss: 0.8389 sup_con_loss: 0.8255 contrastive_loss: 4.5814 
2024-05-15 14:24:20.848 | INFO     | __main__:train:123 - Epoch: [61][380/390]	 loss 3.83670	 cls_loss: 0.2479 cluster_loss: 0.8725 sup_con_loss: 0.5759 contrastive_loss: 4.5865 
2024-05-15 14:24:26.747 | INFO     | __main__:train:126 - Train Epoch: 61 Avg Loss: 3.8843 
2024-05-15 14:24:26.748 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:24:53.837 | INFO     | __main__:train:135 - Train Accuracies: All 0.8149 | Old 0.8391 | New 0.7180
2024-05-15 14:24:57.544 | INFO     | __main__:train:123 - Epoch: [62][0/390]	 loss 3.90503	 cls_loss: 0.2170 cluster_loss: 0.9055 sup_con_loss: 0.7475 contrastive_loss: 4.5829 
2024-05-15 14:25:11.736 | INFO     | __main__:train:123 - Epoch: [62][20/390]	 loss 3.91949	 cls_loss: 0.2266 cluster_loss: 0.9405 sup_con_loss: 0.6968 contrastive_loss: 4.5923 
2024-05-15 14:25:24.751 | INFO     | __main__:train:123 - Epoch: [62][40/390]	 loss 3.82050	 cls_loss: 0.1744 cluster_loss: 0.7683 sup_con_loss: 0.8229 contrastive_loss: 4.5724 
2024-05-15 14:25:37.386 | INFO     | __main__:train:123 - Epoch: [62][60/390]	 loss 3.80554	 cls_loss: 0.1605 cluster_loss: 0.8145 sup_con_loss: 0.7066 contrastive_loss: 4.5734 
2024-05-15 14:25:50.366 | INFO     | __main__:train:123 - Epoch: [62][80/390]	 loss 3.80715	 cls_loss: 0.1834 cluster_loss: 0.8567 sup_con_loss: 0.5897 contrastive_loss: 4.5841 
2024-05-15 14:26:03.393 | INFO     | __main__:train:123 - Epoch: [62][100/390]	 loss 3.88291	 cls_loss: 0.1740 cluster_loss: 0.8719 sup_con_loss: 0.8067 contrastive_loss: 4.5737 
2024-05-15 14:26:16.066 | INFO     | __main__:train:123 - Epoch: [62][120/390]	 loss 3.91518	 cls_loss: 0.1444 cluster_loss: 0.8471 sup_con_loss: 0.9664 contrastive_loss: 4.5781 
2024-05-15 14:26:29.010 | INFO     | __main__:train:123 - Epoch: [62][140/390]	 loss 3.90474	 cls_loss: 0.1607 cluster_loss: 0.8717 sup_con_loss: 0.8607 contrastive_loss: 4.5856 
2024-05-15 14:26:41.557 | INFO     | __main__:train:123 - Epoch: [62][160/390]	 loss 3.89144	 cls_loss: 0.1786 cluster_loss: 0.8865 sup_con_loss: 0.7877 contrastive_loss: 4.5800 
2024-05-15 14:26:54.340 | INFO     | __main__:train:123 - Epoch: [62][180/390]	 loss 3.87230	 cls_loss: 0.1915 cluster_loss: 0.8971 sup_con_loss: 0.7059 contrastive_loss: 4.5771 
2024-05-15 14:27:07.154 | INFO     | __main__:train:123 - Epoch: [62][200/390]	 loss 3.83712	 cls_loss: 0.1813 cluster_loss: 0.8771 sup_con_loss: 0.6480 contrastive_loss: 4.5796 
2024-05-15 14:27:20.453 | INFO     | __main__:train:123 - Epoch: [62][220/390]	 loss 3.85908	 cls_loss: 0.1749 cluster_loss: 0.8740 sup_con_loss: 0.7200 contrastive_loss: 4.5811 
2024-05-15 14:27:33.572 | INFO     | __main__:train:123 - Epoch: [62][240/390]	 loss 3.87933	 cls_loss: 0.1549 cluster_loss: 0.8798 sup_con_loss: 0.7878 contrastive_loss: 4.5808 
2024-05-15 14:27:46.478 | INFO     | __main__:train:123 - Epoch: [62][260/390]	 loss 3.86583	 cls_loss: 0.1321 cluster_loss: 0.8993 sup_con_loss: 0.7401 contrastive_loss: 4.5785 
2024-05-15 14:27:59.833 | INFO     | __main__:train:123 - Epoch: [62][280/390]	 loss 3.82444	 cls_loss: 0.1714 cluster_loss: 0.8589 sup_con_loss: 0.6373 contrastive_loss: 4.5895 
2024-05-15 14:28:12.877 | INFO     | __main__:train:123 - Epoch: [62][300/390]	 loss 3.98774	 cls_loss: 0.1746 cluster_loss: 0.9411 sup_con_loss: 0.9612 contrastive_loss: 4.5823 
2024-05-15 14:28:26.036 | INFO     | __main__:train:123 - Epoch: [62][320/390]	 loss 3.80500	 cls_loss: 0.1493 cluster_loss: 0.8254 sup_con_loss: 0.6927 contrastive_loss: 4.5751 
2024-05-15 14:28:38.786 | INFO     | __main__:train:123 - Epoch: [62][340/390]	 loss 3.80167	 cls_loss: 0.1759 cluster_loss: 0.7850 sup_con_loss: 0.7397 contrastive_loss: 4.5707 
2024-05-15 14:28:51.645 | INFO     | __main__:train:123 - Epoch: [62][360/390]	 loss 3.76846	 cls_loss: 0.1480 cluster_loss: 0.7642 sup_con_loss: 0.6892 contrastive_loss: 4.5826 
2024-05-15 14:29:04.544 | INFO     | __main__:train:123 - Epoch: [62][380/390]	 loss 3.95101	 cls_loss: 0.1579 cluster_loss: 0.9042 sup_con_loss: 0.9334 contrastive_loss: 4.5866 
2024-05-15 14:29:10.102 | INFO     | __main__:train:126 - Train Epoch: 62 Avg Loss: 3.8745 
2024-05-15 14:29:10.103 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:29:37.276 | INFO     | __main__:train:135 - Train Accuracies: All 0.8164 | Old 0.8395 | New 0.7240
2024-05-15 14:29:42.950 | INFO     | __main__:train:123 - Epoch: [63][0/390]	 loss 4.01036	 cls_loss: 0.1614 cluster_loss: 1.0121 sup_con_loss: 0.8968 contrastive_loss: 4.5879 
2024-05-15 14:29:55.821 | INFO     | __main__:train:123 - Epoch: [63][20/390]	 loss 3.83632	 cls_loss: 0.1728 cluster_loss: 0.8792 sup_con_loss: 0.6514 contrastive_loss: 4.5790 
2024-05-15 14:30:08.409 | INFO     | __main__:train:123 - Epoch: [63][40/390]	 loss 4.00688	 cls_loss: 0.2031 cluster_loss: 1.0008 sup_con_loss: 0.8701 contrastive_loss: 4.5858 
2024-05-15 14:30:21.150 | INFO     | __main__:train:123 - Epoch: [63][60/390]	 loss 3.88083	 cls_loss: 0.1933 cluster_loss: 0.8077 sup_con_loss: 0.8988 contrastive_loss: 4.5748 
2024-05-15 14:30:34.222 | INFO     | __main__:train:123 - Epoch: [63][80/390]	 loss 3.95015	 cls_loss: 0.1753 cluster_loss: 0.9755 sup_con_loss: 0.7840 contrastive_loss: 4.5851 
2024-05-15 14:30:47.199 | INFO     | __main__:train:123 - Epoch: [63][100/390]	 loss 3.85904	 cls_loss: 0.1673 cluster_loss: 0.8897 sup_con_loss: 0.6943 contrastive_loss: 4.5834 
2024-05-15 14:31:00.023 | INFO     | __main__:train:123 - Epoch: [63][120/390]	 loss 3.89758	 cls_loss: 0.2310 cluster_loss: 0.9017 sup_con_loss: 0.7029 contrastive_loss: 4.5917 
2024-05-15 14:31:13.097 | INFO     | __main__:train:123 - Epoch: [63][140/390]	 loss 3.90261	 cls_loss: 0.1502 cluster_loss: 0.8728 sup_con_loss: 0.8766 contrastive_loss: 4.5783 
2024-05-15 14:31:26.120 | INFO     | __main__:train:123 - Epoch: [63][160/390]	 loss 3.86145	 cls_loss: 0.1731 cluster_loss: 0.8610 sup_con_loss: 0.7506 contrastive_loss: 4.5823 
2024-05-15 14:31:39.093 | INFO     | __main__:train:123 - Epoch: [63][180/390]	 loss 3.99050	 cls_loss: 0.1397 cluster_loss: 0.9322 sup_con_loss: 1.0167 contrastive_loss: 4.5844 
2024-05-15 14:31:52.097 | INFO     | __main__:train:123 - Epoch: [63][200/390]	 loss 3.98541	 cls_loss: 0.2198 cluster_loss: 0.9625 sup_con_loss: 0.8558 contrastive_loss: 4.5897 
2024-05-15 14:32:05.149 | INFO     | __main__:train:123 - Epoch: [63][220/390]	 loss 3.91598	 cls_loss: 0.1944 cluster_loss: 0.8550 sup_con_loss: 0.8877 contrastive_loss: 4.5869 
2024-05-15 14:32:18.138 | INFO     | __main__:train:123 - Epoch: [63][240/390]	 loss 3.93483	 cls_loss: 0.1916 cluster_loss: 0.9617 sup_con_loss: 0.7495 contrastive_loss: 4.5852 
2024-05-15 14:32:30.914 | INFO     | __main__:train:123 - Epoch: [63][260/390]	 loss 4.00718	 cls_loss: 0.1747 cluster_loss: 0.9552 sup_con_loss: 0.9814 contrastive_loss: 4.5872 
2024-05-15 14:32:43.805 | INFO     | __main__:train:123 - Epoch: [63][280/390]	 loss 3.79792	 cls_loss: 0.1638 cluster_loss: 0.8229 sup_con_loss: 0.6488 contrastive_loss: 4.5824 
2024-05-15 14:32:56.889 | INFO     | __main__:train:123 - Epoch: [63][300/390]	 loss 4.03140	 cls_loss: 0.2141 cluster_loss: 0.9854 sup_con_loss: 0.9533 contrastive_loss: 4.5881 
2024-05-15 14:33:09.822 | INFO     | __main__:train:123 - Epoch: [63][320/390]	 loss 3.78711	 cls_loss: 0.1686 cluster_loss: 0.8544 sup_con_loss: 0.5515 contrastive_loss: 4.5842 
2024-05-15 14:33:22.872 | INFO     | __main__:train:123 - Epoch: [63][340/390]	 loss 4.08576	 cls_loss: 0.1735 cluster_loss: 1.0489 sup_con_loss: 1.0398 contrastive_loss: 4.5836 
2024-05-15 14:33:35.628 | INFO     | __main__:train:123 - Epoch: [63][360/390]	 loss 3.81353	 cls_loss: 0.1703 cluster_loss: 0.8579 sup_con_loss: 0.6180 contrastive_loss: 4.5846 
2024-05-15 14:33:48.224 | INFO     | __main__:train:123 - Epoch: [63][380/390]	 loss 3.84856	 cls_loss: 0.1641 cluster_loss: 0.8497 sup_con_loss: 0.7556 contrastive_loss: 4.5760 
2024-05-15 14:33:54.021 | INFO     | __main__:train:126 - Train Epoch: 63 Avg Loss: 3.8812 
2024-05-15 14:33:54.021 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:34:20.907 | INFO     | __main__:train:135 - Train Accuracies: All 0.8135 | Old 0.8363 | New 0.7225
2024-05-15 14:34:26.185 | INFO     | __main__:train:123 - Epoch: [64][0/390]	 loss 3.97846	 cls_loss: 0.1566 cluster_loss: 0.9592 sup_con_loss: 0.9252 contrastive_loss: 4.5790 
2024-05-15 14:34:39.434 | INFO     | __main__:train:123 - Epoch: [64][20/390]	 loss 3.78237	 cls_loss: 0.1553 cluster_loss: 0.8164 sup_con_loss: 0.6326 contrastive_loss: 4.5784 
2024-05-15 14:34:52.436 | INFO     | __main__:train:123 - Epoch: [64][40/390]	 loss 3.89505	 cls_loss: 0.1873 cluster_loss: 0.9232 sup_con_loss: 0.7014 contrastive_loss: 4.5906 
2024-05-15 14:35:05.293 | INFO     | __main__:train:123 - Epoch: [64][60/390]	 loss 3.96695	 cls_loss: 0.1750 cluster_loss: 0.9966 sup_con_loss: 0.7853 contrastive_loss: 4.5893 
2024-05-15 14:35:18.297 | INFO     | __main__:train:123 - Epoch: [64][80/390]	 loss 3.91604	 cls_loss: 0.2377 cluster_loss: 0.8943 sup_con_loss: 0.7929 contrastive_loss: 4.5755 
2024-05-15 14:35:31.449 | INFO     | __main__:train:123 - Epoch: [64][100/390]	 loss 3.85087	 cls_loss: 0.1864 cluster_loss: 0.8123 sup_con_loss: 0.8113 contrastive_loss: 4.5749 
2024-05-15 14:35:44.339 | INFO     | __main__:train:123 - Epoch: [64][120/390]	 loss 3.71501	 cls_loss: 0.1673 cluster_loss: 0.7381 sup_con_loss: 0.5888 contrastive_loss: 4.5702 
2024-05-15 14:35:57.210 | INFO     | __main__:train:123 - Epoch: [64][140/390]	 loss 3.82502	 cls_loss: 0.1534 cluster_loss: 0.8392 sup_con_loss: 0.7154 contrastive_loss: 4.5776 
2024-05-15 14:36:10.319 | INFO     | __main__:train:123 - Epoch: [64][160/390]	 loss 3.84405	 cls_loss: 0.1821 cluster_loss: 0.8569 sup_con_loss: 0.7095 contrastive_loss: 4.5769 
2024-05-15 14:36:23.281 | INFO     | __main__:train:123 - Epoch: [64][180/390]	 loss 3.83571	 cls_loss: 0.1702 cluster_loss: 0.8528 sup_con_loss: 0.7007 contrastive_loss: 4.5794 
2024-05-15 14:36:36.291 | INFO     | __main__:train:123 - Epoch: [64][200/390]	 loss 3.89471	 cls_loss: 0.1895 cluster_loss: 0.9295 sup_con_loss: 0.7004 contrastive_loss: 4.5832 
2024-05-15 14:36:49.583 | INFO     | __main__:train:123 - Epoch: [64][220/390]	 loss 3.75565	 cls_loss: 0.1453 cluster_loss: 0.7733 sup_con_loss: 0.6454 contrastive_loss: 4.5788 
2024-05-15 14:37:02.471 | INFO     | __main__:train:123 - Epoch: [64][240/390]	 loss 3.96236	 cls_loss: 0.2083 cluster_loss: 0.9562 sup_con_loss: 0.8351 contrastive_loss: 4.5778 
2024-05-15 14:37:15.367 | INFO     | __main__:train:123 - Epoch: [64][260/390]	 loss 3.81564	 cls_loss: 0.1806 cluster_loss: 0.8488 sup_con_loss: 0.6224 contrastive_loss: 4.5890 
2024-05-15 14:37:28.491 | INFO     | __main__:train:123 - Epoch: [64][280/390]	 loss 4.03059	 cls_loss: 0.2140 cluster_loss: 0.9251 sup_con_loss: 1.0686 contrastive_loss: 4.5851 
2024-05-15 14:37:41.498 | INFO     | __main__:train:123 - Epoch: [64][300/390]	 loss 3.81823	 cls_loss: 0.1396 cluster_loss: 0.8565 sup_con_loss: 0.6747 contrastive_loss: 4.5792 
2024-05-15 14:37:54.635 | INFO     | __main__:train:123 - Epoch: [64][320/390]	 loss 3.78307	 cls_loss: 0.1784 cluster_loss: 0.8153 sup_con_loss: 0.6118 contrastive_loss: 4.5793 
2024-05-15 14:38:07.547 | INFO     | __main__:train:123 - Epoch: [64][340/390]	 loss 3.82367	 cls_loss: 0.1455 cluster_loss: 0.8146 sup_con_loss: 0.7575 contrastive_loss: 4.5817 
2024-05-15 14:38:20.373 | INFO     | __main__:train:123 - Epoch: [64][360/390]	 loss 3.91248	 cls_loss: 0.2085 cluster_loss: 0.9355 sup_con_loss: 0.7175 contrastive_loss: 4.5851 
