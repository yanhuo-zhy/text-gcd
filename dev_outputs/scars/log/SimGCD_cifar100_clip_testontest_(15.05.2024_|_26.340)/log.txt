2024-05-15 10:08:26.357 | INFO     | __main__:<module>:262 - Using evaluation function v2 to print results
2024-05-15 10:08:28.373 | INFO     | __main__:<module>:321 - model build
2024-05-15 10:08:36.954 | INFO     | __main__:train:37 - [Parameter containing:
tensor([ 1.2505e+00,  1.7950e+00,  1.4814e+00,  1.1862e+00,  1.2228e+00,
         1.4803e-06,  1.3816e+00,  1.6818e+00,  1.5741e+00,  1.7345e+00,
         1.3017e+00,  1.7853e-06,  1.4591e+00,  1.4010e+00,  1.3987e+00,
         1.4448e+00,  1.6395e+00,  1.4066e+00,  1.6520e+00,  1.1492e+00,
         1.3993e+00,  1.8040e+00,  2.6706e-05,  1.2228e+00,  1.6940e+00,
         1.4137e+00,  1.7989e+00,  1.3575e+00,  1.1653e+00,  1.4848e+00,
         1.3418e+00,  1.7068e-05,  1.1150e+00,  1.6925e+00,  1.6732e+00,
         1.6158e+00,  1.3748e+00,  1.4506e+00,  1.6472e+00,  1.4186e+00,
         1.2377e+00,  1.4147e+00,  1.6829e+00,  1.2952e+00,  1.2543e+00,
         1.6924e+00,  1.7394e+00,  1.0512e+00,  1.6347e+00,  1.5734e+00,
         1.3070e+00,  1.1030e+00,  1.7565e+00,  1.4724e+00,  1.6323e+00,
         1.7146e+00,  1.5611e+00,  1.5932e+00,  1.3064e+00,  1.2644e+00,
         1.5780e+00,  1.3687e+00,  1.6022e+00,  1.4832e+00,  1.0386e+00,
         1.4038e+00,  1.4676e+00,  1.7184e+00,  1.5844e+00,  1.6354e+00,
         1.5182e+00,  6.4233e-01,  1.2944e+00,  8.6831e-05,  1.6459e+00,
         1.4754e+00,  1.4241e+00,  1.1655e+00,  1.1254e+00,  1.5745e+00,
         1.2338e+00,  1.0236e+00,  1.1187e+00,  1.5772e+00,  1.7284e+00,
        -5.8607e-06,  1.5053e+00,  1.7229e+00,  1.4707e+00,  1.5130e+00,
         1.4752e+00,  1.2937e+00,  1.5616e+00,  1.4079e+00,  1.7353e+00,
         1.5934e+00,  2.2488e-04,  1.2376e+00,  1.6666e+00,  8.7118e-01,
         7.9451e-01,  1.1990e+00,  1.2908e+00,  1.5895e+00,  1.1225e+00,
         1.7734e+00,  1.5422e+00,  1.7599e+00,  1.5726e+00,  1.4186e+00,
         1.4901e+00,  1.2850e+00,  1.9334e+00,  4.9599e-01,  1.5160e+00,
         1.7961e+00,  1.1158e+00,  1.6465e+00,  1.4245e+00,  1.5525e+00,
         1.3298e+00,  1.2728e+00,  1.2079e+00,  1.7446e+00,  1.3800e+00,
         2.2044e+00,  1.4784e+00,  1.1268e+00,  1.5838e+00,  1.4474e+00,
         1.6339e+00,  1.3373e+00,  1.1814e+00,  1.6928e+00,  1.6496e+00,
         1.3704e+00,  1.7066e+00,  1.7994e+00,  1.2343e+00,  1.7628e+00,
         1.4018e+00, -3.1390e-05,  1.0339e+00,  1.4138e+00,  1.3423e+00,
         1.6158e+00,  1.7461e+00,  1.4579e+00,  1.5703e+00,  1.2612e+00,
         1.3136e+00,  1.2711e+00,  1.2942e+00,  1.2410e+00,  1.5976e+00,
         1.0244e+00,  1.3902e+00,  1.7761e+00,  1.1767e+00,  1.3887e+00,
         1.2133e+00,  1.0620e-04,  1.6903e+00,  1.5422e+00,  1.3180e+00,
         1.6067e+00,  1.5690e+00,  1.4296e+00,  1.3945e+00,  1.6611e+00,
         1.7479e+00,  1.7614e+00,  1.7711e+00,  1.2193e+00,  5.2926e-06,
         1.5580e+00,  1.7444e+00,  1.4612e+00,  1.5703e+00,  1.7018e+00,
         1.1588e+00,  1.6552e+00,  9.8656e-01,  1.5528e+00,  1.6313e+00,
         1.1482e+00,  4.4462e-01,  1.6245e+00,  1.3555e+00,  1.5268e+00,
         1.2635e+00,  1.3813e+00,  1.3468e+00,  1.6868e+00,  1.4163e+00,
         2.0292e-05,  1.0450e+00,  1.6720e+00,  1.5586e+00,  1.4644e+00,
         1.2731e+00,  1.4921e+00,  1.3819e+00,  1.1632e+00,  8.8199e-01,
         1.6139e+00,  1.6312e+00,  1.1369e+00,  1.6920e+00,  1.3872e+00,
         1.3115e+00,  1.5547e+00,  1.2465e+00,  1.3455e+00,  1.7084e+00,
         1.5367e+00,  1.5961e+00,  1.5917e+00,  1.5601e+00,  1.5481e+00,
         9.9333e-01,  1.3574e+00,  1.4112e+00,  1.6913e+00,  1.6535e+00,
         1.7221e+00,  1.4321e+00,  1.3472e+00,  1.6933e+00,  2.5585e+00,
         1.7771e+00,  1.9919e+00,  1.3485e+00,  1.1141e+00,  1.8257e+00,
         1.1953e+00,  1.3534e+00,  1.8430e+00,  1.5312e+00,  1.6214e+00,
         1.2823e+00,  1.3618e+00,  1.4500e+00,  9.3080e-01,  1.4036e+00,
         1.0668e+00,  1.6209e+00,  1.4177e+00,  1.2058e+00,  1.3846e+00,
         1.1049e+00,  1.8406e+00,  1.3888e+00,  1.5989e+00,  1.7826e+00,
         1.5602e+00,  1.3208e+00,  1.4542e+00,  1.2001e+00, -1.0746e-04,
         1.7455e+00,  1.6977e+00,  1.3199e+00,  1.3859e+00,  1.4745e+00,
         1.2519e+00,  1.5771e+00,  1.1565e+00,  1.7879e+00,  1.4864e+00,
         3.8560e-01,  1.7596e+00,  4.5889e-01,  1.5151e+00,  1.4301e+00,
         1.5044e+00,  1.2652e+00,  1.5515e+00,  1.1864e+00,  1.3839e+00,
         1.2982e+00,  1.6554e+00,  1.7365e+00,  1.4385e+00,  1.7155e+00,
         1.4105e+00,  1.4338e+00,  1.3791e+00,  1.4192e+00,  1.4949e+00,
         1.2547e+00,  1.5954e+00,  1.7297e+00,  1.3636e+00,  1.3632e+00,
         1.5377e+00,  1.7020e+00,  1.6613e+00,  1.6389e+00,  1.4459e+00,
         9.3896e-01,  1.6165e+00,  1.6599e+00,  1.5261e+00,  1.6435e+00,
         1.5838e+00,  1.0791e+00,  1.5280e+00,  1.3796e+00,  1.7406e+00,
         1.1946e+00,  1.3510e+00,  1.4233e+00,  1.4560e+00,  1.4537e+00,
         1.2330e+00,  1.3686e+00,  1.2433e+00,  1.1195e+00,  1.4636e+00,
         1.1970e+00,  1.6757e+00,  1.7105e+00,  1.3250e+00,  1.6082e+00,
         1.4033e+00,  1.8059e+00,  1.2479e+00,  1.4904e+00,  1.4735e+00,
         1.2163e+00,  1.4704e+00,  1.6256e+00,  1.5603e+00,  1.8001e+00,
         8.6470e-01,  1.5954e+00,  1.2486e+00,  1.4000e+00,  1.4293e+00,
         7.7509e-05,  1.1888e+00,  6.6730e-01,  1.3523e+00,  1.1407e+00,
         1.6138e+00,  1.6290e+00,  1.1278e+00,  1.4950e+00,  1.4375e+00,
         1.5252e+00,  1.5568e+00,  1.3888e+00,  1.2885e+00,  7.8910e-01,
         1.7182e+00,  2.9615e-05,  1.3343e+00,  1.5637e+00,  9.4666e-01,
         1.4164e+00,  1.4574e+00,  1.3081e+00,  1.1764e+00,  1.2876e+00,
         1.5991e+00,  1.3571e+00,  1.4329e+00,  1.3638e+00,  1.4738e+00,
         1.2676e+00,  1.4350e+00,  1.2967e+00,  1.7559e+00,  1.6588e+00,
         9.2682e-02,  1.7425e+00,  1.7848e+00,  1.1172e+00,  1.1064e+00,
         1.6518e+00,  1.4107e+00,  1.5011e+00,  1.4069e+00,  1.6104e+00,
         1.4933e+00,  1.4398e+00,  1.6567e+00,  1.4999e+00,  1.2726e+00,
         1.2182e+00,  1.5253e+00,  1.4665e+00,  1.1783e+00,  1.9530e-01,
         1.1190e+00,  1.3473e+00,  1.2944e+00,  1.4710e+00,  1.5982e+00,
         8.9826e-01,  1.6723e+00,  1.6900e+00,  1.5318e+00,  1.6934e+00,
         1.2611e+00,  1.3852e+00,  1.2104e+00,  1.1921e+00,  1.4499e+00,
         1.5427e+00,  1.1465e+00,  1.8840e+00,  1.4057e+00,  1.4588e+00,
         8.9235e-01,  1.3596e+00,  1.5945e+00, -6.6210e-03,  1.7402e+00,
         1.0990e+00,  1.6642e+00,  1.7069e+00,  1.2638e+00,  1.2083e+00,
         1.4975e+00,  9.9842e-01, -1.5694e-05,  1.2582e+00,  1.1776e+00,
         1.5245e+00,  1.6590e+00, -1.3473e-01,  1.5982e+00,  1.5247e+00,
         1.5378e+00,  1.5480e+00,  1.6705e+00,  1.6122e+00,  1.1225e+00,
        -6.5361e-06,  1.1358e+00,  1.7365e+00,  1.3888e+00,  1.4722e+00,
         1.7232e+00,  1.2539e+00,  1.2565e+00,  1.3680e+00,  1.1255e+00,
         1.3535e+00,  1.2495e+00,  1.6040e+00,  1.7439e+00,  1.6276e+00,
         1.7611e+00,  1.1571e+00,  1.7544e+00,  1.5415e+00,  1.4850e+00,
         1.3578e+00,  1.2663e+00,  1.3914e+00,  1.4922e+00,  1.1421e+00,
         1.5825e+00,  1.5777e+00,  1.5943e+00,  1.4732e+00,  1.0814e+00,
         1.2954e+00,  1.5278e+00,  1.4270e+00,  1.4232e+00,  1.2967e+00,
         1.2820e+00,  1.5259e+00,  1.6945e-01,  1.3939e+00,  1.4730e+00,
         1.1973e+00,  1.2938e+00,  1.0509e+00,  1.2064e+00,  1.4626e+00,
         1.3750e+00,  1.1984e+00,  1.3591e+00,  1.4773e+00,  1.2408e+00,
        -1.3592e-05,  1.2293e+00,  1.2715e+00,  1.1957e+00,  2.1751e+00,
         1.8126e+00,  1.1622e+00,  1.1651e+00,  1.4829e+00,  1.5549e+00,
         1.3675e+00,  1.3678e+00,  1.5814e+00,  1.4295e+00,  1.6189e+00,
         1.7048e+00,  1.0042e+00,  1.6366e+00,  1.7083e+00,  1.0667e+00,
         1.1897e+00,  1.1085e+00, -5.0114e-05,  9.0536e-01,  1.5278e+00,
         8.1765e-01,  1.4723e+00,  1.3919e+00,  1.5206e+00,  1.3231e+00,
         1.3511e+00,  6.6787e-01,  1.0118e+00,  1.7007e+00,  1.3869e+00,
         1.6424e+00,  1.2141e+00,  1.3029e+00,  1.3778e+00,  6.3901e-05,
         1.2789e+00,  1.3719e+00,  1.3749e+00,  1.3954e+00,  1.0337e+00,
         1.0983e+00,  1.3942e+00,  1.6173e+00, -1.3156e-05,  1.3541e+00,
         1.6760e+00,  1.6258e+00,  1.2777e+00,  1.2728e+00,  1.4085e+00,
         1.2765e+00,  1.3406e+00,  1.5631e+00,  1.5013e+00,  7.5514e-06,
         8.0783e-01,  1.1069e+00,  1.2548e+00,  1.8344e+00,  1.6113e+00,
         1.4129e+00,  1.7981e+00,  1.2166e+00,  1.3798e+00,  1.4934e+00,
         1.8040e+00,  1.6313e+00,  1.0761e+00,  1.3717e+00,  1.6914e+00,
         1.4608e+00,  1.7111e+00,  3.8322e-05,  9.5215e-01, -4.2810e-05,
         1.6709e+00,  1.6363e+00,  1.7349e+00,  6.6562e-05,  1.3421e+00,
         1.4099e+00,  1.4731e+00,  1.0480e+00,  1.7399e+00,  1.6267e+00,
        -3.1092e-05,  1.2825e+00,  1.3100e+00,  1.6374e+00,  1.3967e+00,
         1.6113e+00,  1.1493e+00,  1.0296e+00,  1.2170e+00,  1.5667e+00,
         1.3754e+00,  6.7746e-01,  2.0829e-05,  1.6572e+00,  1.5289e+00,
         1.4197e+00,  1.6102e+00,  9.7063e-01, -2.3552e-04,  1.7494e+00,
         1.0811e+00,  1.4046e+00,  1.5565e+00,  1.6770e+00,  1.3914e+00,
         1.0698e+00,  1.1566e+00,  1.2511e+00,  1.4659e+00,  1.3767e+00,
         1.3085e+00,  1.0704e+00,  1.2721e+00,  1.6351e+00,  1.5122e+00,
         1.7063e+00,  1.6196e+00,  9.2230e-01,  1.2603e+00,  1.8069e+00,
         1.5508e+00,  1.2251e+00,  1.5813e+00,  1.4296e+00,  1.0980e+00,
         1.7658e+00,  1.4668e+00,  1.3085e+00,  1.4175e+00,  1.3939e+00,
         1.2267e+00,  1.5992e+00,  1.3910e+00,  1.7450e+00,  1.7636e+00,
         1.3904e+00,  1.9031e+00,  1.3063e+00,  1.3598e+00,  1.5164e+00,
         1.5384e+00,  1.3272e+00,  1.2743e+00,  8.5102e-01,  1.6105e+00,
         1.1443e+00,  1.5763e+00,  1.3505e+00,  1.4137e+00,  1.1973e+00,
         1.6556e+00,  1.3202e+00,  1.4703e+00,  3.3475e-05,  1.6929e+00,
         1.7767e+00,  1.8060e+00,  1.5294e+00,  9.5811e-01,  1.2327e+00,
         1.2102e+00,  1.4931e+00,  1.1666e+00,  1.0638e+00,  1.3923e+00,
         1.4398e+00,  1.2291e+00,  1.6711e+00,  1.5738e+00,  1.7011e+00,
         1.3754e+00,  1.4166e+00,  1.8230e+00,  1.6232e+00,  1.3557e+00,
         1.4649e+00,  1.0925e+00,  1.4238e+00,  1.2338e+00,  1.3304e+00,
         5.2322e-01,  4.5587e-05,  1.2277e+00,  1.7084e+00,  1.2525e+00,
         1.5206e+00,  1.6263e+00,  1.3515e+00,  1.2852e+00,  1.4531e+00,
         1.7661e+00,  1.5912e+00,  1.7594e+00, -1.5504e-02,  1.7277e+00,
         1.4857e+00,  1.3581e+00,  1.2837e+00,  1.2904e+00,  1.2187e+00,
         1.5231e+00,  1.4748e+00,  2.3093e+00,  1.2558e+00,  1.9039e+00,
         1.5745e+00,  1.1288e+00,  1.6806e+00,  8.0427e-01,  1.5620e+00,
         1.3410e+00,  1.1626e+00,  1.3581e+00,  1.4374e+00,  1.7680e+00,
         1.5977e+00,  1.5249e+00,  9.1502e-01,  1.1621e+00,  1.2909e+00,
         1.2300e+00,  1.0969e+00,  1.4066e+00,  1.5432e+00,  1.2500e+00,
        -6.1506e-05,  1.5975e+00,  1.3150e+00,  1.4683e+00,  1.6951e+00,
         1.4957e+00, -1.5127e-05,  1.2996e+00,  1.4715e+00,  1.1777e+00,
         1.6820e+00,  1.4384e+00,  1.3005e+00,  1.7947e+00,  1.4873e+00,
         8.7349e-01,  1.6594e+00,  1.5121e+00,  1.6021e+00,  1.6697e+00,
         9.6560e-01,  1.4231e+00,  1.7693e+00,  1.1323e+00,  1.7177e+00,
         1.0862e+00,  1.4322e+00,  1.8170e+00, -3.2709e-05,  1.4917e-01,
        -1.8236e-03,  1.4846e+00,  8.3177e-01,  1.4996e+00,  1.4534e+00,
         1.5688e+00,  1.5060e+00, -1.3188e-03,  1.7226e+00,  1.4962e+00,
         1.1156e+00,  1.4492e+00,  1.6903e+00], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([-5.9209e-02, -5.2768e-02, -3.4505e-01, -1.6351e-01, -1.7916e-02,
        -6.1202e-06, -6.9836e-03,  1.1110e-02,  2.0364e-01,  1.9099e-01,
         9.7642e-02,  8.3109e-06, -9.0536e-02,  1.0088e-01,  4.9682e-03,
        -2.0470e-01,  1.6672e-02,  2.4565e-03,  1.1647e-01, -1.7814e-03,
         1.5497e-02, -2.6907e-03, -1.6102e-06,  2.1106e-01,  7.1408e-02,
        -1.2294e-01,  8.6019e-02,  7.3344e-02, -2.8801e-01, -1.6968e-01,
         7.4380e-02,  6.1896e-06,  7.6873e-02, -1.7837e-01,  3.1501e-02,
        -1.0421e-01,  7.4586e-02,  3.1790e-02, -1.0206e-02,  4.4152e-02,
         9.0982e-03,  1.7358e-02,  2.4111e-01,  2.1504e-01, -1.6977e-01,
         1.7219e-01, -2.3414e-02, -2.1748e-01,  1.1296e-01, -1.1519e-01,
        -1.2656e-02, -2.1825e-01, -4.5680e-01,  1.5590e-01,  1.9888e-01,
         6.8037e-02, -1.6894e-02,  6.9434e-02,  4.1918e-02,  1.5563e-02,
         3.2170e-01, -7.4098e-02, -2.4778e-03,  1.5171e-02, -2.6100e-01,
        -2.4900e-02,  1.6981e-01, -2.4362e-01,  1.7196e-01, -1.2907e-01,
         2.4366e-01,  1.7935e-01, -5.5138e-02,  1.1447e-04,  1.1307e-02,
         1.0859e-01,  1.0495e-01, -2.8890e-01, -1.5098e-01,  8.8300e-02,
        -1.5637e-01,  5.7747e-03, -1.3161e-01, -9.7291e-02, -1.5204e-01,
        -1.0182e-06, -8.3675e-03,  7.5906e-03,  1.1879e-02, -5.8948e-02,
        -1.4573e-01,  1.5941e-02, -1.2784e-01,  2.8235e-01,  1.1901e-01,
         1.0654e-01,  2.0938e-04, -1.3235e-01,  1.1213e-01, -6.6390e-01,
        -2.8214e-01,  4.5894e-01,  9.2324e-02, -8.7135e-02,  9.4956e-02,
         1.2894e-01,  6.8588e-03, -4.1890e-02,  2.5503e-02,  1.5128e-01,
        -2.7975e-01, -8.2265e-02, -4.9456e-02, -3.6847e-02,  2.3976e-01,
         6.6090e-02,  1.3660e-01, -1.8571e-01,  1.0084e-01, -1.2111e-01,
         1.2304e-02, -1.2897e-01,  2.5787e-01, -1.3441e-02, -8.0402e-02,
        -2.0190e-01,  5.1064e-02, -3.5063e-02,  7.0575e-02, -1.4197e-01,
        -1.8083e-01, -9.8257e-02, -2.9618e-03,  3.5502e-02, -8.2362e-02,
         4.2124e-01,  7.3387e-02,  9.1844e-02,  1.3326e-01, -3.4024e-02,
        -4.9519e-02, -9.8711e-07,  2.5918e-01, -7.3830e-02, -4.5120e-03,
        -2.4592e-01, -1.3227e-01,  9.7995e-02, -1.0706e-01,  3.8352e-02,
         5.4037e-02, -2.9223e-03, -5.0214e-02,  8.3107e-02,  8.9646e-02,
        -1.2411e-01,  4.2993e-02,  1.3619e-01,  1.6614e-01, -8.6462e-02,
         1.4318e-01, -7.3038e-05,  9.5023e-02, -3.0474e-02,  1.5658e-01,
         2.5621e-01,  1.6735e-01, -7.7492e-02,  8.0611e-02, -4.2355e-02,
         1.3559e-01,  1.2899e-01, -9.8783e-03,  2.3250e-01, -1.2806e-05,
         2.2140e-01,  6.5219e-02, -1.9658e-02, -1.4244e-01,  1.0922e-01,
        -1.1209e-01,  9.6187e-03, -7.8781e-02, -4.0153e-02, -6.9085e-02,
        -1.7253e-01, -7.3420e-01,  4.3577e-02, -2.1952e-02,  1.6476e-02,
         8.2102e-02,  1.4698e-01, -8.9605e-02, -4.3260e-02, -1.7235e-02,
         8.2321e-06, -2.2048e-01, -2.1097e-02,  5.3156e-02,  2.8835e-02,
         1.4254e-01, -1.8342e-01,  7.5879e-02, -1.3495e-01, -4.9882e-01,
         1.2028e-01, -4.1378e-02,  1.2580e-01, -8.9486e-02, -1.3841e-01,
         7.0368e-02,  8.7971e-02,  3.2079e-02, -1.5554e-01, -3.1407e-02,
         1.9469e-01, -3.0872e-01,  3.0200e-01,  7.6254e-02,  1.2673e-01,
         8.9433e-02,  7.3029e-02,  2.6081e-01, -1.7936e-01, -2.1572e-02,
        -8.8614e-02,  3.4210e-01, -1.4983e-01,  5.2163e-02,  1.0286e-01,
        -3.0498e-01, -1.6364e-01, -1.3004e-02, -1.9007e-01, -9.8637e-02,
        -8.9692e-02, -2.1173e-02,  1.6607e-01,  1.0981e-01,  4.9472e-02,
        -7.7132e-02, -2.0926e-01,  4.7164e-02, -7.7944e-01, -1.2625e-01,
        -9.7225e-03,  3.5724e-02, -2.3993e-01,  1.4284e-01, -7.1304e-03,
         2.0262e-02,  3.2073e-01, -2.0301e-01,  2.7436e-02,  1.1909e-01,
        -4.0200e-02,  1.9319e-01,  4.4777e-02, -8.5672e-02,  1.3235e-05,
         5.9152e-02,  8.0429e-03,  1.7480e-02,  4.2439e-02, -2.3703e-01,
         3.8971e-02,  6.2622e-01,  5.5559e-02,  6.4239e-02, -3.1812e-01,
        -8.6486e-02, -1.4093e-01, -8.8992e-01, -2.1346e-02,  1.0440e-01,
        -4.9601e-02,  1.9048e-01, -8.2631e-02,  3.0662e-01, -6.2486e-02,
         1.0100e-02, -2.7581e-01, -8.9106e-02,  1.4653e-01,  8.8283e-02,
         1.3131e-01,  1.2163e-01,  3.3185e-01,  1.7975e-02, -1.6509e-01,
         1.0493e-01,  1.9934e-01,  1.0033e-01, -1.1582e-01,  6.0018e-03,
        -1.0900e-03,  1.0079e-01, -1.7890e-01,  2.3746e-01, -2.0473e-01,
        -1.4566e-01, -1.8299e-01,  1.0680e-01, -1.1831e-02, -5.0836e-02,
         2.7487e-01,  9.3517e-02,  2.7938e-02,  1.4524e-01, -2.6391e-01,
         1.0004e-01,  1.4494e-01, -1.1805e-01, -1.0324e-02,  1.1357e-01,
        -4.5221e-02, -6.3875e-02,  3.0287e-02, -7.4076e-03, -2.0085e-01,
         5.3425e-02,  5.0244e-02, -2.7949e-02, -1.2780e-01,  5.0908e-02,
         1.4719e-01,  4.8011e-03,  1.8537e-01, -2.1230e-01,  6.5726e-02,
         5.9272e-02,  6.5187e-02, -1.1985e-01,  1.6051e-01,  3.9667e-03,
         4.3127e-02, -1.9664e-02, -3.1367e-01, -2.3444e-01, -7.7297e-02,
        -1.8183e-05, -3.7633e-02,  1.1283e-01,  1.4324e-01,  1.0582e-02,
         6.2235e-02, -1.1150e-02,  3.3057e-01, -1.2008e-01, -1.9060e-01,
        -5.2353e-01, -1.6702e-04,  6.1800e-02, -7.1558e-02,  5.6038e-01,
        -2.1159e-01,  4.6463e-06,  5.2412e-02,  2.6427e-01, -2.3330e-02,
         1.8056e-01,  3.0387e-02, -5.7664e-02, -2.5291e-02, -5.7142e-03,
        -2.7858e-01, -2.1311e-03,  1.0083e-02, -6.2228e-03,  1.2472e-01,
        -1.0074e-01,  1.5083e-01, -1.2360e-01,  1.4374e-02,  5.1325e-02,
        -2.2975e-02,  1.3384e-01, -3.4567e-02,  1.4255e-01, -1.5669e-03,
         1.5303e-01, -7.4872e-02, -6.8726e-02, -6.0582e-02,  8.8146e-02,
         5.1790e-02,  1.3448e-01,  7.6530e-02, -6.5261e-03, -9.3570e-02,
        -9.0801e-02,  1.6550e-01, -5.6144e-02,  2.7656e-02, -4.6630e-01,
         3.9195e-02, -7.7410e-02,  6.0350e-02, -5.0705e-02, -2.6643e-01,
         1.6943e+00, -1.5840e-02, -1.5765e-01, -2.6109e-02, -1.1132e-02,
        -1.6427e-01, -1.6963e-01, -1.2092e-01, -6.6410e-02, -1.6217e-02,
         1.7456e-01, -1.0403e-02,  3.5162e-02, -6.7704e-02,  5.9703e-02,
        -1.3488e-03, -5.8346e-02,  9.4746e-02, -2.5738e-03, -3.3182e-01,
        -1.5123e-01,  5.7448e-02,  4.9125e-02, -2.4912e-01,  7.2794e-02,
         2.2724e-01, -1.8380e-01, -7.4858e-06, -9.3484e-02,  7.0934e-02,
        -9.6452e-02, -1.3445e-01, -1.2464e-01,  1.6199e-02,  1.1594e-01,
        -1.8635e-02,  7.1269e-02,  3.5350e-02,  2.8393e-02, -3.7474e-02,
        -6.6449e-06,  1.6551e-01, -2.8401e-02,  3.0238e-01, -4.5726e-02,
         4.4221e-02,  2.1081e-01, -2.3607e-02, -9.7419e-02,  6.2005e-02,
        -7.5827e-02,  9.4189e-02,  2.6077e-01,  3.8662e-02,  1.2673e-01,
        -2.3672e-01,  4.1761e-02,  1.6129e-01, -1.7885e-01,  8.8373e-02,
        -1.2541e-01, -1.4051e-01, -1.5224e-03,  1.3502e-01,  1.7294e-01,
         7.4632e-02, -5.6330e-02,  1.3065e-01, -3.3917e-02, -2.1413e-01,
         6.5766e-02,  6.0510e-04,  4.2453e-02, -3.9881e-02,  2.1488e-02,
        -7.4692e-02,  5.3163e-02, -3.3939e-01,  1.0569e-01, -1.3507e-01,
        -1.8649e-02, -5.9194e-04,  6.4262e-02, -2.1319e-01,  1.1654e-01,
         1.4172e-02,  4.1454e-02, -1.4489e-01, -4.2065e-02,  9.4641e-02,
         9.2579e-06, -3.2871e-01, -2.2696e-01, -1.8579e-01,  1.0075e+01,
         9.5746e-02,  1.0382e-01, -1.7187e-01, -2.8058e-01, -1.1131e-01,
         3.6123e-02, -4.5311e-02,  1.1394e-01,  6.7171e-02,  6.8682e-02,
         2.0823e-01, -6.2539e-01, -4.1904e-02,  1.1367e-01,  7.0143e-02,
        -6.9878e-02, -9.0111e-03,  2.2144e-05,  2.2492e-01,  4.8424e-03,
        -2.6391e-01, -7.5162e-04,  3.8381e-02,  1.6850e-02, -1.1735e-01,
         1.7478e-01, -6.4362e-01, -4.3992e-01, -7.9428e-02, -2.4979e-02,
        -4.8787e-02,  9.3203e-02,  2.0726e-01,  6.6599e-02,  2.4948e-05,
        -1.4497e-01,  4.5601e-02, -2.0155e-01,  8.1348e-03, -1.1850e-01,
        -1.2056e-01, -1.7481e-01,  1.2135e-01,  1.6957e-05,  2.0086e-02,
         6.9476e-02,  1.1758e-01, -8.0991e-03, -6.5024e-02,  2.8884e-01,
         1.2848e-01, -5.8821e-02,  2.0231e-01,  5.8723e-02, -5.4862e-06,
         5.2944e-01,  7.0827e-02, -1.7815e-01, -1.2977e-01,  8.5833e-02,
         4.2731e-02,  3.2606e-02, -5.5411e-02, -8.7337e-02, -2.3049e-02,
        -2.5327e-02, -1.8436e-02,  1.7468e-01, -1.3529e-02,  1.0569e-01,
         1.6893e-01, -1.0273e-01,  1.3479e-05,  4.0435e-02,  5.6737e-05,
         1.3190e-01, -1.0708e-01, -1.9933e-01,  2.7006e-05, -1.0774e-01,
        -4.9287e-02,  5.1044e-01, -1.6132e-02,  5.3000e-02, -8.4465e-02,
        -1.3794e-05,  9.6694e-03, -2.7137e-02,  7.1366e-02,  2.7544e-01,
         8.6767e-02, -6.3404e-02, -9.5139e-02, -4.7028e-02, -3.1379e-02,
        -2.2714e-01,  2.0639e-01, -2.2556e-05, -1.0833e-01,  2.4347e-02,
        -1.4561e-02, -9.5602e-02,  1.2816e-01,  2.9872e-05,  1.0683e-01,
        -6.0388e-02, -1.4298e-01, -1.6962e-01,  7.2204e-02, -1.0171e-01,
        -3.1981e-01, -1.9100e-01,  3.8995e-02, -1.1359e-01,  5.4421e-02,
         1.0279e-01,  2.5555e-01, -5.1561e-02, -4.4174e-02,  1.3736e-02,
        -1.3608e-03,  1.4602e-01, -2.8588e-01, -8.8012e-02, -4.5207e-02,
        -1.1733e-01, -4.4733e-02, -2.5240e-02,  5.0842e-02, -4.3914e-02,
         4.6639e-01,  1.1582e-02,  3.0588e-02, -1.8382e-02, -5.4641e-02,
        -1.2799e-02, -3.4609e-02, -1.2276e-01,  1.1039e-01,  4.6231e-02,
        -8.3553e-02, -7.8652e-01,  1.8145e-02,  1.1715e-01,  2.2439e-02,
         2.5201e-01,  4.0653e-02,  1.9778e-02,  2.1260e-01, -1.2569e-01,
         1.2215e-01, -2.1285e-02,  6.1757e-02,  1.1102e-01, -5.7473e-02,
         9.5284e-02, -1.8641e-01,  1.2219e-01,  2.3455e-05,  1.1892e-01,
         2.6027e-01,  3.0418e-01, -4.6380e-03, -1.3251e-01, -4.0093e-02,
        -9.8339e-02,  8.7731e-02,  7.6293e-02, -1.5783e-01, -1.5348e-03,
         1.0093e-02, -2.9134e-01, -9.6777e-02,  3.6321e-02,  9.5838e-02,
         5.8800e-02,  8.3120e-03,  4.3760e-03,  5.8265e-02, -6.4203e-02,
         8.9335e-02,  1.4255e-01,  1.6380e-01, -7.0070e-02,  4.6495e-02,
         3.0106e-01,  2.3889e-06,  4.0803e-01,  1.1445e-01,  1.7693e-01,
        -7.1787e-02,  1.6637e-01,  3.7903e-02, -5.5907e-02,  1.8300e-02,
         5.7917e-02, -1.0490e-02, -8.4943e-02,  1.7657e-04, -2.2033e-01,
        -3.6430e-02,  2.2256e-01, -5.7259e-02,  2.5368e-03, -1.2252e-02,
         5.7987e-02,  2.0911e-02, -1.3068e+00, -2.6742e-01,  3.3142e-02,
         1.2981e-01, -3.5030e-02,  2.8411e-01, -1.4590e-01,  4.4483e-02,
         1.6383e-01,  9.3713e-02,  2.8914e-02, -4.2768e-02, -6.8393e-02,
        -9.4468e-02, -1.2752e-01,  3.3487e-01, -6.9179e-02, -1.2910e-01,
        -2.1277e-02,  3.9671e-02, -7.3978e-02, -1.5886e-01, -6.6898e-02,
        -2.0660e-05, -8.8960e-03, -2.2574e-01, -3.3574e-01, -6.0524e-02,
        -4.0602e-02, -1.8936e-05, -9.0038e-02, -4.9287e-03, -8.5164e-02,
         4.9185e-02, -1.2295e-01, -4.0534e-02,  5.7155e-03, -1.9524e-02,
        -5.3329e-02,  5.1300e-03, -1.1853e-01, -1.6203e-02,  8.8018e-02,
         3.5454e-01, -8.1486e-02, -1.2893e-01, -1.1815e-02,  2.6137e-02,
         1.2851e-01,  4.6705e-03,  8.9667e-02, -3.3899e-05,  5.6975e-02,
        -1.3532e-03, -1.2912e-01, -4.6510e-01,  7.2908e-02,  5.4589e-02,
        -8.3421e-02, -9.3318e-02, -1.2037e-03, -4.8082e-02,  1.3122e-02,
         2.8368e-02,  1.7193e-01, -7.7931e-02], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 0.0063, -0.0069, -0.0111,  ..., -0.0136,  0.0083, -0.0113],
        [-0.0045,  0.0162, -0.0085,  ..., -0.0215, -0.0053, -0.0065],
        [-0.0256,  0.0192, -0.0204,  ..., -0.0165, -0.0012, -0.0092],
        ...,
        [-0.0176, -0.0251,  0.0371,  ...,  0.0097, -0.0039,  0.0199],
        [ 0.0036,  0.0177,  0.0220,  ...,  0.0004,  0.0019, -0.0088],
        [-0.0253,  0.0101, -0.0018,  ..., -0.0023, -0.0106, -0.0080]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.3006,  0.0136,  0.0392,  ..., -0.0223, -0.0721, -0.0758],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0048, -0.0116,  0.0299,  ...,  0.0175,  0.0150,  0.0307],
        [-0.0034,  0.0025,  0.0207,  ...,  0.0174, -0.0037,  0.0124],
        [-0.0168, -0.0007, -0.0148,  ..., -0.0277, -0.0276,  0.0064],
        ...,
        [-0.0318,  0.0156,  0.0159,  ..., -0.0040,  0.0066,  0.0098],
        [ 0.0053,  0.0147,  0.0062,  ...,  0.0002,  0.0125,  0.0043],
        [-0.0098,  0.0027, -0.0203,  ..., -0.0283,  0.0054,  0.0198]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 2.5204e-02, -1.4676e-01, -1.1085e-01, -2.5165e-01, -6.7881e-02,
         7.4550e-01, -2.8918e-02,  2.2872e-02,  1.2555e-01, -1.7283e-01,
        -2.2144e-02,  1.9651e-01,  7.4486e-02, -1.3135e-01,  1.5680e-02,
         1.2602e-02,  4.7583e-02,  8.3184e-02,  1.5383e-01,  1.2811e-01,
        -7.2439e-02,  1.1020e-02, -8.3493e-02,  5.1977e-02, -8.8992e-03,
         2.7269e-02,  8.7819e-02,  1.1495e-01,  9.9859e-02,  7.0658e-02,
         1.1240e-01,  3.3918e-02, -3.9440e-02,  1.3818e-01,  6.6409e-02,
        -3.6175e-02, -2.8908e-03, -8.3685e-02,  1.3404e-02, -5.4805e-02,
         2.1146e-02, -6.4215e-02,  2.0196e-02,  1.1712e-01, -6.3860e-02,
         9.1397e-02,  1.2809e-01, -1.0962e-02, -1.4737e-01,  1.0862e-01,
         1.2349e-01, -1.3621e-02, -1.0826e-02,  1.1337e-01, -8.1659e-03,
        -7.2429e-02,  1.7261e-02,  1.8395e-02, -1.1698e-01,  5.5770e-02,
         3.6498e-02,  1.2333e-01,  4.6113e-02, -5.2738e-02, -1.3331e-01,
         5.2245e-02, -1.3447e-02,  2.9030e-01,  6.9156e-04,  1.3906e-01,
         1.7745e-01,  1.2780e-01,  7.2123e-02,  2.0194e+00,  3.0754e-02,
         1.7661e-01,  8.2684e-02, -2.0438e-01,  4.6974e-02, -1.3247e-02,
        -1.7554e-01, -6.5533e-02,  1.2457e-01,  4.6361e-04, -1.7453e-01,
         1.6609e-02,  2.0490e-02,  8.8413e-02, -9.7607e-03,  2.9198e-02,
         3.1662e-02,  7.4465e-02, -8.4767e-02,  4.8407e-02,  6.5173e-02,
         6.3353e-02,  2.5330e-01,  4.5611e-02,  1.1555e-01,  8.6836e-01,
        -1.6196e-01,  7.9984e-02, -1.1950e-01, -1.8678e-01,  9.6892e-03,
         1.8054e-01, -1.2730e-02,  1.1794e-01,  7.8488e-02, -1.2669e-01,
        -2.6026e-01, -6.8944e-04, -2.0563e-01, -3.7120e-02,  1.3525e-01,
        -1.0327e-01,  1.7426e-01, -1.3092e-01, -4.2696e-02,  6.0663e-02,
         1.0944e-01,  2.0969e-02,  7.3625e-02, -4.8487e-02,  6.5356e-02,
         8.2848e-02,  3.7792e-02, -3.4444e-02, -2.8512e-02, -4.7548e-03,
         2.2311e-03, -9.9909e-02, -4.0619e-02,  1.6106e-01, -1.4208e-02,
         6.7521e-03,  1.4778e-01, -4.0878e-01,  2.5596e-02,  1.1379e-01,
         8.7850e-02,  9.5929e-02,  8.6670e-02, -1.2618e-01,  3.6906e-02,
         8.0895e-02,  3.1737e-02,  1.4886e-01,  9.5536e-03,  1.3234e-01,
         7.2235e-02,  4.2063e-02, -8.4018e-02, -9.5633e-03,  5.1898e-02,
         7.4677e-02, -1.4421e-01,  1.1361e-02,  2.7633e-02, -9.5242e-04,
        -2.4405e-02,  3.9176e-02,  4.4086e-02,  1.1936e-01, -2.7680e-02,
        -4.2326e-02, -5.7541e-02, -3.8053e-02,  1.1972e-02, -2.3288e-01,
        -4.4311e-02,  7.1520e-02,  1.0596e-01,  4.3858e-02,  6.9312e-02,
         1.5879e-01, -1.0939e-01,  5.8014e-02,  1.4828e-01,  4.0516e-02,
         5.2580e-03,  2.8658e-01,  2.8793e-03,  9.5742e-02,  9.0563e-02,
         2.8150e-02,  2.9457e+00,  7.3386e-03, -2.0794e-02,  1.6458e-01,
        -1.2993e-01,  7.1108e-02,  3.0981e-02,  1.4927e-02,  7.3440e-02,
        -1.7347e-01, -5.2673e-02,  3.9077e-03,  1.4435e-02,  1.6045e-01,
         5.9171e-02, -8.9524e-02,  1.4242e-02, -9.9387e-02, -5.2939e-02,
        -4.4536e-02, -8.5886e-02, -9.0136e-02, -1.6120e-02, -1.0210e-01,
         1.5979e-01,  3.9959e-02, -1.8292e-02, -1.0411e-02,  4.5605e-02,
        -1.2374e-02, -1.3190e-02,  1.4378e-01,  3.2017e-01,  1.7443e-02,
         4.8731e-02, -5.5286e-02,  2.7535e-02,  1.9803e-01,  5.2934e-02,
        -7.5623e-04, -1.0409e-01, -4.6670e-02,  1.7542e-01, -2.1907e-01,
        -3.4853e-01, -4.9835e-02,  5.6949e-02, -1.0241e-01, -1.1328e-02,
        -9.6633e-02,  4.2811e-02,  4.3588e-02,  1.2037e-02,  7.8788e-02,
        -2.0701e-02, -1.3658e-01,  1.2232e-01,  1.0646e+00, -2.3804e-03,
         2.5755e-02,  4.8900e-02,  3.0552e-02,  1.1159e-01, -9.7154e-04,
         5.6107e-02,  8.5527e-02, -6.3792e-02, -4.4382e-02,  4.8545e-02,
         1.9222e-01,  1.3447e-01,  2.9116e-02, -4.2629e-02, -5.2399e-02,
         3.9627e-02, -1.9796e-02, -2.0867e-02, -6.6185e-02,  5.1226e-02,
        -1.0040e-01, -8.7508e-02,  4.8729e-02,  4.8047e-02,  1.6025e-02,
        -8.3977e-02,  7.6472e-02,  2.1729e+00,  6.4331e-02,  1.6162e-01,
        -8.7606e-02,  1.4174e-01,  3.4655e-02,  4.6869e-02, -5.3468e-02,
         8.4344e-02,  9.4906e-03, -1.6219e-02,  9.2807e-02, -6.1671e-02,
         9.5335e-02, -6.9102e-02,  1.8759e-01, -1.1308e-01,  8.2468e-02,
         1.0566e-01,  4.5688e-02,  3.8485e-02, -3.5568e-04, -1.0497e-02,
         1.8765e-02,  6.2931e-02,  2.7742e-02, -2.2325e-02, -3.2242e-03,
         8.8336e-02,  2.0677e-04,  7.4946e-03,  4.2048e-02,  2.4930e-01,
         3.3632e-02,  1.2429e-01,  6.0794e-03,  9.4705e-02, -3.2016e-01,
        -3.5724e-02, -1.5621e-01,  6.4962e-02, -2.3619e-02,  1.1590e-01,
         3.0589e-02,  4.0862e-02,  2.6329e-02, -1.0830e-01, -3.5587e-02,
        -1.9670e-01,  6.3957e-02, -5.4688e-02, -1.9252e-02, -7.5821e-02,
         1.9083e-01,  1.7937e-02, -4.3595e-01,  2.6858e-02,  4.4909e-02,
         6.9316e-02, -7.2092e-02, -3.1471e-02, -6.1437e-03,  7.8181e-02,
         1.1059e-01, -1.9159e-02, -5.3957e-02, -5.7915e-02,  2.7941e-02,
         4.0491e-02,  3.5114e-02, -8.3909e-02,  6.8203e-02,  2.7627e-02,
         7.5980e-02, -2.5676e-02,  2.8823e-01,  4.1534e-02,  1.4292e-01,
         2.0443e-01, -4.0233e-02,  4.9003e-03,  4.6739e-02,  1.3711e-02,
        -1.0763e-01,  1.1062e-01,  9.1188e-02,  1.2980e-01, -3.9028e-02,
         1.3484e-01,  2.2844e-02,  7.3887e-03,  1.3219e-01,  1.2849e-01,
        -1.6701e-01,  9.5039e-02,  1.6997e-01, -2.1140e-02,  5.4579e-02,
         1.9186e-01,  1.6614e-01, -1.0291e-02, -1.5723e-01,  6.9470e-02,
        -7.9837e-02,  9.1746e-02, -1.3635e-01,  2.4104e-02, -5.4362e-02,
         6.5156e-03,  5.9368e-02, -1.0170e-01, -1.1356e-02,  9.8902e-02,
        -4.8655e-02,  4.5849e-02,  9.3273e-02,  1.8023e-01, -1.3099e-01,
        -7.3838e-02, -2.7955e-02, -1.4207e-01, -1.5098e-01, -2.6259e+00,
         2.0108e-01, -7.9659e-03, -1.8255e-02,  8.3982e-02, -5.6719e-03,
         9.7435e-01,  4.3532e-02,  1.9901e-01, -7.7351e-02,  1.0608e-01,
        -1.3002e-01,  1.6456e-01, -5.9648e-03, -6.2824e-02,  1.0156e-01,
         4.9446e-02,  5.2718e-02, -4.6665e-02, -1.9092e-01,  1.7455e-01,
        -1.0465e-01,  1.9728e-01, -6.2901e-02, -2.0725e-01, -1.0451e-01,
        -6.3247e-02,  3.6625e-03, -7.1540e-02, -9.4895e-02, -2.9849e-02,
         1.3353e-01, -1.8342e-02, -1.8501e-01,  3.6136e-02,  3.1008e-02,
         1.2395e-02,  4.1561e-02,  6.8486e-01, -2.1333e-01,  5.5678e-02,
         7.2924e-02,  7.6079e-02,  1.3231e-01,  3.7862e-02,  2.2681e-02,
         9.2010e-01,  1.6126e-01, -3.7789e-02,  1.4916e-01,  2.1018e-02,
        -5.4641e-02, -3.2506e-02,  1.0358e-01,  4.3190e-02,  1.0362e-02,
         4.1155e-02,  2.3810e-02,  8.2647e-02,  3.5524e-01,  8.5387e-02,
         1.9678e-01, -2.8526e-02,  1.0984e-01,  3.6248e-02,  8.9456e-02,
        -1.2529e-01, -4.1798e-02, -8.2246e-02,  1.6764e-01,  2.2500e-02,
         1.6551e-02, -6.8556e-02,  2.1562e-01,  3.9720e-02, -1.2418e-01,
         2.3201e-02,  1.0247e-01,  4.2534e-02,  8.3268e-02,  3.8081e-02,
         5.1975e-02,  1.4923e-01, -1.6266e+00,  4.8515e-02, -3.1456e-02,
         1.0390e-02, -1.0081e-01, -5.7551e-02, -2.8936e-02, -2.2120e-02,
         7.0234e-02,  1.0959e-01, -2.7500e-02,  1.8936e-02,  2.4679e-01,
        -1.4155e-01, -7.1491e-02, -3.0189e-02, -3.2044e-02,  9.4557e-01,
         5.8264e-02,  5.9201e-02, -1.2087e-01, -9.9765e-02,  4.5261e-03,
        -3.7725e-02,  6.1143e-02,  8.4327e-02, -4.2760e-02, -5.8401e-02,
         8.3739e-02, -1.0522e-01, -3.9312e-01,  1.5280e-02, -6.9650e-02,
        -9.3360e-02, -8.0228e-03,  6.1437e-03,  2.8821e-02,  5.2568e-02,
        -4.8440e-01,  1.7826e-01,  2.6566e-01,  1.8970e-02, -3.5938e-03,
        -6.8847e-02, -1.5588e-01, -3.4141e-01,  4.1832e-02,  7.2181e-02,
        -8.9216e-02,  5.4667e-02,  2.1518e-01, -3.7819e-02,  1.0854e-01,
        -1.0334e-02,  1.4351e-02,  2.9369e-02,  1.1503e-02,  1.7186e-02,
        -1.0593e-01, -1.9882e-01,  9.5092e-02, -1.1791e-01,  1.2562e-02,
        -5.1596e-02, -1.7086e-01,  2.0591e-02,  1.7459e-01, -7.1450e-02,
        -1.7526e-03,  1.5193e-02,  6.0577e-02,  8.1574e-02, -5.3623e-01,
        -4.4659e-01,  7.4511e-03,  9.8437e-02, -1.9230e-01,  2.5519e-02,
         8.9836e-02, -1.0780e-01, -3.6130e-02,  8.1834e-03,  7.4109e-02,
         7.8535e-03,  1.9985e-02,  3.7199e-01, -2.2315e-02,  4.4216e-02,
         8.8010e-02,  9.5637e-02,  3.8409e-01, -4.8981e-02, -5.4720e-01,
         2.3369e-02, -7.7604e-01, -4.4030e-02,  2.3595e-01,  4.4887e-02,
        -1.4884e-02, -2.3726e-01,  6.6309e-02, -2.0153e-01,  4.8480e-02,
         1.8383e-01, -4.8762e-02,  3.9996e-02,  8.7848e-02,  3.5124e-03,
         1.7251e-03,  5.0178e-02,  1.4177e-01,  1.2113e-01, -6.5328e-03,
        -1.9049e-01,  7.7488e-01, -4.1618e-01, -4.6367e-03,  2.0791e-01,
         5.7149e-02, -3.8932e-02,  5.6105e-02,  1.3914e-01,  9.1579e-02,
        -2.1053e-02,  6.3400e-02, -2.2407e-02, -1.3569e-01, -1.1965e-01,
        -1.0849e-01,  1.1125e-02,  1.2957e-02,  1.0216e-01, -7.9980e-03,
        -3.8130e-02,  2.1473e-02,  1.0442e-01, -9.5452e-02,  1.2365e-01,
         5.3784e-02,  4.3555e-02, -7.7990e-02,  5.6901e-02,  1.6475e-01,
        -4.1612e-01, -3.1107e-02,  1.1065e-01,  6.4568e-02,  7.8650e-03,
        -4.0678e-01,  1.8623e-02, -7.5056e-02, -9.6480e-02,  1.4677e-01,
         5.3561e-02,  1.5397e-01, -1.0328e-01, -8.3102e-02,  2.1292e-01,
        -2.6474e-02,  4.7575e-01,  1.1345e-01,  2.9477e-02,  2.3062e-02,
         3.7210e-02,  4.6048e-02, -2.5478e-02,  2.7515e-02,  4.6555e-02,
        -1.6355e-03, -1.4197e-02,  2.9408e-02,  4.8862e-02, -7.2380e-02,
         1.8723e-01,  3.7901e-02,  1.2632e-01,  2.2133e-01,  9.6012e-02,
         6.0383e-02, -2.3615e-03, -1.8729e-02, -1.2401e-01, -4.0226e-02,
        -2.7071e-01,  1.1390e-01,  2.2793e-02, -8.3772e-02, -5.5962e-02,
         8.9546e-02, -1.7674e-02, -1.2787e-01, -1.3489e-02,  1.2081e-01,
         9.0224e-02, -6.9370e-02, -8.7053e-02,  6.0066e-02,  1.3201e-01,
        -7.0824e-02,  2.9838e-02,  9.3361e-02,  7.4621e-02,  4.3134e-02,
         3.2714e-01, -4.0776e-02,  6.9737e-02, -4.3606e-02, -1.5249e-02,
        -1.0278e-01,  2.6362e-03,  3.6257e-02,  2.2025e-02, -1.7667e-01,
         1.6525e-01,  2.2975e-02,  3.2120e-02,  2.2393e-01,  3.1689e-02,
        -3.1127e-02,  2.9421e-02, -6.3129e-02, -8.9659e-02,  1.8561e-03,
        -2.4726e-02,  3.9643e-02, -8.9760e-02, -1.7801e-01,  1.1759e-01,
         1.6275e-01, -6.1061e-03, -6.0398e-03, -4.5551e-02,  1.3076e-01,
         9.7237e-02,  3.8611e-02,  7.7304e-02, -3.3576e-01,  7.5701e-02,
         1.0755e-02,  9.0078e-02,  1.1480e-01, -1.7562e-02, -8.3149e-02,
         4.0478e-02,  3.0223e-02, -3.5331e-02,  1.5424e-01,  2.5245e-03,
         6.8903e-02, -2.8873e-02, -5.0933e-01, -2.0118e-02,  1.0946e-01,
         9.1672e-02,  3.5727e-01,  4.0202e-02,  4.1769e-02,  1.1660e-01,
         5.8208e-04,  1.0447e-01, -5.6721e-02, -8.8299e-02,  1.2456e-02,
         1.3945e-01,  8.1823e-02, -3.7197e-02,  1.0871e-01,  8.5184e-02,
         6.8765e-01, -8.5820e-02,  9.4164e-02,  1.0464e-01, -1.7342e-02,
         6.1707e-02,  1.2247e-01, -6.1066e-02,  7.1153e-01,  2.8669e-01,
         4.3979e-01, -1.4353e-02,  1.2893e-01,  5.8264e-02,  3.6359e-02,
        -2.8831e-03,  4.8507e-02,  6.4796e-01, -1.5176e-02,  4.5346e-02,
         5.1229e-02,  3.9183e-02,  1.4223e-01], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([ 3.8653e+00,  3.1165e+00,  4.5290e+00,  1.6581e+00,  3.6515e+00,
         1.6083e+00,  3.4377e+00,  3.6794e+00,  3.8166e+00,  3.6407e+00,
         3.4594e+00,  1.1493e+00,  3.0919e+00,  3.0618e+00,  4.2050e+00,
         3.0284e+00,  4.5326e+00,  3.0587e+00,  4.1817e+00, -4.1376e+00,
         3.0219e+00,  3.1629e+00,  2.3374e+00,  2.6413e+00,  3.6302e+00,
         2.0773e+00,  3.1919e+00,  4.3215e+00,  3.0816e+00,  3.4885e+00,
         3.4500e+00,  1.6364e+00,  2.9592e-02,  2.5656e+00,  3.1075e+00,
         2.3754e+00,  3.5614e+00,  4.0225e+00,  3.9031e+00,  4.3400e+00,
         3.7831e+00,  4.1174e+00,  4.2838e+00,  4.3583e+00, -3.9438e-03,
         3.4333e+00,  4.0925e+00, -3.0204e+00,  2.5957e+00,  4.0862e+00,
         3.5317e+00,  3.7902e+00,  2.9783e+00,  4.5869e+00,  1.4397e+00,
         4.2127e+00,  2.9209e+00,  2.7688e+00,  3.0986e+00,  3.0154e+00,
         4.4996e+00, -7.0361e-03, -2.2224e+00,  4.0670e+00,  4.1250e-05,
         4.3963e+00,  2.9077e+00, -1.8867e+00,  2.8398e+00,  3.4894e+00,
         2.7322e+00,  3.5967e+00,  3.2891e+00,  1.4144e+00,  3.6632e+00,
         2.8664e+00,  2.3424e+00,  1.1158e+00,  3.1366e+00,  3.9037e+00,
         2.7194e+00,  3.9326e+00,  1.1064e-04,  3.7677e+00,  2.4780e+00,
         1.6537e+00,  3.3926e+00,  3.6510e+00,  3.4254e+00,  2.6829e+00,
         1.5152e+00,  4.1266e+00,  2.7271e+00,  2.9007e+00,  3.5664e+00,
         3.6052e+00,  1.3341e+00,  4.0113e+00,  3.1423e+00, -1.0326e-04,
         2.1316e+00,  3.1253e+00, -2.6277e-03,  2.1456e+00,  3.4065e+00,
         4.1682e+00,  3.7132e+00,  3.8611e+00,  3.6813e+00, -2.2690e+00,
         1.9478e+00,  4.2757e+00,  2.1139e+00,  1.7561e+00,  3.5148e+00,
         3.3863e+00,  1.4889e+00,  2.2887e+00,  3.2048e+00,  4.1658e+00,
         3.5219e+00,  3.1405e+00,  5.7757e-04,  4.1467e+00,  3.5278e+00,
        -7.9522e-04,  3.7840e+00,  3.0982e+00,  4.3053e+00,  3.9867e+00,
         3.7331e+00,  3.1966e+00,  4.0807e+00,  3.7159e+00,  3.8738e+00,
         3.8929e+00,  3.4029e+00,  1.8787e+00,  4.1302e+00, -3.9489e+00,
         2.8961e+00,  2.3566e+00,  2.4809e+00,  2.0301e+00,  3.4692e+00,
         1.0647e-01,  2.5603e+00,  2.5831e+00,  4.1295e+00,  3.0785e+00,
         3.5741e+00,  3.0072e+00,  4.1310e+00,  4.8717e+00,  3.1177e+00,
         3.4960e+00,  2.1775e+00,  2.6923e+00,  3.3190e+00,  3.4729e+00,
         3.7547e+00,  2.1775e+00,  3.3840e+00,  2.3745e+00,  3.9744e+00,
         3.2382e+00,  3.3018e+00,  2.7951e+00,  4.1294e+00,  1.9030e+00,
         3.3698e+00,  3.8765e+00,  4.4529e+00,  1.6363e+00,  1.1373e+00,
         3.5461e+00,  2.8619e+00,  2.0017e+00,  2.2496e+00,  3.3404e+00,
         2.3195e+00,  2.5958e+00,  2.9865e+00,  3.5473e+00,  4.8949e+00,
         4.4801e+00,  7.2535e-01,  3.9030e+00,  4.0998e+00,  4.1367e+00,
         3.1489e+00,  3.4658e+00,  3.5127e+00,  4.6830e+00,  3.4146e+00,
         8.7184e-01,  3.8212e+00,  2.4287e+00,  3.5347e+00,  3.3448e+00,
         3.9384e+00,  2.4383e+00,  3.2260e+00,  2.7112e+00, -6.2123e-04,
         3.6343e+00,  3.4984e+00,  5.2007e-04,  3.8210e+00,  1.6861e+00,
         3.5626e+00,  4.0546e+00,  3.7817e+00,  2.9731e+00,  4.3122e+00,
         3.1081e+00,  4.0373e+00,  2.7450e+00,  1.6859e-03,  3.2499e+00,
         2.5093e+00,  3.5291e+00,  3.3256e+00,  4.0632e+00,  3.4328e+00,
         3.9304e+00,  2.2794e+00,  3.3867e+00,  2.3752e+00,  1.8038e+00,
        -2.4779e-04,  2.5567e+00,  3.2583e+00,  9.3023e-01,  4.3819e+00,
         3.9636e+00,  3.9001e+00,  4.2911e+00,  3.1002e+00,  3.1998e+00,
         4.1374e+00,  7.1580e-02,  3.9907e+00,  1.0032e+00,  1.2704e-02,
         2.4088e+00,  3.9070e+00,  3.6375e+00,  3.6832e+00,  3.8367e+00,
         3.8532e-02,  2.4886e+00,  1.9228e+00,  3.0539e+00,  3.9608e+00,
         2.9584e+00, -2.6252e+00,  3.2666e+00,  3.2123e+00,  1.5541e+00,
         3.7469e+00,  4.5229e+00,  1.2271e+00,  3.2069e+00,  2.8928e+00,
         3.1818e+00,  8.4635e-01,  3.5657e+00,  4.0438e+00,  4.2621e+00,
         2.1627e+00,  3.9381e+00,  2.4560e+00,  3.4299e+00,  4.2743e-04,
         3.9146e+00,  6.3029e-02,  2.1415e+00,  3.3127e+00,  4.4856e+00,
         3.7477e+00,  2.6462e+00,  2.8483e+00,  3.2260e+00,  3.5084e+00,
         4.5317e+00,  2.6413e+00,  2.5403e+00,  2.5541e+00,  3.7172e+00,
         3.4145e+00,  4.1068e+00,  3.5821e+00,  3.4730e+00,  3.0145e+00,
         3.5822e+00,  3.2199e+00,  4.1799e+00,  3.6532e+00,  4.1041e+00,
         3.9260e+00,  2.5267e+00,  3.2483e+00,  3.4546e+00, -6.9367e-03,
         3.5076e+00,  2.8950e+00,  3.4556e+00,  2.9322e+00,  1.8780e+00,
         3.5323e+00,  3.2679e+00,  4.4726e+00,  3.5259e+00,  3.6209e+00,
         3.7377e+00,  4.3861e+00,  2.6278e+00,  3.2014e+00,  2.5317e+00,
         1.6324e+00,  3.1974e+00,  4.1819e+00,  4.1860e+00,  2.8781e+00,
         3.2758e+00,  3.8528e+00,  1.7915e+00,  2.8501e+00,  3.0165e+00,
        -1.5779e+00,  2.2503e+00,  3.1137e+00,  3.7779e+00,  3.4966e+00,
         2.8196e+00, -2.8700e-04,  3.4863e+00,  3.3306e+00,  3.6705e+00,
         2.4159e+00,  3.8003e+00,  8.2212e-02,  3.5358e+00,  4.1861e+00,
         3.7397e+00,  3.3527e+00, -6.5713e-05,  3.8463e+00,  3.7611e+00,
        -9.0628e-02,  4.2595e+00,  4.0079e+00,  4.2197e+00,  1.7247e+00,
         2.4705e+00,  8.4668e-01,  3.9042e+00,  3.3466e+00,  3.2221e+00,
         2.4312e+00,  3.1359e+00,  3.8781e+00,  3.9217e+00,  4.2068e+00,
         2.2260e+00,  3.7071e+00,  3.1913e+00,  3.2796e+00,  2.8853e+00,
         3.1218e+00,  3.7423e+00,  3.6653e+00,  2.5646e+00,  4.3695e+00,
         2.1999e+00,  2.9786e+00,  1.4718e+00,  3.8057e+00,  2.7676e+00,
         4.0317e+00,  3.3795e+00,  2.9125e+00,  4.5216e+00,  3.0238e+00,
         3.2931e+00,  2.5922e+00,  4.5472e+00, -2.1562e+00, -4.9326e-05,
         2.9148e+00,  3.0978e-03,  3.5686e+00,  3.5687e+00,  5.0568e-04,
         2.6303e+00,  3.7423e+00,  3.9560e+00,  2.3392e+00,  2.9434e+00,
        -6.1640e-04,  3.8339e+00,  5.5426e-04,  3.1214e+00,  3.4653e+00,
         3.5810e+00,  3.4742e+00,  3.8396e+00,  3.5853e+00,  2.9728e+00,
         2.5731e+00,  3.9102e+00,  4.3447e+00,  2.5174e+00,  3.4938e+00,
         2.8167e+00,  1.5815e+00,  2.9520e+00,  2.0956e+00,  2.5635e+00,
         1.3676e+00,  3.7521e+00,  3.5313e+00,  1.3147e+00,  2.9331e+00,
         2.5100e+00,  3.2516e+00,  2.2329e+00,  3.6226e+00,  2.9792e+00,
         3.4460e+00,  3.3820e+00, -5.6518e-05,  1.9225e+00,  3.9607e+00,
         3.5181e+00,  3.3777e+00,  3.3577e+00,  3.0281e+00,  3.2415e+00,
        -2.3767e-04,  2.1068e+00,  3.4975e+00,  1.6555e+00,  3.8978e+00,
         4.1638e+00,  2.9894e+00,  2.3906e+00,  2.6347e+00, -1.9361e-03,
         3.4807e+00,  3.6556e+00,  3.1728e+00,  2.1726e+00,  4.4084e+00,
         3.0762e+00,  2.7866e+00,  3.3917e+00,  3.6500e+00,  4.1511e+00,
         1.4027e+00,  4.3715e+00,  3.7856e+00,  3.3241e+00,  2.9393e+00,
         3.8745e+00,  4.0146e+00,  4.2226e-02,  4.1444e+00, -4.3207e-03,
         4.0558e+00,  3.5797e+00,  3.5525e+00,  3.4081e+00,  3.9263e+00,
         3.0155e+00,  2.2074e+00,  5.2980e-01,  3.3494e+00,  3.2191e+00,
         2.8130e+00,  2.7199e+00,  3.5637e+00,  4.6526e+00,  4.0203e+00,
         2.9565e+00,  3.9841e+00,  3.8288e+00,  3.5373e+00, -3.6022e-04,
        -1.6143e+00,  3.6543e+00,  2.4293e+00,  3.5941e+00, -3.1749e-01,
         4.5490e+00,  2.7802e+00,  3.9993e+00,  2.3431e+00,  2.9025e+00,
         3.0749e+00,  3.5290e+00,  2.1428e+00,  3.3854e+00,  2.8383e+00,
         3.9893e+00,  1.8059e+00,  2.2153e+00,  4.2694e+00,  3.3879e+00,
         2.9130e+00,  3.1313e+00,  1.2515e+00,  3.3942e+00,  3.2452e+00,
         1.3342e+00,  2.4612e+00,  1.6861e+00,  4.5725e+00,  1.9277e-04,
         3.1919e+00,  2.0718e+00,  3.0131e-04,  2.9729e+00,  3.7006e+00,
         3.2597e+00,  3.9081e+00,  2.9117e+00,  2.9607e+00,  1.7598e+00,
         3.3503e+00,  3.0719e+00,  3.8285e+00,  3.0814e+00,  3.1737e+00,
         3.0179e+00,  2.6655e+00,  3.9376e+00,  2.0028e+00,  4.1256e+00,
         4.0122e+00,  1.7593e+00,  3.7123e+00,  3.4587e+00,  1.6503e+00,
         3.5956e+00,  3.2066e+00,  4.8301e+00,  2.9145e+00, -7.5316e-02,
         6.4479e-02, -2.3925e-04,  3.5989e+00,  2.7702e+00,  3.6844e+00,
         3.3471e+00,  3.0421e+00,  3.7221e+00,  4.3765e+00,  3.7300e+00,
         3.8157e+00,  4.1610e+00,  3.9255e+00,  3.7783e+00,  4.0711e+00,
         3.5638e+00,  3.2749e+00,  1.7914e+00,  3.5173e+00,  1.7100e+00,
         3.9552e+00,  1.5614e+00,  3.3943e+00,  1.7706e+00,  3.6680e+00,
         3.8299e+00,  3.2760e+00,  2.9876e+00,  2.9626e+00,  4.0743e+00,
         2.1538e+00,  3.7117e+00,  3.5497e+00,  3.7300e+00,  3.8749e+00,
         2.6588e+00,  3.6727e+00,  3.0289e+00,  3.2468e+00,  4.4877e+00,
         2.6150e+00,  4.1823e+00,  1.3555e+00,  3.7501e+00,  6.8052e-03,
         3.3692e+00,  3.6845e+00,  5.3442e-03,  2.0536e+00,  4.1342e+00,
         4.3385e+00,  4.0698e+00,  2.4595e+00,  3.2348e+00,  2.5682e+00,
         2.6737e-04,  3.3638e+00,  3.4750e+00,  2.1245e+00,  3.6775e+00,
         2.9707e+00,  4.5371e+00,  3.3639e+00,  3.8861e+00,  3.4798e+00,
         4.5363e+00,  2.9194e+00,  3.7369e+00,  3.7489e+00,  3.0712e+00,
         2.7413e+00,  3.5640e+00, -2.9846e-04, -4.7880e-02,  4.1277e+00,
         2.0599e+00,  4.8690e+00,  3.4194e+00,  3.4157e+00,  1.9306e+00,
         3.1631e+00,  3.5693e+00,  2.4387e+00,  4.4284e+00,  3.6036e+00,
         3.9395e+00,  1.1678e+00,  2.2277e+00,  3.6891e+00,  3.9389e+00,
         2.3720e+00,  3.3411e+00,  2.9379e+00,  5.8792e+00,  4.2584e+00,
         2.8871e+00,  2.5654e+00,  3.8621e+00,  1.6903e-03,  3.8181e+00,
         3.8377e+00,  4.2851e+00,  3.1167e+00,  2.1207e-01,  3.2695e+00,
         3.2097e+00,  2.5962e+00,  3.5656e+00,  2.4331e-04,  4.2248e+00,
         1.9071e-04,  3.6873e+00,  4.3510e+00, -7.1379e-04,  3.6809e+00,
         3.4676e+00,  2.2374e+00,  2.0259e+00,  4.0962e+00,  2.7214e+00,
         3.5700e+00,  3.1518e+00,  3.5008e+00,  3.2985e+00,  2.7357e+00,
         2.1099e+00,  4.1022e+00,  3.9040e+00,  3.8742e+00,  3.3371e+00,
         4.8824e+00,  1.3807e+00,  3.1213e+00,  3.9656e+00,  3.9793e+00,
         2.7637e+00,  4.1298e+00,  2.4780e+00,  3.2719e+00,  2.2002e+00,
         2.4867e+00,  3.7482e+00,  3.2092e+00,  3.1011e+00,  3.3699e+00,
         3.3762e+00,  4.4621e+00,  2.9772e+00,  3.6100e+00,  3.8410e+00,
         2.7261e+00,  2.6723e+00,  5.9491e+00,  2.3257e+00,  3.0974e+00,
         3.5977e+00,  3.6602e+00,  4.1483e+00,  2.1536e+00,  3.5322e+00,
         3.1865e+00,  4.0348e+00,  4.0583e+00,  1.5135e+00,  3.5035e+00,
         4.1253e+00,  3.4117e+00,  2.3134e+00,  3.9590e+00,  2.5033e+00,
         3.3419e+00,  3.3033e+00,  3.2803e+00,  3.2586e+00,  3.7452e+00,
         1.9358e+00,  3.5736e+00, -9.8807e-04,  3.1859e+00,  3.3065e+00,
         2.6896e+00, -1.1634e-01,  3.4076e+00,  3.8979e+00,  3.3343e+00,
         3.5605e+00,  3.5416e+00,  3.0370e+00,  4.4013e+00,  2.2894e+00,
         2.6079e+00,  3.9566e+00,  3.1758e+00,  3.9136e+00,  1.7508e+00,
         3.2554e-04,  2.1533e+00,  2.6767e+00,  2.5342e+00,  3.5996e+00,
         3.5100e+00,  3.2550e+00,  4.0269e+00,  1.9960e+00,  3.0641e+00,
         3.3546e+00,  2.7171e+00, -6.7951e-03,  2.8085e+00,  3.4320e+00,
         3.7631e+00, -3.5781e+00, -8.9921e-01,  3.7588e+00,  3.8471e+00,
         3.7831e+00,  2.8728e+00,  3.2199e+00], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([-9.7484e-02,  5.6902e-01, -4.6968e-01,  3.2140e-01,  3.0194e-01,
         2.1172e-01,  2.7744e-01,  8.5267e-02,  3.3437e-01,  1.3296e+00,
         6.2620e-01, -1.6493e+00, -4.7276e-01,  8.0249e-01,  4.5049e-02,
        -4.1751e-01,  1.3805e-02, -3.0555e-01, -2.5895e-01,  4.2820e-01,
         3.6498e-01,  2.7136e-02, -3.8449e-01,  5.3296e-01,  2.2479e-01,
        -4.5301e-01,  1.6172e-02, -1.0590e-01, -1.0782e+00, -3.5429e-01,
        -4.1093e-02, -6.0708e-01,  1.2691e-02, -5.4530e-01,  8.9710e-02,
         5.5804e-02,  8.3810e-02,  4.5617e-01,  9.4301e-02,  5.7691e-01,
         1.0292e-01,  4.7243e-01,  7.2416e-01,  5.5186e-01,  3.0296e-04,
         7.4049e-02, -5.4222e-01,  5.7659e-01,  7.4357e-01, -8.4169e-01,
        -5.0381e-01, -6.8478e-01, -6.7817e-01,  1.2001e-01,  1.4228e-01,
         8.4998e-01, -5.8020e-03,  2.3015e-01,  5.0663e-01,  3.9178e-02,
         1.2269e+00,  3.2394e-04,  9.8683e-02,  4.4620e-01, -3.4137e-05,
        -1.7969e-01,  5.2487e-01,  2.8738e-01,  4.8312e-01, -8.6785e-01,
        -1.1298e-02,  9.2246e-01, -2.5673e-01, -8.6341e-01,  2.6100e-02,
        -2.7345e-01,  4.6976e-02, -8.6560e-02, -4.7107e-01,  4.8977e-01,
         1.9709e-01,  5.2972e-01, -2.4313e-04, -2.3650e-02,  2.5917e-01,
        -7.0771e-01,  1.0101e-01, -2.5844e-01,  2.3854e-01, -4.8508e-02,
        -1.3626e-01, -1.2972e-01,  2.7464e-01,  5.9622e-01,  1.3487e-01,
         1.0433e-01,  1.2873e+00, -3.0692e-01, -1.1349e-01,  2.0205e-04,
        -3.5793e-01,  1.0750e+00, -1.4235e-03,  1.8748e-01,  5.6353e-01,
        -8.0498e-02,  1.4993e-01, -6.5382e-01, -6.7865e-02, -5.6622e-01,
         3.4820e-01, -6.9409e-02,  2.1410e-01, -1.1561e-01,  2.7485e-01,
         4.7580e-01, -2.4369e-02,  2.0227e-02,  6.4899e-01, -4.9263e-01,
        -3.2916e-01, -2.5118e-01,  5.6195e-04,  3.8367e-01, -3.1181e-01,
        -1.4097e-03,  1.5120e-02, -4.7378e-02,  5.0222e-01, -3.1497e-01,
        -3.6823e-01,  1.8156e-01,  3.6251e-01, -4.3723e-01, -1.0093e-01,
         1.5098e+00, -1.4597e-01,  9.4389e-01,  3.1714e-01,  5.6053e-01,
        -9.3297e-02,  4.8828e-01,  4.5397e-01,  2.2534e-01,  6.5987e-02,
        -7.1127e-02, -2.1649e-01, -2.1632e-01, -1.8267e-01, -1.4317e-01,
        -5.8505e-02, -1.0546e-01,  3.6690e-01,  6.2943e-01,  1.2185e-01,
        -6.4551e-01,  4.4640e-01,  3.9495e-01,  6.2211e-01, -1.9490e-01,
         9.4710e-01, -1.1166e+00,  4.0503e-01, -3.0921e-01,  7.1014e-01,
         6.5745e-01,  6.2015e-01,  7.2518e-02,  3.9815e-01,  4.8317e-01,
         6.3726e-01,  3.5259e-01, -2.8051e-01,  3.7037e-01, -1.1520e+00,
        -6.8311e-02,  5.6710e-01, -6.5703e-02, -5.7750e-01,  2.6568e-01,
        -2.0187e-01, -3.8216e-01, -5.2650e-02, -3.9794e-01, -6.2709e-01,
        -9.4528e-01, -3.9432e+00,  3.1726e-01,  2.1078e-01, -3.9869e-01,
         9.8251e-01,  3.6200e-01, -3.0470e-01, -1.2183e-01, -2.4963e-01,
         6.6312e-03, -5.4332e-01, -1.0990e-01,  1.4444e-01, -3.4709e-01,
         5.0297e-01,  7.5252e-02,  2.8666e-01,  1.2925e-02, -3.0368e-04,
         7.2028e-01,  1.5800e-01,  5.0554e-04, -1.5220e-01, -6.9810e-02,
        -2.2954e-01,  2.1814e-01,  4.8142e-01, -3.2094e-01, -9.3545e-02,
         4.7320e-01, -6.7127e-01,  2.6978e-01, -2.7419e-03,  4.2761e-01,
         3.3211e-01,  6.4086e-01,  7.8721e-01, -9.8475e-01, -1.5272e-01,
        -2.5117e-01,  1.3470e+00,  1.4989e-02, -1.2141e-01,  5.4030e-02,
         5.9382e-04, -4.5727e-02, -1.9832e-01,  8.2416e-02,  4.2481e-03,
         8.4224e-02, -1.2540e-01,  3.0216e-01,  3.1064e-01, -2.8028e-01,
         2.7703e-02,  2.1448e-02, -2.9076e-01, -1.9064e+00,  1.9664e-03,
         3.1586e-02,  3.1318e-02, -8.1416e-01,  3.1955e-01,  1.7222e-01,
         1.3468e-02,  3.0597e-01,  6.7196e-03,  2.2497e-01,  3.6282e-01,
        -3.7732e-01, -9.4471e-02,  1.2307e-01,  5.3786e-02, -3.6008e-01,
         2.1328e-01,  3.0911e-01,  2.9255e-02,  5.0711e-01, -4.5868e-01,
         5.1919e-01,  4.3625e-01,  1.8168e-01, -5.5767e-02, -8.9798e-01,
         8.8103e-02, -6.7206e-01,  9.2677e+00,  1.3844e-01, -1.1530e-03,
         2.0218e-01,  2.0589e-02, -1.9576e-01,  8.1757e-01,  1.1819e-01,
        -1.5091e-01, -4.6339e-01, -1.4974e-01,  3.5963e-02,  7.5227e-01,
         7.7564e-02,  4.9079e-01,  1.5953e-01,  4.8051e-01, -6.6454e-01,
         5.5871e-02,  5.5325e-01,  1.8891e-01, -2.5345e-01,  9.2568e-02,
        -1.3534e-01,  1.7035e-02, -4.4323e-01,  8.7209e-01, -6.9571e-01,
        -6.7856e-01, -3.4932e-01,  3.4631e-01, -1.4266e-01, -1.6692e-03,
         5.3150e-01,  6.2719e-02,  2.1717e-01,  8.9105e-02,  3.3517e-01,
         6.3095e-01,  1.2375e+00, -7.1619e-01,  3.3974e-01, -2.3551e-02,
        -3.5789e-02, -2.8336e-01,  6.9395e-02,  6.4937e-01, -1.2543e-01,
         5.8805e-01,  7.0710e-02,  5.4155e-01, -1.4333e-01,  4.4404e-01,
        -1.6259e-01,  2.1047e-01,  1.2811e+00, -3.3844e-01,  5.5718e-02,
        -1.2373e-02,  2.5825e-01, -2.1133e-01,  5.4085e-01, -1.7035e-01,
        -1.3842e-01,  2.7044e-04, -4.8924e-01, -1.5452e-01, -2.2621e-01,
        -1.9004e+00,  1.2676e-02,  6.7194e-02,  1.1559e-01,  1.5438e-01,
         1.6636e-01,  8.6411e-02, -3.4787e-04, -5.6098e-01, -1.0485e+00,
         1.3577e-02,  6.8804e-01,  3.2318e-01, -3.7677e-01,  9.7465e-01,
        -2.3231e-02, -1.7795e-01, -1.4290e-01,  2.9665e-01,  1.3633e-01,
         1.1182e-01,  1.1296e-01, -1.2311e-01, -6.5858e-01, -3.5720e-01,
        -1.1104e-01, -2.7181e-01, -3.9215e-01,  2.1458e-01,  2.3627e-01,
        -9.5227e-01, -2.6372e-01, -4.5967e-01,  5.0104e-01, -9.3809e-02,
         3.7204e-01,  8.8322e-02,  4.3875e-01,  5.7194e-01,  2.5099e-01,
         4.8540e-01, -3.3244e-01,  4.6529e-01, -1.3071e-01,  8.4006e-02,
         2.9924e-01,  8.3355e-02,  8.6818e-02,  3.2150e-01, -4.5623e-04,
         8.0699e-02,  1.4717e-04,  3.5946e-01,  8.6057e-01, -6.0676e-05,
        -3.4311e-01,  6.9983e-04,  4.3940e-01, -2.7946e-01, -4.4893e-01,
        -2.8425e-04, -2.0501e-01, -6.5174e-04,  2.2570e-01, -1.7314e-01,
         6.1289e-02, -1.0303e+00, -3.1230e-01,  3.8574e-03, -3.8238e-01,
         3.7009e-01, -3.0534e-01,  4.1858e-01,  1.0766e+00, -5.2668e-01,
         7.0852e-01, -3.2406e-01,  2.5173e-01,  8.3430e-01, -5.0772e-01,
         2.1062e-01,  3.3559e-01,  4.9194e-01,  1.1399e-01,  3.4512e-01,
         8.7735e-02, -3.6899e-01,  4.3592e-01, -2.7638e-01,  1.7805e-01,
        -2.4465e-01, -1.4868e-01, -2.2159e-06,  5.7571e-01,  2.6543e-01,
        -3.0325e-01, -1.7568e-01, -2.8756e-01,  6.7290e-02, -1.6069e-01,
         4.9859e-04,  9.7020e-02,  1.0772e-01,  3.4898e-02, -2.1877e-01,
         5.1196e-01,  6.3516e-01, -1.2289e-01, -2.9608e-01, -1.0385e-03,
        -3.4227e-01,  5.2099e-01,  5.5720e-01, -6.9667e-01,  6.5551e-02,
        -8.6432e-01,  3.4075e-01,  5.6029e-02, -4.2334e-01,  6.0621e-03,
         3.2926e-01, -3.0589e-01,  4.5676e-01, -2.3554e-01,  5.6265e-01,
         1.9850e-01,  3.2241e-01, -1.4407e-02, -8.4698e-02,  2.1265e-03,
         3.6593e-01, -1.6022e-01,  7.6383e-02, -4.2135e-01,  2.0392e-02,
        -3.2482e-01, -3.3382e-02,  2.8000e-01,  8.5534e-02, -2.0941e-02,
         6.9992e-02,  2.2699e-01,  6.8410e-01, -7.0308e-01,  5.8668e-01,
         2.4510e-02, -2.3814e-01, -1.0122e-01, -1.4849e-01,  5.5165e-04,
         9.6652e-01, -4.7909e-01, -2.0958e-01, -3.4437e-01,  7.6096e-02,
         2.8297e-01,  3.4340e-01, -5.0304e-02, -7.4096e-01, -3.9395e-01,
         4.1021e-01,  4.6465e-02,  1.9436e-01,  5.6049e-01,  3.4729e-01,
         4.8854e-01,  2.9430e-02,  9.1861e-01,  4.0835e-01,  6.7103e-01,
         2.7128e-01,  2.4395e-01, -7.7566e-02,  1.2424e+00, -1.0368e-01,
         1.6321e-01, -1.5717e-01, -1.1468e-01,  1.2738e-01,  5.4700e-06,
         8.1447e-01, -3.6846e-01, -1.9615e-05, -2.3154e-01, -2.8030e-01,
         3.0372e-01,  2.1022e-01, -1.2260e-01,  3.0862e-01,  1.6327e+00,
        -1.9297e-01,  2.5285e-01, -6.3982e-01,  1.7750e-01, -1.9318e-01,
         2.8310e-01,  3.0362e-01,  1.1620e-01, -9.4228e-01,  9.1405e-02,
         5.0763e-01,  3.4950e-01,  1.3577e-01, -7.4698e-01,  6.2979e-01,
         7.3192e-01, -2.1286e-02,  5.2864e-01,  1.2730e-02, -4.3799e-02,
         1.7699e-01, -9.7363e-05, -8.3526e-01,  3.4650e-01,  1.2268e-01,
        -1.4678e-01,  3.5796e-01,  9.7326e-02, -2.2141e-01, -2.6542e-01,
         3.8145e-02,  1.0002e-01,  3.6836e-01,  3.8001e-01,  3.1593e-01,
         2.0860e-01, -4.7621e-01,  3.8306e-01,  4.2335e-01, -9.4643e-01,
         3.9496e-01,  9.3140e-01, -3.1888e-01,  4.9871e-01, -4.0882e-01,
         8.3727e-02,  2.1403e+00, -1.3239e-01,  8.1997e-01, -5.6595e-01,
         5.4734e-01,  2.3250e-01, -1.6457e-01,  1.5878e-02,  1.3331e+00,
         1.7161e-01, -4.1075e-01, -8.3826e-01, -3.0978e-01,  8.9526e-02,
         1.7128e-01, -5.7087e-01, -8.1335e-01,  1.4282e-02, -6.7671e-03,
        -9.4589e-02,  1.1866e-01, -1.5126e-02, -5.7748e-01,  4.4894e-02,
         9.3146e-02, -6.3449e-01, -2.8269e-01,  7.1664e-01,  3.9586e-01,
        -1.9582e-04, -6.8679e-01,  2.0763e-01, -1.5304e-01,  3.8786e-01,
         3.8097e-01,  1.3043e+00, -3.5134e-01,  2.1608e-01, -2.5468e-01,
        -5.7364e-02,  2.0509e-01, -9.3000e-01, -2.8236e-01, -4.6326e-01,
         1.1722e+00,  9.0574e-02, -4.1199e-04,  1.2345e-02, -1.6841e-01,
         1.7000e+00,  2.2140e-01,  5.1252e-01,  3.0566e-01, -4.4666e-01,
        -1.1180e-01, -4.8205e-01, -2.7119e-02,  8.6481e-01, -5.6711e-01,
        -6.7565e-02, -1.0528e+00, -1.0161e-01,  4.1712e-01,  1.6258e-01,
         3.2856e-01,  1.4491e-01,  1.7947e-01,  1.9221e+00, -4.0700e-01,
         4.9105e-01,  3.9097e-02,  1.8570e-01, -2.9750e-04,  2.6533e-01,
        -2.5029e-01, -5.8985e-01, -2.0063e-01, -6.6683e-02,  1.9062e-02,
         5.9849e-01,  6.1312e-01,  2.4529e-01,  1.9244e-04,  1.7670e-01,
        -9.7840e-04, -1.0746e-01,  4.5170e-01,  2.4027e-04,  3.6712e-01,
        -1.8153e-01, -2.7021e-01,  2.6855e-01,  3.2896e-01, -5.4589e-02,
        -2.6625e-01,  3.2036e-01,  4.2279e-01,  5.3833e-02, -2.9319e-01,
         5.2283e-01,  5.7344e-01,  2.2706e-01, -4.3813e-01,  5.5155e-02,
         1.8987e+00,  4.1386e-01,  7.0525e-01,  6.1034e-01,  9.8500e-01,
         1.4129e-01,  5.8295e-01,  1.1732e-01, -7.1918e-02,  5.0518e-01,
        -2.2765e-01, -6.2554e-02, -2.2903e-01, -7.9104e-01, -4.1275e-01,
         2.4737e-01,  9.2875e-01,  1.1827e-01,  5.4653e-01, -2.5171e-02,
         1.4722e-01,  1.4636e-01, -1.5214e+00,  3.5526e-02, -2.2526e-01,
        -1.2744e-01,  8.7494e-02,  8.6312e-01, -2.6749e-01, -4.0782e-01,
         3.3625e-01,  4.1112e-01, -3.2597e-03,  5.3532e-01, -3.2127e-01,
        -1.5272e-01, -4.3328e-01,  6.2625e-01, -4.6366e-02, -6.8110e-02,
         1.7899e-01,  1.4630e-01,  4.7515e-02, -6.7495e-01, -1.1843e-01,
         3.5687e-01,  9.2315e-02, -1.8205e-03, -6.2719e-01, -6.0913e-01,
        -3.6412e-01,  9.4658e-02, -3.0682e-01, -2.0901e-01, -4.8793e-01,
         6.6064e-02, -7.5259e-01,  3.9913e-01,  5.3244e-01,  2.1897e-01,
        -4.8314e-01, -1.2580e-01,  1.3100e-01, -3.2855e-01,  9.2786e-03,
        -6.8991e-04,  1.9442e-01, -3.4117e-01,  3.2392e-01,  2.6179e-01,
         5.3533e-01, -2.8967e-01,  5.7741e-01,  7.0399e-01,  1.8483e-01,
        -9.4652e-03, -2.7149e-01,  5.7139e-03, -1.7765e-02,  6.5116e-02,
        -1.9484e-01,  1.7539e-01, -5.2911e-01,  7.4880e-02,  9.3777e-02,
         5.4853e-02,  4.8387e-01, -4.9404e-01], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[-0.0185,  0.0128,  0.0068,  ..., -0.0075,  0.0040,  0.0046],
        [ 0.0031, -0.0158, -0.0029,  ..., -0.0123,  0.0079,  0.0051],
        [-0.0040,  0.0051, -0.0042,  ...,  0.0075, -0.0370,  0.0110],
        ...,
        [-0.0043, -0.0036,  0.0162,  ..., -0.0080, -0.0058, -0.0166],
        [-0.0057, -0.0148,  0.0177,  ...,  0.0054,  0.0029, -0.0105],
        [ 0.0147, -0.0108, -0.0096,  ..., -0.0060,  0.0085, -0.0116]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-4.3312, -5.0542, -4.9329,  ..., -5.1544, -5.0990, -5.1538],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0096,  0.0129,  0.0285,  ..., -0.0044,  0.0048,  0.0127],
        [-0.0071, -0.0034, -0.0003,  ..., -0.0050, -0.0046,  0.0041],
        [-0.0152, -0.0358,  0.0546,  ..., -0.0105,  0.0135,  0.0021],
        ...,
        [ 0.0067,  0.0091,  0.0069,  ...,  0.0017, -0.0014, -0.0096],
        [ 0.0169,  0.0041,  0.0333,  ...,  0.0046,  0.0299, -0.0105],
        [ 0.0205, -0.0171, -0.0211,  ..., -0.0035,  0.0018, -0.0039]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 8.7858e-02,  6.8485e-02, -9.4689e-02,  1.1237e-01,  1.6171e-01,
         1.0083e-01, -4.0980e-02,  2.3847e-02,  1.3448e-01,  7.7147e-02,
         2.2004e-01, -5.5500e-01, -2.2711e-01,  2.0241e-01, -5.6965e-03,
         4.1227e-02, -1.4804e-02,  2.6183e-02, -2.0395e-01, -6.0915e-02,
         4.1660e-02, -8.5721e-02,  6.0316e-02,  2.1076e-01, -4.6952e-03,
        -2.8458e-01, -4.5952e-02, -8.8157e-02, -6.9793e-02, -4.1130e-02,
        -1.2395e-01, -1.5501e-01,  5.7983e-02, -2.2546e-01,  1.4980e-01,
         6.3761e-02,  6.9051e-02, -1.0327e-01,  8.5448e-02,  4.0364e-02,
        -1.1287e-02,  1.0031e-01,  6.1512e-02,  5.3133e-02, -6.9990e-02,
         2.3347e-02, -1.1291e-01,  1.8666e-02,  1.0759e-01,  7.2377e-02,
        -6.2828e-02,  9.0125e-02, -1.9110e-01,  2.4328e-02,  5.2331e-02,
        -7.7534e-03,  6.7656e-02, -6.5712e-02,  1.4086e-01, -8.3639e-02,
        -9.1600e-03,  5.1479e-02, -3.1074e-02,  1.1218e-01, -2.4088e-01,
         6.0351e-02, -1.3792e-01, -2.6311e-01,  2.3906e-01, -4.5298e-02,
         9.6074e-02,  1.0733e-02, -1.0981e-01, -1.9196e+00, -9.4813e-02,
        -1.2073e-01, -8.9985e-03, -1.4317e-01,  3.0385e-02, -6.2018e-02,
         1.1693e-01,  1.4078e-01, -2.2896e-01,  6.1573e-02,  1.0923e-01,
        -3.3417e-01, -7.7330e-02,  1.6092e-02,  1.4399e-01, -1.7138e-01,
        -1.2858e-01,  1.4314e-01,  9.8052e-02,  1.4666e-01,  6.8101e-02,
        -2.0541e-02,  6.4347e-01,  1.0687e-03, -9.4947e-02, -1.0475e+00,
        -1.4121e-01,  1.1811e-01,  2.5496e-01, -2.3431e-02, -1.5489e-02,
        -1.7899e-01,  2.9365e-02, -1.6999e-01, -3.3154e-02,  2.7768e-01,
        -9.4571e-03, -3.4787e-02,  2.0496e-02, -7.2604e-02,  1.8767e-02,
         1.0067e-01,  2.6595e-02,  7.9647e-02,  1.5597e-01,  1.1296e-02,
        -1.1450e-02, -1.7792e-01,  2.2050e-01,  9.5171e-02, -2.8125e-03,
        -2.4041e-02, -7.9723e-02,  7.3706e-02, -1.4754e-01,  3.0383e-01,
        -1.2168e-01,  6.7152e-03,  2.8623e-01,  4.2366e-02,  2.1715e-01,
         1.9562e-01,  1.1379e-02,  3.3652e-01,  2.6619e-01, -8.5347e-02,
        -5.0044e-02, -1.9659e-02,  1.0558e-01,  5.4501e-02, -9.8598e-02,
        -1.5916e-01, -3.5430e-02, -7.0248e-02,  1.7230e-02, -1.7610e-01,
         3.7837e-02, -1.3444e-02,  7.0766e-03, -1.3738e-01,  1.0387e-01,
        -1.6165e-02,  1.5519e-01,  1.4127e-02,  5.6342e-02, -9.9774e-02,
         2.9382e-02,  1.8373e-01,  2.1298e-01, -2.5120e-02,  6.5080e-02,
        -1.3132e-01,  1.4370e-01, -7.7668e-02, -9.9610e-03,  2.0029e-01,
         1.6923e-01,  5.8793e-02, -1.4502e-01,  2.1613e-01, -5.1185e-01,
         1.4234e-03,  1.6525e-01, -1.2988e-01, -1.7943e-01,  9.3696e-02,
        -8.2199e-02, -1.4235e-01, -1.3404e-01,  6.7234e-02, -7.8838e-02,
        -1.4593e-02, -4.5020e+00,  1.2377e-01,  4.0894e-02, -1.7472e-01,
         1.3933e-01,  1.3499e-01,  5.5441e-02,  1.2400e-01, -5.3515e-02,
        -3.9923e-02, -1.5102e-01,  4.0503e-02,  1.5436e-01,  6.8166e-03,
         6.9146e-02, -3.8246e-02,  1.6590e-01, -1.3184e-02, -3.7661e-01,
         1.4292e-01,  6.0477e-02,  2.4387e-01, -6.0087e-03, -6.5263e-02,
         4.9381e-03,  1.0994e-01, -1.7069e-01, -1.3449e-03,  6.2988e-02,
         1.1637e-01, -7.7580e-02, -3.5093e-02, -2.2328e-01,  1.9291e-01,
         3.7808e-02,  1.9098e-01,  2.5876e-02, -1.0397e-01,  2.8700e-02,
         3.0584e-02, -4.3139e-02,  1.0287e-01,  1.3306e-01,  3.0708e-02,
         2.5859e-01, -2.4253e-04,  2.7416e-02, -4.1919e-01,  8.5059e-02,
         1.9847e-01, -1.7631e-02,  1.3505e-01, -2.7558e-02, -1.1137e-01,
         2.1571e-02, -9.5818e-02,  1.1280e-01, -5.7215e-01,  4.7990e-02,
        -3.7045e-02,  6.0540e-02, -8.9568e-02,  2.1658e-02,  1.1277e-02,
         6.8174e-02,  1.2445e-01, -2.1292e-01, -1.9056e-02, -4.7701e-03,
        -4.4919e-02,  1.8657e-01,  1.0267e-01,  6.7242e-02,  2.1359e-01,
         8.6867e-02, -1.7425e-01, -6.4454e-02,  2.5982e-01, -2.0440e-01,
         1.2881e-01,  3.5972e-01, -2.1667e-02, -2.8284e-02, -2.8184e-02,
         5.8791e-02, -8.1949e-02,  3.5934e+00,  9.1368e-02, -1.7669e-01,
        -7.2062e-02,  1.6805e-01, -1.7104e-01, -1.3824e-02,  5.2400e-02,
         8.6239e-02, -1.1514e-01,  1.6844e-03,  8.1537e-03,  3.3268e-01,
        -1.3614e-01,  1.1263e-01,  1.5441e-02,  6.6890e-02,  1.9496e-02,
         4.9465e-02,  4.3088e-02,  2.8112e-02,  1.4617e-01,  8.8883e-02,
        -2.1138e-02,  5.0601e-02, -9.9735e-02,  1.8272e-01, -1.1156e-01,
        -1.3084e-01,  1.8035e-03, -3.0013e-02, -1.8717e-01, -3.2544e-01,
        -7.6239e-02,  2.3195e-02,  2.7944e-02, -2.9968e-02,  7.4618e-02,
         3.4899e-01, -1.1067e-01,  7.4016e-02, -1.0560e-01, -2.1044e-01,
        -1.0136e-01,  3.5866e-02, -2.4500e-02, -3.1685e-02, -3.2100e-02,
         6.5873e-02, -6.8225e-02, -3.4466e-03, -6.4872e-02,  3.7173e-02,
        -8.4355e-03, -9.5365e-03,  3.6555e-01, -1.0341e-01, -4.4842e-02,
        -4.8094e-02,  1.3583e-01, -7.4273e-02, -1.0994e-01, -1.0448e-01,
        -4.3233e-02, -1.1012e-02,  6.2817e-02, -2.0302e-02,  1.7504e-01,
        -6.0285e-01, -1.0286e-01,  2.7109e-01, -7.4577e-02,  9.7853e-02,
         3.5644e-02, -4.1614e-02, -4.8146e-02, -2.2387e-01, -3.5649e-02,
        -2.8273e-01,  8.7865e-03, -5.0214e-02, -6.4593e-03,  3.5199e-01,
         4.7891e-02, -2.7229e-01, -1.0626e-01, -6.6050e-02,  1.1070e-01,
        -7.8940e-02, -3.8638e-03,  2.0971e-02,  6.5002e-02,  1.9313e-02,
        -4.0176e-02, -8.4295e-02, -9.0769e-02,  1.3767e-01, -1.5862e-01,
        -2.3877e-01, -1.4036e-01,  3.6712e-02,  1.3053e-02,  7.7399e-02,
         3.7604e-01,  1.8848e-01,  1.0471e-02, -2.8212e-02,  2.5851e-02,
         1.1604e-01, -2.3274e-03,  1.7115e-01,  6.3610e-02,  8.8991e-02,
         1.2148e-02,  1.0084e-01,  3.3853e-02, -1.6689e-01, -3.2607e-02,
         1.7856e-01,  1.1220e-01,  1.0607e-01,  1.0094e-01, -1.6382e+00,
        -1.2208e-01, -1.2451e-02,  1.0692e-01, -9.5128e-02,  2.0454e-02,
         6.7885e-01, -3.5769e-02, -1.7533e-01,  5.6995e-02, -9.8644e-02,
        -1.9517e-02, -1.8481e-01,  1.2416e-01,  1.1021e-01, -1.6452e-01,
         1.6912e-01,  1.4217e-01,  1.5011e-02, -9.8491e-02, -1.8868e-01,
         1.7103e-01, -1.2778e-01, -7.9839e-03,  8.0338e-02, -1.3336e-01,
        -1.8946e-01, -9.4729e-03,  2.3725e-01,  1.4395e-01, -1.1926e-01,
         8.5909e-02, -9.5583e-02,  1.1921e-01,  8.8042e-02,  2.6016e-02,
        -1.8503e-01, -7.4718e-02,  5.1545e-01,  2.1839e-01,  1.7369e-02,
        -8.5426e-03,  3.9846e-02, -7.3030e-02, -1.5144e-01,  9.1896e-02,
         4.6792e-01,  3.6060e-03,  7.5704e-02, -6.7343e-02,  7.7555e-02,
         3.4799e-02,  8.2126e-02,  1.9653e-01, -9.5600e-02,  1.9749e-01,
        -3.2906e-02,  2.7350e-02,  1.8474e-01, -1.1078e-01, -1.2593e-02,
        -2.5271e-01,  6.4132e-02, -4.3392e-02,  7.2425e-02,  1.0440e-02,
         1.6389e-01, -1.1760e-02,  1.3156e-01, -8.7216e-02, -1.9717e-02,
         1.6508e-01,  2.6169e-01, -1.8179e-01, -5.3347e-02, -6.5802e-02,
         8.6229e-02, -1.5759e-01,  2.7184e-02, -2.9918e-02, -1.0028e-02,
        -5.0935e-02,  6.2068e-02, -7.6400e-01,  4.3433e-02,  3.5628e-03,
        -6.9015e-02, -6.4498e-02,  2.9897e-01,  7.6045e-02,  9.6593e-02,
        -2.1013e-02, -1.0868e-01, -9.1305e-02, -3.0553e-02, -9.3752e-02,
        -1.0525e-01, -3.6441e-02, -9.2039e-02, -1.4150e-02,  8.4031e-01,
         1.5972e-02,  9.0259e-02,  5.3165e-03, -1.8870e-01,  9.3341e-02,
        -1.0377e-02, -1.9550e-02,  4.5466e-03,  3.4962e-02,  9.6973e-02,
        -6.5136e-02, -1.8623e-02,  3.6416e-01, -8.7581e-02,  1.9295e-01,
        -2.8300e-02, -1.4172e-01, -3.3494e-02,  1.3051e-01, -1.1193e-01,
         7.5109e-02,  1.9578e-02,  2.3519e-02, -1.2044e-01,  2.0719e-03,
         1.3134e-01, -8.7169e-02,  6.1953e-02, -1.0636e-01,  1.2751e-01,
         6.0106e-02,  1.1438e-01, -3.0576e-02,  6.1610e-03,  4.5109e-01,
         3.4375e-02,  4.8626e-02, -1.8583e-02, -6.1685e-02, -8.6194e-03,
        -2.6896e-02,  2.1391e-02,  9.1741e-02,  2.2605e-02, -2.7712e-02,
         1.4158e-01,  2.2449e-01,  1.8829e-01, -1.7448e-01, -6.7159e-02,
         1.1642e-01, -2.1588e-02, -1.3143e-01,  2.0775e-02, -5.2900e-01,
         8.9629e-01,  7.6719e-03,  3.0294e-02,  1.8724e-02,  6.8431e-02,
         1.2600e-01,  5.5317e-02,  1.5825e-01,  2.0695e-02,  1.1859e-02,
         2.4746e-02,  1.7369e-02, -3.9022e-01,  1.1733e-01,  3.3748e-02,
         2.5647e-02, -9.3572e-02,  1.3847e-01,  8.1838e-02,  4.1207e-02,
         3.8778e-02,  2.2542e-01, -1.0419e-01, -1.3600e-01, -1.7695e-02,
         1.1201e-01,  3.3297e-01, -7.8916e-02,  1.4284e-01, -5.3299e-02,
        -1.0439e-01,  1.3080e-01, -8.1232e-03, -2.1532e-01,  3.4838e-02,
        -4.5737e-03,  1.5023e-01, -1.6142e-01, -1.5071e-01, -1.1321e-01,
        -3.9390e-02, -6.9676e-01, -1.7920e-01, -3.9135e-02, -9.3569e-02,
         6.3767e-02,  5.6826e-02,  2.2182e-01,  2.0160e-01, -2.4882e-02,
        -8.0718e-02,  3.6525e-02, -1.8609e-01,  1.7135e-01, -7.9903e-03,
        -1.3266e-01,  4.5399e-02, -7.9829e-02, -2.3008e-02, -1.8440e-02,
         1.0227e-01,  1.2385e-01, -3.7695e-02,  1.3774e-01, -1.0991e-01,
         7.4495e-02,  1.3709e-01,  6.7307e-02, -3.2222e-02, -1.8478e-01,
         1.6878e-01, -4.6568e-02, -9.5325e-02, -3.2407e-02, -7.9363e-03,
         4.7699e-01, -8.2508e-03,  9.5388e-02,  9.6013e-02, -3.1300e-01,
        -1.8204e-01, -6.7545e-02, -1.1519e-01, -3.5458e-02, -1.6376e-01,
        -1.0018e-02, -3.6243e-01, -6.4890e-02,  1.9269e-02,  1.0950e-01,
        -3.1893e-02,  3.9511e-02,  3.2733e-02, -1.6695e-01, -4.8210e-02,
         8.8567e-02, -1.5570e-01,  8.8592e-02, -1.1953e-03,  8.8017e-03,
         2.5445e-02,  1.0233e-01,  1.2830e-01,  4.9740e-01, -3.2483e-02,
         7.5547e-02,  6.8581e-02,  2.3985e-02, -1.8314e-01, -3.7670e-02,
         4.4190e-02, -4.8116e-03, -6.1745e-02, -7.5571e-02,  6.4337e-02,
        -1.8442e-01, -2.3253e-01,  1.1038e-01,  4.4361e-02, -3.3439e-02,
         1.7653e-02,  9.2851e-02,  6.6199e-02,  6.9094e-02, -1.1801e-01,
         3.6094e-02, -1.8010e-02, -6.1672e-02,  7.1056e-02, -1.3262e-01,
        -2.0788e-01,  8.1037e-02, -4.4617e-04,  6.9792e-02,  3.3864e-02,
        -8.1383e-02,  1.2466e-01,  4.8332e-02, -4.6180e-02,  7.0561e-02,
        -9.3636e-02, -1.6211e-02, -7.1664e-02, -7.4098e-01, -7.8872e-02,
        -1.5815e-02,  3.0219e-03,  8.6269e-02,  8.0929e-02, -1.9344e-01,
         4.0638e-03,  6.5358e-02, -9.2687e-02, -2.5527e-02, -7.7478e-02,
        -1.1468e-01,  5.0995e-02,  6.9982e-02, -3.8581e-02, -1.6173e-01,
         7.6995e-02, -1.7860e-02, -1.1515e-01,  4.0278e-02,  8.5927e-02,
        -1.0771e-01, -1.4709e-01, -2.2180e-02, -3.4174e-02,  3.9868e-02,
        -4.4907e-02,  5.0666e-02, -1.2785e-01, -1.0556e-01, -7.8794e-02,
         1.3681e-01, -1.7313e-02,  4.2390e-01, -1.6909e-01, -1.8453e-01,
        -1.8109e-01,  3.3584e-02, -1.3543e-01,  4.0614e-02, -1.6447e-01,
         6.5140e-03, -1.0357e-01,  7.5707e-02,  1.3948e-01,  1.9312e-01,
        -5.2018e-02, -8.9340e-02,  3.3937e-02, -6.7607e-02,  6.3463e-02,
        -7.3600e-02,  3.9170e-02, -1.3810e-01,  1.8326e-02,  2.3135e-02,
         3.6882e-01,  3.9995e-02,  1.3364e-01,  2.6600e-01, -2.7374e-01,
        -5.6863e-01, -3.9388e-02, -5.2296e-01, -2.4309e-02, -6.1552e-02,
        -6.5935e-02,  7.6693e-03,  1.1322e+00,  3.1588e-02,  8.9532e-02,
         4.5770e-02,  1.4155e-01, -2.0149e-01], device='cuda:0',
       requires_grad=True)]
2024-05-15 10:08:37.026 | INFO     | __main__:train:38 - [Parameter containing:
tensor([[-0.0209,  0.0015, -0.0096,  ..., -0.0132, -0.0120, -0.0025],
        [ 0.0158,  0.0068, -0.0087,  ..., -0.0080,  0.0171, -0.0256],
        [ 0.0090,  0.0195, -0.0171,  ..., -0.0168,  0.0224,  0.0026],
        ...,
        [-0.0174, -0.0419,  0.0279,  ...,  0.0398,  0.0322, -0.0135],
        [-0.0101, -0.0026, -0.0056,  ..., -0.0073, -0.0049,  0.0053],
        [-0.0075,  0.0008,  0.0068,  ..., -0.0527, -0.0120, -0.0220]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0217, -0.0037, -0.0286,  ..., -0.0266, -0.0071, -0.0099],
        [ 0.0082,  0.0291, -0.0091,  ...,  0.0227, -0.0088, -0.0139],
        [-0.0290,  0.0016,  0.0114,  ..., -0.0221, -0.0064, -0.0015],
        ...,
        [ 0.0013, -0.0146,  0.0251,  ...,  0.0174, -0.0345, -0.0066],
        [ 0.0308, -0.0225,  0.0237,  ...,  0.0055,  0.0013,  0.0192],
        [-0.0324, -0.0109, -0.0120,  ...,  0.0152, -0.0227, -0.0161]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 2.0191e-02, -1.8776e-02,  1.1265e-03,  ..., -7.9299e-05,
         -1.7314e-02,  2.9485e-03],
        [ 1.0581e-02, -1.2457e-02, -2.3850e-04,  ...,  4.7228e-03,
         -1.9095e-02,  8.2643e-03],
        [-2.2856e-02, -1.2452e-03, -1.8784e-02,  ...,  2.9030e-03,
          2.6138e-02,  3.3134e-02],
        ...,
        [-1.1522e-02,  1.6920e-04, -2.1725e-02,  ..., -1.3156e-02,
          2.4672e-03, -5.0111e-03],
        [ 3.1364e-02,  1.3190e-02,  1.5364e-02,  ..., -2.4913e-02,
          2.0206e-02, -2.6461e-02],
        [ 1.9361e-02,  1.5720e-03,  1.2424e-02,  ...,  2.7742e-03,
          1.8372e-02, -1.6623e-04]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-1.0260e-02,  8.6990e-03, -6.8350e-03,  ...,  1.8515e-02,
         -5.1672e-04,  7.0726e-03],
        [-8.8910e-05, -3.2449e-02, -4.5956e-03,  ...,  2.9294e-02,
          1.4969e-02,  1.6834e-02],
        [-2.5577e-02,  2.2456e-02,  2.1305e-02,  ..., -2.7277e-02,
         -1.8768e-02,  2.1622e-02],
        ...,
        [ 1.1959e-02,  2.4361e-02,  2.0456e-02,  ..., -2.7859e-02,
         -2.1358e-03,  4.4417e-03],
        [-2.5409e-02,  1.3041e-02,  3.1839e-02,  ..., -2.4660e-03,
          8.6101e-03,  1.6648e-02],
        [-3.3437e-02,  2.2489e-02, -6.2109e-03,  ...,  6.5903e-04,
          6.8532e-04, -2.8414e-02]], device='cuda:0', requires_grad=True)]
2024-05-15 10:08:44.095 | INFO     | __main__:train:123 - Epoch: [0][0/390]	 loss 8.60933	 cls_loss: 4.6408 cluster_loss: 4.5379 sup_con_loss: 2.2017 contrastive_loss: 5.0228 
2024-05-15 10:09:01.446 | INFO     | __main__:train:123 - Epoch: [0][20/390]	 loss 7.45246	 cls_loss: 2.8059 cluster_loss: 3.9111 sup_con_loss: 2.3319 contrastive_loss: 4.7876 
2024-05-15 10:09:16.689 | INFO     | __main__:train:123 - Epoch: [0][40/390]	 loss 6.13580	 cls_loss: 1.6435 cluster_loss: 2.8448 sup_con_loss: 1.8764 contrastive_loss: 4.6996 
2024-05-15 10:09:31.468 | INFO     | __main__:train:123 - Epoch: [0][60/390]	 loss 6.09867	 cls_loss: 1.7003 cluster_loss: 2.7602 sup_con_loss: 1.9211 contrastive_loss: 4.6724 
2024-05-15 10:09:46.353 | INFO     | __main__:train:123 - Epoch: [0][80/390]	 loss 5.77343	 cls_loss: 1.4680 cluster_loss: 2.6639 sup_con_loss: 1.4021 contrastive_loss: 4.6729 
2024-05-15 10:10:01.372 | INFO     | __main__:train:123 - Epoch: [0][100/390]	 loss 5.93463	 cls_loss: 1.4211 cluster_loss: 2.5709 sup_con_loss: 2.0656 contrastive_loss: 4.6818 
2024-05-15 10:10:16.508 | INFO     | __main__:train:123 - Epoch: [0][120/390]	 loss 5.53123	 cls_loss: 1.3012 cluster_loss: 2.3739 sup_con_loss: 1.4454 contrastive_loss: 4.6568 
2024-05-15 10:10:31.576 | INFO     | __main__:train:123 - Epoch: [0][140/390]	 loss 5.51449	 cls_loss: 1.3797 cluster_loss: 2.3407 sup_con_loss: 1.3387 contrastive_loss: 4.6794 
2024-05-15 10:10:46.439 | INFO     | __main__:train:123 - Epoch: [0][160/390]	 loss 5.71842	 cls_loss: 1.3192 cluster_loss: 2.3669 sup_con_loss: 2.0030 contrastive_loss: 4.6418 
2024-05-15 10:11:01.407 | INFO     | __main__:train:123 - Epoch: [0][180/390]	 loss 5.72045	 cls_loss: 1.3723 cluster_loss: 2.3549 sup_con_loss: 1.9275 contrastive_loss: 4.6689 
2024-05-15 10:11:16.482 | INFO     | __main__:train:123 - Epoch: [0][200/390]	 loss 5.72115	 cls_loss: 1.3877 cluster_loss: 2.2561 sup_con_loss: 2.1728 contrastive_loss: 4.6285 
2024-05-15 10:11:31.668 | INFO     | __main__:train:123 - Epoch: [0][220/390]	 loss 5.41514	 cls_loss: 1.2904 cluster_loss: 2.3198 sup_con_loss: 1.2710 contrastive_loss: 4.6320 
2024-05-15 10:11:46.658 | INFO     | __main__:train:123 - Epoch: [0][240/390]	 loss 5.67517	 cls_loss: 1.3093 cluster_loss: 2.4288 sup_con_loss: 1.7546 contrastive_loss: 4.6524 
2024-05-15 10:12:01.538 | INFO     | __main__:train:123 - Epoch: [0][260/390]	 loss 5.31395	 cls_loss: 1.2130 cluster_loss: 2.1565 sup_con_loss: 1.3767 contrastive_loss: 4.6244 
2024-05-15 10:12:16.744 | INFO     | __main__:train:123 - Epoch: [0][280/390]	 loss 5.34032	 cls_loss: 1.2625 cluster_loss: 2.0714 sup_con_loss: 1.5655 contrastive_loss: 4.6217 
2024-05-15 10:12:32.071 | INFO     | __main__:train:123 - Epoch: [0][300/390]	 loss 5.43873	 cls_loss: 1.2243 cluster_loss: 2.0380 sup_con_loss: 1.9338 contrastive_loss: 4.6288 
2024-05-15 10:12:47.061 | INFO     | __main__:train:123 - Epoch: [0][320/390]	 loss 5.29200	 cls_loss: 1.1571 cluster_loss: 2.0838 sup_con_loss: 1.5143 contrastive_loss: 4.6193 
2024-05-15 10:13:02.252 | INFO     | __main__:train:123 - Epoch: [0][340/390]	 loss 5.46490	 cls_loss: 1.2718 cluster_loss: 2.2114 sup_con_loss: 1.6611 contrastive_loss: 4.6169 
2024-05-15 10:13:17.293 | INFO     | __main__:train:123 - Epoch: [0][360/390]	 loss 4.96451	 cls_loss: 0.9453 cluster_loss: 1.8586 sup_con_loss: 1.2208 contrastive_loss: 4.6128 
2024-05-15 10:13:31.881 | INFO     | __main__:train:123 - Epoch: [0][380/390]	 loss 5.13067	 cls_loss: 1.0657 cluster_loss: 1.9411 sup_con_loss: 1.4292 contrastive_loss: 4.6088 
2024-05-15 10:13:38.330 | INFO     | __main__:train:126 - Train Epoch: 0 Avg Loss: 5.7668 
2024-05-15 10:13:38.330 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:14:06.975 | INFO     | __main__:train:135 - Train Accuracies: All 0.7079 | Old 0.7921 | New 0.3710
2024-05-15 10:14:10.320 | INFO     | __main__:train:123 - Epoch: [1][0/390]	 loss 5.11636	 cls_loss: 1.2027 cluster_loss: 1.9786 sup_con_loss: 1.1586 contrastive_loss: 4.6213 
2024-05-15 10:14:26.327 | INFO     | __main__:train:123 - Epoch: [1][20/390]	 loss 4.97466	 cls_loss: 1.0622 cluster_loss: 1.7893 sup_con_loss: 1.2465 contrastive_loss: 4.6209 
2024-05-15 10:14:41.212 | INFO     | __main__:train:123 - Epoch: [1][40/390]	 loss 5.16518	 cls_loss: 1.2247 cluster_loss: 1.8708 sup_con_loss: 1.5003 contrastive_loss: 4.6083 
2024-05-15 10:14:55.957 | INFO     | __main__:train:123 - Epoch: [1][60/390]	 loss 5.21366	 cls_loss: 1.1825 cluster_loss: 1.8594 sup_con_loss: 1.6972 contrastive_loss: 4.6110 
2024-05-15 10:15:11.190 | INFO     | __main__:train:123 - Epoch: [1][80/390]	 loss 5.05027	 cls_loss: 1.1167 cluster_loss: 1.8462 sup_con_loss: 1.3352 contrastive_loss: 4.6032 
2024-05-15 10:15:26.144 | INFO     | __main__:train:123 - Epoch: [1][100/390]	 loss 5.15082	 cls_loss: 1.2345 cluster_loss: 1.9692 sup_con_loss: 1.2441 contrastive_loss: 4.6205 
2024-05-15 10:15:41.181 | INFO     | __main__:train:123 - Epoch: [1][120/390]	 loss 5.14677	 cls_loss: 1.0893 cluster_loss: 1.8343 sup_con_loss: 1.6333 contrastive_loss: 4.6178 
2024-05-15 10:15:56.202 | INFO     | __main__:train:123 - Epoch: [1][140/390]	 loss 4.95129	 cls_loss: 1.1351 cluster_loss: 1.7768 sup_con_loss: 1.1453 contrastive_loss: 4.6126 
2024-05-15 10:16:11.115 | INFO     | __main__:train:123 - Epoch: [1][160/390]	 loss 4.92180	 cls_loss: 1.0294 cluster_loss: 1.7243 sup_con_loss: 1.2734 contrastive_loss: 4.6078 
2024-05-15 10:16:26.042 | INFO     | __main__:train:123 - Epoch: [1][180/390]	 loss 5.28424	 cls_loss: 1.1944 cluster_loss: 1.9392 sup_con_loss: 1.7328 contrastive_loss: 4.6142 
2024-05-15 10:16:41.219 | INFO     | __main__:train:123 - Epoch: [1][200/390]	 loss 5.02793	 cls_loss: 1.0276 cluster_loss: 1.8773 sup_con_loss: 1.2749 contrastive_loss: 4.6181 
2024-05-15 10:16:56.146 | INFO     | __main__:train:123 - Epoch: [1][220/390]	 loss 4.75290	 cls_loss: 1.0568 cluster_loss: 1.7009 sup_con_loss: 0.8086 contrastive_loss: 4.6068 
2024-05-15 10:17:11.182 | INFO     | __main__:train:123 - Epoch: [1][240/390]	 loss 5.18802	 cls_loss: 1.1734 cluster_loss: 1.9307 sup_con_loss: 1.5069 contrastive_loss: 4.6076 
2024-05-15 10:17:26.155 | INFO     | __main__:train:123 - Epoch: [1][260/390]	 loss 5.14539	 cls_loss: 1.1931 cluster_loss: 1.8469 sup_con_loss: 1.5191 contrastive_loss: 4.6086 
2024-05-15 10:17:41.074 | INFO     | __main__:train:123 - Epoch: [1][280/390]	 loss 5.15494	 cls_loss: 1.1444 cluster_loss: 1.7630 sup_con_loss: 1.7576 contrastive_loss: 4.6051 
2024-05-15 10:17:56.121 | INFO     | __main__:train:123 - Epoch: [1][300/390]	 loss 4.92434	 cls_loss: 1.0102 cluster_loss: 1.8873 sup_con_loss: 0.9608 contrastive_loss: 4.6273 
2024-05-15 10:18:11.237 | INFO     | __main__:train:123 - Epoch: [1][320/390]	 loss 4.92431	 cls_loss: 0.9932 cluster_loss: 1.8331 sup_con_loss: 1.0973 contrastive_loss: 4.6171 
2024-05-15 10:18:26.020 | INFO     | __main__:train:123 - Epoch: [1][340/390]	 loss 5.01583	 cls_loss: 1.0835 cluster_loss: 1.6974 sup_con_loss: 1.5395 contrastive_loss: 4.6068 
2024-05-15 10:18:40.886 | INFO     | __main__:train:123 - Epoch: [1][360/390]	 loss 5.13792	 cls_loss: 1.2290 cluster_loss: 1.8256 sup_con_loss: 1.5115 contrastive_loss: 4.6032 
2024-05-15 10:18:55.535 | INFO     | __main__:train:123 - Epoch: [1][380/390]	 loss 4.84789	 cls_loss: 0.9935 cluster_loss: 1.7437 sup_con_loss: 1.0710 contrastive_loss: 4.6029 
2024-05-15 10:19:01.948 | INFO     | __main__:train:126 - Train Epoch: 1 Avg Loss: 5.0339 
2024-05-15 10:19:01.948 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:19:30.537 | INFO     | __main__:train:135 - Train Accuracies: All 0.7332 | Old 0.7949 | New 0.4865
2024-05-15 10:19:35.217 | INFO     | __main__:train:123 - Epoch: [2][0/390]	 loss 4.75278	 cls_loss: 0.9828 cluster_loss: 1.5294 sup_con_loss: 1.2200 contrastive_loss: 4.5964 
2024-05-15 10:19:50.457 | INFO     | __main__:train:123 - Epoch: [2][20/390]	 loss 4.94010	 cls_loss: 1.0654 cluster_loss: 1.7307 sup_con_loss: 1.2828 contrastive_loss: 4.6050 
2024-05-15 10:20:05.620 | INFO     | __main__:train:123 - Epoch: [2][40/390]	 loss 4.82357	 cls_loss: 0.8944 cluster_loss: 1.5784 sup_con_loss: 1.4108 contrastive_loss: 4.6012 
2024-05-15 10:20:20.495 | INFO     | __main__:train:123 - Epoch: [2][60/390]	 loss 4.95695	 cls_loss: 1.1220 cluster_loss: 1.7402 sup_con_loss: 1.2638 contrastive_loss: 4.6012 
2024-05-15 10:20:35.815 | INFO     | __main__:train:123 - Epoch: [2][80/390]	 loss 4.94154	 cls_loss: 1.0814 cluster_loss: 1.7151 sup_con_loss: 1.3081 contrastive_loss: 4.6006 
2024-05-15 10:20:50.839 | INFO     | __main__:train:123 - Epoch: [2][100/390]	 loss 4.95920	 cls_loss: 1.0353 cluster_loss: 1.7820 sup_con_loss: 1.2425 contrastive_loss: 4.6210 
2024-05-15 10:21:05.859 | INFO     | __main__:train:123 - Epoch: [2][120/390]	 loss 5.02263	 cls_loss: 1.0578 cluster_loss: 1.8288 sup_con_loss: 1.3322 contrastive_loss: 4.6114 
2024-05-15 10:21:21.162 | INFO     | __main__:train:123 - Epoch: [2][140/390]	 loss 4.44803	 cls_loss: 0.6892 cluster_loss: 1.4405 sup_con_loss: 0.8122 contrastive_loss: 4.5942 
2024-05-15 10:21:36.267 | INFO     | __main__:train:123 - Epoch: [2][160/390]	 loss 4.62996	 cls_loss: 0.8101 cluster_loss: 1.5288 sup_con_loss: 1.0397 contrastive_loss: 4.5982 
2024-05-15 10:21:51.299 | INFO     | __main__:train:123 - Epoch: [2][180/390]	 loss 4.94461	 cls_loss: 0.9682 cluster_loss: 1.7217 sup_con_loss: 1.4056 contrastive_loss: 4.6072 
2024-05-15 10:22:06.258 | INFO     | __main__:train:123 - Epoch: [2][200/390]	 loss 4.81311	 cls_loss: 0.9021 cluster_loss: 1.6518 sup_con_loss: 1.2209 contrastive_loss: 4.6098 
2024-05-15 10:22:21.424 | INFO     | __main__:train:123 - Epoch: [2][220/390]	 loss 4.90508	 cls_loss: 1.0857 cluster_loss: 1.7362 sup_con_loss: 1.1466 contrastive_loss: 4.6081 
2024-05-15 10:22:36.571 | INFO     | __main__:train:123 - Epoch: [2][240/390]	 loss 4.98574	 cls_loss: 1.2155 cluster_loss: 1.6974 sup_con_loss: 1.3178 contrastive_loss: 4.6089 
2024-05-15 10:22:51.112 | INFO     | __main__:train:123 - Epoch: [2][260/390]	 loss 4.98551	 cls_loss: 1.1397 cluster_loss: 1.6795 sup_con_loss: 1.4468 contrastive_loss: 4.5978 
2024-05-15 10:23:06.196 | INFO     | __main__:train:123 - Epoch: [2][280/390]	 loss 4.99889	 cls_loss: 1.1461 cluster_loss: 1.6733 sup_con_loss: 1.4949 contrastive_loss: 4.5952 
2024-05-15 10:23:21.053 | INFO     | __main__:train:123 - Epoch: [2][300/390]	 loss 4.74683	 cls_loss: 1.0367 cluster_loss: 1.5570 sup_con_loss: 1.0924 contrastive_loss: 4.5994 
2024-05-15 10:23:35.746 | INFO     | __main__:train:123 - Epoch: [2][320/390]	 loss 4.74299	 cls_loss: 0.8562 cluster_loss: 1.6293 sup_con_loss: 1.1349 contrastive_loss: 4.5955 
2024-05-15 10:23:50.581 | INFO     | __main__:train:123 - Epoch: [2][340/390]	 loss 4.87650	 cls_loss: 1.1073 cluster_loss: 1.7290 sup_con_loss: 1.0727 contrastive_loss: 4.5995 
2024-05-15 10:24:05.411 | INFO     | __main__:train:123 - Epoch: [2][360/390]	 loss 4.87805	 cls_loss: 1.0076 cluster_loss: 1.7024 sup_con_loss: 1.2344 contrastive_loss: 4.5950 
2024-05-15 10:24:19.695 | INFO     | __main__:train:123 - Epoch: [2][380/390]	 loss 4.97312	 cls_loss: 1.0500 cluster_loss: 1.7161 sup_con_loss: 1.4265 contrastive_loss: 4.6013 
2024-05-15 10:24:26.021 | INFO     | __main__:train:126 - Train Epoch: 2 Avg Loss: 4.8572 
2024-05-15 10:24:26.022 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:24:54.612 | INFO     | __main__:train:135 - Train Accuracies: All 0.7469 | Old 0.7936 | New 0.5600
2024-05-15 10:24:57.981 | INFO     | __main__:train:123 - Epoch: [3][0/390]	 loss 4.90725	 cls_loss: 1.0519 cluster_loss: 1.5894 sup_con_loss: 1.4689 contrastive_loss: 4.6028 
2024-05-15 10:25:13.059 | INFO     | __main__:train:123 - Epoch: [3][20/390]	 loss 4.63186	 cls_loss: 0.9887 cluster_loss: 1.5184 sup_con_loss: 0.8789 contrastive_loss: 4.6019 
2024-05-15 10:25:28.019 | INFO     | __main__:train:123 - Epoch: [3][40/390]	 loss 4.73627	 cls_loss: 0.9640 cluster_loss: 1.5340 sup_con_loss: 1.1645 contrastive_loss: 4.6065 
2024-05-15 10:25:42.914 | INFO     | __main__:train:123 - Epoch: [3][60/390]	 loss 4.64733	 cls_loss: 0.7990 cluster_loss: 1.5406 sup_con_loss: 1.0650 contrastive_loss: 4.6055 
2024-05-15 10:25:57.843 | INFO     | __main__:train:123 - Epoch: [3][80/390]	 loss 4.85455	 cls_loss: 1.0634 cluster_loss: 1.5454 sup_con_loss: 1.3953 contrastive_loss: 4.5992 
2024-05-15 10:26:12.679 | INFO     | __main__:train:123 - Epoch: [3][100/390]	 loss 4.65585	 cls_loss: 0.8601 cluster_loss: 1.4874 sup_con_loss: 1.1409 contrastive_loss: 4.5979 
2024-05-15 10:26:27.784 | INFO     | __main__:train:123 - Epoch: [3][120/390]	 loss 4.63389	 cls_loss: 1.0196 cluster_loss: 1.6066 sup_con_loss: 0.7016 contrastive_loss: 4.5957 
2024-05-15 10:26:42.775 | INFO     | __main__:train:123 - Epoch: [3][140/390]	 loss 4.63233	 cls_loss: 0.9603 cluster_loss: 1.5155 sup_con_loss: 0.9258 contrastive_loss: 4.5956 
2024-05-15 10:26:57.897 | INFO     | __main__:train:123 - Epoch: [3][160/390]	 loss 4.84327	 cls_loss: 0.9981 cluster_loss: 1.6036 sup_con_loss: 1.3084 contrastive_loss: 4.6056 
2024-05-15 10:27:12.631 | INFO     | __main__:train:123 - Epoch: [3][180/390]	 loss 4.91391	 cls_loss: 1.0753 cluster_loss: 1.6696 sup_con_loss: 1.3143 contrastive_loss: 4.6036 
2024-05-15 10:27:27.504 | INFO     | __main__:train:123 - Epoch: [3][200/390]	 loss 4.78537	 cls_loss: 1.0837 cluster_loss: 1.5901 sup_con_loss: 1.0925 contrastive_loss: 4.6002 
2024-05-15 10:27:42.812 | INFO     | __main__:train:123 - Epoch: [3][220/390]	 loss 4.85684	 cls_loss: 0.9925 cluster_loss: 1.6722 sup_con_loss: 1.2256 contrastive_loss: 4.6055 
2024-05-15 10:27:57.880 | INFO     | __main__:train:123 - Epoch: [3][240/390]	 loss 4.71521	 cls_loss: 0.9262 cluster_loss: 1.7263 sup_con_loss: 0.7849 contrastive_loss: 4.6066 
2024-05-15 10:28:12.709 | INFO     | __main__:train:123 - Epoch: [3][260/390]	 loss 4.69951	 cls_loss: 0.8858 cluster_loss: 1.5463 sup_con_loss: 1.1377 contrastive_loss: 4.5942 
2024-05-15 10:28:27.949 | INFO     | __main__:train:123 - Epoch: [3][280/390]	 loss 4.60505	 cls_loss: 0.8522 cluster_loss: 1.5355 sup_con_loss: 0.9172 contrastive_loss: 4.5965 
2024-05-15 10:28:43.084 | INFO     | __main__:train:123 - Epoch: [3][300/390]	 loss 4.75278	 cls_loss: 0.9321 cluster_loss: 1.5931 sup_con_loss: 1.1471 contrastive_loss: 4.5993 
2024-05-15 10:28:58.091 | INFO     | __main__:train:123 - Epoch: [3][320/390]	 loss 4.77831	 cls_loss: 0.9189 cluster_loss: 1.5902 sup_con_loss: 1.2221 contrastive_loss: 4.6082 
2024-05-15 10:29:13.130 | INFO     | __main__:train:123 - Epoch: [3][340/390]	 loss 4.68766	 cls_loss: 0.8723 cluster_loss: 1.6427 sup_con_loss: 0.9274 contrastive_loss: 4.6000 
2024-05-15 10:29:27.956 | INFO     | __main__:train:123 - Epoch: [3][360/390]	 loss 4.82792	 cls_loss: 1.0304 cluster_loss: 1.6533 sup_con_loss: 1.1637 contrastive_loss: 4.5928 
2024-05-15 10:29:42.261 | INFO     | __main__:train:123 - Epoch: [3][380/390]	 loss 4.68157	 cls_loss: 0.8606 cluster_loss: 1.6053 sup_con_loss: 1.0068 contrastive_loss: 4.5916 
2024-05-15 10:29:48.528 | INFO     | __main__:train:126 - Train Epoch: 3 Avg Loss: 4.7440 
2024-05-15 10:29:48.528 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:30:17.212 | INFO     | __main__:train:135 - Train Accuracies: All 0.7487 | Old 0.8086 | New 0.5090
2024-05-15 10:30:21.351 | INFO     | __main__:train:123 - Epoch: [4][0/390]	 loss 4.63033	 cls_loss: 0.8614 cluster_loss: 1.5171 sup_con_loss: 1.0211 contrastive_loss: 4.5928 
2024-05-15 10:30:36.314 | INFO     | __main__:train:123 - Epoch: [4][20/390]	 loss 4.70204	 cls_loss: 0.9447 cluster_loss: 1.5345 sup_con_loss: 1.1038 contrastive_loss: 4.5964 
2024-05-15 10:30:51.481 | INFO     | __main__:train:123 - Epoch: [4][40/390]	 loss 4.74677	 cls_loss: 1.1052 cluster_loss: 1.4627 sup_con_loss: 1.2101 contrastive_loss: 4.5934 
2024-05-15 10:31:06.534 | INFO     | __main__:train:123 - Epoch: [4][60/390]	 loss 4.59086	 cls_loss: 0.8947 cluster_loss: 1.5568 sup_con_loss: 0.8025 contrastive_loss: 4.5922 
2024-05-15 10:31:21.498 | INFO     | __main__:train:123 - Epoch: [4][80/390]	 loss 4.89370	 cls_loss: 1.1514 cluster_loss: 1.6109 sup_con_loss: 1.3048 contrastive_loss: 4.5953 
2024-05-15 10:31:36.448 | INFO     | __main__:train:123 - Epoch: [4][100/390]	 loss 4.54923	 cls_loss: 0.9598 cluster_loss: 1.4415 sup_con_loss: 0.8338 contrastive_loss: 4.5915 
2024-05-15 10:31:51.588 | INFO     | __main__:train:123 - Epoch: [4][120/390]	 loss 4.45483	 cls_loss: 0.7498 cluster_loss: 1.4803 sup_con_loss: 0.6932 contrastive_loss: 4.5963 
2024-05-15 10:32:06.270 | INFO     | __main__:train:123 - Epoch: [4][140/390]	 loss 4.74616	 cls_loss: 1.1587 cluster_loss: 1.6039 sup_con_loss: 0.8926 contrastive_loss: 4.5934 
2024-05-15 10:32:21.198 | INFO     | __main__:train:123 - Epoch: [4][160/390]	 loss 4.79724	 cls_loss: 1.0746 cluster_loss: 1.5227 sup_con_loss: 1.2792 contrastive_loss: 4.5902 
2024-05-15 10:32:36.053 | INFO     | __main__:train:123 - Epoch: [4][180/390]	 loss 4.59824	 cls_loss: 0.7204 cluster_loss: 1.4547 sup_con_loss: 1.1767 contrastive_loss: 4.5981 
2024-05-15 10:32:51.091 | INFO     | __main__:train:123 - Epoch: [4][200/390]	 loss 4.75695	 cls_loss: 0.9628 cluster_loss: 1.5198 sup_con_loss: 1.2710 contrastive_loss: 4.5957 
2024-05-15 10:33:05.790 | INFO     | __main__:train:123 - Epoch: [4][220/390]	 loss 4.82235	 cls_loss: 0.9477 cluster_loss: 1.5155 sup_con_loss: 1.4783 contrastive_loss: 4.5972 
2024-05-15 10:33:20.592 | INFO     | __main__:train:123 - Epoch: [4][240/390]	 loss 4.43482	 cls_loss: 0.7467 cluster_loss: 1.4564 sup_con_loss: 0.6807 contrastive_loss: 4.5978 
2024-05-15 10:33:35.624 | INFO     | __main__:train:123 - Epoch: [4][260/390]	 loss 4.56344	 cls_loss: 0.9596 cluster_loss: 1.3950 sup_con_loss: 0.9681 contrastive_loss: 4.5877 
2024-05-15 10:33:50.612 | INFO     | __main__:train:123 - Epoch: [4][280/390]	 loss 4.64418	 cls_loss: 0.8704 cluster_loss: 1.4881 sup_con_loss: 1.0994 contrastive_loss: 4.5961 
2024-05-15 10:34:05.493 | INFO     | __main__:train:123 - Epoch: [4][300/390]	 loss 4.87407	 cls_loss: 1.0819 cluster_loss: 1.6451 sup_con_loss: 1.2436 contrastive_loss: 4.6013 
2024-05-15 10:34:20.389 | INFO     | __main__:train:123 - Epoch: [4][320/390]	 loss 4.61824	 cls_loss: 0.9636 cluster_loss: 1.5863 sup_con_loss: 0.7536 contrastive_loss: 4.5940 
2024-05-15 10:34:35.233 | INFO     | __main__:train:123 - Epoch: [4][340/390]	 loss 4.50224	 cls_loss: 0.7154 cluster_loss: 1.3945 sup_con_loss: 1.0311 contrastive_loss: 4.5916 
2024-05-15 10:34:50.136 | INFO     | __main__:train:123 - Epoch: [4][360/390]	 loss 4.54957	 cls_loss: 0.7953 cluster_loss: 1.4392 sup_con_loss: 0.9883 contrastive_loss: 4.5997 
2024-05-15 10:35:04.937 | INFO     | __main__:train:123 - Epoch: [4][380/390]	 loss 4.61809	 cls_loss: 1.0942 cluster_loss: 1.4811 sup_con_loss: 0.8242 contrastive_loss: 4.5907 
2024-05-15 10:35:11.477 | INFO     | __main__:train:126 - Train Epoch: 4 Avg Loss: 4.6620 
2024-05-15 10:35:11.478 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:35:39.933 | INFO     | __main__:train:135 - Train Accuracies: All 0.7537 | Old 0.7965 | New 0.5825
2024-05-15 10:35:43.423 | INFO     | __main__:train:123 - Epoch: [5][0/390]	 loss 4.61967	 cls_loss: 0.8882 cluster_loss: 1.4962 sup_con_loss: 1.0053 contrastive_loss: 4.5914 
2024-05-15 10:35:59.012 | INFO     | __main__:train:123 - Epoch: [5][20/390]	 loss 4.56126	 cls_loss: 0.8081 cluster_loss: 1.4776 sup_con_loss: 0.9453 contrastive_loss: 4.5956 
2024-05-15 10:36:13.939 | INFO     | __main__:train:123 - Epoch: [5][40/390]	 loss 4.42124	 cls_loss: 0.7391 cluster_loss: 1.4312 sup_con_loss: 0.7063 contrastive_loss: 4.5925 
2024-05-15 10:36:28.978 | INFO     | __main__:train:123 - Epoch: [5][60/390]	 loss 4.52775	 cls_loss: 0.7882 cluster_loss: 1.4017 sup_con_loss: 1.0210 contrastive_loss: 4.5899 
2024-05-15 10:36:44.035 | INFO     | __main__:train:123 - Epoch: [5][80/390]	 loss 4.63220	 cls_loss: 0.8454 cluster_loss: 1.4510 sup_con_loss: 1.1623 contrastive_loss: 4.5944 
2024-05-15 10:36:58.992 | INFO     | __main__:train:123 - Epoch: [5][100/390]	 loss 4.72507	 cls_loss: 0.9513 cluster_loss: 1.5495 sup_con_loss: 1.1352 contrastive_loss: 4.5963 
2024-05-15 10:37:14.250 | INFO     | __main__:train:123 - Epoch: [5][120/390]	 loss 4.64012	 cls_loss: 0.8999 cluster_loss: 1.4815 sup_con_loss: 1.0790 contrastive_loss: 4.5916 
2024-05-15 10:37:29.525 | INFO     | __main__:train:123 - Epoch: [5][140/390]	 loss 4.66048	 cls_loss: 0.8474 cluster_loss: 1.6889 sup_con_loss: 0.7963 contrastive_loss: 4.5960 
2024-05-15 10:37:44.543 | INFO     | __main__:train:123 - Epoch: [5][160/390]	 loss 4.55904	 cls_loss: 0.7552 cluster_loss: 1.4484 sup_con_loss: 1.0643 contrastive_loss: 4.5857 
2024-05-15 10:37:59.419 | INFO     | __main__:train:123 - Epoch: [5][180/390]	 loss 4.53140	 cls_loss: 0.8445 cluster_loss: 1.3007 sup_con_loss: 1.1664 contrastive_loss: 4.5879 
2024-05-15 10:38:14.538 | INFO     | __main__:train:123 - Epoch: [5][200/390]	 loss 4.54714	 cls_loss: 0.8706 cluster_loss: 1.3386 sup_con_loss: 1.0985 contrastive_loss: 4.5967 
2024-05-15 10:38:29.701 | INFO     | __main__:train:123 - Epoch: [5][220/390]	 loss 4.46116	 cls_loss: 0.8653 cluster_loss: 1.4484 sup_con_loss: 0.6618 contrastive_loss: 4.5927 
2024-05-15 10:38:44.638 | INFO     | __main__:train:123 - Epoch: [5][240/390]	 loss 4.52217	 cls_loss: 0.8117 cluster_loss: 1.4588 sup_con_loss: 0.8657 contrastive_loss: 4.5952 
2024-05-15 10:38:59.900 | INFO     | __main__:train:123 - Epoch: [5][260/390]	 loss 4.49426	 cls_loss: 0.7378 cluster_loss: 1.3651 sup_con_loss: 1.0344 contrastive_loss: 4.5949 
2024-05-15 10:39:14.817 | INFO     | __main__:train:123 - Epoch: [5][280/390]	 loss 4.39101	 cls_loss: 0.7613 cluster_loss: 1.3569 sup_con_loss: 0.7472 contrastive_loss: 4.5862 
2024-05-15 10:39:29.912 | INFO     | __main__:train:123 - Epoch: [5][300/390]	 loss 4.51667	 cls_loss: 0.8996 cluster_loss: 1.4341 sup_con_loss: 0.8244 contrastive_loss: 4.5863 
2024-05-15 10:39:44.815 | INFO     | __main__:train:123 - Epoch: [5][320/390]	 loss 4.83304	 cls_loss: 0.9758 cluster_loss: 1.5394 sup_con_loss: 1.4419 contrastive_loss: 4.5942 
2024-05-15 10:39:59.656 | INFO     | __main__:train:123 - Epoch: [5][340/390]	 loss 4.63089	 cls_loss: 1.0873 cluster_loss: 1.5054 sup_con_loss: 0.8206 contrastive_loss: 4.5917 
2024-05-15 10:40:14.692 | INFO     | __main__:train:123 - Epoch: [5][360/390]	 loss 4.61655	 cls_loss: 0.9268 cluster_loss: 1.4391 sup_con_loss: 1.0658 contrastive_loss: 4.5904 
2024-05-15 10:40:29.175 | INFO     | __main__:train:123 - Epoch: [5][380/390]	 loss 4.63041	 cls_loss: 0.8852 cluster_loss: 1.5610 sup_con_loss: 0.9059 contrastive_loss: 4.5982 
2024-05-15 10:40:35.540 | INFO     | __main__:train:126 - Train Epoch: 5 Avg Loss: 4.5770 
2024-05-15 10:40:35.540 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:41:03.964 | INFO     | __main__:train:135 - Train Accuracies: All 0.7565 | Old 0.8056 | New 0.5600
2024-05-15 10:41:07.135 | INFO     | __main__:train:123 - Epoch: [6][0/390]	 loss 4.47779	 cls_loss: 0.8184 cluster_loss: 1.3399 sup_con_loss: 0.9641 contrastive_loss: 4.5892 
2024-05-15 10:41:23.076 | INFO     | __main__:train:123 - Epoch: [6][20/390]	 loss 4.53076	 cls_loss: 0.7855 cluster_loss: 1.3969 sup_con_loss: 1.0561 contrastive_loss: 4.5819 
2024-05-15 10:41:37.948 | INFO     | __main__:train:123 - Epoch: [6][40/390]	 loss 4.62607	 cls_loss: 0.9529 cluster_loss: 1.4411 sup_con_loss: 1.0684 contrastive_loss: 4.5875 
2024-05-15 10:41:52.966 | INFO     | __main__:train:123 - Epoch: [6][60/390]	 loss 4.61313	 cls_loss: 0.8291 cluster_loss: 1.3767 sup_con_loss: 1.2756 contrastive_loss: 4.5871 
2024-05-15 10:42:07.779 | INFO     | __main__:train:123 - Epoch: [6][80/390]	 loss 4.65294	 cls_loss: 0.9738 cluster_loss: 1.5826 sup_con_loss: 0.8431 contrastive_loss: 4.5975 
2024-05-15 10:42:22.620 | INFO     | __main__:train:123 - Epoch: [6][100/390]	 loss 4.53030	 cls_loss: 0.8251 cluster_loss: 1.4280 sup_con_loss: 0.9367 contrastive_loss: 4.5931 
2024-05-15 10:42:37.438 | INFO     | __main__:train:123 - Epoch: [6][120/390]	 loss 4.32949	 cls_loss: 0.7635 cluster_loss: 1.3863 sup_con_loss: 0.4971 contrastive_loss: 4.5956 
2024-05-15 10:42:52.443 | INFO     | __main__:train:123 - Epoch: [6][140/390]	 loss 4.47607	 cls_loss: 0.8827 cluster_loss: 1.3232 sup_con_loss: 0.9207 contrastive_loss: 4.5920 
2024-05-15 10:43:07.317 | INFO     | __main__:train:123 - Epoch: [6][160/390]	 loss 4.57093	 cls_loss: 0.9115 cluster_loss: 1.3832 sup_con_loss: 1.0507 contrastive_loss: 4.5925 
2024-05-15 10:43:22.345 | INFO     | __main__:train:123 - Epoch: [6][180/390]	 loss 4.50872	 cls_loss: 0.8760 cluster_loss: 1.4486 sup_con_loss: 0.7851 contrastive_loss: 4.5935 
2024-05-15 10:43:37.473 | INFO     | __main__:train:123 - Epoch: [6][200/390]	 loss 4.59799	 cls_loss: 0.8944 cluster_loss: 1.3895 sup_con_loss: 1.1381 contrastive_loss: 4.5899 
2024-05-15 10:43:52.516 | INFO     | __main__:train:123 - Epoch: [6][220/390]	 loss 4.46577	 cls_loss: 0.7964 cluster_loss: 1.3561 sup_con_loss: 0.9166 contrastive_loss: 4.5919 
2024-05-15 10:44:07.394 | INFO     | __main__:train:123 - Epoch: [6][240/390]	 loss 4.52941	 cls_loss: 0.8375 cluster_loss: 1.5001 sup_con_loss: 0.7644 contrastive_loss: 4.6056 
2024-05-15 10:44:22.258 | INFO     | __main__:train:123 - Epoch: [6][260/390]	 loss 4.53278	 cls_loss: 0.8234 cluster_loss: 1.4457 sup_con_loss: 0.9218 contrastive_loss: 4.5880 
2024-05-15 10:44:37.238 | INFO     | __main__:train:123 - Epoch: [6][280/390]	 loss 4.50867	 cls_loss: 0.8173 cluster_loss: 1.4106 sup_con_loss: 0.9200 contrastive_loss: 4.5903 
2024-05-15 10:44:52.343 | INFO     | __main__:train:123 - Epoch: [6][300/390]	 loss 4.45890	 cls_loss: 0.7387 cluster_loss: 1.3593 sup_con_loss: 0.9313 contrastive_loss: 4.6014 
2024-05-15 10:45:07.548 | INFO     | __main__:train:123 - Epoch: [6][320/390]	 loss 4.63753	 cls_loss: 0.8586 cluster_loss: 1.4016 sup_con_loss: 1.2692 contrastive_loss: 4.5873 
2024-05-15 10:45:22.545 | INFO     | __main__:train:123 - Epoch: [6][340/390]	 loss 4.45714	 cls_loss: 0.7149 cluster_loss: 1.3993 sup_con_loss: 0.8993 contrastive_loss: 4.5887 
2024-05-15 10:45:37.538 | INFO     | __main__:train:123 - Epoch: [6][360/390]	 loss 4.49043	 cls_loss: 0.8370 cluster_loss: 1.3946 sup_con_loss: 0.8852 contrastive_loss: 4.5864 
2024-05-15 10:45:52.058 | INFO     | __main__:train:123 - Epoch: [6][380/390]	 loss 4.54356	 cls_loss: 0.8776 cluster_loss: 1.4297 sup_con_loss: 0.9203 contrastive_loss: 4.5923 
2024-05-15 10:45:58.489 | INFO     | __main__:train:126 - Train Epoch: 6 Avg Loss: 4.5281 
2024-05-15 10:45:58.489 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:46:27.383 | INFO     | __main__:train:135 - Train Accuracies: All 0.7577 | Old 0.8085 | New 0.5545
2024-05-15 10:46:31.446 | INFO     | __main__:train:123 - Epoch: [7][0/390]	 loss 4.74576	 cls_loss: 1.0881 cluster_loss: 1.5188 sup_con_loss: 1.1084 contrastive_loss: 4.5996 
2024-05-15 10:46:46.641 | INFO     | __main__:train:123 - Epoch: [7][20/390]	 loss 4.38210	 cls_loss: 0.7679 cluster_loss: 1.3157 sup_con_loss: 0.7930 contrastive_loss: 4.5855 
2024-05-15 10:47:02.044 | INFO     | __main__:train:123 - Epoch: [7][40/390]	 loss 4.65667	 cls_loss: 1.0455 cluster_loss: 1.4669 sup_con_loss: 1.0090 contrastive_loss: 4.5910 
2024-05-15 10:47:17.382 | INFO     | __main__:train:123 - Epoch: [7][60/390]	 loss 4.51977	 cls_loss: 0.8587 cluster_loss: 1.4049 sup_con_loss: 0.9139 contrastive_loss: 4.5942 
2024-05-15 10:47:32.200 | INFO     | __main__:train:123 - Epoch: [7][80/390]	 loss 4.68736	 cls_loss: 1.1474 cluster_loss: 1.4792 sup_con_loss: 0.9649 contrastive_loss: 4.5947 
2024-05-15 10:47:47.232 | INFO     | __main__:train:123 - Epoch: [7][100/390]	 loss 4.34834	 cls_loss: 0.6144 cluster_loss: 1.3321 sup_con_loss: 0.8173 contrastive_loss: 4.5867 
2024-05-15 10:48:02.422 | INFO     | __main__:train:123 - Epoch: [7][120/390]	 loss 4.36034	 cls_loss: 0.6965 cluster_loss: 1.3321 sup_con_loss: 0.7641 contrastive_loss: 4.5896 
2024-05-15 10:48:17.523 | INFO     | __main__:train:123 - Epoch: [7][140/390]	 loss 4.40804	 cls_loss: 0.7223 cluster_loss: 1.3908 sup_con_loss: 0.7733 contrastive_loss: 4.5855 
2024-05-15 10:48:32.679 | INFO     | __main__:train:123 - Epoch: [7][160/390]	 loss 4.24701	 cls_loss: 0.6287 cluster_loss: 1.2729 sup_con_loss: 0.6220 contrastive_loss: 4.5876 
2024-05-15 10:48:47.675 | INFO     | __main__:train:123 - Epoch: [7][180/390]	 loss 4.44321	 cls_loss: 0.9004 cluster_loss: 1.3329 sup_con_loss: 0.7857 contrastive_loss: 4.5949 
2024-05-15 10:49:02.782 | INFO     | __main__:train:123 - Epoch: [7][200/390]	 loss 4.27706	 cls_loss: 0.7100 cluster_loss: 1.2609 sup_con_loss: 0.6544 contrastive_loss: 4.5845 
2024-05-15 10:49:17.613 | INFO     | __main__:train:123 - Epoch: [7][220/390]	 loss 4.54178	 cls_loss: 0.9675 cluster_loss: 1.4206 sup_con_loss: 0.8424 contrastive_loss: 4.5921 
2024-05-15 10:49:32.421 | INFO     | __main__:train:123 - Epoch: [7][240/390]	 loss 4.41117	 cls_loss: 0.9063 cluster_loss: 1.3998 sup_con_loss: 0.5706 contrastive_loss: 4.5914 
2024-05-15 10:49:47.488 | INFO     | __main__:train:123 - Epoch: [7][260/390]	 loss 4.40224	 cls_loss: 0.6824 cluster_loss: 1.3853 sup_con_loss: 0.8009 contrastive_loss: 4.5887 
2024-05-15 10:50:02.447 | INFO     | __main__:train:123 - Epoch: [7][280/390]	 loss 4.52778	 cls_loss: 0.8548 cluster_loss: 1.4341 sup_con_loss: 0.8963 contrastive_loss: 4.5888 
2024-05-15 10:50:17.547 | INFO     | __main__:train:123 - Epoch: [7][300/390]	 loss 4.30039	 cls_loss: 0.6964 cluster_loss: 1.2608 sup_con_loss: 0.7299 contrastive_loss: 4.5872 
2024-05-15 10:50:32.635 | INFO     | __main__:train:123 - Epoch: [7][320/390]	 loss 4.43936	 cls_loss: 0.7854 cluster_loss: 1.3895 sup_con_loss: 0.7858 contrastive_loss: 4.5942 
2024-05-15 10:50:47.568 | INFO     | __main__:train:123 - Epoch: [7][340/390]	 loss 4.47134	 cls_loss: 0.6936 cluster_loss: 1.3660 sup_con_loss: 1.0246 contrastive_loss: 4.5878 
2024-05-15 10:51:02.376 | INFO     | __main__:train:123 - Epoch: [7][360/390]	 loss 4.47146	 cls_loss: 0.8735 cluster_loss: 1.3307 sup_con_loss: 0.9101 contrastive_loss: 4.5880 
2024-05-15 10:51:16.693 | INFO     | __main__:train:123 - Epoch: [7][380/390]	 loss 4.54920	 cls_loss: 0.8770 cluster_loss: 1.3697 sup_con_loss: 1.0586 contrastive_loss: 4.5869 
2024-05-15 10:51:23.049 | INFO     | __main__:train:126 - Train Epoch: 7 Avg Loss: 4.4722 
2024-05-15 10:51:23.051 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:51:51.708 | INFO     | __main__:train:135 - Train Accuracies: All 0.7626 | Old 0.8064 | New 0.5875
2024-05-15 10:51:56.072 | INFO     | __main__:train:123 - Epoch: [8][0/390]	 loss 4.55591	 cls_loss: 0.9118 cluster_loss: 1.3761 sup_con_loss: 1.0350 contrastive_loss: 4.5847 
2024-05-15 10:52:11.154 | INFO     | __main__:train:123 - Epoch: [8][20/390]	 loss 4.34794	 cls_loss: 0.7673 cluster_loss: 1.3158 sup_con_loss: 0.6776 contrastive_loss: 4.5953 
2024-05-15 10:52:26.120 | INFO     | __main__:train:123 - Epoch: [8][40/390]	 loss 4.45050	 cls_loss: 0.8385 cluster_loss: 1.3717 sup_con_loss: 0.8109 contrastive_loss: 4.5871 
2024-05-15 10:52:40.988 | INFO     | __main__:train:123 - Epoch: [8][60/390]	 loss 4.45205	 cls_loss: 0.7964 cluster_loss: 1.4146 sup_con_loss: 0.7707 contrastive_loss: 4.5908 
2024-05-15 10:52:55.901 | INFO     | __main__:train:123 - Epoch: [8][80/390]	 loss 4.39752	 cls_loss: 0.7816 cluster_loss: 1.2710 sup_con_loss: 0.9074 contrastive_loss: 4.5849 
2024-05-15 10:53:10.827 | INFO     | __main__:train:123 - Epoch: [8][100/390]	 loss 4.44730	 cls_loss: 0.8982 cluster_loss: 1.3970 sup_con_loss: 0.7010 contrastive_loss: 4.5840 
2024-05-15 10:53:26.009 | INFO     | __main__:train:123 - Epoch: [8][120/390]	 loss 4.34734	 cls_loss: 0.7734 cluster_loss: 1.2560 sup_con_loss: 0.7952 contrastive_loss: 4.5876 
2024-05-15 10:53:41.245 | INFO     | __main__:train:123 - Epoch: [8][140/390]	 loss 4.40602	 cls_loss: 0.7316 cluster_loss: 1.3394 sup_con_loss: 0.8527 contrastive_loss: 4.5860 
2024-05-15 10:53:56.063 | INFO     | __main__:train:123 - Epoch: [8][160/390]	 loss 4.42179	 cls_loss: 0.8480 cluster_loss: 1.4131 sup_con_loss: 0.6365 contrastive_loss: 4.5903 
2024-05-15 10:54:10.794 | INFO     | __main__:train:123 - Epoch: [8][180/390]	 loss 4.49652	 cls_loss: 0.7896 cluster_loss: 1.3644 sup_con_loss: 0.9963 contrastive_loss: 4.5916 
2024-05-15 10:54:26.016 | INFO     | __main__:train:123 - Epoch: [8][200/390]	 loss 4.35205	 cls_loss: 0.6884 cluster_loss: 1.3012 sup_con_loss: 0.8062 contrastive_loss: 4.5895 
2024-05-15 10:54:40.883 | INFO     | __main__:train:123 - Epoch: [8][220/390]	 loss 4.35730	 cls_loss: 0.6892 cluster_loss: 1.3186 sup_con_loss: 0.7889 contrastive_loss: 4.5890 
2024-05-15 10:54:55.968 | INFO     | __main__:train:123 - Epoch: [8][240/390]	 loss 4.42752	 cls_loss: 0.7519 cluster_loss: 1.3028 sup_con_loss: 0.9498 contrastive_loss: 4.5924 
2024-05-15 10:55:11.064 | INFO     | __main__:train:123 - Epoch: [8][260/390]	 loss 4.26102	 cls_loss: 0.7054 cluster_loss: 1.2044 sup_con_loss: 0.7185 contrastive_loss: 4.5843 
2024-05-15 10:55:26.192 | INFO     | __main__:train:123 - Epoch: [8][280/390]	 loss 4.33474	 cls_loss: 0.7741 cluster_loss: 1.3503 sup_con_loss: 0.5782 contrastive_loss: 4.5904 
2024-05-15 10:55:41.150 | INFO     | __main__:train:123 - Epoch: [8][300/390]	 loss 4.46727	 cls_loss: 0.7724 cluster_loss: 1.3021 sup_con_loss: 1.0560 contrastive_loss: 4.5861 
2024-05-15 10:55:56.357 | INFO     | __main__:train:123 - Epoch: [8][320/390]	 loss 4.50869	 cls_loss: 1.0109 cluster_loss: 1.3133 sup_con_loss: 0.9160 contrastive_loss: 4.5856 
2024-05-15 10:56:11.365 | INFO     | __main__:train:123 - Epoch: [8][340/390]	 loss 4.17580	 cls_loss: 0.7616 cluster_loss: 1.1535 sup_con_loss: 0.5237 contrastive_loss: 4.5787 
2024-05-15 10:56:26.332 | INFO     | __main__:train:123 - Epoch: [8][360/390]	 loss 4.49020	 cls_loss: 0.8418 cluster_loss: 1.3204 sup_con_loss: 1.0151 contrastive_loss: 4.5877 
2024-05-15 10:56:40.834 | INFO     | __main__:train:123 - Epoch: [8][380/390]	 loss 4.41556	 cls_loss: 0.7440 cluster_loss: 1.3060 sup_con_loss: 0.9302 contrastive_loss: 4.5856 
2024-05-15 10:56:47.245 | INFO     | __main__:train:126 - Train Epoch: 8 Avg Loss: 4.4301 
2024-05-15 10:56:47.247 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:57:15.964 | INFO     | __main__:train:135 - Train Accuracies: All 0.7630 | Old 0.8129 | New 0.5635
2024-05-15 10:57:19.155 | INFO     | __main__:train:123 - Epoch: [9][0/390]	 loss 4.32375	 cls_loss: 0.7527 cluster_loss: 1.2428 sup_con_loss: 0.7797 contrastive_loss: 4.5840 
2024-05-15 10:57:35.267 | INFO     | __main__:train:123 - Epoch: [9][20/390]	 loss 4.27474	 cls_loss: 0.6483 cluster_loss: 1.2750 sup_con_loss: 0.6678 contrastive_loss: 4.5929 
2024-05-15 10:57:50.286 | INFO     | __main__:train:123 - Epoch: [9][40/390]	 loss 4.44133	 cls_loss: 0.7405 cluster_loss: 1.3270 sup_con_loss: 0.9647 contrastive_loss: 4.5876 
2024-05-15 10:58:05.473 | INFO     | __main__:train:123 - Epoch: [9][60/390]	 loss 4.40314	 cls_loss: 0.7341 cluster_loss: 1.3416 sup_con_loss: 0.8383 contrastive_loss: 4.5858 
2024-05-15 10:58:20.520 | INFO     | __main__:train:123 - Epoch: [9][80/390]	 loss 4.29309	 cls_loss: 0.6025 cluster_loss: 1.3196 sup_con_loss: 0.6982 contrastive_loss: 4.5848 
2024-05-15 10:58:35.500 | INFO     | __main__:train:123 - Epoch: [9][100/390]	 loss 4.22875	 cls_loss: 0.6360 cluster_loss: 1.1850 sup_con_loss: 0.7376 contrastive_loss: 4.5811 
2024-05-15 10:58:50.792 | INFO     | __main__:train:123 - Epoch: [9][120/390]	 loss 4.50599	 cls_loss: 0.8431 cluster_loss: 1.2780 sup_con_loss: 1.1433 contrastive_loss: 4.5847 
2024-05-15 10:59:06.075 | INFO     | __main__:train:123 - Epoch: [9][140/390]	 loss 4.37370	 cls_loss: 0.6778 cluster_loss: 1.2306 sup_con_loss: 1.0118 contrastive_loss: 4.5884 
2024-05-15 10:59:20.868 | INFO     | __main__:train:123 - Epoch: [9][160/390]	 loss 4.36136	 cls_loss: 0.6673 cluster_loss: 1.2035 sup_con_loss: 1.0388 contrastive_loss: 4.5876 
2024-05-15 10:59:35.991 | INFO     | __main__:train:123 - Epoch: [9][180/390]	 loss 4.50205	 cls_loss: 0.7477 cluster_loss: 1.3057 sup_con_loss: 1.1703 contrastive_loss: 4.5878 
2024-05-15 10:59:51.060 | INFO     | __main__:train:123 - Epoch: [9][200/390]	 loss 4.45748	 cls_loss: 0.7883 cluster_loss: 1.3020 sup_con_loss: 1.0155 contrastive_loss: 4.5844 
2024-05-15 11:00:06.097 | INFO     | __main__:train:123 - Epoch: [9][220/390]	 loss 4.32580	 cls_loss: 0.7350 cluster_loss: 1.2541 sup_con_loss: 0.7872 contrastive_loss: 4.5813 
2024-05-15 11:00:20.884 | INFO     | __main__:train:123 - Epoch: [9][240/390]	 loss 4.18648	 cls_loss: 0.6452 cluster_loss: 1.1780 sup_con_loss: 0.6149 contrastive_loss: 4.5842 
2024-05-15 11:00:35.900 | INFO     | __main__:train:123 - Epoch: [9][260/390]	 loss 4.53873	 cls_loss: 1.0158 cluster_loss: 1.3797 sup_con_loss: 0.8745 contrastive_loss: 4.5851 
2024-05-15 11:00:50.966 | INFO     | __main__:train:123 - Epoch: [9][280/390]	 loss 4.36803	 cls_loss: 0.7287 cluster_loss: 1.2558 sup_con_loss: 0.8992 contrastive_loss: 4.5878 
2024-05-15 11:01:06.004 | INFO     | __main__:train:123 - Epoch: [9][300/390]	 loss 4.44109	 cls_loss: 0.8206 cluster_loss: 1.2985 sup_con_loss: 0.9449 contrastive_loss: 4.5833 
2024-05-15 11:01:20.962 | INFO     | __main__:train:123 - Epoch: [9][320/390]	 loss 4.34242	 cls_loss: 0.7091 cluster_loss: 1.2844 sup_con_loss: 0.7962 contrastive_loss: 4.5857 
2024-05-15 11:01:36.076 | INFO     | __main__:train:123 - Epoch: [9][340/390]	 loss 4.43655	 cls_loss: 0.8335 cluster_loss: 1.2730 sup_con_loss: 0.9701 contrastive_loss: 4.5812 
2024-05-15 11:01:51.041 | INFO     | __main__:train:123 - Epoch: [9][360/390]	 loss 4.36765	 cls_loss: 0.6778 cluster_loss: 1.3224 sup_con_loss: 0.8388 contrastive_loss: 4.5804 
2024-05-15 11:02:05.534 | INFO     | __main__:train:123 - Epoch: [9][380/390]	 loss 4.45393	 cls_loss: 0.6881 cluster_loss: 1.2900 sup_con_loss: 1.1335 contrastive_loss: 4.5813 
2024-05-15 11:02:11.923 | INFO     | __main__:train:126 - Train Epoch: 9 Avg Loss: 4.3847 
2024-05-15 11:02:11.925 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:02:40.592 | INFO     | __main__:train:135 - Train Accuracies: All 0.7665 | Old 0.8154 | New 0.5710
2024-05-15 11:02:45.408 | INFO     | __main__:train:123 - Epoch: [10][0/390]	 loss 4.19056	 cls_loss: 0.5936 cluster_loss: 1.2363 sup_con_loss: 0.5730 contrastive_loss: 4.5826 
2024-05-15 11:03:00.484 | INFO     | __main__:train:123 - Epoch: [10][20/390]	 loss 4.40919	 cls_loss: 0.8029 cluster_loss: 1.3386 sup_con_loss: 0.7863 contrastive_loss: 4.5890 
2024-05-15 11:03:16.022 | INFO     | __main__:train:123 - Epoch: [10][40/390]	 loss 4.48491	 cls_loss: 1.0023 cluster_loss: 1.3152 sup_con_loss: 0.8583 contrastive_loss: 4.5828 
2024-05-15 11:03:30.954 | INFO     | __main__:train:123 - Epoch: [10][60/390]	 loss 4.37216	 cls_loss: 0.6999 cluster_loss: 1.3007 sup_con_loss: 0.8589 contrastive_loss: 4.5864 
2024-05-15 11:03:45.849 | INFO     | __main__:train:123 - Epoch: [10][80/390]	 loss 4.31145	 cls_loss: 0.8301 cluster_loss: 1.2094 sup_con_loss: 0.7334 contrastive_loss: 4.5817 
2024-05-15 11:04:00.988 | INFO     | __main__:train:123 - Epoch: [10][100/390]	 loss 4.23945	 cls_loss: 0.7114 cluster_loss: 1.1821 sup_con_loss: 0.7047 contrastive_loss: 4.5776 
2024-05-15 11:04:15.999 | INFO     | __main__:train:123 - Epoch: [10][120/390]	 loss 4.40574	 cls_loss: 0.8339 cluster_loss: 1.1856 sup_con_loss: 1.0359 contrastive_loss: 4.5856 
2024-05-15 11:04:31.192 | INFO     | __main__:train:123 - Epoch: [10][140/390]	 loss 4.37103	 cls_loss: 0.8467 cluster_loss: 1.2551 sup_con_loss: 0.8015 contrastive_loss: 4.5821 
2024-05-15 11:04:46.295 | INFO     | __main__:train:123 - Epoch: [10][160/390]	 loss 4.39624	 cls_loss: 0.7434 cluster_loss: 1.2574 sup_con_loss: 0.9650 contrastive_loss: 4.5862 
2024-05-15 11:05:01.601 | INFO     | __main__:train:123 - Epoch: [10][180/390]	 loss 4.32831	 cls_loss: 0.7761 cluster_loss: 1.2826 sup_con_loss: 0.6861 contrastive_loss: 4.5890 
2024-05-15 11:05:16.442 | INFO     | __main__:train:123 - Epoch: [10][200/390]	 loss 4.35957	 cls_loss: 0.7001 cluster_loss: 1.2556 sup_con_loss: 0.9115 contrastive_loss: 4.5837 
2024-05-15 11:05:31.622 | INFO     | __main__:train:123 - Epoch: [10][220/390]	 loss 4.28373	 cls_loss: 0.6342 cluster_loss: 1.2387 sup_con_loss: 0.7851 contrastive_loss: 4.5874 
2024-05-15 11:05:46.483 | INFO     | __main__:train:123 - Epoch: [10][240/390]	 loss 4.40749	 cls_loss: 0.8579 cluster_loss: 1.2455 sup_con_loss: 0.9066 contrastive_loss: 4.5852 
2024-05-15 11:06:01.462 | INFO     | __main__:train:123 - Epoch: [10][260/390]	 loss 4.35802	 cls_loss: 0.6039 cluster_loss: 1.3634 sup_con_loss: 0.7940 contrastive_loss: 4.5885 
2024-05-15 11:06:16.683 | INFO     | __main__:train:123 - Epoch: [10][280/390]	 loss 4.24715	 cls_loss: 0.5793 cluster_loss: 1.1789 sup_con_loss: 0.8558 contrastive_loss: 4.5824 
2024-05-15 11:06:32.141 | INFO     | __main__:train:123 - Epoch: [10][300/390]	 loss 4.36226	 cls_loss: 0.8747 cluster_loss: 1.2163 sup_con_loss: 0.8250 contrastive_loss: 4.5796 
2024-05-15 11:06:47.306 | INFO     | __main__:train:123 - Epoch: [10][320/390]	 loss 4.37833	 cls_loss: 0.7755 cluster_loss: 1.2465 sup_con_loss: 0.9052 contrastive_loss: 4.5844 
2024-05-15 11:07:02.344 | INFO     | __main__:train:123 - Epoch: [10][340/390]	 loss 4.38245	 cls_loss: 0.7225 cluster_loss: 1.2550 sup_con_loss: 0.9564 contrastive_loss: 4.5832 
2024-05-15 11:07:17.323 | INFO     | __main__:train:123 - Epoch: [10][360/390]	 loss 4.40054	 cls_loss: 0.8020 cluster_loss: 1.2516 sup_con_loss: 0.9328 contrastive_loss: 4.5844 
2024-05-15 11:07:31.766 | INFO     | __main__:train:123 - Epoch: [10][380/390]	 loss 4.33708	 cls_loss: 0.6896 cluster_loss: 1.2709 sup_con_loss: 0.8305 contrastive_loss: 4.5830 
2024-05-15 11:07:38.275 | INFO     | __main__:train:126 - Train Epoch: 10 Avg Loss: 4.3527 
2024-05-15 11:07:38.275 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:08:07.023 | INFO     | __main__:train:135 - Train Accuracies: All 0.7704 | Old 0.8249 | New 0.5525
2024-05-15 11:08:10.367 | INFO     | __main__:train:123 - Epoch: [11][0/390]	 loss 4.41927	 cls_loss: 0.7659 cluster_loss: 1.2640 sup_con_loss: 1.0007 contrastive_loss: 4.5836 
2024-05-15 11:08:26.096 | INFO     | __main__:train:123 - Epoch: [11][20/390]	 loss 4.48673	 cls_loss: 0.8107 cluster_loss: 1.3485 sup_con_loss: 0.9922 contrastive_loss: 4.5833 
2024-05-15 11:08:41.117 | INFO     | __main__:train:123 - Epoch: [11][40/390]	 loss 4.40334	 cls_loss: 0.7506 cluster_loss: 1.2524 sup_con_loss: 0.9875 contrastive_loss: 4.5861 
2024-05-15 11:08:56.093 | INFO     | __main__:train:123 - Epoch: [11][60/390]	 loss 4.26192	 cls_loss: 0.6407 cluster_loss: 1.2618 sup_con_loss: 0.6815 contrastive_loss: 4.5831 
2024-05-15 11:09:10.913 | INFO     | __main__:train:123 - Epoch: [11][80/390]	 loss 4.19129	 cls_loss: 0.7228 cluster_loss: 1.2007 sup_con_loss: 0.5105 contrastive_loss: 4.5834 
2024-05-15 11:09:25.943 | INFO     | __main__:train:123 - Epoch: [11][100/390]	 loss 4.19849	 cls_loss: 0.7120 cluster_loss: 1.2467 sup_con_loss: 0.4534 contrastive_loss: 4.5850 
2024-05-15 11:09:40.974 | INFO     | __main__:train:123 - Epoch: [11][120/390]	 loss 4.27913	 cls_loss: 0.6968 cluster_loss: 1.2029 sup_con_loss: 0.7923 contrastive_loss: 4.5785 
2024-05-15 11:09:56.089 | INFO     | __main__:train:123 - Epoch: [11][140/390]	 loss 4.31420	 cls_loss: 0.7771 cluster_loss: 1.2378 sup_con_loss: 0.7392 contrastive_loss: 4.5829 
2024-05-15 11:10:10.892 | INFO     | __main__:train:123 - Epoch: [11][160/390]	 loss 4.43952	 cls_loss: 0.8447 cluster_loss: 1.2800 sup_con_loss: 0.9554 contrastive_loss: 4.5808 
2024-05-15 11:10:26.009 | INFO     | __main__:train:123 - Epoch: [11][180/390]	 loss 4.39239	 cls_loss: 0.7545 cluster_loss: 1.3481 sup_con_loss: 0.7708 contrastive_loss: 4.5881 
2024-05-15 11:10:40.642 | INFO     | __main__:train:123 - Epoch: [11][200/390]	 loss 4.25443	 cls_loss: 0.7030 cluster_loss: 1.1924 sup_con_loss: 0.7324 contrastive_loss: 4.5800 
2024-05-15 11:10:55.672 | INFO     | __main__:train:123 - Epoch: [11][220/390]	 loss 4.24961	 cls_loss: 0.7514 cluster_loss: 1.2051 sup_con_loss: 0.6348 contrastive_loss: 4.5864 
2024-05-15 11:11:10.675 | INFO     | __main__:train:123 - Epoch: [11][240/390]	 loss 4.25473	 cls_loss: 0.5922 cluster_loss: 1.1382 sup_con_loss: 0.9482 contrastive_loss: 4.5782 
2024-05-15 11:11:25.682 | INFO     | __main__:train:123 - Epoch: [11][260/390]	 loss 4.40095	 cls_loss: 0.8827 cluster_loss: 1.2962 sup_con_loss: 0.7799 contrastive_loss: 4.5793 
2024-05-15 11:11:40.650 | INFO     | __main__:train:123 - Epoch: [11][280/390]	 loss 4.34655	 cls_loss: 0.7988 cluster_loss: 1.1870 sup_con_loss: 0.9080 contrastive_loss: 4.5810 
2024-05-15 11:11:55.632 | INFO     | __main__:train:123 - Epoch: [11][300/390]	 loss 4.25433	 cls_loss: 0.6108 cluster_loss: 1.2266 sup_con_loss: 0.7579 contrastive_loss: 4.5815 
2024-05-15 11:12:10.849 | INFO     | __main__:train:123 - Epoch: [11][320/390]	 loss 4.29532	 cls_loss: 0.6827 cluster_loss: 1.2435 sup_con_loss: 0.7650 contrastive_loss: 4.5852 
2024-05-15 11:12:25.642 | INFO     | __main__:train:123 - Epoch: [11][340/390]	 loss 4.48887	 cls_loss: 0.9866 cluster_loss: 1.3124 sup_con_loss: 0.8881 contrastive_loss: 4.5841 
2024-05-15 11:12:40.572 | INFO     | __main__:train:123 - Epoch: [11][360/390]	 loss 4.36838	 cls_loss: 0.8822 cluster_loss: 1.2563 sup_con_loss: 0.7699 contrastive_loss: 4.5747 
2024-05-15 11:12:55.074 | INFO     | __main__:train:123 - Epoch: [11][380/390]	 loss 4.15497	 cls_loss: 0.5402 cluster_loss: 1.1443 sup_con_loss: 0.6960 contrastive_loss: 4.5823 
2024-05-15 11:13:01.396 | INFO     | __main__:train:126 - Train Epoch: 11 Avg Loss: 4.3283 
2024-05-15 11:13:01.398 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:13:30.383 | INFO     | __main__:train:135 - Train Accuracies: All 0.7711 | Old 0.8174 | New 0.5860
2024-05-15 11:13:34.765 | INFO     | __main__:train:123 - Epoch: [12][0/390]	 loss 4.34779	 cls_loss: 0.9246 cluster_loss: 1.1927 sup_con_loss: 0.7725 contrastive_loss: 4.5825 
2024-05-15 11:13:49.829 | INFO     | __main__:train:123 - Epoch: [12][20/390]	 loss 4.51638	 cls_loss: 0.7963 cluster_loss: 1.3178 sup_con_loss: 1.1482 contrastive_loss: 4.5835 
2024-05-15 11:14:04.852 | INFO     | __main__:train:123 - Epoch: [12][40/390]	 loss 4.27194	 cls_loss: 0.7296 cluster_loss: 1.2326 sup_con_loss: 0.6874 contrastive_loss: 4.5767 
2024-05-15 11:14:20.048 | INFO     | __main__:train:123 - Epoch: [12][60/390]	 loss 4.36425	 cls_loss: 0.8708 cluster_loss: 1.2293 sup_con_loss: 0.7966 contrastive_loss: 4.5871 
2024-05-15 11:14:35.107 | INFO     | __main__:train:123 - Epoch: [12][80/390]	 loss 4.31805	 cls_loss: 0.6235 cluster_loss: 1.2194 sup_con_loss: 0.9312 contrastive_loss: 4.5866 
2024-05-15 11:14:49.943 | INFO     | __main__:train:123 - Epoch: [12][100/390]	 loss 4.35322	 cls_loss: 0.7092 cluster_loss: 1.2093 sup_con_loss: 0.9824 contrastive_loss: 4.5771 
2024-05-15 11:15:05.122 | INFO     | __main__:train:123 - Epoch: [12][120/390]	 loss 4.24484	 cls_loss: 0.6477 cluster_loss: 1.2019 sup_con_loss: 0.7317 contrastive_loss: 4.5858 
2024-05-15 11:15:20.196 | INFO     | __main__:train:123 - Epoch: [12][140/390]	 loss 4.33170	 cls_loss: 0.7890 cluster_loss: 1.1836 sup_con_loss: 0.8831 contrastive_loss: 4.5802 
2024-05-15 11:15:35.198 | INFO     | __main__:train:123 - Epoch: [12][160/390]	 loss 4.24189	 cls_loss: 0.7727 cluster_loss: 1.1659 sup_con_loss: 0.6693 contrastive_loss: 4.5836 
2024-05-15 11:15:50.062 | INFO     | __main__:train:123 - Epoch: [12][180/390]	 loss 4.39341	 cls_loss: 0.8118 cluster_loss: 1.2458 sup_con_loss: 0.9227 contrastive_loss: 4.5793 
2024-05-15 11:16:05.237 | INFO     | __main__:train:123 - Epoch: [12][200/390]	 loss 4.20011	 cls_loss: 0.5903 cluster_loss: 1.1794 sup_con_loss: 0.7066 contrastive_loss: 4.5840 
2024-05-15 11:16:20.187 | INFO     | __main__:train:123 - Epoch: [12][220/390]	 loss 4.20407	 cls_loss: 0.6902 cluster_loss: 1.1119 sup_con_loss: 0.7540 contrastive_loss: 4.5783 
2024-05-15 11:16:35.087 | INFO     | __main__:train:123 - Epoch: [12][240/390]	 loss 4.17678	 cls_loss: 0.5975 cluster_loss: 1.0817 sup_con_loss: 0.8257 contrastive_loss: 4.5778 
2024-05-15 11:16:50.160 | INFO     | __main__:train:123 - Epoch: [12][260/390]	 loss 4.28607	 cls_loss: 0.6894 cluster_loss: 1.2136 sup_con_loss: 0.8021 contrastive_loss: 4.5772 
2024-05-15 11:17:05.109 | INFO     | __main__:train:123 - Epoch: [12][280/390]	 loss 4.21783	 cls_loss: 0.6647 cluster_loss: 1.2556 sup_con_loss: 0.5528 contrastive_loss: 4.5779 
2024-05-15 11:17:20.286 | INFO     | __main__:train:123 - Epoch: [12][300/390]	 loss 4.22103	 cls_loss: 0.5263 cluster_loss: 1.1968 sup_con_loss: 0.8072 contrastive_loss: 4.5791 
2024-05-15 11:17:35.461 | INFO     | __main__:train:123 - Epoch: [12][320/390]	 loss 4.32098	 cls_loss: 0.7377 cluster_loss: 1.3005 sup_con_loss: 0.6696 contrastive_loss: 4.5894 
2024-05-15 11:17:50.252 | INFO     | __main__:train:123 - Epoch: [12][340/390]	 loss 4.23124	 cls_loss: 0.6104 cluster_loss: 1.2480 sup_con_loss: 0.6548 contrastive_loss: 4.5803 
2024-05-15 11:18:04.950 | INFO     | __main__:train:123 - Epoch: [12][360/390]	 loss 4.24747	 cls_loss: 0.5929 cluster_loss: 1.1695 sup_con_loss: 0.8481 contrastive_loss: 4.5892 
2024-05-15 11:18:19.434 | INFO     | __main__:train:123 - Epoch: [12][380/390]	 loss 4.34486	 cls_loss: 0.7854 cluster_loss: 1.2444 sup_con_loss: 0.8073 contrastive_loss: 4.5824 
2024-05-15 11:18:25.798 | INFO     | __main__:train:126 - Train Epoch: 12 Avg Loss: 4.2971 
2024-05-15 11:18:25.799 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:18:54.208 | INFO     | __main__:train:135 - Train Accuracies: All 0.7730 | Old 0.8297 | New 0.5460
2024-05-15 11:18:58.916 | INFO     | __main__:train:123 - Epoch: [13][0/390]	 loss 4.29205	 cls_loss: 0.6074 cluster_loss: 1.1537 sup_con_loss: 1.0148 contrastive_loss: 4.5760 
2024-05-15 11:19:13.878 | INFO     | __main__:train:123 - Epoch: [13][20/390]	 loss 4.41551	 cls_loss: 0.7591 cluster_loss: 1.3628 sup_con_loss: 0.8046 contrastive_loss: 4.5883 
2024-05-15 11:19:28.915 | INFO     | __main__:train:123 - Epoch: [13][40/390]	 loss 4.36880	 cls_loss: 0.6559 cluster_loss: 1.2550 sup_con_loss: 0.9850 contrastive_loss: 4.5828 
2024-05-15 11:19:43.768 | INFO     | __main__:train:123 - Epoch: [13][60/390]	 loss 4.40545	 cls_loss: 0.8574 cluster_loss: 1.2935 sup_con_loss: 0.8082 contrastive_loss: 4.5873 
2024-05-15 11:19:58.841 | INFO     | __main__:train:123 - Epoch: [13][80/390]	 loss 4.35483	 cls_loss: 0.8298 cluster_loss: 1.2161 sup_con_loss: 0.8458 contrastive_loss: 4.5814 
2024-05-15 11:20:13.816 | INFO     | __main__:train:123 - Epoch: [13][100/390]	 loss 4.17141	 cls_loss: 0.7431 cluster_loss: 1.1960 sup_con_loss: 0.4395 contrastive_loss: 4.5848 
2024-05-15 11:20:28.728 | INFO     | __main__:train:123 - Epoch: [13][120/390]	 loss 4.38704	 cls_loss: 0.7757 cluster_loss: 1.2347 sup_con_loss: 0.9587 contrastive_loss: 4.5807 
2024-05-15 11:20:43.642 | INFO     | __main__:train:123 - Epoch: [13][140/390]	 loss 4.44006	 cls_loss: 0.8702 cluster_loss: 1.2202 sup_con_loss: 1.0306 contrastive_loss: 4.5871 
2024-05-15 11:20:58.599 | INFO     | __main__:train:123 - Epoch: [13][160/390]	 loss 4.30041	 cls_loss: 0.6232 cluster_loss: 1.2375 sup_con_loss: 0.8629 contrastive_loss: 4.5783 
2024-05-15 11:21:13.571 | INFO     | __main__:train:123 - Epoch: [13][180/390]	 loss 4.24076	 cls_loss: 0.8178 cluster_loss: 1.1791 sup_con_loss: 0.6013 contrastive_loss: 4.5810 
2024-05-15 11:21:28.530 | INFO     | __main__:train:123 - Epoch: [13][200/390]	 loss 4.20625	 cls_loss: 0.6904 cluster_loss: 1.1607 sup_con_loss: 0.6623 contrastive_loss: 4.5821 
2024-05-15 11:21:43.721 | INFO     | __main__:train:123 - Epoch: [13][220/390]	 loss 4.35035	 cls_loss: 0.7144 cluster_loss: 1.2257 sup_con_loss: 0.9235 contrastive_loss: 4.5852 
2024-05-15 11:21:58.508 | INFO     | __main__:train:123 - Epoch: [13][240/390]	 loss 4.28206	 cls_loss: 0.5587 cluster_loss: 1.1901 sup_con_loss: 0.9455 contrastive_loss: 4.5878 
2024-05-15 11:22:13.819 | INFO     | __main__:train:123 - Epoch: [13][260/390]	 loss 4.34471	 cls_loss: 0.8571 cluster_loss: 1.1938 sup_con_loss: 0.8338 contrastive_loss: 4.5799 
2024-05-15 11:22:29.116 | INFO     | __main__:train:123 - Epoch: [13][280/390]	 loss 4.23569	 cls_loss: 0.7156 cluster_loss: 1.2083 sup_con_loss: 0.6434 contrastive_loss: 4.5763 
2024-05-15 11:22:44.468 | INFO     | __main__:train:123 - Epoch: [13][300/390]	 loss 4.23876	 cls_loss: 0.6662 cluster_loss: 1.1544 sup_con_loss: 0.7950 contrastive_loss: 4.5800 
2024-05-15 11:22:59.459 | INFO     | __main__:train:123 - Epoch: [13][320/390]	 loss 4.28495	 cls_loss: 0.6785 cluster_loss: 1.2172 sup_con_loss: 0.7932 contrastive_loss: 4.5826 
2024-05-15 11:23:14.655 | INFO     | __main__:train:123 - Epoch: [13][340/390]	 loss 4.23065	 cls_loss: 0.6702 cluster_loss: 1.2093 sup_con_loss: 0.6636 contrastive_loss: 4.5812 
2024-05-15 11:23:29.513 | INFO     | __main__:train:123 - Epoch: [13][360/390]	 loss 4.28790	 cls_loss: 0.5756 cluster_loss: 1.2494 sup_con_loss: 0.8471 contrastive_loss: 4.5813 
2024-05-15 11:23:43.911 | INFO     | __main__:train:123 - Epoch: [13][380/390]	 loss 4.37971	 cls_loss: 0.7638 cluster_loss: 1.2522 sup_con_loss: 0.9138 contrastive_loss: 4.5826 
2024-05-15 11:23:50.299 | INFO     | __main__:train:126 - Train Epoch: 13 Avg Loss: 4.2606 
2024-05-15 11:23:50.301 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:24:18.971 | INFO     | __main__:train:135 - Train Accuracies: All 0.7747 | Old 0.8217 | New 0.5865
2024-05-15 11:24:22.337 | INFO     | __main__:train:123 - Epoch: [14][0/390]	 loss 4.27878	 cls_loss: 0.6564 cluster_loss: 1.2036 sup_con_loss: 0.8259 contrastive_loss: 4.5810 
2024-05-15 11:24:37.530 | INFO     | __main__:train:123 - Epoch: [14][20/390]	 loss 4.11155	 cls_loss: 0.5476 cluster_loss: 1.0813 sup_con_loss: 0.6797 contrastive_loss: 4.5833 
2024-05-15 11:24:52.700 | INFO     | __main__:train:123 - Epoch: [14][40/390]	 loss 4.15640	 cls_loss: 0.6903 cluster_loss: 1.1007 sup_con_loss: 0.6472 contrastive_loss: 4.5736 
2024-05-15 11:25:07.553 | INFO     | __main__:train:123 - Epoch: [14][60/390]	 loss 4.28836	 cls_loss: 0.6701 cluster_loss: 1.1941 sup_con_loss: 0.8576 contrastive_loss: 4.5807 
2024-05-15 11:25:22.700 | INFO     | __main__:train:123 - Epoch: [14][80/390]	 loss 4.44329	 cls_loss: 0.7589 cluster_loss: 1.3227 sup_con_loss: 0.9517 contrastive_loss: 4.5921 
2024-05-15 11:25:37.773 | INFO     | __main__:train:123 - Epoch: [14][100/390]	 loss 4.32846	 cls_loss: 0.8001 cluster_loss: 1.1364 sup_con_loss: 0.9455 contrastive_loss: 4.5829 
2024-05-15 11:25:53.049 | INFO     | __main__:train:123 - Epoch: [14][120/390]	 loss 4.35124	 cls_loss: 0.7310 cluster_loss: 1.2327 sup_con_loss: 0.9041 contrastive_loss: 4.5812 
2024-05-15 11:26:08.038 | INFO     | __main__:train:123 - Epoch: [14][140/390]	 loss 4.24759	 cls_loss: 0.6461 cluster_loss: 1.2194 sup_con_loss: 0.7136 contrastive_loss: 4.5832 
2024-05-15 11:26:23.070 | INFO     | __main__:train:123 - Epoch: [14][160/390]	 loss 4.11809	 cls_loss: 0.6269 cluster_loss: 1.0563 sup_con_loss: 0.6699 contrastive_loss: 4.5809 
2024-05-15 11:26:38.082 | INFO     | __main__:train:123 - Epoch: [14][180/390]	 loss 4.15583	 cls_loss: 0.6861 cluster_loss: 1.1586 sup_con_loss: 0.5345 contrastive_loss: 4.5777 
2024-05-15 11:26:53.054 | INFO     | __main__:train:123 - Epoch: [14][200/390]	 loss 4.32925	 cls_loss: 0.7555 cluster_loss: 1.2355 sup_con_loss: 0.8015 contrastive_loss: 4.5865 
2024-05-15 11:27:08.086 | INFO     | __main__:train:123 - Epoch: [14][220/390]	 loss 4.33575	 cls_loss: 0.7747 cluster_loss: 1.2622 sup_con_loss: 0.7554 contrastive_loss: 4.5843 
2024-05-15 11:27:22.705 | INFO     | __main__:train:123 - Epoch: [14][240/390]	 loss 4.35344	 cls_loss: 0.8335 cluster_loss: 1.1567 sup_con_loss: 0.9518 contrastive_loss: 4.5796 
2024-05-15 11:27:37.684 | INFO     | __main__:train:123 - Epoch: [14][260/390]	 loss 4.31689	 cls_loss: 0.8088 cluster_loss: 1.1434 sup_con_loss: 0.8970 contrastive_loss: 4.5794 
2024-05-15 11:27:52.472 | INFO     | __main__:train:123 - Epoch: [14][280/390]	 loss 4.11939	 cls_loss: 0.5319 cluster_loss: 1.0708 sup_con_loss: 0.7503 contrastive_loss: 4.5763 
2024-05-15 11:28:07.289 | INFO     | __main__:train:123 - Epoch: [14][300/390]	 loss 4.38545	 cls_loss: 0.8202 cluster_loss: 1.1438 sup_con_loss: 1.0894 contrastive_loss: 4.5748 
2024-05-15 11:28:22.042 | INFO     | __main__:train:123 - Epoch: [14][320/390]	 loss 4.28851	 cls_loss: 0.7319 cluster_loss: 1.1713 sup_con_loss: 0.8338 contrastive_loss: 4.5833 
2024-05-15 11:28:37.007 | INFO     | __main__:train:123 - Epoch: [14][340/390]	 loss 4.32108	 cls_loss: 0.8039 cluster_loss: 1.2138 sup_con_loss: 0.7860 contrastive_loss: 4.5780 
2024-05-15 11:28:51.977 | INFO     | __main__:train:123 - Epoch: [14][360/390]	 loss 4.13338	 cls_loss: 0.6870 cluster_loss: 1.0310 sup_con_loss: 0.7075 contrastive_loss: 4.5772 
2024-05-15 11:29:06.376 | INFO     | __main__:train:123 - Epoch: [14][380/390]	 loss 4.20400	 cls_loss: 0.5865 cluster_loss: 1.0985 sup_con_loss: 0.8685 contrastive_loss: 4.5857 
2024-05-15 11:29:12.657 | INFO     | __main__:train:126 - Train Epoch: 14 Avg Loss: 4.2425 
2024-05-15 11:29:12.658 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:29:41.466 | INFO     | __main__:train:135 - Train Accuracies: All 0.7777 | Old 0.8204 | New 0.6070
2024-05-15 11:29:44.543 | INFO     | __main__:train:123 - Epoch: [15][0/390]	 loss 4.22627	 cls_loss: 0.7576 cluster_loss: 1.1282 sup_con_loss: 0.7204 contrastive_loss: 4.5779 
2024-05-15 11:29:59.823 | INFO     | __main__:train:123 - Epoch: [15][20/390]	 loss 4.24208	 cls_loss: 0.8339 cluster_loss: 1.0838 sup_con_loss: 0.7760 contrastive_loss: 4.5756 
2024-05-15 11:30:14.490 | INFO     | __main__:train:123 - Epoch: [15][40/390]	 loss 4.25753	 cls_loss: 0.5985 cluster_loss: 1.1542 sup_con_loss: 0.9180 contrastive_loss: 4.5792 
2024-05-15 11:30:29.242 | INFO     | __main__:train:123 - Epoch: [15][60/390]	 loss 4.21866	 cls_loss: 0.7179 cluster_loss: 1.1070 sup_con_loss: 0.7801 contrastive_loss: 4.5766 
2024-05-15 11:30:44.055 | INFO     | __main__:train:123 - Epoch: [15][80/390]	 loss 4.24444	 cls_loss: 0.5842 cluster_loss: 1.1940 sup_con_loss: 0.8189 contrastive_loss: 4.5803 
2024-05-15 11:30:58.600 | INFO     | __main__:train:123 - Epoch: [15][100/390]	 loss 4.19128	 cls_loss: 0.4582 cluster_loss: 1.1749 sup_con_loss: 0.8213 contrastive_loss: 4.5842 
2024-05-15 11:31:13.628 | INFO     | __main__:train:123 - Epoch: [15][120/390]	 loss 4.19331	 cls_loss: 0.7126 cluster_loss: 1.0601 sup_con_loss: 0.7923 contrastive_loss: 4.5808 
2024-05-15 11:31:28.634 | INFO     | __main__:train:123 - Epoch: [15][140/390]	 loss 4.14458	 cls_loss: 0.6663 cluster_loss: 1.0873 sup_con_loss: 0.6591 contrastive_loss: 4.5754 
2024-05-15 11:31:43.635 | INFO     | __main__:train:123 - Epoch: [15][160/390]	 loss 4.39522	 cls_loss: 0.6896 cluster_loss: 1.3235 sup_con_loss: 0.8750 contrastive_loss: 4.5959 
2024-05-15 11:31:58.653 | INFO     | __main__:train:123 - Epoch: [15][180/390]	 loss 4.21808	 cls_loss: 0.5952 cluster_loss: 1.1045 sup_con_loss: 0.9029 contrastive_loss: 4.5782 
2024-05-15 11:32:13.643 | INFO     | __main__:train:123 - Epoch: [15][200/390]	 loss 4.18047	 cls_loss: 0.7105 cluster_loss: 1.1731 sup_con_loss: 0.5440 contrastive_loss: 4.5829 
2024-05-15 11:32:28.718 | INFO     | __main__:train:123 - Epoch: [15][220/390]	 loss 4.15284	 cls_loss: 0.6882 cluster_loss: 1.0687 sup_con_loss: 0.6969 contrastive_loss: 4.5745 
2024-05-15 11:32:43.738 | INFO     | __main__:train:123 - Epoch: [15][240/390]	 loss 4.30520	 cls_loss: 0.6167 cluster_loss: 1.2502 sup_con_loss: 0.8473 contrastive_loss: 4.5849 
2024-05-15 11:32:58.673 | INFO     | __main__:train:123 - Epoch: [15][260/390]	 loss 4.37994	 cls_loss: 0.8256 cluster_loss: 1.2017 sup_con_loss: 0.9348 contrastive_loss: 4.5888 
2024-05-15 11:33:13.603 | INFO     | __main__:train:123 - Epoch: [15][280/390]	 loss 4.24947	 cls_loss: 0.6167 cluster_loss: 1.1601 sup_con_loss: 0.8566 contrastive_loss: 4.5842 
2024-05-15 11:33:28.806 | INFO     | __main__:train:123 - Epoch: [15][300/390]	 loss 4.39507	 cls_loss: 0.7826 cluster_loss: 1.1619 sup_con_loss: 1.1137 contrastive_loss: 4.5787 
2024-05-15 11:33:44.016 | INFO     | __main__:train:123 - Epoch: [15][320/390]	 loss 4.17392	 cls_loss: 0.6451 cluster_loss: 1.0860 sup_con_loss: 0.7658 contrastive_loss: 4.5757 
2024-05-15 11:33:59.069 | INFO     | __main__:train:123 - Epoch: [15][340/390]	 loss 4.19501	 cls_loss: 0.6150 cluster_loss: 1.0972 sup_con_loss: 0.8317 contrastive_loss: 4.5777 
2024-05-15 11:34:13.920 | INFO     | __main__:train:123 - Epoch: [15][360/390]	 loss 4.32809	 cls_loss: 0.8183 cluster_loss: 1.1043 sup_con_loss: 1.0020 contrastive_loss: 4.5741 
2024-05-15 11:34:28.688 | INFO     | __main__:train:123 - Epoch: [15][380/390]	 loss 4.16235	 cls_loss: 0.5997 cluster_loss: 1.1346 sup_con_loss: 0.6832 contrastive_loss: 4.5783 
2024-05-15 11:34:35.074 | INFO     | __main__:train:126 - Train Epoch: 15 Avg Loss: 4.2202 
2024-05-15 11:34:35.074 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:35:03.579 | INFO     | __main__:train:135 - Train Accuracies: All 0.7787 | Old 0.8319 | New 0.5660
2024-05-15 11:35:08.113 | INFO     | __main__:train:123 - Epoch: [16][0/390]	 loss 4.22207	 cls_loss: 0.6254 cluster_loss: 1.1400 sup_con_loss: 0.8148 contrastive_loss: 4.5800 
2024-05-15 11:35:23.200 | INFO     | __main__:train:123 - Epoch: [16][20/390]	 loss 4.16033	 cls_loss: 0.5395 cluster_loss: 1.0604 sup_con_loss: 0.8743 contrastive_loss: 4.5788 
2024-05-15 11:35:38.433 | INFO     | __main__:train:123 - Epoch: [16][40/390]	 loss 4.13884	 cls_loss: 0.5331 cluster_loss: 1.0763 sup_con_loss: 0.7906 contrastive_loss: 4.5784 
2024-05-15 11:35:53.320 | INFO     | __main__:train:123 - Epoch: [16][60/390]	 loss 4.33882	 cls_loss: 0.7606 cluster_loss: 1.1933 sup_con_loss: 0.9119 contrastive_loss: 4.5813 
2024-05-15 11:36:08.154 | INFO     | __main__:train:123 - Epoch: [16][80/390]	 loss 4.23985	 cls_loss: 0.6737 cluster_loss: 1.2050 sup_con_loss: 0.6995 contrastive_loss: 4.5784 
2024-05-15 11:36:23.274 | INFO     | __main__:train:123 - Epoch: [16][100/390]	 loss 4.01530	 cls_loss: 0.5236 cluster_loss: 1.0104 sup_con_loss: 0.5715 contrastive_loss: 4.5773 
2024-05-15 11:36:38.332 | INFO     | __main__:train:123 - Epoch: [16][120/390]	 loss 4.24981	 cls_loss: 0.5243 cluster_loss: 1.1691 sup_con_loss: 0.9404 contrastive_loss: 4.5804 
2024-05-15 11:36:53.284 | INFO     | __main__:train:123 - Epoch: [16][140/390]	 loss 4.29988	 cls_loss: 0.7138 cluster_loss: 1.1324 sup_con_loss: 0.9581 contrastive_loss: 4.5826 
2024-05-15 11:37:08.284 | INFO     | __main__:train:123 - Epoch: [16][160/390]	 loss 4.29048	 cls_loss: 0.6354 cluster_loss: 1.1434 sup_con_loss: 1.0022 contrastive_loss: 4.5756 
2024-05-15 11:37:23.137 | INFO     | __main__:train:123 - Epoch: [16][180/390]	 loss 4.19641	 cls_loss: 0.6514 cluster_loss: 1.0765 sup_con_loss: 0.8356 contrastive_loss: 4.5788 
2024-05-15 11:37:38.304 | INFO     | __main__:train:123 - Epoch: [16][200/390]	 loss 4.39681	 cls_loss: 0.8103 cluster_loss: 1.2789 sup_con_loss: 0.8670 contrastive_loss: 4.5822 
2024-05-15 11:37:53.376 | INFO     | __main__:train:123 - Epoch: [16][220/390]	 loss 4.26080	 cls_loss: 0.7248 cluster_loss: 1.1313 sup_con_loss: 0.8508 contrastive_loss: 4.5754 
2024-05-15 11:38:08.264 | INFO     | __main__:train:123 - Epoch: [16][240/390]	 loss 4.16218	 cls_loss: 0.7027 cluster_loss: 1.0840 sup_con_loss: 0.6656 contrastive_loss: 4.5825 
2024-05-15 11:38:23.117 | INFO     | __main__:train:123 - Epoch: [16][260/390]	 loss 4.21490	 cls_loss: 0.6681 cluster_loss: 1.1573 sup_con_loss: 0.7248 contrastive_loss: 4.5771 
2024-05-15 11:38:38.159 | INFO     | __main__:train:123 - Epoch: [16][280/390]	 loss 4.22998	 cls_loss: 0.6625 cluster_loss: 1.1512 sup_con_loss: 0.7826 contrastive_loss: 4.5784 
2024-05-15 11:38:52.941 | INFO     | __main__:train:123 - Epoch: [16][300/390]	 loss 4.33518	 cls_loss: 0.7162 cluster_loss: 1.1421 sup_con_loss: 1.0449 contrastive_loss: 4.5791 
2024-05-15 11:39:08.222 | INFO     | __main__:train:123 - Epoch: [16][320/390]	 loss 4.17941	 cls_loss: 0.5811 cluster_loss: 1.0556 sup_con_loss: 0.8928 contrastive_loss: 4.5806 
2024-05-15 11:39:23.027 | INFO     | __main__:train:123 - Epoch: [16][340/390]	 loss 4.10242	 cls_loss: 0.5722 cluster_loss: 1.0438 sup_con_loss: 0.7101 contrastive_loss: 4.5772 
2024-05-15 11:39:37.828 | INFO     | __main__:train:123 - Epoch: [16][360/390]	 loss 4.32273	 cls_loss: 0.7139 cluster_loss: 1.1886 sup_con_loss: 0.9287 contrastive_loss: 4.5773 
2024-05-15 11:39:52.439 | INFO     | __main__:train:123 - Epoch: [16][380/390]	 loss 4.41815	 cls_loss: 0.6359 cluster_loss: 1.2611 sup_con_loss: 1.1298 contrastive_loss: 4.5853 
2024-05-15 11:39:58.843 | INFO     | __main__:train:126 - Train Epoch: 16 Avg Loss: 4.2075 
2024-05-15 11:39:58.844 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:40:27.657 | INFO     | __main__:train:135 - Train Accuracies: All 0.7799 | Old 0.8306 | New 0.5770
2024-05-15 11:40:32.185 | INFO     | __main__:train:123 - Epoch: [17][0/390]	 loss 4.05789	 cls_loss: 0.5722 cluster_loss: 1.0502 sup_con_loss: 0.5699 contrastive_loss: 4.5777 
2024-05-15 11:40:47.114 | INFO     | __main__:train:123 - Epoch: [17][20/390]	 loss 4.30332	 cls_loss: 0.7558 cluster_loss: 1.1302 sup_con_loss: 0.9373 contrastive_loss: 4.5787 
2024-05-15 11:41:02.019 | INFO     | __main__:train:123 - Epoch: [17][40/390]	 loss 4.20614	 cls_loss: 0.6548 cluster_loss: 1.0868 sup_con_loss: 0.8253 contrastive_loss: 4.5872 
2024-05-15 11:41:17.191 | INFO     | __main__:train:123 - Epoch: [17][60/390]	 loss 4.18927	 cls_loss: 0.7066 cluster_loss: 1.0435 sup_con_loss: 0.8215 contrastive_loss: 4.5787 
2024-05-15 11:41:32.271 | INFO     | __main__:train:123 - Epoch: [17][80/390]	 loss 4.17217	 cls_loss: 0.6359 cluster_loss: 1.1427 sup_con_loss: 0.6474 contrastive_loss: 4.5850 
2024-05-15 11:41:47.272 | INFO     | __main__:train:123 - Epoch: [17][100/390]	 loss 4.17184	 cls_loss: 0.6242 cluster_loss: 1.1028 sup_con_loss: 0.7411 contrastive_loss: 4.5803 
2024-05-15 11:42:02.589 | INFO     | __main__:train:123 - Epoch: [17][120/390]	 loss 4.18520	 cls_loss: 0.6834 cluster_loss: 1.0955 sup_con_loss: 0.7331 contrastive_loss: 4.5805 
2024-05-15 11:42:17.559 | INFO     | __main__:train:123 - Epoch: [17][140/390]	 loss 4.21813	 cls_loss: 0.7069 cluster_loss: 1.0296 sup_con_loss: 0.9349 contrastive_loss: 4.5757 
2024-05-15 11:42:32.609 | INFO     | __main__:train:123 - Epoch: [17][160/390]	 loss 4.38046	 cls_loss: 0.6488 cluster_loss: 1.2362 sup_con_loss: 1.0499 contrastive_loss: 4.5882 
2024-05-15 11:42:47.628 | INFO     | __main__:train:123 - Epoch: [17][180/390]	 loss 4.04843	 cls_loss: 0.4990 cluster_loss: 1.0418 sup_con_loss: 0.6303 contrastive_loss: 4.5784 
2024-05-15 11:43:02.621 | INFO     | __main__:train:123 - Epoch: [17][200/390]	 loss 4.22827	 cls_loss: 0.6551 cluster_loss: 1.0824 sup_con_loss: 0.9091 contrastive_loss: 4.5804 
2024-05-15 11:43:17.798 | INFO     | __main__:train:123 - Epoch: [17][220/390]	 loss 4.13260	 cls_loss: 0.5870 cluster_loss: 1.0776 sup_con_loss: 0.7246 contrastive_loss: 4.5740 
2024-05-15 11:43:32.938 | INFO     | __main__:train:123 - Epoch: [17][240/390]	 loss 4.12884	 cls_loss: 0.6490 cluster_loss: 1.0817 sup_con_loss: 0.6433 contrastive_loss: 4.5745 
2024-05-15 11:43:47.966 | INFO     | __main__:train:123 - Epoch: [17][260/390]	 loss 4.18109	 cls_loss: 0.5768 cluster_loss: 1.0758 sup_con_loss: 0.8714 contrastive_loss: 4.5768 
2024-05-15 11:44:02.812 | INFO     | __main__:train:123 - Epoch: [17][280/390]	 loss 4.10257	 cls_loss: 0.5348 cluster_loss: 1.0481 sup_con_loss: 0.7437 contrastive_loss: 4.5751 
2024-05-15 11:44:17.739 | INFO     | __main__:train:123 - Epoch: [17][300/390]	 loss 3.97453	 cls_loss: 0.4529 cluster_loss: 0.9654 sup_con_loss: 0.6175 contrastive_loss: 4.5729 
2024-05-15 11:44:32.968 | INFO     | __main__:train:123 - Epoch: [17][320/390]	 loss 4.13792	 cls_loss: 0.5474 cluster_loss: 1.1046 sup_con_loss: 0.7273 contrastive_loss: 4.5751 
2024-05-15 11:44:47.988 | INFO     | __main__:train:123 - Epoch: [17][340/390]	 loss 4.13494	 cls_loss: 0.7597 cluster_loss: 1.0241 sup_con_loss: 0.6514 contrastive_loss: 4.5775 
2024-05-15 11:45:03.074 | INFO     | __main__:train:123 - Epoch: [17][360/390]	 loss 4.10847	 cls_loss: 0.5397 cluster_loss: 1.0142 sup_con_loss: 0.8225 contrastive_loss: 4.5731 
2024-05-15 11:45:17.371 | INFO     | __main__:train:123 - Epoch: [17][380/390]	 loss 4.27086	 cls_loss: 0.7579 cluster_loss: 1.0954 sup_con_loss: 0.9092 contrastive_loss: 4.5775 
2024-05-15 11:45:23.741 | INFO     | __main__:train:126 - Train Epoch: 17 Avg Loss: 4.1778 
2024-05-15 11:45:23.742 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:45:52.287 | INFO     | __main__:train:135 - Train Accuracies: All 0.7813 | Old 0.8361 | New 0.5620
2024-05-15 11:45:56.860 | INFO     | __main__:train:123 - Epoch: [18][0/390]	 loss 4.21842	 cls_loss: 0.6498 cluster_loss: 1.0803 sup_con_loss: 0.8963 contrastive_loss: 4.5770 
2024-05-15 11:46:11.595 | INFO     | __main__:train:123 - Epoch: [18][20/390]	 loss 4.11957	 cls_loss: 0.6146 cluster_loss: 1.0645 sup_con_loss: 0.6694 contrastive_loss: 4.5819 
2024-05-15 11:46:26.566 | INFO     | __main__:train:123 - Epoch: [18][40/390]	 loss 4.24975	 cls_loss: 0.6122 cluster_loss: 1.1776 sup_con_loss: 0.8409 contrastive_loss: 4.5781 
2024-05-15 11:46:41.676 | INFO     | __main__:train:123 - Epoch: [18][60/390]	 loss 4.13702	 cls_loss: 0.5920 cluster_loss: 1.0659 sup_con_loss: 0.7541 contrastive_loss: 4.5739 
2024-05-15 11:46:56.508 | INFO     | __main__:train:123 - Epoch: [18][80/390]	 loss 4.23525	 cls_loss: 0.6151 cluster_loss: 1.0941 sup_con_loss: 0.9442 contrastive_loss: 4.5820 
2024-05-15 11:47:11.428 | INFO     | __main__:train:123 - Epoch: [18][100/390]	 loss 4.23536	 cls_loss: 0.5285 cluster_loss: 1.1296 sup_con_loss: 0.9759 contrastive_loss: 4.5763 
2024-05-15 11:47:26.224 | INFO     | __main__:train:123 - Epoch: [18][120/390]	 loss 4.23629	 cls_loss: 0.6524 cluster_loss: 1.0953 sup_con_loss: 0.9143 contrastive_loss: 4.5785 
2024-05-15 11:47:41.228 | INFO     | __main__:train:123 - Epoch: [18][140/390]	 loss 4.11313	 cls_loss: 0.5437 cluster_loss: 1.0302 sup_con_loss: 0.8036 contrastive_loss: 4.5722 
2024-05-15 11:47:56.074 | INFO     | __main__:train:123 - Epoch: [18][160/390]	 loss 4.34347	 cls_loss: 0.6911 cluster_loss: 1.1434 sup_con_loss: 1.0863 contrastive_loss: 4.5819 
2024-05-15 11:48:10.986 | INFO     | __main__:train:123 - Epoch: [18][180/390]	 loss 4.33255	 cls_loss: 0.7370 cluster_loss: 1.1681 sup_con_loss: 0.9726 contrastive_loss: 4.5768 
2024-05-15 11:48:25.697 | INFO     | __main__:train:123 - Epoch: [18][200/390]	 loss 4.03535	 cls_loss: 0.5469 cluster_loss: 0.9954 sup_con_loss: 0.6428 contrastive_loss: 4.5722 
2024-05-15 11:48:40.566 | INFO     | __main__:train:123 - Epoch: [18][220/390]	 loss 4.23406	 cls_loss: 0.7164 cluster_loss: 1.1241 sup_con_loss: 0.7862 contrastive_loss: 4.5807 
2024-05-15 11:48:55.724 | INFO     | __main__:train:123 - Epoch: [18][240/390]	 loss 4.12019	 cls_loss: 0.5975 cluster_loss: 1.0359 sup_con_loss: 0.7614 contrastive_loss: 4.5711 
2024-05-15 11:49:10.585 | INFO     | __main__:train:123 - Epoch: [18][260/390]	 loss 4.34117	 cls_loss: 0.6490 cluster_loss: 1.1909 sup_con_loss: 1.0297 contrastive_loss: 4.5839 
2024-05-15 11:49:25.414 | INFO     | __main__:train:123 - Epoch: [18][280/390]	 loss 4.07770	 cls_loss: 0.5759 cluster_loss: 0.9944 sup_con_loss: 0.7375 contrastive_loss: 4.5718 
2024-05-15 11:49:40.663 | INFO     | __main__:train:123 - Epoch: [18][300/390]	 loss 4.03376	 cls_loss: 0.5222 cluster_loss: 0.9803 sup_con_loss: 0.6837 contrastive_loss: 4.5762 
2024-05-15 11:49:55.723 | INFO     | __main__:train:123 - Epoch: [18][320/390]	 loss 4.18247	 cls_loss: 0.5164 cluster_loss: 1.0822 sup_con_loss: 0.9235 contrastive_loss: 4.5770 
2024-05-15 11:50:10.628 | INFO     | __main__:train:123 - Epoch: [18][340/390]	 loss 4.13634	 cls_loss: 0.5119 cluster_loss: 1.0455 sup_con_loss: 0.8686 contrastive_loss: 4.5747 
2024-05-15 11:50:25.424 | INFO     | __main__:train:123 - Epoch: [18][360/390]	 loss 4.13476	 cls_loss: 0.5404 cluster_loss: 1.0473 sup_con_loss: 0.8145 contrastive_loss: 4.5842 
2024-05-15 11:50:39.915 | INFO     | __main__:train:123 - Epoch: [18][380/390]	 loss 4.10841	 cls_loss: 0.5157 cluster_loss: 1.0288 sup_con_loss: 0.8096 contrastive_loss: 4.5782 
2024-05-15 11:50:46.284 | INFO     | __main__:train:126 - Train Epoch: 18 Avg Loss: 4.1611 
2024-05-15 11:50:46.285 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:51:15.145 | INFO     | __main__:train:135 - Train Accuracies: All 0.7849 | Old 0.8351 | New 0.5840
2024-05-15 11:51:19.881 | INFO     | __main__:train:123 - Epoch: [19][0/390]	 loss 4.17482	 cls_loss: 0.7344 cluster_loss: 1.0917 sup_con_loss: 0.6653 contrastive_loss: 4.5775 
2024-05-15 11:51:34.503 | INFO     | __main__:train:123 - Epoch: [19][20/390]	 loss 4.05151	 cls_loss: 0.5293 cluster_loss: 1.0400 sup_con_loss: 0.6121 contrastive_loss: 4.5785 
2024-05-15 11:51:49.403 | INFO     | __main__:train:123 - Epoch: [19][40/390]	 loss 4.15527	 cls_loss: 0.7877 cluster_loss: 0.9860 sup_con_loss: 0.7546 contrastive_loss: 4.5763 
2024-05-15 11:52:04.434 | INFO     | __main__:train:123 - Epoch: [19][60/390]	 loss 4.04012	 cls_loss: 0.5171 cluster_loss: 1.0405 sup_con_loss: 0.5903 contrastive_loss: 4.5788 
2024-05-15 11:52:19.374 | INFO     | __main__:train:123 - Epoch: [19][80/390]	 loss 4.17080	 cls_loss: 0.6193 cluster_loss: 1.0942 sup_con_loss: 0.7644 contrastive_loss: 4.5774 
2024-05-15 11:52:34.027 | INFO     | __main__:train:123 - Epoch: [19][100/390]	 loss 4.15976	 cls_loss: 0.5281 cluster_loss: 1.0178 sup_con_loss: 0.9475 contrastive_loss: 4.5873 
2024-05-15 11:52:49.057 | INFO     | __main__:train:123 - Epoch: [19][120/390]	 loss 4.17889	 cls_loss: 0.6795 cluster_loss: 1.0624 sup_con_loss: 0.7896 contrastive_loss: 4.5756 
2024-05-15 11:53:04.440 | INFO     | __main__:train:123 - Epoch: [19][140/390]	 loss 4.12878	 cls_loss: 0.5538 cluster_loss: 1.0799 sup_con_loss: 0.7295 contrastive_loss: 4.5811 
2024-05-15 11:53:19.568 | INFO     | __main__:train:123 - Epoch: [19][160/390]	 loss 4.23054	 cls_loss: 0.5969 cluster_loss: 1.0705 sup_con_loss: 0.9993 contrastive_loss: 4.5785 
2024-05-15 11:53:34.362 | INFO     | __main__:train:123 - Epoch: [19][180/390]	 loss 4.24250	 cls_loss: 0.7622 cluster_loss: 1.1534 sup_con_loss: 0.7182 contrastive_loss: 4.5764 
2024-05-15 11:53:49.402 | INFO     | __main__:train:123 - Epoch: [19][200/390]	 loss 4.25227	 cls_loss: 0.6218 cluster_loss: 1.1317 sup_con_loss: 0.9071 contrastive_loss: 4.5870 
2024-05-15 11:54:04.408 | INFO     | __main__:train:123 - Epoch: [19][220/390]	 loss 4.26559	 cls_loss: 0.6375 cluster_loss: 0.9947 sup_con_loss: 1.2100 contrastive_loss: 4.5729 
2024-05-15 11:54:19.467 | INFO     | __main__:train:123 - Epoch: [19][240/390]	 loss 4.23958	 cls_loss: 0.7153 cluster_loss: 1.0239 sup_con_loss: 1.0013 contrastive_loss: 4.5742 
2024-05-15 11:54:34.470 | INFO     | __main__:train:123 - Epoch: [19][260/390]	 loss 4.19616	 cls_loss: 0.7690 cluster_loss: 0.9835 sup_con_loss: 0.9033 contrastive_loss: 4.5717 
2024-05-15 11:54:49.342 | INFO     | __main__:train:123 - Epoch: [19][280/390]	 loss 4.08594	 cls_loss: 0.4571 cluster_loss: 1.0963 sup_con_loss: 0.6802 contrastive_loss: 4.5774 
2024-05-15 11:55:04.355 | INFO     | __main__:train:123 - Epoch: [19][300/390]	 loss 4.17461	 cls_loss: 0.7430 cluster_loss: 1.0513 sup_con_loss: 0.7341 contrastive_loss: 4.5759 
2024-05-15 11:55:19.233 | INFO     | __main__:train:123 - Epoch: [19][320/390]	 loss 4.23621	 cls_loss: 0.5787 cluster_loss: 1.1050 sup_con_loss: 0.9687 contrastive_loss: 4.5791 
2024-05-15 11:55:34.241 | INFO     | __main__:train:123 - Epoch: [19][340/390]	 loss 4.10466	 cls_loss: 0.5555 cluster_loss: 1.0384 sup_con_loss: 0.7466 contrastive_loss: 4.5753 
2024-05-15 11:55:49.293 | INFO     | __main__:train:123 - Epoch: [19][360/390]	 loss 4.34369	 cls_loss: 0.8170 cluster_loss: 1.0792 sup_con_loss: 1.0896 contrastive_loss: 4.5767 
2024-05-15 11:56:04.023 | INFO     | __main__:train:123 - Epoch: [19][380/390]	 loss 4.14948	 cls_loss: 0.6870 cluster_loss: 1.0337 sup_con_loss: 0.7458 contrastive_loss: 4.5785 
2024-05-15 11:56:10.527 | INFO     | __main__:train:126 - Train Epoch: 19 Avg Loss: 4.1477 
2024-05-15 11:56:10.528 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:56:38.974 | INFO     | __main__:train:135 - Train Accuracies: All 0.7884 | Old 0.8366 | New 0.5955
2024-05-15 11:56:43.977 | INFO     | __main__:train:123 - Epoch: [20][0/390]	 loss 4.21563	 cls_loss: 0.6081 cluster_loss: 1.1301 sup_con_loss: 0.8357 contrastive_loss: 4.5780 
2024-05-15 11:56:58.981 | INFO     | __main__:train:123 - Epoch: [20][20/390]	 loss 4.09816	 cls_loss: 0.5834 cluster_loss: 1.0232 sup_con_loss: 0.7345 contrastive_loss: 4.5720 
2024-05-15 11:57:13.699 | INFO     | __main__:train:123 - Epoch: [20][40/390]	 loss 4.10419	 cls_loss: 0.4661 cluster_loss: 1.0624 sup_con_loss: 0.7859 contrastive_loss: 4.5776 
2024-05-15 11:57:28.619 | INFO     | __main__:train:123 - Epoch: [20][60/390]	 loss 4.08748	 cls_loss: 0.6602 cluster_loss: 1.0232 sup_con_loss: 0.6175 contrastive_loss: 4.5773 
2024-05-15 11:57:43.664 | INFO     | __main__:train:123 - Epoch: [20][80/390]	 loss 4.06498	 cls_loss: 0.5150 cluster_loss: 1.1093 sup_con_loss: 0.5259 contrastive_loss: 4.5840 
2024-05-15 11:57:58.822 | INFO     | __main__:train:123 - Epoch: [20][100/390]	 loss 4.23320	 cls_loss: 0.7041 cluster_loss: 1.1263 sup_con_loss: 0.7956 contrastive_loss: 4.5787 
2024-05-15 11:58:14.034 | INFO     | __main__:train:123 - Epoch: [20][120/390]	 loss 4.07315	 cls_loss: 0.5491 cluster_loss: 0.9783 sup_con_loss: 0.7791 contrastive_loss: 4.5729 
2024-05-15 11:58:29.004 | INFO     | __main__:train:123 - Epoch: [20][140/390]	 loss 4.14377	 cls_loss: 0.5736 cluster_loss: 1.0734 sup_con_loss: 0.7733 contrastive_loss: 4.5764 
2024-05-15 11:58:44.146 | INFO     | __main__:train:123 - Epoch: [20][160/390]	 loss 3.97963	 cls_loss: 0.4841 cluster_loss: 0.9437 sup_con_loss: 0.6427 contrastive_loss: 4.5721 
2024-05-15 11:58:59.008 | INFO     | __main__:train:123 - Epoch: [20][180/390]	 loss 4.16512	 cls_loss: 0.5918 cluster_loss: 1.0883 sup_con_loss: 0.7806 contrastive_loss: 4.5806 
2024-05-15 11:59:13.780 | INFO     | __main__:train:123 - Epoch: [20][200/390]	 loss 4.08277	 cls_loss: 0.4617 cluster_loss: 1.0627 sup_con_loss: 0.7342 contrastive_loss: 4.5745 
2024-05-15 11:59:29.057 | INFO     | __main__:train:123 - Epoch: [20][220/390]	 loss 4.04171	 cls_loss: 0.5394 cluster_loss: 0.9893 sup_con_loss: 0.6690 contrastive_loss: 4.5780 
2024-05-15 11:59:44.115 | INFO     | __main__:train:123 - Epoch: [20][240/390]	 loss 4.10253	 cls_loss: 0.6345 cluster_loss: 0.9950 sup_con_loss: 0.7350 contrastive_loss: 4.5792 
2024-05-15 11:59:58.930 | INFO     | __main__:train:123 - Epoch: [20][260/390]	 loss 4.09502	 cls_loss: 0.5642 cluster_loss: 1.0376 sup_con_loss: 0.7194 contrastive_loss: 4.5712 
2024-05-15 12:00:14.081 | INFO     | __main__:train:123 - Epoch: [20][280/390]	 loss 4.06430	 cls_loss: 0.5031 cluster_loss: 0.9789 sup_con_loss: 0.8000 contrastive_loss: 4.5721 
2024-05-15 12:00:29.468 | INFO     | __main__:train:123 - Epoch: [20][300/390]	 loss 4.11931	 cls_loss: 0.5190 cluster_loss: 1.0845 sup_con_loss: 0.7339 contrastive_loss: 4.5782 
2024-05-15 12:00:44.505 | INFO     | __main__:train:123 - Epoch: [20][320/390]	 loss 4.23741	 cls_loss: 0.5682 cluster_loss: 1.1260 sup_con_loss: 0.9410 contrastive_loss: 4.5805 
2024-05-15 12:00:59.675 | INFO     | __main__:train:123 - Epoch: [20][340/390]	 loss 4.10032	 cls_loss: 0.5463 cluster_loss: 1.0276 sup_con_loss: 0.7615 contrastive_loss: 4.5764 
2024-05-15 12:01:14.787 | INFO     | __main__:train:123 - Epoch: [20][360/390]	 loss 4.01462	 cls_loss: 0.4477 cluster_loss: 0.9396 sup_con_loss: 0.7853 contrastive_loss: 4.5728 
2024-05-15 12:01:29.368 | INFO     | __main__:train:123 - Epoch: [20][380/390]	 loss 4.08696	 cls_loss: 0.5332 cluster_loss: 1.0485 sup_con_loss: 0.7012 contrastive_loss: 4.5745 
2024-05-15 12:01:35.905 | INFO     | __main__:train:126 - Train Epoch: 20 Avg Loss: 4.1272 
2024-05-15 12:01:35.905 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:02:04.540 | INFO     | __main__:train:135 - Train Accuracies: All 0.7904 | Old 0.8389 | New 0.5965
2024-05-15 12:02:08.139 | INFO     | __main__:train:123 - Epoch: [21][0/390]	 loss 4.22013	 cls_loss: 0.5927 cluster_loss: 1.0804 sup_con_loss: 0.9535 contrastive_loss: 4.5796 
2024-05-15 12:02:23.796 | INFO     | __main__:train:123 - Epoch: [21][20/390]	 loss 4.10704	 cls_loss: 0.7318 cluster_loss: 0.9848 sup_con_loss: 0.6650 contrastive_loss: 4.5815 
2024-05-15 12:02:39.026 | INFO     | __main__:train:123 - Epoch: [21][40/390]	 loss 4.00896	 cls_loss: 0.6165 cluster_loss: 0.9605 sup_con_loss: 0.5641 contrastive_loss: 4.5715 
2024-05-15 12:02:54.099 | INFO     | __main__:train:123 - Epoch: [21][60/390]	 loss 4.16529	 cls_loss: 0.6772 cluster_loss: 1.0945 sup_con_loss: 0.6916 contrastive_loss: 4.5767 
2024-05-15 12:03:08.694 | INFO     | __main__:train:123 - Epoch: [21][80/390]	 loss 4.10169	 cls_loss: 0.5804 cluster_loss: 1.1112 sup_con_loss: 0.5787 contrastive_loss: 4.5749 
2024-05-15 12:03:23.628 | INFO     | __main__:train:123 - Epoch: [21][100/390]	 loss 4.16272	 cls_loss: 0.6064 cluster_loss: 1.0056 sup_con_loss: 0.9262 contrastive_loss: 4.5733 
2024-05-15 12:03:38.887 | INFO     | __main__:train:123 - Epoch: [21][120/390]	 loss 3.95231	 cls_loss: 0.4022 cluster_loss: 0.9010 sup_con_loss: 0.7143 contrastive_loss: 4.5783 
2024-05-15 12:03:53.758 | INFO     | __main__:train:123 - Epoch: [21][140/390]	 loss 4.04761	 cls_loss: 0.5884 cluster_loss: 0.9675 sup_con_loss: 0.6902 contrastive_loss: 4.5711 
2024-05-15 12:04:08.479 | INFO     | __main__:train:123 - Epoch: [21][160/390]	 loss 4.08185	 cls_loss: 0.5097 cluster_loss: 1.0036 sup_con_loss: 0.7981 contrastive_loss: 4.5720 
2024-05-15 12:04:23.542 | INFO     | __main__:train:123 - Epoch: [21][180/390]	 loss 4.08782	 cls_loss: 0.5269 cluster_loss: 1.0528 sup_con_loss: 0.6927 contrastive_loss: 4.5794 
2024-05-15 12:04:38.435 | INFO     | __main__:train:123 - Epoch: [21][200/390]	 loss 4.22539	 cls_loss: 0.6640 cluster_loss: 1.1196 sup_con_loss: 0.8359 contrastive_loss: 4.5733 
2024-05-15 12:04:53.319 | INFO     | __main__:train:123 - Epoch: [21][220/390]	 loss 4.26731	 cls_loss: 0.8065 cluster_loss: 1.1430 sup_con_loss: 0.7630 contrastive_loss: 4.5769 
2024-05-15 12:05:08.290 | INFO     | __main__:train:123 - Epoch: [21][240/390]	 loss 4.13055	 cls_loss: 0.5356 cluster_loss: 1.1297 sup_con_loss: 0.6618 contrastive_loss: 4.5802 
2024-05-15 12:05:23.386 | INFO     | __main__:train:123 - Epoch: [21][260/390]	 loss 4.15703	 cls_loss: 0.5223 cluster_loss: 1.0332 sup_con_loss: 0.9444 contrastive_loss: 4.5725 
2024-05-15 12:05:38.288 | INFO     | __main__:train:123 - Epoch: [21][280/390]	 loss 4.21210	 cls_loss: 0.5270 cluster_loss: 1.1317 sup_con_loss: 0.8967 contrastive_loss: 4.5819 
2024-05-15 12:05:53.497 | INFO     | __main__:train:123 - Epoch: [21][300/390]	 loss 4.22023	 cls_loss: 0.5928 cluster_loss: 1.0704 sup_con_loss: 0.9803 contrastive_loss: 4.5752 
2024-05-15 12:06:08.457 | INFO     | __main__:train:123 - Epoch: [21][320/390]	 loss 4.12036	 cls_loss: 0.5749 cluster_loss: 1.0186 sup_con_loss: 0.8128 contrastive_loss: 4.5732 
2024-05-15 12:06:23.199 | INFO     | __main__:train:123 - Epoch: [21][340/390]	 loss 4.24118	 cls_loss: 0.6687 cluster_loss: 1.0989 sup_con_loss: 0.9067 contrastive_loss: 4.5777 
2024-05-15 12:06:37.920 | INFO     | __main__:train:123 - Epoch: [21][360/390]	 loss 4.13903	 cls_loss: 0.5451 cluster_loss: 1.0834 sup_con_loss: 0.7561 contrastive_loss: 4.5837 
2024-05-15 12:06:52.257 | INFO     | __main__:train:123 - Epoch: [21][380/390]	 loss 4.15805	 cls_loss: 0.5541 cluster_loss: 1.0822 sup_con_loss: 0.7989 contrastive_loss: 4.5862 
2024-05-15 12:06:58.679 | INFO     | __main__:train:126 - Train Epoch: 21 Avg Loss: 4.1191 
2024-05-15 12:06:58.680 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:07:27.631 | INFO     | __main__:train:135 - Train Accuracies: All 0.7875 | Old 0.8375 | New 0.5875
2024-05-15 12:07:32.757 | INFO     | __main__:train:123 - Epoch: [22][0/390]	 loss 4.12844	 cls_loss: 0.5507 cluster_loss: 1.1064 sup_con_loss: 0.6819 contrastive_loss: 4.5813 
2024-05-15 12:07:47.369 | INFO     | __main__:train:123 - Epoch: [22][20/390]	 loss 4.12531	 cls_loss: 0.5969 cluster_loss: 1.0439 sup_con_loss: 0.7424 contrastive_loss: 4.5815 
2024-05-15 12:08:02.396 | INFO     | __main__:train:123 - Epoch: [22][40/390]	 loss 4.11420	 cls_loss: 0.6024 cluster_loss: 1.1072 sup_con_loss: 0.5876 contrastive_loss: 4.5816 
2024-05-15 12:08:17.584 | INFO     | __main__:train:123 - Epoch: [22][60/390]	 loss 4.11720	 cls_loss: 0.6119 cluster_loss: 1.0042 sup_con_loss: 0.7825 contrastive_loss: 4.5792 
2024-05-15 12:08:32.684 | INFO     | __main__:train:123 - Epoch: [22][80/390]	 loss 4.04848	 cls_loss: 0.5340 cluster_loss: 1.0190 sup_con_loss: 0.6374 contrastive_loss: 4.5787 
2024-05-15 12:08:47.740 | INFO     | __main__:train:123 - Epoch: [22][100/390]	 loss 4.08676	 cls_loss: 0.6259 cluster_loss: 0.9475 sup_con_loss: 0.8046 contrastive_loss: 4.5695 
2024-05-15 12:09:03.031 | INFO     | __main__:train:123 - Epoch: [22][120/390]	 loss 4.10340	 cls_loss: 0.6874 cluster_loss: 0.9879 sup_con_loss: 0.7061 contrastive_loss: 4.5746 
2024-05-15 12:09:18.243 | INFO     | __main__:train:123 - Epoch: [22][140/390]	 loss 4.09684	 cls_loss: 0.5506 cluster_loss: 1.0114 sup_con_loss: 0.7755 contrastive_loss: 4.5774 
2024-05-15 12:09:33.125 | INFO     | __main__:train:123 - Epoch: [22][160/390]	 loss 4.10599	 cls_loss: 0.5932 cluster_loss: 1.0818 sup_con_loss: 0.6371 contrastive_loss: 4.5727 
2024-05-15 12:09:48.066 | INFO     | __main__:train:123 - Epoch: [22][180/390]	 loss 4.20132	 cls_loss: 0.6809 cluster_loss: 1.0242 sup_con_loss: 0.9289 contrastive_loss: 4.5725 
2024-05-15 12:10:03.017 | INFO     | __main__:train:123 - Epoch: [22][200/390]	 loss 4.08567	 cls_loss: 0.5344 cluster_loss: 1.0085 sup_con_loss: 0.7733 contrastive_loss: 4.5730 
2024-05-15 12:10:17.964 | INFO     | __main__:train:123 - Epoch: [22][220/390]	 loss 4.04169	 cls_loss: 0.6284 cluster_loss: 0.9115 sup_con_loss: 0.7420 contrastive_loss: 4.5685 
2024-05-15 12:10:33.121 | INFO     | __main__:train:123 - Epoch: [22][240/390]	 loss 4.05328	 cls_loss: 0.6000 cluster_loss: 0.9845 sup_con_loss: 0.6599 contrastive_loss: 4.5728 
2024-05-15 12:10:48.298 | INFO     | __main__:train:123 - Epoch: [22][260/390]	 loss 3.95255	 cls_loss: 0.4651 cluster_loss: 0.9059 sup_con_loss: 0.6448 contrastive_loss: 4.5772 
2024-05-15 12:11:03.191 | INFO     | __main__:train:123 - Epoch: [22][280/390]	 loss 4.21758	 cls_loss: 0.7262 cluster_loss: 1.0864 sup_con_loss: 0.7955 contrastive_loss: 4.5828 
2024-05-15 12:11:18.491 | INFO     | __main__:train:123 - Epoch: [22][300/390]	 loss 4.02435	 cls_loss: 0.5675 cluster_loss: 0.9871 sup_con_loss: 0.6009 contrastive_loss: 4.5751 
2024-05-15 12:11:33.614 | INFO     | __main__:train:123 - Epoch: [22][320/390]	 loss 4.00976	 cls_loss: 0.5935 cluster_loss: 0.8693 sup_con_loss: 0.7659 contrastive_loss: 4.5676 
2024-05-15 12:11:48.665 | INFO     | __main__:train:123 - Epoch: [22][340/390]	 loss 4.03002	 cls_loss: 0.5401 cluster_loss: 0.9551 sup_con_loss: 0.7093 contrastive_loss: 4.5722 
2024-05-15 12:12:03.622 | INFO     | __main__:train:123 - Epoch: [22][360/390]	 loss 4.03929	 cls_loss: 0.6046 cluster_loss: 1.0564 sup_con_loss: 0.4772 contrastive_loss: 4.5754 
2024-05-15 12:12:18.259 | INFO     | __main__:train:123 - Epoch: [22][380/390]	 loss 4.16147	 cls_loss: 0.5726 cluster_loss: 1.0810 sup_con_loss: 0.7991 contrastive_loss: 4.5827 
2024-05-15 12:12:24.784 | INFO     | __main__:train:126 - Train Epoch: 22 Avg Loss: 4.1041 
2024-05-15 12:12:24.785 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:12:53.246 | INFO     | __main__:train:135 - Train Accuracies: All 0.7875 | Old 0.8381 | New 0.5850
2024-05-15 12:12:58.128 | INFO     | __main__:train:123 - Epoch: [23][0/390]	 loss 4.12403	 cls_loss: 0.4567 cluster_loss: 1.0152 sup_con_loss: 0.9388 contrastive_loss: 4.5781 
2024-05-15 12:13:13.112 | INFO     | __main__:train:123 - Epoch: [23][20/390]	 loss 4.12231	 cls_loss: 0.5425 cluster_loss: 1.0609 sup_con_loss: 0.7689 contrastive_loss: 4.5750 
2024-05-15 12:13:28.119 | INFO     | __main__:train:123 - Epoch: [23][40/390]	 loss 4.15841	 cls_loss: 0.6239 cluster_loss: 1.0342 sup_con_loss: 0.8360 contrastive_loss: 4.5773 
2024-05-15 12:13:43.360 | INFO     | __main__:train:123 - Epoch: [23][60/390]	 loss 4.10808	 cls_loss: 0.5694 cluster_loss: 0.9499 sup_con_loss: 0.9125 contrastive_loss: 4.5722 
2024-05-15 12:13:58.173 | INFO     | __main__:train:123 - Epoch: [23][80/390]	 loss 4.10012	 cls_loss: 0.4180 cluster_loss: 0.9772 sup_con_loss: 0.9883 contrastive_loss: 4.5734 
2024-05-15 12:14:13.062 | INFO     | __main__:train:123 - Epoch: [23][100/390]	 loss 4.07137	 cls_loss: 0.4446 cluster_loss: 1.0419 sup_con_loss: 0.7528 contrastive_loss: 4.5769 
2024-05-15 12:14:27.926 | INFO     | __main__:train:123 - Epoch: [23][120/390]	 loss 3.96616	 cls_loss: 0.6052 cluster_loss: 0.9352 sup_con_loss: 0.5050 contrastive_loss: 4.5688 
2024-05-15 12:14:42.843 | INFO     | __main__:train:123 - Epoch: [23][140/390]	 loss 4.08374	 cls_loss: 0.5516 cluster_loss: 1.0178 sup_con_loss: 0.7296 contrastive_loss: 4.5751 
2024-05-15 12:14:57.497 | INFO     | __main__:train:123 - Epoch: [23][160/390]	 loss 4.08778	 cls_loss: 0.4910 cluster_loss: 0.9870 sup_con_loss: 0.8562 contrastive_loss: 4.5765 
2024-05-15 12:15:12.551 | INFO     | __main__:train:123 - Epoch: [23][180/390]	 loss 4.18764	 cls_loss: 0.5209 cluster_loss: 1.1290 sup_con_loss: 0.8400 contrastive_loss: 4.5808 
2024-05-15 12:15:27.384 | INFO     | __main__:train:123 - Epoch: [23][200/390]	 loss 4.02265	 cls_loss: 0.3937 cluster_loss: 1.0247 sup_con_loss: 0.6960 contrastive_loss: 4.5772 
2024-05-15 12:15:42.237 | INFO     | __main__:train:123 - Epoch: [23][220/390]	 loss 4.00430	 cls_loss: 0.5710 cluster_loss: 0.9017 sup_con_loss: 0.7012 contrastive_loss: 4.5737 
2024-05-15 12:15:57.484 | INFO     | __main__:train:123 - Epoch: [23][240/390]	 loss 4.11765	 cls_loss: 0.5108 cluster_loss: 1.0493 sup_con_loss: 0.8109 contrastive_loss: 4.5739 
2024-05-15 12:16:12.511 | INFO     | __main__:train:123 - Epoch: [23][260/390]	 loss 4.09473	 cls_loss: 0.5117 cluster_loss: 1.0334 sup_con_loss: 0.7710 contrastive_loss: 4.5755 
2024-05-15 12:16:27.548 | INFO     | __main__:train:123 - Epoch: [23][280/390]	 loss 4.16913	 cls_loss: 0.5955 cluster_loss: 1.1487 sup_con_loss: 0.6854 contrastive_loss: 4.5756 
2024-05-15 12:16:42.521 | INFO     | __main__:train:123 - Epoch: [23][300/390]	 loss 4.00449	 cls_loss: 0.4689 cluster_loss: 1.0041 sup_con_loss: 0.6066 contrastive_loss: 4.5776 
2024-05-15 12:16:57.542 | INFO     | __main__:train:123 - Epoch: [23][320/390]	 loss 4.04139	 cls_loss: 0.4061 cluster_loss: 0.9944 sup_con_loss: 0.8012 contrastive_loss: 4.5731 
2024-05-15 12:17:12.541 | INFO     | __main__:train:123 - Epoch: [23][340/390]	 loss 4.08553	 cls_loss: 0.5495 cluster_loss: 0.9975 sup_con_loss: 0.7819 contrastive_loss: 4.5710 
2024-05-15 12:17:27.323 | INFO     | __main__:train:123 - Epoch: [23][360/390]	 loss 4.03940	 cls_loss: 0.5561 cluster_loss: 1.0017 sup_con_loss: 0.6283 contrastive_loss: 4.5750 
2024-05-15 12:17:41.908 | INFO     | __main__:train:123 - Epoch: [23][380/390]	 loss 4.01345	 cls_loss: 0.4660 cluster_loss: 1.0158 sup_con_loss: 0.6180 contrastive_loss: 4.5751 
2024-05-15 12:17:48.301 | INFO     | __main__:train:126 - Train Epoch: 23 Avg Loss: 4.0880 
2024-05-15 12:17:48.302 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:18:16.804 | INFO     | __main__:train:135 - Train Accuracies: All 0.7896 | Old 0.8405 | New 0.5860
2024-05-15 12:18:20.331 | INFO     | __main__:train:123 - Epoch: [24][0/390]	 loss 4.01733	 cls_loss: 0.4841 cluster_loss: 0.9793 sup_con_loss: 0.6802 contrastive_loss: 4.5743 
2024-05-15 12:18:36.093 | INFO     | __main__:train:123 - Epoch: [24][20/390]	 loss 4.07851	 cls_loss: 0.5898 cluster_loss: 0.9845 sup_con_loss: 0.7422 contrastive_loss: 4.5729 
2024-05-15 12:18:51.205 | INFO     | __main__:train:123 - Epoch: [24][40/390]	 loss 4.01303	 cls_loss: 0.5379 cluster_loss: 0.9759 sup_con_loss: 0.6106 contrastive_loss: 4.5795 
2024-05-15 12:19:06.524 | INFO     | __main__:train:123 - Epoch: [24][60/390]	 loss 3.98440	 cls_loss: 0.4672 cluster_loss: 0.9311 sup_con_loss: 0.6810 contrastive_loss: 4.5805 
2024-05-15 12:19:21.329 | INFO     | __main__:train:123 - Epoch: [24][80/390]	 loss 4.05727	 cls_loss: 0.4692 cluster_loss: 1.0089 sup_con_loss: 0.7526 contrastive_loss: 4.5752 
2024-05-15 12:19:36.314 | INFO     | __main__:train:123 - Epoch: [24][100/390]	 loss 4.15129	 cls_loss: 0.5815 cluster_loss: 1.0566 sup_con_loss: 0.8181 contrastive_loss: 4.5764 
2024-05-15 12:19:51.143 | INFO     | __main__:train:123 - Epoch: [24][120/390]	 loss 4.01322	 cls_loss: 0.5697 cluster_loss: 0.9218 sup_con_loss: 0.6888 contrastive_loss: 4.5747 
2024-05-15 12:20:06.184 | INFO     | __main__:train:123 - Epoch: [24][140/390]	 loss 4.03354	 cls_loss: 0.5767 cluster_loss: 0.9206 sup_con_loss: 0.7458 contrastive_loss: 4.5727 
2024-05-15 12:20:21.369 | INFO     | __main__:train:123 - Epoch: [24][160/390]	 loss 4.16938	 cls_loss: 0.6599 cluster_loss: 0.9902 sup_con_loss: 0.9194 contrastive_loss: 4.5739 
2024-05-15 12:20:36.524 | INFO     | __main__:train:123 - Epoch: [24][180/390]	 loss 4.18005	 cls_loss: 0.5035 cluster_loss: 1.0313 sup_con_loss: 1.0313 contrastive_loss: 4.5731 
2024-05-15 12:20:51.460 | INFO     | __main__:train:123 - Epoch: [24][200/390]	 loss 4.13433	 cls_loss: 0.5878 cluster_loss: 1.0256 sup_con_loss: 0.8132 contrastive_loss: 4.5806 
2024-05-15 12:21:06.462 | INFO     | __main__:train:123 - Epoch: [24][220/390]	 loss 4.00316	 cls_loss: 0.5249 cluster_loss: 0.9837 sup_con_loss: 0.5918 contrastive_loss: 4.5737 
2024-05-15 12:21:21.566 | INFO     | __main__:train:123 - Epoch: [24][240/390]	 loss 4.04884	 cls_loss: 0.4574 cluster_loss: 1.0270 sup_con_loss: 0.7078 contrastive_loss: 4.5745 
2024-05-15 12:21:36.430 | INFO     | __main__:train:123 - Epoch: [24][260/390]	 loss 4.08672	 cls_loss: 0.5322 cluster_loss: 1.0561 sup_con_loss: 0.6862 contrastive_loss: 4.5751 
2024-05-15 12:21:51.482 | INFO     | __main__:train:123 - Epoch: [24][280/390]	 loss 4.00395	 cls_loss: 0.4260 cluster_loss: 0.9390 sup_con_loss: 0.7772 contrastive_loss: 4.5731 
2024-05-15 12:22:06.557 | INFO     | __main__:train:123 - Epoch: [24][300/390]	 loss 4.12111	 cls_loss: 0.5765 cluster_loss: 1.0417 sup_con_loss: 0.7671 contrastive_loss: 4.5750 
2024-05-15 12:22:21.260 | INFO     | __main__:train:123 - Epoch: [24][320/390]	 loss 4.21583	 cls_loss: 0.7161 cluster_loss: 1.0357 sup_con_loss: 0.9127 contrastive_loss: 4.5732 
2024-05-15 12:22:36.142 | INFO     | __main__:train:123 - Epoch: [24][340/390]	 loss 4.00413	 cls_loss: 0.4795 cluster_loss: 0.9751 sup_con_loss: 0.6498 contrastive_loss: 4.5770 
2024-05-15 12:22:51.117 | INFO     | __main__:train:123 - Epoch: [24][360/390]	 loss 3.97714	 cls_loss: 0.4680 cluster_loss: 0.9697 sup_con_loss: 0.6055 contrastive_loss: 4.5709 
2024-05-15 12:23:05.636 | INFO     | __main__:train:123 - Epoch: [24][380/390]	 loss 4.09393	 cls_loss: 0.4292 cluster_loss: 1.0062 sup_con_loss: 0.9029 contrastive_loss: 4.5749 
2024-05-15 12:23:12.087 | INFO     | __main__:train:126 - Train Epoch: 24 Avg Loss: 4.0733 
2024-05-15 12:23:12.088 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:23:40.961 | INFO     | __main__:train:135 - Train Accuracies: All 0.7931 | Old 0.8425 | New 0.5955
2024-05-15 12:23:46.031 | INFO     | __main__:train:123 - Epoch: [25][0/390]	 loss 4.12322	 cls_loss: 0.5068 cluster_loss: 0.9424 sup_con_loss: 1.0257 contrastive_loss: 4.5759 
2024-05-15 12:24:00.927 | INFO     | __main__:train:123 - Epoch: [25][20/390]	 loss 4.10074	 cls_loss: 0.5664 cluster_loss: 1.0371 sup_con_loss: 0.7122 contrastive_loss: 4.5832 
2024-05-15 12:24:15.989 | INFO     | __main__:train:123 - Epoch: [25][40/390]	 loss 4.25199	 cls_loss: 0.6814 cluster_loss: 1.0170 sup_con_loss: 1.0759 contrastive_loss: 4.5783 
2024-05-15 12:24:31.118 | INFO     | __main__:train:123 - Epoch: [25][60/390]	 loss 4.15551	 cls_loss: 0.6369 cluster_loss: 1.0216 sup_con_loss: 0.8382 contrastive_loss: 4.5772 
2024-05-15 12:24:46.099 | INFO     | __main__:train:123 - Epoch: [25][80/390]	 loss 4.04347	 cls_loss: 0.4952 cluster_loss: 0.9769 sup_con_loss: 0.7421 contrastive_loss: 4.5776 
2024-05-15 12:25:00.838 | INFO     | __main__:train:123 - Epoch: [25][100/390]	 loss 4.06439	 cls_loss: 0.4881 cluster_loss: 1.0372 sup_con_loss: 0.6973 contrastive_loss: 4.5773 
2024-05-15 12:25:16.218 | INFO     | __main__:train:123 - Epoch: [25][120/390]	 loss 4.16177	 cls_loss: 0.4933 cluster_loss: 1.1153 sup_con_loss: 0.8240 contrastive_loss: 4.5782 
2024-05-15 12:25:31.199 | INFO     | __main__:train:123 - Epoch: [25][140/390]	 loss 4.09542	 cls_loss: 0.5710 cluster_loss: 0.9844 sup_con_loss: 0.8042 contrastive_loss: 4.5757 
2024-05-15 12:25:46.042 | INFO     | __main__:train:123 - Epoch: [25][160/390]	 loss 4.14166	 cls_loss: 0.5852 cluster_loss: 1.0265 sup_con_loss: 0.8490 contrastive_loss: 4.5730 
2024-05-15 12:26:01.282 | INFO     | __main__:train:123 - Epoch: [25][180/390]	 loss 3.97284	 cls_loss: 0.4838 cluster_loss: 0.9472 sup_con_loss: 0.6071 contrastive_loss: 4.5774 
2024-05-15 12:26:16.403 | INFO     | __main__:train:123 - Epoch: [25][200/390]	 loss 3.99892	 cls_loss: 0.5738 cluster_loss: 0.9149 sup_con_loss: 0.6562 contrastive_loss: 4.5750 
2024-05-15 12:26:31.600 | INFO     | __main__:train:123 - Epoch: [25][220/390]	 loss 4.05413	 cls_loss: 0.4601 cluster_loss: 0.9483 sup_con_loss: 0.8699 contrastive_loss: 4.5727 
2024-05-15 12:26:46.496 | INFO     | __main__:train:123 - Epoch: [25][240/390]	 loss 4.05809	 cls_loss: 0.5811 cluster_loss: 0.9846 sup_con_loss: 0.6897 contrastive_loss: 4.5744 
2024-05-15 12:27:01.472 | INFO     | __main__:train:123 - Epoch: [25][260/390]	 loss 4.06847	 cls_loss: 0.4850 cluster_loss: 1.0221 sup_con_loss: 0.7398 contrastive_loss: 4.5776 
2024-05-15 12:27:16.513 | INFO     | __main__:train:123 - Epoch: [25][280/390]	 loss 4.25332	 cls_loss: 0.7153 cluster_loss: 1.1009 sup_con_loss: 0.8934 contrastive_loss: 4.5765 
2024-05-15 12:27:31.630 | INFO     | __main__:train:123 - Epoch: [25][300/390]	 loss 4.10670	 cls_loss: 0.6714 cluster_loss: 0.9949 sup_con_loss: 0.7263 contrastive_loss: 4.5705 
2024-05-15 12:27:46.468 | INFO     | __main__:train:123 - Epoch: [25][320/390]	 loss 3.95866	 cls_loss: 0.3633 cluster_loss: 0.9242 sup_con_loss: 0.7448 contrastive_loss: 4.5694 
2024-05-15 12:28:01.594 | INFO     | __main__:train:123 - Epoch: [25][340/390]	 loss 4.09620	 cls_loss: 0.5391 cluster_loss: 0.9439 sup_con_loss: 0.9210 contrastive_loss: 4.5717 
2024-05-15 12:28:16.579 | INFO     | __main__:train:123 - Epoch: [25][360/390]	 loss 4.16236	 cls_loss: 0.5787 cluster_loss: 1.0069 sup_con_loss: 0.9559 contrastive_loss: 4.5703 
2024-05-15 12:28:31.155 | INFO     | __main__:train:123 - Epoch: [25][380/390]	 loss 3.94841	 cls_loss: 0.6148 cluster_loss: 0.9005 sup_con_loss: 0.5079 contrastive_loss: 4.5695 
2024-05-15 12:28:37.565 | INFO     | __main__:train:126 - Train Epoch: 25 Avg Loss: 4.0737 
2024-05-15 12:28:37.566 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:29:06.087 | INFO     | __main__:train:135 - Train Accuracies: All 0.7928 | Old 0.8394 | New 0.6065
2024-05-15 12:29:09.477 | INFO     | __main__:train:123 - Epoch: [26][0/390]	 loss 4.09087	 cls_loss: 0.4567 cluster_loss: 0.9820 sup_con_loss: 0.9131 contrastive_loss: 4.5741 
2024-05-15 12:29:24.603 | INFO     | __main__:train:123 - Epoch: [26][20/390]	 loss 3.93525	 cls_loss: 0.4768 cluster_loss: 0.9433 sup_con_loss: 0.5148 contrastive_loss: 4.5770 
2024-05-15 12:29:39.823 | INFO     | __main__:train:123 - Epoch: [26][40/390]	 loss 4.05362	 cls_loss: 0.5388 cluster_loss: 1.0481 sup_con_loss: 0.5733 contrastive_loss: 4.5894 
2024-05-15 12:29:55.105 | INFO     | __main__:train:123 - Epoch: [26][60/390]	 loss 4.04963	 cls_loss: 0.5108 cluster_loss: 0.9225 sup_con_loss: 0.8639 contrastive_loss: 4.5675 
2024-05-15 12:30:10.241 | INFO     | __main__:train:123 - Epoch: [26][80/390]	 loss 4.04902	 cls_loss: 0.5299 cluster_loss: 0.9002 sup_con_loss: 0.8832 contrastive_loss: 4.5682 
2024-05-15 12:30:24.873 | INFO     | __main__:train:123 - Epoch: [26][100/390]	 loss 4.21769	 cls_loss: 0.5604 cluster_loss: 1.1243 sup_con_loss: 0.9067 contrastive_loss: 4.5744 
2024-05-15 12:30:39.874 | INFO     | __main__:train:123 - Epoch: [26][120/390]	 loss 4.05905	 cls_loss: 0.5343 cluster_loss: 0.9219 sup_con_loss: 0.8575 contrastive_loss: 4.5734 
2024-05-15 12:30:55.039 | INFO     | __main__:train:123 - Epoch: [26][140/390]	 loss 4.05361	 cls_loss: 0.5608 cluster_loss: 0.9172 sup_con_loss: 0.8274 contrastive_loss: 4.5717 
2024-05-15 12:31:10.073 | INFO     | __main__:train:123 - Epoch: [26][160/390]	 loss 4.07537	 cls_loss: 0.5260 cluster_loss: 1.0371 sup_con_loss: 0.6928 contrastive_loss: 4.5764 
2024-05-15 12:31:25.121 | INFO     | __main__:train:123 - Epoch: [26][180/390]	 loss 4.16466	 cls_loss: 0.5726 cluster_loss: 1.0934 sup_con_loss: 0.8013 contrastive_loss: 4.5739 
2024-05-15 12:31:40.057 | INFO     | __main__:train:123 - Epoch: [26][200/390]	 loss 4.03403	 cls_loss: 0.4978 cluster_loss: 0.9508 sup_con_loss: 0.7655 contrastive_loss: 4.5752 
2024-05-15 12:31:54.492 | INFO     | __main__:train:123 - Epoch: [26][220/390]	 loss 4.08676	 cls_loss: 0.4452 cluster_loss: 0.9838 sup_con_loss: 0.9094 contrastive_loss: 4.5741 
2024-05-15 12:32:09.356 | INFO     | __main__:train:123 - Epoch: [26][240/390]	 loss 4.06673	 cls_loss: 0.5174 cluster_loss: 1.0471 sup_con_loss: 0.6573 contrastive_loss: 4.5769 
2024-05-15 12:32:24.215 | INFO     | __main__:train:123 - Epoch: [26][260/390]	 loss 3.99520	 cls_loss: 0.4863 cluster_loss: 0.9483 sup_con_loss: 0.6683 contrastive_loss: 4.5765 
2024-05-15 12:32:39.273 | INFO     | __main__:train:123 - Epoch: [26][280/390]	 loss 4.01650	 cls_loss: 0.6274 cluster_loss: 0.9200 sup_con_loss: 0.6591 contrastive_loss: 4.5666 
2024-05-15 12:32:54.146 | INFO     | __main__:train:123 - Epoch: [26][300/390]	 loss 4.15037	 cls_loss: 0.4523 cluster_loss: 1.0443 sup_con_loss: 0.9665 contrastive_loss: 4.5770 
2024-05-15 12:33:09.122 | INFO     | __main__:train:123 - Epoch: [26][320/390]	 loss 4.36947	 cls_loss: 0.7785 cluster_loss: 1.1859 sup_con_loss: 0.9893 contrastive_loss: 4.5845 
2024-05-15 12:33:23.862 | INFO     | __main__:train:123 - Epoch: [26][340/390]	 loss 4.18682	 cls_loss: 0.4265 cluster_loss: 1.0605 sup_con_loss: 1.0534 contrastive_loss: 4.5839 
2024-05-15 12:33:38.637 | INFO     | __main__:train:123 - Epoch: [26][360/390]	 loss 4.05555	 cls_loss: 0.5659 cluster_loss: 0.9619 sup_con_loss: 0.7390 contrastive_loss: 4.5748 
2024-05-15 12:33:53.088 | INFO     | __main__:train:123 - Epoch: [26][380/390]	 loss 4.02960	 cls_loss: 0.4853 cluster_loss: 0.9391 sup_con_loss: 0.7930 contrastive_loss: 4.5719 
2024-05-15 12:33:59.457 | INFO     | __main__:train:126 - Train Epoch: 26 Avg Loss: 4.0558 
2024-05-15 12:33:59.457 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:34:27.667 | INFO     | __main__:train:135 - Train Accuracies: All 0.7901 | Old 0.8403 | New 0.5895
2024-05-15 12:34:32.078 | INFO     | __main__:train:123 - Epoch: [27][0/390]	 loss 4.05610	 cls_loss: 0.5291 cluster_loss: 0.9725 sup_con_loss: 0.7490 contrastive_loss: 4.5794 
2024-05-15 12:34:47.126 | INFO     | __main__:train:123 - Epoch: [27][20/390]	 loss 4.06969	 cls_loss: 0.5925 cluster_loss: 0.9868 sup_con_loss: 0.7009 contrastive_loss: 4.5778 
2024-05-15 12:35:02.353 | INFO     | __main__:train:123 - Epoch: [27][40/390]	 loss 4.11038	 cls_loss: 0.5408 cluster_loss: 0.9985 sup_con_loss: 0.8473 contrastive_loss: 4.5777 
2024-05-15 12:35:17.338 | INFO     | __main__:train:123 - Epoch: [27][60/390]	 loss 4.00099	 cls_loss: 0.4097 cluster_loss: 0.8995 sup_con_loss: 0.8642 contrastive_loss: 4.5700 
2024-05-15 12:35:32.383 | INFO     | __main__:train:123 - Epoch: [27][80/390]	 loss 4.09366	 cls_loss: 0.5656 cluster_loss: 0.9745 sup_con_loss: 0.8195 contrastive_loss: 4.5776 
2024-05-15 12:35:47.348 | INFO     | __main__:train:123 - Epoch: [27][100/390]	 loss 4.09187	 cls_loss: 0.5075 cluster_loss: 1.0254 sup_con_loss: 0.7703 contrastive_loss: 4.5817 
2024-05-15 12:36:02.731 | INFO     | __main__:train:123 - Epoch: [27][120/390]	 loss 4.08720	 cls_loss: 0.5631 cluster_loss: 1.0422 sup_con_loss: 0.6655 contrastive_loss: 4.5843 
2024-05-15 12:36:18.050 | INFO     | __main__:train:123 - Epoch: [27][140/390]	 loss 4.17942	 cls_loss: 0.4957 cluster_loss: 1.0394 sup_con_loss: 1.0184 contrastive_loss: 4.5751 
2024-05-15 12:36:33.194 | INFO     | __main__:train:123 - Epoch: [27][160/390]	 loss 4.01701	 cls_loss: 0.4001 cluster_loss: 1.0277 sup_con_loss: 0.6742 contrastive_loss: 4.5738 
2024-05-15 12:36:48.307 | INFO     | __main__:train:123 - Epoch: [27][180/390]	 loss 4.01591	 cls_loss: 0.4633 cluster_loss: 0.8814 sup_con_loss: 0.8797 contrastive_loss: 4.5738 
2024-05-15 12:37:03.089 | INFO     | __main__:train:123 - Epoch: [27][200/390]	 loss 4.04878	 cls_loss: 0.4840 cluster_loss: 0.9089 sup_con_loss: 0.9053 contrastive_loss: 4.5719 
2024-05-15 12:37:18.214 | INFO     | __main__:train:123 - Epoch: [27][220/390]	 loss 4.02081	 cls_loss: 0.5357 cluster_loss: 0.9869 sup_con_loss: 0.6334 contrastive_loss: 4.5695 
2024-05-15 12:37:33.043 | INFO     | __main__:train:123 - Epoch: [27][240/390]	 loss 3.94140	 cls_loss: 0.5439 cluster_loss: 0.8667 sup_con_loss: 0.6319 contrastive_loss: 4.5638 
2024-05-15 12:37:48.246 | INFO     | __main__:train:123 - Epoch: [27][260/390]	 loss 4.10387	 cls_loss: 0.5802 cluster_loss: 0.9708 sup_con_loss: 0.8469 contrastive_loss: 4.5744 
2024-05-15 12:38:03.229 | INFO     | __main__:train:123 - Epoch: [27][280/390]	 loss 4.06474	 cls_loss: 0.4336 cluster_loss: 0.9661 sup_con_loss: 0.8846 contrastive_loss: 4.5775 
2024-05-15 12:38:18.296 | INFO     | __main__:train:123 - Epoch: [27][300/390]	 loss 4.05664	 cls_loss: 0.4579 cluster_loss: 0.9970 sup_con_loss: 0.7809 contrastive_loss: 4.5769 
2024-05-15 12:38:33.391 | INFO     | __main__:train:123 - Epoch: [27][320/390]	 loss 4.18972	 cls_loss: 0.5948 cluster_loss: 1.0792 sup_con_loss: 0.8548 contrastive_loss: 4.5860 
2024-05-15 12:38:48.341 | INFO     | __main__:train:123 - Epoch: [27][340/390]	 loss 4.01239	 cls_loss: 0.3778 cluster_loss: 0.9375 sup_con_loss: 0.8359 contrastive_loss: 4.5819 
2024-05-15 12:39:03.053 | INFO     | __main__:train:123 - Epoch: [27][360/390]	 loss 4.04741	 cls_loss: 0.5239 cluster_loss: 0.9994 sup_con_loss: 0.6809 contrastive_loss: 4.5787 
2024-05-15 12:39:17.765 | INFO     | __main__:train:123 - Epoch: [27][380/390]	 loss 3.98741	 cls_loss: 0.4327 cluster_loss: 0.9393 sup_con_loss: 0.7170 contrastive_loss: 4.5761 
2024-05-15 12:39:24.275 | INFO     | __main__:train:126 - Train Epoch: 27 Avg Loss: 4.0477 
2024-05-15 12:39:24.275 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:39:53.161 | INFO     | __main__:train:135 - Train Accuracies: All 0.7917 | Old 0.8433 | New 0.5855
2024-05-15 12:39:57.607 | INFO     | __main__:train:123 - Epoch: [28][0/390]	 loss 4.02493	 cls_loss: 0.4868 cluster_loss: 0.9912 sup_con_loss: 0.6691 contrastive_loss: 4.5786 
2024-05-15 12:40:12.121 | INFO     | __main__:train:123 - Epoch: [28][20/390]	 loss 4.03639	 cls_loss: 0.4174 cluster_loss: 0.9783 sup_con_loss: 0.8073 contrastive_loss: 4.5721 
2024-05-15 12:40:27.287 | INFO     | __main__:train:123 - Epoch: [28][40/390]	 loss 4.05347	 cls_loss: 0.4195 cluster_loss: 0.9897 sup_con_loss: 0.8132 contrastive_loss: 4.5827 
2024-05-15 12:40:42.252 | INFO     | __main__:train:123 - Epoch: [28][60/390]	 loss 4.00846	 cls_loss: 0.5824 cluster_loss: 0.8964 sup_con_loss: 0.7163 contrastive_loss: 4.5712 
2024-05-15 12:40:56.961 | INFO     | __main__:train:123 - Epoch: [28][80/390]	 loss 4.14685	 cls_loss: 0.7009 cluster_loss: 1.0443 sup_con_loss: 0.7074 contrastive_loss: 4.5771 
2024-05-15 12:41:12.108 | INFO     | __main__:train:123 - Epoch: [28][100/390]	 loss 3.91660	 cls_loss: 0.4083 cluster_loss: 0.9023 sup_con_loss: 0.6219 contrastive_loss: 4.5685 
2024-05-15 12:41:26.632 | INFO     | __main__:train:123 - Epoch: [28][120/390]	 loss 4.15509	 cls_loss: 0.5585 cluster_loss: 1.0021 sup_con_loss: 0.9612 contrastive_loss: 4.5721 
2024-05-15 12:41:41.376 | INFO     | __main__:train:123 - Epoch: [28][140/390]	 loss 3.95216	 cls_loss: 0.4590 cluster_loss: 0.8689 sup_con_loss: 0.7269 contrastive_loss: 4.5728 
2024-05-15 12:41:56.395 | INFO     | __main__:train:123 - Epoch: [28][160/390]	 loss 4.10972	 cls_loss: 0.4070 cluster_loss: 1.0318 sup_con_loss: 0.9193 contrastive_loss: 4.5766 
2024-05-15 12:42:11.256 | INFO     | __main__:train:123 - Epoch: [28][180/390]	 loss 4.21397	 cls_loss: 0.6411 cluster_loss: 1.0277 sup_con_loss: 0.9984 contrastive_loss: 4.5725 
2024-05-15 12:42:26.074 | INFO     | __main__:train:123 - Epoch: [28][200/390]	 loss 4.06480	 cls_loss: 0.5732 cluster_loss: 1.0099 sup_con_loss: 0.6699 contrastive_loss: 4.5742 
2024-05-15 12:42:41.058 | INFO     | __main__:train:123 - Epoch: [28][220/390]	 loss 4.09532	 cls_loss: 0.6224 cluster_loss: 0.9783 sup_con_loss: 0.7698 contrastive_loss: 4.5725 
2024-05-15 12:42:56.145 | INFO     | __main__:train:123 - Epoch: [28][240/390]	 loss 3.93593	 cls_loss: 0.4765 cluster_loss: 0.8325 sup_con_loss: 0.7305 contrastive_loss: 4.5729 
2024-05-15 12:43:10.935 | INFO     | __main__:train:123 - Epoch: [28][260/390]	 loss 4.06518	 cls_loss: 0.6201 cluster_loss: 0.9406 sup_con_loss: 0.7518 contrastive_loss: 4.5748 
2024-05-15 12:43:25.713 | INFO     | __main__:train:123 - Epoch: [28][280/390]	 loss 3.96726	 cls_loss: 0.4224 cluster_loss: 0.9132 sup_con_loss: 0.7225 contrastive_loss: 4.5738 
2024-05-15 12:43:40.971 | INFO     | __main__:train:123 - Epoch: [28][300/390]	 loss 4.17464	 cls_loss: 0.5616 cluster_loss: 1.0144 sup_con_loss: 0.9901 contrastive_loss: 4.5726 
2024-05-15 12:43:55.885 | INFO     | __main__:train:123 - Epoch: [28][320/390]	 loss 4.06195	 cls_loss: 0.5597 cluster_loss: 0.9585 sup_con_loss: 0.7733 contrastive_loss: 4.5729 
2024-05-15 12:44:11.014 | INFO     | __main__:train:123 - Epoch: [28][340/390]	 loss 4.04853	 cls_loss: 0.5225 cluster_loss: 1.0269 sup_con_loss: 0.6260 contrastive_loss: 4.5832 
2024-05-15 12:44:25.926 | INFO     | __main__:train:123 - Epoch: [28][360/390]	 loss 3.93088	 cls_loss: 0.4418 cluster_loss: 0.8707 sup_con_loss: 0.6754 contrastive_loss: 4.5752 
2024-05-15 12:44:40.474 | INFO     | __main__:train:123 - Epoch: [28][380/390]	 loss 4.02850	 cls_loss: 0.5298 cluster_loss: 0.9261 sup_con_loss: 0.7625 contrastive_loss: 4.5758 
2024-05-15 12:44:46.779 | INFO     | __main__:train:126 - Train Epoch: 28 Avg Loss: 4.0435 
2024-05-15 12:44:46.780 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:45:15.420 | INFO     | __main__:train:135 - Train Accuracies: All 0.7922 | Old 0.8445 | New 0.5830
2024-05-15 12:45:18.937 | INFO     | __main__:train:123 - Epoch: [29][0/390]	 loss 3.87491	 cls_loss: 0.3259 cluster_loss: 0.8419 sup_con_loss: 0.6887 contrastive_loss: 4.5731 
2024-05-15 12:45:33.995 | INFO     | __main__:train:123 - Epoch: [29][20/390]	 loss 3.97875	 cls_loss: 0.6092 cluster_loss: 0.8848 sup_con_loss: 0.6263 contrastive_loss: 4.5711 
2024-05-15 12:45:49.148 | INFO     | __main__:train:123 - Epoch: [29][40/390]	 loss 4.03722	 cls_loss: 0.4818 cluster_loss: 1.0213 sup_con_loss: 0.6614 contrastive_loss: 4.5742 
2024-05-15 12:46:04.172 | INFO     | __main__:train:123 - Epoch: [29][60/390]	 loss 4.07172	 cls_loss: 0.5203 cluster_loss: 0.9386 sup_con_loss: 0.8709 contrastive_loss: 4.5765 
2024-05-15 12:46:19.411 | INFO     | __main__:train:123 - Epoch: [29][80/390]	 loss 3.96678	 cls_loss: 0.4031 cluster_loss: 0.9217 sup_con_loss: 0.7278 contrastive_loss: 4.5721 
2024-05-15 12:46:34.378 | INFO     | __main__:train:123 - Epoch: [29][100/390]	 loss 4.24757	 cls_loss: 0.4325 cluster_loss: 1.1389 sup_con_loss: 1.0721 contrastive_loss: 4.5857 
2024-05-15 12:46:49.454 | INFO     | __main__:train:123 - Epoch: [29][120/390]	 loss 4.02111	 cls_loss: 0.4908 cluster_loss: 0.9215 sup_con_loss: 0.7870 contrastive_loss: 4.5768 
2024-05-15 12:47:04.451 | INFO     | __main__:train:123 - Epoch: [29][140/390]	 loss 4.07629	 cls_loss: 0.4829 cluster_loss: 0.9989 sup_con_loss: 0.7996 contrastive_loss: 4.5817 
2024-05-15 12:47:19.603 | INFO     | __main__:train:123 - Epoch: [29][160/390]	 loss 4.01096	 cls_loss: 0.4405 cluster_loss: 0.9964 sup_con_loss: 0.6609 contrastive_loss: 4.5812 
2024-05-15 12:47:34.657 | INFO     | __main__:train:123 - Epoch: [29][180/390]	 loss 4.16002	 cls_loss: 0.5933 cluster_loss: 1.0511 sup_con_loss: 0.8349 contrastive_loss: 4.5799 
2024-05-15 12:47:49.617 | INFO     | __main__:train:123 - Epoch: [29][200/390]	 loss 3.89977	 cls_loss: 0.4251 cluster_loss: 0.8165 sup_con_loss: 0.7214 contrastive_loss: 4.5658 
2024-05-15 12:48:04.647 | INFO     | __main__:train:123 - Epoch: [29][220/390]	 loss 4.10128	 cls_loss: 0.6413 cluster_loss: 1.0211 sup_con_loss: 0.6859 contrastive_loss: 4.5739 
2024-05-15 12:48:19.797 | INFO     | __main__:train:123 - Epoch: [29][240/390]	 loss 4.05303	 cls_loss: 0.4689 cluster_loss: 0.9900 sup_con_loss: 0.7803 contrastive_loss: 4.5727 
2024-05-15 12:48:34.674 | INFO     | __main__:train:123 - Epoch: [29][260/390]	 loss 4.03021	 cls_loss: 0.4944 cluster_loss: 0.9372 sup_con_loss: 0.7832 contrastive_loss: 4.5752 
2024-05-15 12:48:49.552 | INFO     | __main__:train:123 - Epoch: [29][280/390]	 loss 3.97560	 cls_loss: 0.5101 cluster_loss: 1.0190 sup_con_loss: 0.4568 contrastive_loss: 4.5766 
2024-05-15 12:49:04.617 | INFO     | __main__:train:123 - Epoch: [29][300/390]	 loss 4.01902	 cls_loss: 0.4991 cluster_loss: 0.9538 sup_con_loss: 0.7206 contrastive_loss: 4.5725 
2024-05-15 12:49:19.858 | INFO     | __main__:train:123 - Epoch: [29][320/390]	 loss 4.03745	 cls_loss: 0.4581 cluster_loss: 0.9857 sup_con_loss: 0.7427 contrastive_loss: 4.5792 
2024-05-15 12:49:34.754 | INFO     | __main__:train:123 - Epoch: [29][340/390]	 loss 4.12954	 cls_loss: 0.4902 cluster_loss: 0.9738 sup_con_loss: 1.0077 contrastive_loss: 4.5728 
2024-05-15 12:49:49.708 | INFO     | __main__:train:123 - Epoch: [29][360/390]	 loss 3.99744	 cls_loss: 0.5640 cluster_loss: 0.9471 sup_con_loss: 0.6054 contrastive_loss: 4.5731 
2024-05-15 12:50:04.246 | INFO     | __main__:train:123 - Epoch: [29][380/390]	 loss 4.03221	 cls_loss: 0.5199 cluster_loss: 0.9807 sup_con_loss: 0.6837 contrastive_loss: 4.5747 
2024-05-15 12:50:10.452 | INFO     | __main__:train:126 - Train Epoch: 29 Avg Loss: 4.0255 
2024-05-15 12:50:10.452 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:50:38.765 | INFO     | __main__:train:135 - Train Accuracies: All 0.7988 | Old 0.8460 | New 0.6100
2024-05-15 12:50:43.081 | INFO     | __main__:train:123 - Epoch: [30][0/390]	 loss 3.93683	 cls_loss: 0.4841 cluster_loss: 0.8480 sup_con_loss: 0.6978 contrastive_loss: 4.5722 
2024-05-15 12:50:57.772 | INFO     | __main__:train:123 - Epoch: [30][20/390]	 loss 4.04016	 cls_loss: 0.3933 cluster_loss: 0.9928 sup_con_loss: 0.8113 contrastive_loss: 4.5742 
2024-05-15 12:51:12.566 | INFO     | __main__:train:123 - Epoch: [30][40/390]	 loss 3.92717	 cls_loss: 0.3385 cluster_loss: 0.9504 sup_con_loss: 0.6230 contrastive_loss: 4.5737 
2024-05-15 12:51:27.598 | INFO     | __main__:train:123 - Epoch: [30][60/390]	 loss 4.12794	 cls_loss: 0.4837 cluster_loss: 1.0953 sup_con_loss: 0.7784 contrastive_loss: 4.5758 
2024-05-15 12:51:42.718 | INFO     | __main__:train:123 - Epoch: [30][80/390]	 loss 4.18600	 cls_loss: 0.5683 cluster_loss: 1.0207 sup_con_loss: 0.9895 contrastive_loss: 4.5805 
2024-05-15 12:51:57.543 | INFO     | __main__:train:123 - Epoch: [30][100/390]	 loss 3.90440	 cls_loss: 0.3913 cluster_loss: 0.8927 sup_con_loss: 0.6159 contrastive_loss: 4.5717 
2024-05-15 12:52:12.483 | INFO     | __main__:train:123 - Epoch: [30][120/390]	 loss 4.04549	 cls_loss: 0.4425 cluster_loss: 0.9450 sup_con_loss: 0.8690 contrastive_loss: 4.5727 
2024-05-15 12:52:27.448 | INFO     | __main__:train:123 - Epoch: [30][140/390]	 loss 4.00552	 cls_loss: 0.4482 cluster_loss: 0.9173 sup_con_loss: 0.7900 contrastive_loss: 4.5784 
2024-05-15 12:52:42.291 | INFO     | __main__:train:123 - Epoch: [30][160/390]	 loss 4.04875	 cls_loss: 0.4618 cluster_loss: 0.9593 sup_con_loss: 0.8348 contrastive_loss: 4.5714 
2024-05-15 12:52:57.089 | INFO     | __main__:train:123 - Epoch: [30][180/390]	 loss 4.07239	 cls_loss: 0.4777 cluster_loss: 0.9988 sup_con_loss: 0.8079 contrastive_loss: 4.5742 
2024-05-15 12:53:11.946 | INFO     | __main__:train:123 - Epoch: [30][200/390]	 loss 4.10198	 cls_loss: 0.4995 cluster_loss: 0.9990 sup_con_loss: 0.8647 contrastive_loss: 4.5771 
2024-05-15 12:53:26.773 | INFO     | __main__:train:123 - Epoch: [30][220/390]	 loss 3.85837	 cls_loss: 0.4039 cluster_loss: 0.8828 sup_con_loss: 0.4863 contrastive_loss: 4.5739 
2024-05-15 12:53:41.940 | INFO     | __main__:train:123 - Epoch: [30][240/390]	 loss 4.07869	 cls_loss: 0.6399 cluster_loss: 0.9469 sup_con_loss: 0.7680 contrastive_loss: 4.5699 
2024-05-15 12:53:56.880 | INFO     | __main__:train:123 - Epoch: [30][260/390]	 loss 4.05943	 cls_loss: 0.4806 cluster_loss: 1.0127 sup_con_loss: 0.7305 contrastive_loss: 4.5804 
2024-05-15 12:54:11.797 | INFO     | __main__:train:123 - Epoch: [30][280/390]	 loss 3.89064	 cls_loss: 0.4602 cluster_loss: 0.8235 sup_con_loss: 0.6436 contrastive_loss: 4.5677 
2024-05-15 12:54:26.753 | INFO     | __main__:train:123 - Epoch: [30][300/390]	 loss 4.12499	 cls_loss: 0.4387 cluster_loss: 1.0662 sup_con_loss: 0.8619 contrastive_loss: 4.5797 
2024-05-15 12:54:42.030 | INFO     | __main__:train:123 - Epoch: [30][320/390]	 loss 4.17693	 cls_loss: 0.5476 cluster_loss: 1.0489 sup_con_loss: 0.9462 contrastive_loss: 4.5728 
2024-05-15 12:54:56.755 | INFO     | __main__:train:123 - Epoch: [30][340/390]	 loss 4.02291	 cls_loss: 0.4682 cluster_loss: 0.8616 sup_con_loss: 0.9375 contrastive_loss: 4.5706 
2024-05-15 12:55:11.585 | INFO     | __main__:train:123 - Epoch: [30][360/390]	 loss 3.92495	 cls_loss: 0.4090 cluster_loss: 0.9171 sup_con_loss: 0.6056 contrastive_loss: 4.5750 
2024-05-15 12:55:26.179 | INFO     | __main__:train:123 - Epoch: [30][380/390]	 loss 3.97619	 cls_loss: 0.5402 cluster_loss: 0.9039 sup_con_loss: 0.6504 contrastive_loss: 4.5723 
2024-05-15 12:55:32.647 | INFO     | __main__:train:126 - Train Epoch: 30 Avg Loss: 4.0200 
2024-05-15 12:55:32.648 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:56:01.370 | INFO     | __main__:train:135 - Train Accuracies: All 0.7989 | Old 0.8423 | New 0.6255
2024-05-15 12:56:06.055 | INFO     | __main__:train:123 - Epoch: [31][0/390]	 loss 4.02774	 cls_loss: 0.6432 cluster_loss: 0.8650 sup_con_loss: 0.7754 contrastive_loss: 4.5677 
2024-05-15 12:56:21.098 | INFO     | __main__:train:123 - Epoch: [31][20/390]	 loss 4.04476	 cls_loss: 0.4403 cluster_loss: 1.0114 sup_con_loss: 0.7390 contrastive_loss: 4.5763 
2024-05-15 12:56:35.984 | INFO     | __main__:train:123 - Epoch: [31][40/390]	 loss 4.02455	 cls_loss: 0.4699 cluster_loss: 0.9916 sup_con_loss: 0.6869 contrastive_loss: 4.5771 
2024-05-15 12:56:51.059 | INFO     | __main__:train:123 - Epoch: [31][60/390]	 loss 4.08354	 cls_loss: 0.3676 cluster_loss: 1.0653 sup_con_loss: 0.8189 contrastive_loss: 4.5781 
2024-05-15 12:57:06.230 | INFO     | __main__:train:123 - Epoch: [31][80/390]	 loss 4.04664	 cls_loss: 0.5372 cluster_loss: 0.9427 sup_con_loss: 0.7761 contrastive_loss: 4.5758 
2024-05-15 12:57:21.023 | INFO     | __main__:train:123 - Epoch: [31][100/390]	 loss 3.98469	 cls_loss: 0.4128 cluster_loss: 0.9168 sup_con_loss: 0.7734 contrastive_loss: 4.5748 
2024-05-15 12:57:36.219 | INFO     | __main__:train:123 - Epoch: [31][120/390]	 loss 4.08130	 cls_loss: 0.4398 cluster_loss: 0.9766 sup_con_loss: 0.8972 contrastive_loss: 4.5825 
2024-05-15 12:57:51.377 | INFO     | __main__:train:123 - Epoch: [31][140/390]	 loss 4.05364	 cls_loss: 0.4992 cluster_loss: 0.9844 sup_con_loss: 0.7613 contrastive_loss: 4.5733 
2024-05-15 12:58:06.311 | INFO     | __main__:train:123 - Epoch: [31][160/390]	 loss 4.00016	 cls_loss: 0.4489 cluster_loss: 0.8579 sup_con_loss: 0.9012 contrastive_loss: 4.5692 
2024-05-15 12:58:21.252 | INFO     | __main__:train:123 - Epoch: [31][180/390]	 loss 4.18229	 cls_loss: 0.5552 cluster_loss: 0.9893 sup_con_loss: 1.0648 contrastive_loss: 4.5727 
2024-05-15 12:58:36.308 | INFO     | __main__:train:123 - Epoch: [31][200/390]	 loss 3.90852	 cls_loss: 0.4512 cluster_loss: 0.9037 sup_con_loss: 0.5486 contrastive_loss: 4.5711 
2024-05-15 12:58:51.502 | INFO     | __main__:train:123 - Epoch: [31][220/390]	 loss 3.89931	 cls_loss: 0.4108 cluster_loss: 0.8510 sup_con_loss: 0.6612 contrastive_loss: 4.5707 
2024-05-15 12:59:06.637 | INFO     | __main__:train:123 - Epoch: [31][240/390]	 loss 3.82907	 cls_loss: 0.4528 cluster_loss: 0.7878 sup_con_loss: 0.5459 contrastive_loss: 4.5653 
2024-05-15 12:59:21.492 | INFO     | __main__:train:123 - Epoch: [31][260/390]	 loss 3.89793	 cls_loss: 0.3854 cluster_loss: 0.8163 sup_con_loss: 0.7506 contrastive_loss: 4.5688 
2024-05-15 12:59:36.586 | INFO     | __main__:train:123 - Epoch: [31][280/390]	 loss 4.06756	 cls_loss: 0.5433 cluster_loss: 0.9451 sup_con_loss: 0.8310 contrastive_loss: 4.5727 
2024-05-15 12:59:51.421 | INFO     | __main__:train:123 - Epoch: [31][300/390]	 loss 3.95519	 cls_loss: 0.4684 cluster_loss: 0.9241 sup_con_loss: 0.6247 contrastive_loss: 4.5723 
2024-05-15 13:00:06.314 | INFO     | __main__:train:123 - Epoch: [31][320/390]	 loss 4.06999	 cls_loss: 0.3596 cluster_loss: 0.9525 sup_con_loss: 1.0077 contrastive_loss: 4.5728 
2024-05-15 13:00:21.389 | INFO     | __main__:train:123 - Epoch: [31][340/390]	 loss 3.91302	 cls_loss: 0.3290 cluster_loss: 0.8438 sup_con_loss: 0.7977 contrastive_loss: 4.5695 
2024-05-15 13:00:36.327 | INFO     | __main__:train:123 - Epoch: [31][360/390]	 loss 3.98771	 cls_loss: 0.3988 cluster_loss: 0.9427 sup_con_loss: 0.7474 contrastive_loss: 4.5751 
2024-05-15 13:00:50.820 | INFO     | __main__:train:123 - Epoch: [31][380/390]	 loss 3.86139	 cls_loss: 0.4596 cluster_loss: 0.8702 sup_con_loss: 0.4730 contrastive_loss: 4.5683 
2024-05-15 13:00:57.185 | INFO     | __main__:train:126 - Train Epoch: 31 Avg Loss: 4.0133 
2024-05-15 13:00:57.185 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:01:25.651 | INFO     | __main__:train:135 - Train Accuracies: All 0.7990 | Old 0.8426 | New 0.6245
2024-05-15 13:01:29.043 | INFO     | __main__:train:123 - Epoch: [32][0/390]	 loss 3.88855	 cls_loss: 0.3905 cluster_loss: 0.9453 sup_con_loss: 0.4749 contrastive_loss: 4.5710 
2024-05-15 13:01:44.725 | INFO     | __main__:train:123 - Epoch: [32][20/390]	 loss 3.87747	 cls_loss: 0.4727 cluster_loss: 0.9124 sup_con_loss: 0.4218 contrastive_loss: 4.5712 
2024-05-15 13:01:59.734 | INFO     | __main__:train:123 - Epoch: [32][40/390]	 loss 3.95680	 cls_loss: 0.4130 cluster_loss: 0.9224 sup_con_loss: 0.6819 contrastive_loss: 4.5754 
2024-05-15 13:02:14.876 | INFO     | __main__:train:123 - Epoch: [32][60/390]	 loss 4.01502	 cls_loss: 0.4530 cluster_loss: 0.9735 sup_con_loss: 0.7108 contrastive_loss: 4.5769 
2024-05-15 13:02:29.737 | INFO     | __main__:train:123 - Epoch: [32][80/390]	 loss 3.99157	 cls_loss: 0.3645 cluster_loss: 0.9879 sup_con_loss: 0.7081 contrastive_loss: 4.5754 
2024-05-15 13:02:44.866 | INFO     | __main__:train:123 - Epoch: [32][100/390]	 loss 3.95026	 cls_loss: 0.3357 cluster_loss: 0.8585 sup_con_loss: 0.8724 contrastive_loss: 4.5683 
2024-05-15 13:02:59.822 | INFO     | __main__:train:123 - Epoch: [32][120/390]	 loss 3.96731	 cls_loss: 0.4974 cluster_loss: 0.9227 sup_con_loss: 0.6294 contrastive_loss: 4.5741 
2024-05-15 13:03:14.764 | INFO     | __main__:train:123 - Epoch: [32][140/390]	 loss 3.93085	 cls_loss: 0.4439 cluster_loss: 0.9374 sup_con_loss: 0.5482 contrastive_loss: 4.5758 
2024-05-15 13:03:29.895 | INFO     | __main__:train:123 - Epoch: [32][160/390]	 loss 3.96152	 cls_loss: 0.4666 cluster_loss: 0.9141 sup_con_loss: 0.6661 contrastive_loss: 4.5706 
2024-05-15 13:03:44.933 | INFO     | __main__:train:123 - Epoch: [32][180/390]	 loss 4.00174	 cls_loss: 0.4577 cluster_loss: 0.9249 sup_con_loss: 0.7727 contrastive_loss: 4.5691 
2024-05-15 13:03:59.829 | INFO     | __main__:train:123 - Epoch: [32][200/390]	 loss 4.06650	 cls_loss: 0.4528 cluster_loss: 0.9886 sup_con_loss: 0.8307 contrastive_loss: 4.5764 
2024-05-15 13:04:14.925 | INFO     | __main__:train:123 - Epoch: [32][220/390]	 loss 4.03123	 cls_loss: 0.4760 cluster_loss: 0.9272 sup_con_loss: 0.8303 contrastive_loss: 4.5713 
2024-05-15 13:04:29.970 | INFO     | __main__:train:123 - Epoch: [32][240/390]	 loss 3.96445	 cls_loss: 0.5127 cluster_loss: 0.9664 sup_con_loss: 0.5266 contrastive_loss: 4.5732 
2024-05-15 13:04:44.881 | INFO     | __main__:train:123 - Epoch: [32][260/390]	 loss 3.91265	 cls_loss: 0.4189 cluster_loss: 0.9074 sup_con_loss: 0.5741 contrastive_loss: 4.5774 
2024-05-15 13:04:59.843 | INFO     | __main__:train:123 - Epoch: [32][280/390]	 loss 4.15063	 cls_loss: 0.4785 cluster_loss: 1.0475 sup_con_loss: 0.9361 contrastive_loss: 4.5763 
2024-05-15 13:05:15.033 | INFO     | __main__:train:123 - Epoch: [32][300/390]	 loss 3.94011	 cls_loss: 0.3929 cluster_loss: 0.9308 sup_con_loss: 0.6402 contrastive_loss: 4.5747 
2024-05-15 13:05:30.336 | INFO     | __main__:train:123 - Epoch: [32][320/390]	 loss 3.96011	 cls_loss: 0.5075 cluster_loss: 0.9454 sup_con_loss: 0.5583 contrastive_loss: 4.5732 
2024-05-15 13:05:45.533 | INFO     | __main__:train:123 - Epoch: [32][340/390]	 loss 4.03632	 cls_loss: 0.4796 cluster_loss: 0.9761 sup_con_loss: 0.7448 contrastive_loss: 4.5743 
2024-05-15 13:06:00.568 | INFO     | __main__:train:123 - Epoch: [32][360/390]	 loss 4.05762	 cls_loss: 0.4365 cluster_loss: 0.9865 sup_con_loss: 0.8278 contrastive_loss: 4.5753 
2024-05-15 13:06:15.192 | INFO     | __main__:train:123 - Epoch: [32][380/390]	 loss 3.87134	 cls_loss: 0.3787 cluster_loss: 0.8851 sup_con_loss: 0.5451 contrastive_loss: 4.5734 
2024-05-15 13:06:21.643 | INFO     | __main__:train:126 - Train Epoch: 32 Avg Loss: 4.0043 
2024-05-15 13:06:21.644 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:06:50.423 | INFO     | __main__:train:135 - Train Accuracies: All 0.8041 | Old 0.8445 | New 0.6425
2024-05-15 13:06:54.442 | INFO     | __main__:train:123 - Epoch: [33][0/390]	 loss 4.04160	 cls_loss: 0.5480 cluster_loss: 1.0123 sup_con_loss: 0.6236 contrastive_loss: 4.5747 
2024-05-15 13:07:09.281 | INFO     | __main__:train:123 - Epoch: [33][20/390]	 loss 4.03633	 cls_loss: 0.6160 cluster_loss: 0.9341 sup_con_loss: 0.6787 contrastive_loss: 4.5785 
2024-05-15 13:07:24.359 | INFO     | __main__:train:123 - Epoch: [33][40/390]	 loss 3.91816	 cls_loss: 0.4522 cluster_loss: 0.8522 sup_con_loss: 0.6720 contrastive_loss: 4.5705 
2024-05-15 13:07:39.541 | INFO     | __main__:train:123 - Epoch: [33][60/390]	 loss 4.00975	 cls_loss: 0.3868 cluster_loss: 0.9264 sup_con_loss: 0.8566 contrastive_loss: 4.5730 
2024-05-15 13:07:54.668 | INFO     | __main__:train:123 - Epoch: [33][80/390]	 loss 3.87623	 cls_loss: 0.4474 cluster_loss: 0.8760 sup_con_loss: 0.5070 contrastive_loss: 4.5736 
2024-05-15 13:08:09.516 | INFO     | __main__:train:123 - Epoch: [33][100/390]	 loss 4.03237	 cls_loss: 0.4560 cluster_loss: 0.9047 sup_con_loss: 0.8926 contrastive_loss: 4.5727 
2024-05-15 13:08:24.579 | INFO     | __main__:train:123 - Epoch: [33][120/390]	 loss 3.99470	 cls_loss: 0.4559 cluster_loss: 0.8882 sup_con_loss: 0.8189 contrastive_loss: 4.5710 
2024-05-15 13:08:39.518 | INFO     | __main__:train:123 - Epoch: [33][140/390]	 loss 4.11725	 cls_loss: 0.5459 cluster_loss: 0.9587 sup_con_loss: 0.9400 contrastive_loss: 4.5754 
2024-05-15 13:08:54.369 | INFO     | __main__:train:123 - Epoch: [33][160/390]	 loss 3.93917	 cls_loss: 0.3571 cluster_loss: 0.8625 sup_con_loss: 0.8102 contrastive_loss: 4.5693 
2024-05-15 13:09:09.279 | INFO     | __main__:train:123 - Epoch: [33][180/390]	 loss 4.10854	 cls_loss: 0.4954 cluster_loss: 0.9993 sup_con_loss: 0.8840 contrastive_loss: 4.5788 
2024-05-15 13:09:24.233 | INFO     | __main__:train:123 - Epoch: [33][200/390]	 loss 4.05633	 cls_loss: 0.4645 cluster_loss: 0.9870 sup_con_loss: 0.8061 contrastive_loss: 4.5693 
2024-05-15 13:09:39.167 | INFO     | __main__:train:123 - Epoch: [33][220/390]	 loss 4.05568	 cls_loss: 0.3762 cluster_loss: 0.9854 sup_con_loss: 0.8884 contrastive_loss: 4.5732 
2024-05-15 13:09:54.390 | INFO     | __main__:train:123 - Epoch: [33][240/390]	 loss 3.99863	 cls_loss: 0.4516 cluster_loss: 0.9565 sup_con_loss: 0.6999 contrastive_loss: 4.5751 
2024-05-15 13:10:09.449 | INFO     | __main__:train:123 - Epoch: [33][260/390]	 loss 3.94817	 cls_loss: 0.3178 cluster_loss: 0.9152 sup_con_loss: 0.7751 contrastive_loss: 4.5704 
2024-05-15 13:10:24.143 | INFO     | __main__:train:123 - Epoch: [33][280/390]	 loss 3.85289	 cls_loss: 0.3102 cluster_loss: 0.8309 sup_con_loss: 0.6739 contrastive_loss: 4.5668 
2024-05-15 13:10:39.060 | INFO     | __main__:train:123 - Epoch: [33][300/390]	 loss 4.03890	 cls_loss: 0.4708 cluster_loss: 0.9930 sup_con_loss: 0.7262 contrastive_loss: 4.5762 
2024-05-15 13:10:54.125 | INFO     | __main__:train:123 - Epoch: [33][320/390]	 loss 4.00290	 cls_loss: 0.3626 cluster_loss: 0.9780 sup_con_loss: 0.7533 contrastive_loss: 4.5794 
2024-05-15 13:11:08.970 | INFO     | __main__:train:123 - Epoch: [33][340/390]	 loss 3.99335	 cls_loss: 0.3841 cluster_loss: 0.9675 sup_con_loss: 0.7370 contrastive_loss: 4.5724 
2024-05-15 13:11:23.896 | INFO     | __main__:train:123 - Epoch: [33][360/390]	 loss 3.94596	 cls_loss: 0.3323 cluster_loss: 0.9103 sup_con_loss: 0.7584 contrastive_loss: 4.5731 
2024-05-15 13:11:38.470 | INFO     | __main__:train:123 - Epoch: [33][380/390]	 loss 3.96047	 cls_loss: 0.3938 cluster_loss: 0.9074 sup_con_loss: 0.7352 contrastive_loss: 4.5777 
2024-05-15 13:11:44.932 | INFO     | __main__:train:126 - Train Epoch: 33 Avg Loss: 3.9977 
2024-05-15 13:11:44.933 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:12:13.131 | INFO     | __main__:train:135 - Train Accuracies: All 0.8023 | Old 0.8454 | New 0.6300
2024-05-15 13:12:18.036 | INFO     | __main__:train:123 - Epoch: [34][0/390]	 loss 4.03839	 cls_loss: 0.4215 cluster_loss: 0.9198 sup_con_loss: 0.9134 contrastive_loss: 4.5744 
2024-05-15 13:12:32.953 | INFO     | __main__:train:123 - Epoch: [34][20/390]	 loss 4.17260	 cls_loss: 0.4146 cluster_loss: 1.0329 sup_con_loss: 1.0862 contrastive_loss: 4.5784 
2024-05-15 13:12:47.862 | INFO     | __main__:train:123 - Epoch: [34][40/390]	 loss 3.96033	 cls_loss: 0.3846 cluster_loss: 0.9821 sup_con_loss: 0.6149 contrastive_loss: 4.5725 
2024-05-15 13:13:02.999 | INFO     | __main__:train:123 - Epoch: [34][60/390]	 loss 3.87818	 cls_loss: 0.4245 cluster_loss: 0.8724 sup_con_loss: 0.5499 contrastive_loss: 4.5693 
2024-05-15 13:13:17.766 | INFO     | __main__:train:123 - Epoch: [34][80/390]	 loss 3.93864	 cls_loss: 0.4980 cluster_loss: 0.8584 sup_con_loss: 0.6764 contrastive_loss: 4.5687 
2024-05-15 13:13:32.881 | INFO     | __main__:train:123 - Epoch: [34][100/390]	 loss 3.92675	 cls_loss: 0.3343 cluster_loss: 0.8706 sup_con_loss: 0.7797 contrastive_loss: 4.5707 
2024-05-15 13:13:47.809 | INFO     | __main__:train:123 - Epoch: [34][120/390]	 loss 3.95780	 cls_loss: 0.3716 cluster_loss: 0.9225 sup_con_loss: 0.7326 contrastive_loss: 4.5718 
2024-05-15 13:14:02.744 | INFO     | __main__:train:123 - Epoch: [34][140/390]	 loss 3.87434	 cls_loss: 0.3462 cluster_loss: 0.8661 sup_con_loss: 0.6190 contrastive_loss: 4.5746 
2024-05-15 13:14:17.849 | INFO     | __main__:train:123 - Epoch: [34][160/390]	 loss 4.01568	 cls_loss: 0.4730 cluster_loss: 0.9472 sup_con_loss: 0.7602 contrastive_loss: 4.5667 
2024-05-15 13:14:32.924 | INFO     | __main__:train:123 - Epoch: [34][180/390]	 loss 3.90322	 cls_loss: 0.3530 cluster_loss: 0.8247 sup_con_loss: 0.7826 contrastive_loss: 4.5688 
2024-05-15 13:14:47.812 | INFO     | __main__:train:123 - Epoch: [34][200/390]	 loss 3.91724	 cls_loss: 0.3556 cluster_loss: 0.9366 sup_con_loss: 0.5993 contrastive_loss: 4.5758 
2024-05-15 13:15:02.989 | INFO     | __main__:train:123 - Epoch: [34][220/390]	 loss 3.87560	 cls_loss: 0.3827 cluster_loss: 0.9070 sup_con_loss: 0.5226 contrastive_loss: 4.5679 
2024-05-15 13:15:17.962 | INFO     | __main__:train:123 - Epoch: [34][240/390]	 loss 3.95113	 cls_loss: 0.2878 cluster_loss: 0.9625 sup_con_loss: 0.7261 contrastive_loss: 4.5702 
2024-05-15 13:15:32.788 | INFO     | __main__:train:123 - Epoch: [34][260/390]	 loss 3.97241	 cls_loss: 0.3316 cluster_loss: 0.9579 sup_con_loss: 0.7380 contrastive_loss: 4.5776 
2024-05-15 13:15:47.768 | INFO     | __main__:train:123 - Epoch: [34][280/390]	 loss 3.93254	 cls_loss: 0.3676 cluster_loss: 0.9176 sup_con_loss: 0.6749 contrastive_loss: 4.5711 
2024-05-15 13:16:02.718 | INFO     | __main__:train:123 - Epoch: [34][300/390]	 loss 4.05474	 cls_loss: 0.4974 cluster_loss: 0.9835 sup_con_loss: 0.7674 contrastive_loss: 4.5735 
2024-05-15 13:16:18.283 | INFO     | __main__:train:123 - Epoch: [34][320/390]	 loss 4.01861	 cls_loss: 0.4779 cluster_loss: 0.9531 sup_con_loss: 0.7365 contrastive_loss: 4.5754 
2024-05-15 13:16:33.389 | INFO     | __main__:train:123 - Epoch: [34][340/390]	 loss 3.89446	 cls_loss: 0.2958 cluster_loss: 0.8395 sup_con_loss: 0.7931 contrastive_loss: 4.5656 
2024-05-15 13:16:48.359 | INFO     | __main__:train:123 - Epoch: [34][360/390]	 loss 3.98087	 cls_loss: 0.4522 cluster_loss: 0.8575 sup_con_loss: 0.8419 contrastive_loss: 4.5701 
2024-05-15 13:17:02.884 | INFO     | __main__:train:123 - Epoch: [34][380/390]	 loss 3.95230	 cls_loss: 0.3682 cluster_loss: 0.9457 sup_con_loss: 0.6757 contrastive_loss: 4.5727 
2024-05-15 13:17:09.400 | INFO     | __main__:train:126 - Train Epoch: 34 Avg Loss: 3.9901 
2024-05-15 13:17:09.401 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:17:37.717 | INFO     | __main__:train:135 - Train Accuracies: All 0.8026 | Old 0.8445 | New 0.6350
2024-05-15 13:17:41.655 | INFO     | __main__:train:123 - Epoch: [35][0/390]	 loss 3.98113	 cls_loss: 0.3936 cluster_loss: 0.9654 sup_con_loss: 0.6959 contrastive_loss: 4.5728 
2024-05-15 13:17:57.004 | INFO     | __main__:train:123 - Epoch: [35][20/390]	 loss 3.98578	 cls_loss: 0.4638 cluster_loss: 0.9498 sup_con_loss: 0.6588 contrastive_loss: 4.5778 
2024-05-15 13:18:12.002 | INFO     | __main__:train:123 - Epoch: [35][40/390]	 loss 4.12777	 cls_loss: 0.4215 cluster_loss: 1.0372 sup_con_loss: 0.9545 contrastive_loss: 4.5723 
2024-05-15 13:18:26.919 | INFO     | __main__:train:123 - Epoch: [35][60/390]	 loss 3.91157	 cls_loss: 0.3230 cluster_loss: 0.9242 sup_con_loss: 0.6458 contrastive_loss: 4.5719 
2024-05-15 13:18:41.764 | INFO     | __main__:train:123 - Epoch: [35][80/390]	 loss 3.98231	 cls_loss: 0.4818 cluster_loss: 0.9124 sup_con_loss: 0.7035 contrastive_loss: 4.5760 
2024-05-15 13:18:56.958 | INFO     | __main__:train:123 - Epoch: [35][100/390]	 loss 3.96314	 cls_loss: 0.4477 cluster_loss: 0.9116 sup_con_loss: 0.6846 contrastive_loss: 4.5758 
2024-05-15 13:19:11.120 | INFO     | __main__:train:123 - Epoch: [35][120/390]	 loss 4.08075	 cls_loss: 0.4112 cluster_loss: 1.0275 sup_con_loss: 0.8477 contrastive_loss: 4.5727 
2024-05-15 13:19:25.344 | INFO     | __main__:train:123 - Epoch: [35][140/390]	 loss 4.03967	 cls_loss: 0.5425 cluster_loss: 0.8903 sup_con_loss: 0.8590 contrastive_loss: 4.5699 
2024-05-15 13:19:39.478 | INFO     | __main__:train:123 - Epoch: [35][160/390]	 loss 3.96853	 cls_loss: 0.4278 cluster_loss: 0.9197 sup_con_loss: 0.7174 contrastive_loss: 4.5691 
2024-05-15 13:19:53.500 | INFO     | __main__:train:123 - Epoch: [35][180/390]	 loss 3.87901	 cls_loss: 0.2618 cluster_loss: 0.9043 sup_con_loss: 0.6532 contrastive_loss: 4.5707 
2024-05-15 13:20:07.349 | INFO     | __main__:train:123 - Epoch: [35][200/390]	 loss 3.96704	 cls_loss: 0.3887 cluster_loss: 0.9317 sup_con_loss: 0.7216 contrastive_loss: 4.5736 
2024-05-15 13:20:21.681 | INFO     | __main__:train:123 - Epoch: [35][220/390]	 loss 4.02336	 cls_loss: 0.6137 cluster_loss: 0.8324 sup_con_loss: 0.8494 contrastive_loss: 4.5696 
2024-05-15 13:20:35.819 | INFO     | __main__:train:123 - Epoch: [35][240/390]	 loss 3.98815	 cls_loss: 0.5014 cluster_loss: 0.9511 sup_con_loss: 0.6394 contrastive_loss: 4.5702 
2024-05-15 13:20:50.101 | INFO     | __main__:train:123 - Epoch: [35][260/390]	 loss 3.98875	 cls_loss: 0.3746 cluster_loss: 0.9281 sup_con_loss: 0.8061 contrastive_loss: 4.5727 
2024-05-15 13:21:04.209 | INFO     | __main__:train:123 - Epoch: [35][280/390]	 loss 4.01036	 cls_loss: 0.4276 cluster_loss: 0.9737 sup_con_loss: 0.7247 contrastive_loss: 4.5756 
2024-05-15 13:21:18.328 | INFO     | __main__:train:123 - Epoch: [35][300/390]	 loss 4.12417	 cls_loss: 0.5060 cluster_loss: 1.0576 sup_con_loss: 0.8064 contrastive_loss: 4.5806 
2024-05-15 13:21:32.446 | INFO     | __main__:train:123 - Epoch: [35][320/390]	 loss 4.09927	 cls_loss: 0.4709 cluster_loss: 0.9566 sup_con_loss: 0.9731 contrastive_loss: 4.5724 
2024-05-15 13:21:46.556 | INFO     | __main__:train:123 - Epoch: [35][340/390]	 loss 4.00339	 cls_loss: 0.3741 cluster_loss: 0.9563 sup_con_loss: 0.7893 contrastive_loss: 4.5763 
2024-05-15 13:22:00.729 | INFO     | __main__:train:123 - Epoch: [35][360/390]	 loss 4.07703	 cls_loss: 0.3871 cluster_loss: 1.0066 sup_con_loss: 0.8981 contrastive_loss: 4.5737 
2024-05-15 13:22:14.314 | INFO     | __main__:train:123 - Epoch: [35][380/390]	 loss 3.95533	 cls_loss: 0.3862 cluster_loss: 0.9076 sup_con_loss: 0.7452 contrastive_loss: 4.5682 
2024-05-15 13:22:20.379 | INFO     | __main__:train:126 - Train Epoch: 35 Avg Loss: 3.9789 
2024-05-15 13:22:20.379 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:22:48.146 | INFO     | __main__:train:135 - Train Accuracies: All 0.8054 | Old 0.8476 | New 0.6365
2024-05-15 13:22:52.698 | INFO     | __main__:train:123 - Epoch: [36][0/390]	 loss 3.90648	 cls_loss: 0.3747 cluster_loss: 0.8648 sup_con_loss: 0.6972 contrastive_loss: 4.5680 
2024-05-15 13:23:06.840 | INFO     | __main__:train:123 - Epoch: [36][20/390]	 loss 4.00775	 cls_loss: 0.4072 cluster_loss: 0.9289 sup_con_loss: 0.8298 contrastive_loss: 4.5708 
2024-05-15 13:23:21.002 | INFO     | __main__:train:123 - Epoch: [36][40/390]	 loss 3.96337	 cls_loss: 0.3515 cluster_loss: 0.9710 sup_con_loss: 0.6703 contrastive_loss: 4.5764 
2024-05-15 13:23:34.967 | INFO     | __main__:train:123 - Epoch: [36][60/390]	 loss 3.94937	 cls_loss: 0.5200 cluster_loss: 0.8695 sup_con_loss: 0.6631 contrastive_loss: 4.5694 
2024-05-15 13:23:49.128 | INFO     | __main__:train:123 - Epoch: [36][80/390]	 loss 3.86290	 cls_loss: 0.3909 cluster_loss: 0.8111 sup_con_loss: 0.6636 contrastive_loss: 4.5641 
2024-05-15 13:24:03.236 | INFO     | __main__:train:123 - Epoch: [36][100/390]	 loss 4.06785	 cls_loss: 0.5589 cluster_loss: 0.9939 sup_con_loss: 0.7249 contrastive_loss: 4.5731 
2024-05-15 13:24:17.428 | INFO     | __main__:train:123 - Epoch: [36][120/390]	 loss 3.97872	 cls_loss: 0.3618 cluster_loss: 0.9359 sup_con_loss: 0.7733 contrastive_loss: 4.5739 
2024-05-15 13:24:31.447 | INFO     | __main__:train:123 - Epoch: [36][140/390]	 loss 4.00860	 cls_loss: 0.3397 cluster_loss: 0.9621 sup_con_loss: 0.8367 contrastive_loss: 4.5716 
2024-05-15 13:24:45.490 | INFO     | __main__:train:123 - Epoch: [36][160/390]	 loss 4.11694	 cls_loss: 0.4735 cluster_loss: 1.0060 sup_con_loss: 0.9351 contrastive_loss: 4.5692 
2024-05-15 13:24:59.660 | INFO     | __main__:train:123 - Epoch: [36][180/390]	 loss 3.99588	 cls_loss: 0.3599 cluster_loss: 0.9724 sup_con_loss: 0.7433 contrastive_loss: 4.5811 
2024-05-15 13:25:13.684 | INFO     | __main__:train:123 - Epoch: [36][200/390]	 loss 3.97827	 cls_loss: 0.3612 cluster_loss: 0.9454 sup_con_loss: 0.7524 contrastive_loss: 4.5754 
2024-05-15 13:25:27.809 | INFO     | __main__:train:123 - Epoch: [36][220/390]	 loss 3.91401	 cls_loss: 0.2816 cluster_loss: 0.8918 sup_con_loss: 0.7654 contrastive_loss: 4.5659 
2024-05-15 13:25:42.074 | INFO     | __main__:train:123 - Epoch: [36][240/390]	 loss 3.94054	 cls_loss: 0.3422 cluster_loss: 0.8788 sup_con_loss: 0.7954 contrastive_loss: 4.5710 
2024-05-15 13:25:56.380 | INFO     | __main__:train:123 - Epoch: [36][260/390]	 loss 3.91963	 cls_loss: 0.4502 cluster_loss: 0.8640 sup_con_loss: 0.6563 contrastive_loss: 4.5703 
2024-05-15 13:26:10.522 | INFO     | __main__:train:123 - Epoch: [36][280/390]	 loss 3.98248	 cls_loss: 0.5063 cluster_loss: 0.9207 sup_con_loss: 0.6738 contrastive_loss: 4.5707 
2024-05-15 13:26:24.618 | INFO     | __main__:train:123 - Epoch: [36][300/390]	 loss 3.89453	 cls_loss: 0.4121 cluster_loss: 0.8919 sup_con_loss: 0.5721 contrastive_loss: 4.5697 
2024-05-15 13:26:38.695 | INFO     | __main__:train:123 - Epoch: [36][320/390]	 loss 3.96508	 cls_loss: 0.4270 cluster_loss: 0.9733 sup_con_loss: 0.6103 contrastive_loss: 4.5683 
2024-05-15 13:26:52.930 | INFO     | __main__:train:123 - Epoch: [36][340/390]	 loss 3.98805	 cls_loss: 0.4341 cluster_loss: 0.9483 sup_con_loss: 0.6972 contrastive_loss: 4.5780 
2024-05-15 13:27:07.245 | INFO     | __main__:train:123 - Epoch: [36][360/390]	 loss 4.02372	 cls_loss: 0.4314 cluster_loss: 0.9386 sup_con_loss: 0.8359 contrastive_loss: 4.5693 
2024-05-15 13:27:21.133 | INFO     | __main__:train:123 - Epoch: [36][380/390]	 loss 3.91127	 cls_loss: 0.2858 cluster_loss: 0.8969 sup_con_loss: 0.7421 contrastive_loss: 4.5670 
2024-05-15 13:27:27.299 | INFO     | __main__:train:126 - Train Epoch: 36 Avg Loss: 3.9780 
2024-05-15 13:27:27.300 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:27:55.243 | INFO     | __main__:train:135 - Train Accuracies: All 0.8064 | Old 0.8499 | New 0.6325
2024-05-15 13:27:58.638 | INFO     | __main__:train:123 - Epoch: [37][0/390]	 loss 3.86704	 cls_loss: 0.3204 cluster_loss: 0.8691 sup_con_loss: 0.6180 contrastive_loss: 4.5749 
2024-05-15 13:28:13.089 | INFO     | __main__:train:123 - Epoch: [37][20/390]	 loss 3.92040	 cls_loss: 0.2769 cluster_loss: 0.8945 sup_con_loss: 0.7714 contrastive_loss: 4.5724 
2024-05-15 13:28:27.314 | INFO     | __main__:train:123 - Epoch: [37][40/390]	 loss 4.10677	 cls_loss: 0.4930 cluster_loss: 1.0204 sup_con_loss: 0.8576 contrastive_loss: 4.5704 
2024-05-15 13:28:41.553 | INFO     | __main__:train:123 - Epoch: [37][60/390]	 loss 3.90369	 cls_loss: 0.3739 cluster_loss: 0.8856 sup_con_loss: 0.6442 contrastive_loss: 4.5719 
2024-05-15 13:28:55.655 | INFO     | __main__:train:123 - Epoch: [37][80/390]	 loss 4.05771	 cls_loss: 0.5035 cluster_loss: 0.9907 sup_con_loss: 0.7617 contrastive_loss: 4.5707 
2024-05-15 13:29:09.662 | INFO     | __main__:train:123 - Epoch: [37][100/390]	 loss 3.95394	 cls_loss: 0.4825 cluster_loss: 0.9345 sup_con_loss: 0.5962 contrastive_loss: 4.5677 
2024-05-15 13:29:24.226 | INFO     | __main__:train:123 - Epoch: [37][120/390]	 loss 4.07582	 cls_loss: 0.4896 cluster_loss: 0.9587 sup_con_loss: 0.8876 contrastive_loss: 4.5702 
2024-05-15 13:29:38.420 | INFO     | __main__:train:123 - Epoch: [37][140/390]	 loss 3.94116	 cls_loss: 0.3742 cluster_loss: 0.8865 sup_con_loss: 0.7571 contrastive_loss: 4.5676 
2024-05-15 13:29:52.518 | INFO     | __main__:train:123 - Epoch: [37][160/390]	 loss 3.93318	 cls_loss: 0.3539 cluster_loss: 0.9007 sup_con_loss: 0.7222 contrastive_loss: 4.5709 
2024-05-15 13:30:06.808 | INFO     | __main__:train:123 - Epoch: [37][180/390]	 loss 3.94019	 cls_loss: 0.3704 cluster_loss: 0.8955 sup_con_loss: 0.7331 contrastive_loss: 4.5721 
2024-05-15 13:30:20.899 | INFO     | __main__:train:123 - Epoch: [37][200/390]	 loss 4.16898	 cls_loss: 0.5134 cluster_loss: 1.1064 sup_con_loss: 0.8340 contrastive_loss: 4.5819 
2024-05-15 13:30:35.005 | INFO     | __main__:train:123 - Epoch: [37][220/390]	 loss 4.06409	 cls_loss: 0.3924 cluster_loss: 0.9840 sup_con_loss: 0.8993 contrastive_loss: 4.5730 
2024-05-15 13:30:49.174 | INFO     | __main__:train:123 - Epoch: [37][240/390]	 loss 4.03728	 cls_loss: 0.3318 cluster_loss: 1.0398 sup_con_loss: 0.7745 contrastive_loss: 4.5757 
2024-05-15 13:31:03.405 | INFO     | __main__:train:123 - Epoch: [37][260/390]	 loss 4.02484	 cls_loss: 0.3719 cluster_loss: 0.9538 sup_con_loss: 0.8486 contrastive_loss: 4.5811 
2024-05-15 13:31:17.608 | INFO     | __main__:train:123 - Epoch: [37][280/390]	 loss 3.91121	 cls_loss: 0.4018 cluster_loss: 0.9030 sup_con_loss: 0.6113 contrastive_loss: 4.5687 
2024-05-15 13:31:31.894 | INFO     | __main__:train:123 - Epoch: [37][300/390]	 loss 3.90509	 cls_loss: 0.4189 cluster_loss: 0.8086 sup_con_loss: 0.7563 contrastive_loss: 4.5665 
2024-05-15 13:31:46.059 | INFO     | __main__:train:123 - Epoch: [37][320/390]	 loss 3.94135	 cls_loss: 0.3533 cluster_loss: 0.8887 sup_con_loss: 0.7624 contrastive_loss: 4.5741 
2024-05-15 13:32:00.611 | INFO     | __main__:train:123 - Epoch: [37][340/390]	 loss 3.92999	 cls_loss: 0.4243 cluster_loss: 0.8786 sup_con_loss: 0.6792 contrastive_loss: 4.5733 
2024-05-15 13:32:14.661 | INFO     | __main__:train:123 - Epoch: [37][360/390]	 loss 3.96676	 cls_loss: 0.4441 cluster_loss: 0.8983 sup_con_loss: 0.7308 contrastive_loss: 4.5717 
2024-05-15 13:32:28.563 | INFO     | __main__:train:123 - Epoch: [37][380/390]	 loss 3.88107	 cls_loss: 0.3048 cluster_loss: 0.8635 sup_con_loss: 0.6869 contrastive_loss: 4.5733 
2024-05-15 13:32:34.865 | INFO     | __main__:train:126 - Train Epoch: 37 Avg Loss: 3.9774 
2024-05-15 13:32:34.865 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:33:02.509 | INFO     | __main__:train:135 - Train Accuracies: All 0.8045 | Old 0.8461 | New 0.6380
2024-05-15 13:33:06.844 | INFO     | __main__:train:123 - Epoch: [38][0/390]	 loss 3.99055	 cls_loss: 0.4106 cluster_loss: 0.9833 sup_con_loss: 0.6749 contrastive_loss: 4.5715 
2024-05-15 13:33:21.394 | INFO     | __main__:train:123 - Epoch: [38][20/390]	 loss 4.06722	 cls_loss: 0.5389 cluster_loss: 0.9664 sup_con_loss: 0.7917 contrastive_loss: 4.5744 
2024-05-15 13:33:35.712 | INFO     | __main__:train:123 - Epoch: [38][40/390]	 loss 3.97887	 cls_loss: 0.3384 cluster_loss: 0.9037 sup_con_loss: 0.8566 contrastive_loss: 4.5741 
2024-05-15 13:33:50.069 | INFO     | __main__:train:123 - Epoch: [38][60/390]	 loss 3.82859	 cls_loss: 0.2217 cluster_loss: 0.7860 sup_con_loss: 0.7710 contrastive_loss: 4.5696 
2024-05-15 13:34:04.648 | INFO     | __main__:train:123 - Epoch: [38][80/390]	 loss 4.03656	 cls_loss: 0.5052 cluster_loss: 0.9408 sup_con_loss: 0.7880 contrastive_loss: 4.5729 
2024-05-15 13:34:18.880 | INFO     | __main__:train:123 - Epoch: [38][100/390]	 loss 3.93976	 cls_loss: 0.5080 cluster_loss: 0.8309 sup_con_loss: 0.7157 contrastive_loss: 4.5714 
2024-05-15 13:34:33.493 | INFO     | __main__:train:123 - Epoch: [38][120/390]	 loss 3.93119	 cls_loss: 0.3322 cluster_loss: 0.8903 sup_con_loss: 0.7600 contrastive_loss: 4.5696 
2024-05-15 13:34:47.975 | INFO     | __main__:train:123 - Epoch: [38][140/390]	 loss 4.00624	 cls_loss: 0.4085 cluster_loss: 0.8968 sup_con_loss: 0.8835 contrastive_loss: 4.5710 
2024-05-15 13:35:02.283 | INFO     | __main__:train:123 - Epoch: [38][160/390]	 loss 3.98135	 cls_loss: 0.4788 cluster_loss: 0.9043 sup_con_loss: 0.7053 contrastive_loss: 4.5833 
2024-05-15 13:35:16.602 | INFO     | __main__:train:123 - Epoch: [38][180/390]	 loss 3.90885	 cls_loss: 0.3099 cluster_loss: 0.8608 sup_con_loss: 0.7527 contrastive_loss: 4.5807 
2024-05-15 13:35:30.831 | INFO     | __main__:train:123 - Epoch: [38][200/390]	 loss 3.88405	 cls_loss: 0.3297 cluster_loss: 0.8766 sup_con_loss: 0.6539 contrastive_loss: 4.5693 
2024-05-15 13:35:45.012 | INFO     | __main__:train:123 - Epoch: [38][220/390]	 loss 3.93313	 cls_loss: 0.4204 cluster_loss: 0.8564 sup_con_loss: 0.7521 contrastive_loss: 4.5632 
2024-05-15 13:35:59.286 | INFO     | __main__:train:123 - Epoch: [38][240/390]	 loss 3.96030	 cls_loss: 0.3649 cluster_loss: 0.9553 sup_con_loss: 0.6845 contrastive_loss: 4.5725 
2024-05-15 13:36:13.884 | INFO     | __main__:train:123 - Epoch: [38][260/390]	 loss 4.03578	 cls_loss: 0.3624 cluster_loss: 1.0048 sup_con_loss: 0.8079 contrastive_loss: 4.5740 
2024-05-15 13:36:28.368 | INFO     | __main__:train:123 - Epoch: [38][280/390]	 loss 4.04333	 cls_loss: 0.4424 cluster_loss: 0.9433 sup_con_loss: 0.8697 contrastive_loss: 4.5707 
2024-05-15 13:36:42.737 | INFO     | __main__:train:123 - Epoch: [38][300/390]	 loss 4.04681	 cls_loss: 0.4463 cluster_loss: 0.9133 sup_con_loss: 0.9341 contrastive_loss: 4.5693 
2024-05-15 13:36:57.280 | INFO     | __main__:train:123 - Epoch: [38][320/390]	 loss 3.76983	 cls_loss: 0.3102 cluster_loss: 0.8429 sup_con_loss: 0.4094 contrastive_loss: 4.5693 
2024-05-15 13:37:11.747 | INFO     | __main__:train:123 - Epoch: [38][340/390]	 loss 3.95186	 cls_loss: 0.4995 cluster_loss: 0.8971 sup_con_loss: 0.6399 contrastive_loss: 4.5691 
2024-05-15 13:37:25.786 | INFO     | __main__:train:123 - Epoch: [38][360/390]	 loss 4.06481	 cls_loss: 0.4196 cluster_loss: 0.9692 sup_con_loss: 0.9011 contrastive_loss: 4.5732 
2024-05-15 13:37:39.851 | INFO     | __main__:train:123 - Epoch: [38][380/390]	 loss 3.96074	 cls_loss: 0.3615 cluster_loss: 0.9065 sup_con_loss: 0.7851 contrastive_loss: 4.5696 
2024-05-15 13:37:46.131 | INFO     | __main__:train:126 - Train Epoch: 38 Avg Loss: 3.9796 
2024-05-15 13:37:46.131 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:38:13.784 | INFO     | __main__:train:135 - Train Accuracies: All 0.8053 | Old 0.8458 | New 0.6435
2024-05-15 13:38:16.793 | INFO     | __main__:train:123 - Epoch: [39][0/390]	 loss 4.04963	 cls_loss: 0.4143 cluster_loss: 0.9781 sup_con_loss: 0.8551 contrastive_loss: 4.5685 
2024-05-15 13:38:31.562 | INFO     | __main__:train:123 - Epoch: [39][20/390]	 loss 3.97071	 cls_loss: 0.3060 cluster_loss: 0.8821 sup_con_loss: 0.9125 contrastive_loss: 4.5706 
2024-05-15 13:38:46.041 | INFO     | __main__:train:123 - Epoch: [39][40/390]	 loss 3.90743	 cls_loss: 0.4218 cluster_loss: 0.8780 sup_con_loss: 0.6266 contrastive_loss: 4.5690 
2024-05-15 13:39:00.484 | INFO     | __main__:train:123 - Epoch: [39][60/390]	 loss 3.95726	 cls_loss: 0.4015 cluster_loss: 0.9557 sup_con_loss: 0.6336 contrastive_loss: 4.5750 
2024-05-15 13:39:14.862 | INFO     | __main__:train:123 - Epoch: [39][80/390]	 loss 4.03800	 cls_loss: 0.5663 cluster_loss: 0.8541 sup_con_loss: 0.8942 contrastive_loss: 4.5718 
2024-05-15 13:39:29.265 | INFO     | __main__:train:123 - Epoch: [39][100/390]	 loss 3.85767	 cls_loss: 0.4203 cluster_loss: 0.8449 sup_con_loss: 0.5470 contrastive_loss: 4.5691 
2024-05-15 13:39:43.839 | INFO     | __main__:train:123 - Epoch: [39][120/390]	 loss 4.04309	 cls_loss: 0.4754 cluster_loss: 0.9186 sup_con_loss: 0.8847 contrastive_loss: 4.5691 
2024-05-15 13:39:58.423 | INFO     | __main__:train:123 - Epoch: [39][140/390]	 loss 4.00410	 cls_loss: 0.3824 cluster_loss: 0.9699 sup_con_loss: 0.7600 contrastive_loss: 4.5751 
2024-05-15 13:40:12.893 | INFO     | __main__:train:123 - Epoch: [39][160/390]	 loss 3.88226	 cls_loss: 0.3058 cluster_loss: 0.9012 sup_con_loss: 0.6245 contrastive_loss: 4.5705 
2024-05-15 13:40:27.016 | INFO     | __main__:train:123 - Epoch: [39][180/390]	 loss 3.83888	 cls_loss: 0.3281 cluster_loss: 0.7917 sup_con_loss: 0.6901 contrastive_loss: 4.5660 
2024-05-15 13:40:41.323 | INFO     | __main__:train:123 - Epoch: [39][200/390]	 loss 3.99427	 cls_loss: 0.4294 cluster_loss: 0.8847 sup_con_loss: 0.8417 contrastive_loss: 4.5759 
2024-05-15 13:40:55.889 | INFO     | __main__:train:123 - Epoch: [39][220/390]	 loss 4.03828	 cls_loss: 0.2649 cluster_loss: 0.9821 sup_con_loss: 0.9549 contrastive_loss: 4.5738 
2024-05-15 13:41:10.203 | INFO     | __main__:train:123 - Epoch: [39][240/390]	 loss 3.92516	 cls_loss: 0.3561 cluster_loss: 0.8388 sup_con_loss: 0.8151 contrastive_loss: 4.5692 
2024-05-15 13:41:24.801 | INFO     | __main__:train:123 - Epoch: [39][260/390]	 loss 3.93227	 cls_loss: 0.4043 cluster_loss: 0.8165 sup_con_loss: 0.8307 contrastive_loss: 4.5682 
2024-05-15 13:41:39.464 | INFO     | __main__:train:123 - Epoch: [39][280/390]	 loss 3.82249	 cls_loss: 0.4494 cluster_loss: 0.7367 sup_con_loss: 0.6216 contrastive_loss: 4.5673 
2024-05-15 13:41:53.796 | INFO     | __main__:train:123 - Epoch: [39][300/390]	 loss 3.92299	 cls_loss: 0.2293 cluster_loss: 0.9620 sup_con_loss: 0.7045 contrastive_loss: 4.5706 
2024-05-15 13:42:08.255 | INFO     | __main__:train:123 - Epoch: [39][320/390]	 loss 4.05272	 cls_loss: 0.4406 cluster_loss: 0.9747 sup_con_loss: 0.8365 contrastive_loss: 4.5726 
2024-05-15 13:42:22.939 | INFO     | __main__:train:123 - Epoch: [39][340/390]	 loss 3.88721	 cls_loss: 0.3795 cluster_loss: 0.8940 sup_con_loss: 0.5776 contrastive_loss: 4.5710 
2024-05-15 13:42:37.396 | INFO     | __main__:train:123 - Epoch: [39][360/390]	 loss 3.94169	 cls_loss: 0.4224 cluster_loss: 0.8698 sup_con_loss: 0.7377 contrastive_loss: 4.5696 
2024-05-15 13:42:51.398 | INFO     | __main__:train:123 - Epoch: [39][380/390]	 loss 3.98371	 cls_loss: 0.4007 cluster_loss: 0.8522 sup_con_loss: 0.9077 contrastive_loss: 4.5721 
2024-05-15 13:42:57.855 | INFO     | __main__:train:126 - Train Epoch: 39 Avg Loss: 3.9696 
2024-05-15 13:42:57.856 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:43:25.475 | INFO     | __main__:train:135 - Train Accuracies: All 0.8057 | Old 0.8486 | New 0.6340
2024-05-15 13:43:29.298 | INFO     | __main__:train:123 - Epoch: [40][0/390]	 loss 3.91169	 cls_loss: 0.4260 cluster_loss: 0.9084 sup_con_loss: 0.5778 contrastive_loss: 4.5690 
2024-05-15 13:43:43.933 | INFO     | __main__:train:123 - Epoch: [40][20/390]	 loss 3.99939	 cls_loss: 0.3703 cluster_loss: 0.9029 sup_con_loss: 0.8949 contrastive_loss: 4.5688 
2024-05-15 13:43:58.459 | INFO     | __main__:train:123 - Epoch: [40][40/390]	 loss 4.07641	 cls_loss: 0.3520 cluster_loss: 1.0075 sup_con_loss: 0.9176 contrastive_loss: 4.5802 
2024-05-15 13:44:12.638 | INFO     | __main__:train:123 - Epoch: [40][60/390]	 loss 3.91305	 cls_loss: 0.2726 cluster_loss: 0.8364 sup_con_loss: 0.8706 contrastive_loss: 4.5681 
2024-05-15 13:44:26.996 | INFO     | __main__:train:123 - Epoch: [40][80/390]	 loss 3.82390	 cls_loss: 0.3080 cluster_loss: 0.8487 sup_con_loss: 0.5512 contrastive_loss: 4.5715 
2024-05-15 13:44:41.535 | INFO     | __main__:train:123 - Epoch: [40][100/390]	 loss 3.81829	 cls_loss: 0.3023 cluster_loss: 0.8725 sup_con_loss: 0.4983 contrastive_loss: 4.5707 
2024-05-15 13:44:55.744 | INFO     | __main__:train:123 - Epoch: [40][120/390]	 loss 3.97442	 cls_loss: 0.3183 cluster_loss: 0.9431 sup_con_loss: 0.7961 contrastive_loss: 4.5713 
2024-05-15 13:45:10.018 | INFO     | __main__:train:123 - Epoch: [40][140/390]	 loss 3.86602	 cls_loss: 0.4554 cluster_loss: 0.8970 sup_con_loss: 0.4433 contrastive_loss: 4.5669 
2024-05-15 13:45:24.364 | INFO     | __main__:train:123 - Epoch: [40][160/390]	 loss 4.02363	 cls_loss: 0.3963 cluster_loss: 0.9846 sup_con_loss: 0.7879 contrastive_loss: 4.5679 
2024-05-15 13:45:38.820 | INFO     | __main__:train:123 - Epoch: [40][180/390]	 loss 3.83007	 cls_loss: 0.3537 cluster_loss: 0.8518 sup_con_loss: 0.5200 contrastive_loss: 4.5702 
2024-05-15 13:45:53.105 | INFO     | __main__:train:123 - Epoch: [40][200/390]	 loss 3.97641	 cls_loss: 0.3797 cluster_loss: 0.9347 sup_con_loss: 0.7507 contrastive_loss: 4.5742 
2024-05-15 13:46:07.698 | INFO     | __main__:train:123 - Epoch: [40][220/390]	 loss 4.11587	 cls_loss: 0.3227 cluster_loss: 1.0872 sup_con_loss: 0.9264 contrastive_loss: 4.5724 
2024-05-15 13:46:22.077 | INFO     | __main__:train:123 - Epoch: [40][240/390]	 loss 3.91419	 cls_loss: 0.5105 cluster_loss: 0.8398 sup_con_loss: 0.6289 contrastive_loss: 4.5686 
2024-05-15 13:46:36.525 | INFO     | __main__:train:123 - Epoch: [40][260/390]	 loss 3.94707	 cls_loss: 0.4125 cluster_loss: 0.8935 sup_con_loss: 0.7217 contrastive_loss: 4.5682 
2024-05-15 13:46:50.943 | INFO     | __main__:train:123 - Epoch: [40][280/390]	 loss 4.00721	 cls_loss: 0.4521 cluster_loss: 0.9096 sup_con_loss: 0.8198 contrastive_loss: 4.5705 
2024-05-15 13:47:05.575 | INFO     | __main__:train:123 - Epoch: [40][300/390]	 loss 3.88288	 cls_loss: 0.4174 cluster_loss: 0.8689 sup_con_loss: 0.5699 contrastive_loss: 4.5731 
2024-05-15 13:47:20.042 | INFO     | __main__:train:123 - Epoch: [40][320/390]	 loss 4.08295	 cls_loss: 0.3552 cluster_loss: 0.9843 sup_con_loss: 0.9790 contrastive_loss: 4.5787 
2024-05-15 13:47:34.537 | INFO     | __main__:train:123 - Epoch: [40][340/390]	 loss 3.96139	 cls_loss: 0.3320 cluster_loss: 0.9231 sup_con_loss: 0.7709 contrastive_loss: 4.5775 
2024-05-15 13:47:49.100 | INFO     | __main__:train:123 - Epoch: [40][360/390]	 loss 3.96788	 cls_loss: 0.4272 cluster_loss: 0.9326 sup_con_loss: 0.6769 contrastive_loss: 4.5773 
2024-05-15 13:48:02.921 | INFO     | __main__:train:123 - Epoch: [40][380/390]	 loss 3.93184	 cls_loss: 0.2896 cluster_loss: 0.8950 sup_con_loss: 0.7858 contrastive_loss: 4.5749 
2024-05-15 13:48:09.282 | INFO     | __main__:train:126 - Train Epoch: 40 Avg Loss: 3.9656 
2024-05-15 13:48:09.283 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:48:36.960 | INFO     | __main__:train:135 - Train Accuracies: All 0.8056 | Old 0.8494 | New 0.6305
2024-05-15 13:48:40.357 | INFO     | __main__:train:123 - Epoch: [41][0/390]	 loss 4.01590	 cls_loss: 0.3990 cluster_loss: 0.8961 sup_con_loss: 0.9244 contrastive_loss: 4.5696 
2024-05-15 13:48:54.860 | INFO     | __main__:train:123 - Epoch: [41][20/390]	 loss 4.00890	 cls_loss: 0.4353 cluster_loss: 1.0050 sup_con_loss: 0.6504 contrastive_loss: 4.5779 
2024-05-15 13:49:09.187 | INFO     | __main__:train:123 - Epoch: [41][40/390]	 loss 4.04882	 cls_loss: 0.3973 cluster_loss: 0.9844 sup_con_loss: 0.8520 contrastive_loss: 4.5718 
2024-05-15 13:49:23.589 | INFO     | __main__:train:123 - Epoch: [41][60/390]	 loss 4.00999	 cls_loss: 0.4911 cluster_loss: 0.9351 sup_con_loss: 0.7373 contrastive_loss: 4.5726 
2024-05-15 13:49:37.984 | INFO     | __main__:train:123 - Epoch: [41][80/390]	 loss 4.10060	 cls_loss: 0.4257 cluster_loss: 1.0030 sup_con_loss: 0.9307 contrastive_loss: 4.5753 
2024-05-15 13:49:52.322 | INFO     | __main__:train:123 - Epoch: [41][100/390]	 loss 4.07177	 cls_loss: 0.3837 cluster_loss: 0.9905 sup_con_loss: 0.9142 contrastive_loss: 4.5748 
2024-05-15 13:50:06.913 | INFO     | __main__:train:123 - Epoch: [41][120/390]	 loss 3.88023	 cls_loss: 0.3179 cluster_loss: 0.8510 sup_con_loss: 0.6978 contrastive_loss: 4.5717 
2024-05-15 13:50:21.477 | INFO     | __main__:train:123 - Epoch: [41][140/390]	 loss 3.94230	 cls_loss: 0.4280 cluster_loss: 0.8504 sup_con_loss: 0.7709 contrastive_loss: 4.5691 
2024-05-15 13:50:35.916 | INFO     | __main__:train:123 - Epoch: [41][160/390]	 loss 3.97159	 cls_loss: 0.3948 cluster_loss: 0.9529 sup_con_loss: 0.6826 contrastive_loss: 4.5772 
2024-05-15 13:50:50.463 | INFO     | __main__:train:123 - Epoch: [41][180/390]	 loss 3.89087	 cls_loss: 0.3827 cluster_loss: 0.8662 sup_con_loss: 0.6460 contrastive_loss: 4.5658 
2024-05-15 13:51:04.731 | INFO     | __main__:train:123 - Epoch: [41][200/390]	 loss 3.93339	 cls_loss: 0.3083 cluster_loss: 0.9205 sup_con_loss: 0.7296 contrastive_loss: 4.5720 
2024-05-15 13:51:19.103 | INFO     | __main__:train:123 - Epoch: [41][220/390]	 loss 4.06789	 cls_loss: 0.5004 cluster_loss: 0.9777 sup_con_loss: 0.8170 contrastive_loss: 4.5713 
2024-05-15 13:51:33.546 | INFO     | __main__:train:123 - Epoch: [41][240/390]	 loss 4.06490	 cls_loss: 0.5429 cluster_loss: 1.0707 sup_con_loss: 0.5809 contrastive_loss: 4.5779 
2024-05-15 13:51:48.084 | INFO     | __main__:train:123 - Epoch: [41][260/390]	 loss 3.97751	 cls_loss: 0.4815 cluster_loss: 0.9100 sup_con_loss: 0.7091 contrastive_loss: 4.5681 
2024-05-15 13:52:02.492 | INFO     | __main__:train:123 - Epoch: [41][280/390]	 loss 4.00839	 cls_loss: 0.4514 cluster_loss: 0.9820 sup_con_loss: 0.6727 contrastive_loss: 4.5795 
2024-05-15 13:52:17.000 | INFO     | __main__:train:123 - Epoch: [41][300/390]	 loss 3.92579	 cls_loss: 0.3390 cluster_loss: 0.8353 sup_con_loss: 0.8392 contrastive_loss: 4.5700 
2024-05-15 13:52:31.482 | INFO     | __main__:train:123 - Epoch: [41][320/390]	 loss 4.03882	 cls_loss: 0.3557 cluster_loss: 0.9520 sup_con_loss: 0.9218 contrastive_loss: 4.5737 
2024-05-15 13:52:45.869 | INFO     | __main__:train:123 - Epoch: [41][340/390]	 loss 4.02344	 cls_loss: 0.3963 cluster_loss: 1.0261 sup_con_loss: 0.6855 contrastive_loss: 4.5814 
2024-05-15 13:53:00.112 | INFO     | __main__:train:123 - Epoch: [41][360/390]	 loss 3.98627	 cls_loss: 0.4328 cluster_loss: 0.8786 sup_con_loss: 0.8310 contrastive_loss: 4.5736 
2024-05-15 13:53:14.160 | INFO     | __main__:train:123 - Epoch: [41][380/390]	 loss 3.83553	 cls_loss: 0.3363 cluster_loss: 0.8241 sup_con_loss: 0.6026 contrastive_loss: 4.5712 
2024-05-15 13:53:20.375 | INFO     | __main__:train:126 - Train Epoch: 41 Avg Loss: 3.9653 
2024-05-15 13:53:20.376 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:53:47.834 | INFO     | __main__:train:135 - Train Accuracies: All 0.8058 | Old 0.8495 | New 0.6310
2024-05-15 13:53:51.936 | INFO     | __main__:train:123 - Epoch: [42][0/390]	 loss 3.84690	 cls_loss: 0.3046 cluster_loss: 0.8579 sup_con_loss: 0.6107 contrastive_loss: 4.5676 
2024-05-15 13:54:06.435 | INFO     | __main__:train:123 - Epoch: [42][20/390]	 loss 3.97498	 cls_loss: 0.4242 cluster_loss: 0.8699 sup_con_loss: 0.8363 contrastive_loss: 4.5667 
2024-05-15 13:54:20.799 | INFO     | __main__:train:123 - Epoch: [42][40/390]	 loss 3.90037	 cls_loss: 0.3406 cluster_loss: 0.9378 sup_con_loss: 0.5736 contrastive_loss: 4.5705 
2024-05-15 13:54:35.157 | INFO     | __main__:train:123 - Epoch: [42][60/390]	 loss 3.86431	 cls_loss: 0.3110 cluster_loss: 0.8320 sup_con_loss: 0.6987 contrastive_loss: 4.5694 
2024-05-15 13:54:49.588 | INFO     | __main__:train:123 - Epoch: [42][80/390]	 loss 3.99599	 cls_loss: 0.3681 cluster_loss: 1.0017 sup_con_loss: 0.6915 contrastive_loss: 4.5754 
2024-05-15 13:55:03.950 | INFO     | __main__:train:123 - Epoch: [42][100/390]	 loss 3.94242	 cls_loss: 0.3862 cluster_loss: 0.9013 sup_con_loss: 0.7096 contrastive_loss: 4.5740 
2024-05-15 13:55:18.319 | INFO     | __main__:train:123 - Epoch: [42][120/390]	 loss 3.82905	 cls_loss: 0.3462 cluster_loss: 0.8184 sup_con_loss: 0.5888 contrastive_loss: 4.5690 
2024-05-15 13:55:32.881 | INFO     | __main__:train:123 - Epoch: [42][140/390]	 loss 4.00056	 cls_loss: 0.3883 cluster_loss: 0.8944 sup_con_loss: 0.8935 contrastive_loss: 4.5701 
2024-05-15 13:55:47.462 | INFO     | __main__:train:123 - Epoch: [42][160/390]	 loss 4.07495	 cls_loss: 0.3610 cluster_loss: 1.0193 sup_con_loss: 0.8960 contrastive_loss: 4.5729 
2024-05-15 13:56:01.967 | INFO     | __main__:train:123 - Epoch: [42][180/390]	 loss 3.99137	 cls_loss: 0.3274 cluster_loss: 0.9272 sup_con_loss: 0.8695 contrastive_loss: 4.5688 
2024-05-15 13:56:16.442 | INFO     | __main__:train:123 - Epoch: [42][200/390]	 loss 3.99433	 cls_loss: 0.4404 cluster_loss: 0.9107 sup_con_loss: 0.7771 contrastive_loss: 4.5789 
2024-05-15 13:56:31.025 | INFO     | __main__:train:123 - Epoch: [42][220/390]	 loss 3.97869	 cls_loss: 0.3334 cluster_loss: 0.9180 sup_con_loss: 0.8225 contrastive_loss: 4.5807 
2024-05-15 13:56:45.291 | INFO     | __main__:train:123 - Epoch: [42][240/390]	 loss 3.97142	 cls_loss: 0.3110 cluster_loss: 0.9331 sup_con_loss: 0.8221 contrastive_loss: 4.5667 
2024-05-15 13:56:59.835 | INFO     | __main__:train:123 - Epoch: [42][260/390]	 loss 3.92456	 cls_loss: 0.3665 cluster_loss: 0.8855 sup_con_loss: 0.7048 contrastive_loss: 4.5754 
2024-05-15 13:57:14.445 | INFO     | __main__:train:123 - Epoch: [42][280/390]	 loss 4.07603	 cls_loss: 0.3785 cluster_loss: 0.9576 sup_con_loss: 0.9960 contrastive_loss: 4.5732 
2024-05-15 13:57:29.030 | INFO     | __main__:train:123 - Epoch: [42][300/390]	 loss 3.83775	 cls_loss: 0.3560 cluster_loss: 0.8236 sup_con_loss: 0.5909 contrastive_loss: 4.5707 
2024-05-15 13:57:43.628 | INFO     | __main__:train:123 - Epoch: [42][320/390]	 loss 3.91123	 cls_loss: 0.4462 cluster_loss: 0.8575 sup_con_loss: 0.6479 contrastive_loss: 4.5706 
2024-05-15 13:57:58.163 | INFO     | __main__:train:123 - Epoch: [42][340/390]	 loss 3.85409	 cls_loss: 0.3136 cluster_loss: 0.8882 sup_con_loss: 0.5557 contrastive_loss: 4.5731 
2024-05-15 13:58:12.751 | INFO     | __main__:train:123 - Epoch: [42][360/390]	 loss 3.86741	 cls_loss: 0.2927 cluster_loss: 0.8183 sup_con_loss: 0.7603 contrastive_loss: 4.5646 
2024-05-15 13:58:27.002 | INFO     | __main__:train:123 - Epoch: [42][380/390]	 loss 3.91564	 cls_loss: 0.3429 cluster_loss: 0.8454 sup_con_loss: 0.7868 contrastive_loss: 4.5704 
2024-05-15 13:58:33.373 | INFO     | __main__:train:126 - Train Epoch: 42 Avg Loss: 3.9592 
2024-05-15 13:58:33.374 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:59:01.025 | INFO     | __main__:train:135 - Train Accuracies: All 0.8049 | Old 0.8478 | New 0.6335
2024-05-15 13:59:04.730 | INFO     | __main__:train:123 - Epoch: [43][0/390]	 loss 3.96762	 cls_loss: 0.4394 cluster_loss: 0.8528 sup_con_loss: 0.8263 contrastive_loss: 4.5696 
2024-05-15 13:59:19.381 | INFO     | __main__:train:123 - Epoch: [43][20/390]	 loss 4.19745	 cls_loss: 0.4641 cluster_loss: 1.0764 sup_con_loss: 1.0272 contrastive_loss: 4.5782 
2024-05-15 13:59:33.797 | INFO     | __main__:train:123 - Epoch: [43][40/390]	 loss 3.98329	 cls_loss: 0.3890 cluster_loss: 0.9285 sup_con_loss: 0.7659 contrastive_loss: 4.5778 
2024-05-15 13:59:48.150 | INFO     | __main__:train:123 - Epoch: [43][60/390]	 loss 4.06924	 cls_loss: 0.3890 cluster_loss: 0.9661 sup_con_loss: 0.9443 contrastive_loss: 4.5764 
2024-05-15 14:00:02.631 | INFO     | __main__:train:123 - Epoch: [43][80/390]	 loss 3.87462	 cls_loss: 0.3182 cluster_loss: 0.8489 sup_con_loss: 0.6918 contrastive_loss: 4.5682 
2024-05-15 14:00:16.936 | INFO     | __main__:train:123 - Epoch: [43][100/390]	 loss 3.91594	 cls_loss: 0.3884 cluster_loss: 0.8752 sup_con_loss: 0.6923 contrastive_loss: 4.5674 
2024-05-15 14:00:31.378 | INFO     | __main__:train:123 - Epoch: [43][120/390]	 loss 3.95716	 cls_loss: 0.4841 cluster_loss: 0.8857 sup_con_loss: 0.6858 contrastive_loss: 4.5723 
2024-05-15 14:00:45.831 | INFO     | __main__:train:123 - Epoch: [43][140/390]	 loss 4.00958	 cls_loss: 0.3379 cluster_loss: 0.9559 sup_con_loss: 0.8565 contrastive_loss: 4.5695 
2024-05-15 14:01:00.244 | INFO     | __main__:train:123 - Epoch: [43][160/390]	 loss 4.03606	 cls_loss: 0.4524 cluster_loss: 0.9073 sup_con_loss: 0.9025 contrastive_loss: 4.5724 
2024-05-15 14:01:14.807 | INFO     | __main__:train:123 - Epoch: [43][180/390]	 loss 4.07638	 cls_loss: 0.2968 cluster_loss: 1.0810 sup_con_loss: 0.8444 contrastive_loss: 4.5758 
2024-05-15 14:01:29.121 | INFO     | __main__:train:123 - Epoch: [43][200/390]	 loss 3.94124	 cls_loss: 0.3565 cluster_loss: 0.9191 sup_con_loss: 0.7038 contrastive_loss: 4.5734 
2024-05-15 14:01:43.601 | INFO     | __main__:train:123 - Epoch: [43][220/390]	 loss 3.98666	 cls_loss: 0.3574 cluster_loss: 0.9042 sup_con_loss: 0.8734 contrastive_loss: 4.5664 
2024-05-15 14:01:58.020 | INFO     | __main__:train:123 - Epoch: [43][240/390]	 loss 3.99237	 cls_loss: 0.3358 cluster_loss: 0.9286 sup_con_loss: 0.8576 contrastive_loss: 4.5709 
2024-05-15 14:02:12.453 | INFO     | __main__:train:123 - Epoch: [43][260/390]	 loss 3.89522	 cls_loss: 0.5027 cluster_loss: 0.7644 sup_con_loss: 0.7234 contrastive_loss: 4.5680 
2024-05-15 14:02:26.734 | INFO     | __main__:train:123 - Epoch: [43][280/390]	 loss 3.94259	 cls_loss: 0.4214 cluster_loss: 0.8599 sup_con_loss: 0.7598 contrastive_loss: 4.5696 
2024-05-15 14:02:41.366 | INFO     | __main__:train:123 - Epoch: [43][300/390]	 loss 3.92700	 cls_loss: 0.3721 cluster_loss: 0.8634 sup_con_loss: 0.7580 contrastive_loss: 4.5696 
2024-05-15 14:02:55.774 | INFO     | __main__:train:123 - Epoch: [43][320/390]	 loss 3.90563	 cls_loss: 0.2547 cluster_loss: 0.8879 sup_con_loss: 0.7596 contrastive_loss: 4.5746 
2024-05-15 14:03:10.209 | INFO     | __main__:train:123 - Epoch: [43][340/390]	 loss 3.90215	 cls_loss: 0.2647 cluster_loss: 0.9237 sup_con_loss: 0.6733 contrastive_loss: 4.5745 
2024-05-15 14:03:24.528 | INFO     | __main__:train:123 - Epoch: [43][360/390]	 loss 3.98780	 cls_loss: 0.3468 cluster_loss: 0.9086 sup_con_loss: 0.8719 contrastive_loss: 4.5703 
2024-05-15 14:03:38.467 | INFO     | __main__:train:123 - Epoch: [43][380/390]	 loss 3.93549	 cls_loss: 0.3044 cluster_loss: 0.9572 sup_con_loss: 0.6694 contrastive_loss: 4.5731 
2024-05-15 14:03:44.590 | INFO     | __main__:train:126 - Train Epoch: 43 Avg Loss: 3.9532 
2024-05-15 14:03:44.591 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:04:12.566 | INFO     | __main__:train:135 - Train Accuracies: All 0.8066 | Old 0.8454 | New 0.6515
2024-05-15 14:04:16.601 | INFO     | __main__:train:123 - Epoch: [44][0/390]	 loss 3.83560	 cls_loss: 0.3708 cluster_loss: 0.8446 sup_con_loss: 0.5391 contrastive_loss: 4.5664 
2024-05-15 14:04:30.712 | INFO     | __main__:train:123 - Epoch: [44][20/390]	 loss 3.81401	 cls_loss: 0.2960 cluster_loss: 0.7949 sup_con_loss: 0.6464 contrastive_loss: 4.5654 
2024-05-15 14:04:45.071 | INFO     | __main__:train:123 - Epoch: [44][40/390]	 loss 4.01067	 cls_loss: 0.3824 cluster_loss: 0.9387 sup_con_loss: 0.8489 contrastive_loss: 4.5685 
2024-05-15 14:04:59.139 | INFO     | __main__:train:123 - Epoch: [44][60/390]	 loss 3.92889	 cls_loss: 0.3517 cluster_loss: 0.8975 sup_con_loss: 0.7180 contrastive_loss: 4.5710 
2024-05-15 14:05:13.614 | INFO     | __main__:train:123 - Epoch: [44][80/390]	 loss 3.96948	 cls_loss: 0.3730 cluster_loss: 0.9647 sup_con_loss: 0.6795 contrastive_loss: 4.5755 
2024-05-15 14:05:27.715 | INFO     | __main__:train:123 - Epoch: [44][100/390]	 loss 3.78977	 cls_loss: 0.3381 cluster_loss: 0.8413 sup_con_loss: 0.4429 contrastive_loss: 4.5686 
2024-05-15 14:05:42.209 | INFO     | __main__:train:123 - Epoch: [44][120/390]	 loss 4.00048	 cls_loss: 0.3571 cluster_loss: 0.8639 sup_con_loss: 0.9804 contrastive_loss: 4.5706 
2024-05-15 14:05:56.648 | INFO     | __main__:train:123 - Epoch: [44][140/390]	 loss 3.95611	 cls_loss: 0.3938 cluster_loss: 0.8882 sup_con_loss: 0.7577 contrastive_loss: 4.5780 
2024-05-15 14:06:11.207 | INFO     | __main__:train:123 - Epoch: [44][160/390]	 loss 3.99798	 cls_loss: 0.3172 cluster_loss: 0.9657 sup_con_loss: 0.8194 contrastive_loss: 4.5730 
2024-05-15 14:06:25.543 | INFO     | __main__:train:123 - Epoch: [44][180/390]	 loss 3.86271	 cls_loss: 0.2407 cluster_loss: 0.8453 sup_con_loss: 0.7402 contrastive_loss: 4.5691 
2024-05-15 14:06:39.785 | INFO     | __main__:train:123 - Epoch: [44][200/390]	 loss 4.10191	 cls_loss: 0.4806 cluster_loss: 1.0938 sup_con_loss: 0.6979 contrastive_loss: 4.5822 
2024-05-15 14:06:54.286 | INFO     | __main__:train:123 - Epoch: [44][220/390]	 loss 3.97806	 cls_loss: 0.4050 cluster_loss: 0.9274 sup_con_loss: 0.7458 contrastive_loss: 4.5730 
2024-05-15 14:07:08.668 | INFO     | __main__:train:123 - Epoch: [44][240/390]	 loss 4.07710	 cls_loss: 0.4132 cluster_loss: 0.9240 sup_con_loss: 1.0207 contrastive_loss: 4.5764 
2024-05-15 14:07:23.055 | INFO     | __main__:train:123 - Epoch: [44][260/390]	 loss 4.05045	 cls_loss: 0.4172 cluster_loss: 0.9626 sup_con_loss: 0.8756 contrastive_loss: 4.5727 
2024-05-15 14:07:37.414 | INFO     | __main__:train:123 - Epoch: [44][280/390]	 loss 3.97178	 cls_loss: 0.4304 cluster_loss: 0.8850 sup_con_loss: 0.7866 contrastive_loss: 4.5701 
2024-05-15 14:07:51.944 | INFO     | __main__:train:123 - Epoch: [44][300/390]	 loss 3.95588	 cls_loss: 0.3368 cluster_loss: 0.8946 sup_con_loss: 0.8118 contrastive_loss: 4.5729 
2024-05-15 14:08:06.035 | INFO     | __main__:train:123 - Epoch: [44][320/390]	 loss 3.97941	 cls_loss: 0.3223 cluster_loss: 0.9010 sup_con_loss: 0.8818 contrastive_loss: 4.5728 
2024-05-15 14:08:20.375 | INFO     | __main__:train:123 - Epoch: [44][340/390]	 loss 3.90730	 cls_loss: 0.3410 cluster_loss: 0.9022 sup_con_loss: 0.6531 contrastive_loss: 4.5737 
2024-05-15 14:08:34.710 | INFO     | __main__:train:123 - Epoch: [44][360/390]	 loss 4.03045	 cls_loss: 0.4794 cluster_loss: 0.9377 sup_con_loss: 0.8041 contrastive_loss: 4.5719 
2024-05-15 14:08:48.833 | INFO     | __main__:train:123 - Epoch: [44][380/390]	 loss 3.92011	 cls_loss: 0.3996 cluster_loss: 0.8178 sup_con_loss: 0.8022 contrastive_loss: 4.5660 
2024-05-15 14:08:55.080 | INFO     | __main__:train:126 - Train Epoch: 44 Avg Loss: 3.9411 
2024-05-15 14:08:55.081 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:09:22.625 | INFO     | __main__:train:135 - Train Accuracies: All 0.8023 | Old 0.8441 | New 0.6350
2024-05-15 14:09:26.478 | INFO     | __main__:train:123 - Epoch: [45][0/390]	 loss 3.81937	 cls_loss: 0.3240 cluster_loss: 0.8596 sup_con_loss: 0.5011 contrastive_loss: 4.5721 
2024-05-15 14:09:40.809 | INFO     | __main__:train:123 - Epoch: [45][20/390]	 loss 4.00918	 cls_loss: 0.4683 cluster_loss: 0.9565 sup_con_loss: 0.7074 contrastive_loss: 4.5784 
2024-05-15 14:09:55.151 | INFO     | __main__:train:123 - Epoch: [45][40/390]	 loss 3.96218	 cls_loss: 0.3824 cluster_loss: 0.8645 sup_con_loss: 0.8539 contrastive_loss: 4.5655 
2024-05-15 14:10:09.628 | INFO     | __main__:train:123 - Epoch: [45][60/390]	 loss 3.94480	 cls_loss: 0.3572 cluster_loss: 0.8184 sup_con_loss: 0.9016 contrastive_loss: 4.5727 
2024-05-15 14:10:23.987 | INFO     | __main__:train:123 - Epoch: [45][80/390]	 loss 3.85585	 cls_loss: 0.3933 cluster_loss: 0.8758 sup_con_loss: 0.5034 contrastive_loss: 4.5734 
2024-05-15 14:10:38.325 | INFO     | __main__:train:123 - Epoch: [45][100/390]	 loss 3.93987	 cls_loss: 0.3963 cluster_loss: 0.8417 sup_con_loss: 0.8139 contrastive_loss: 4.5681 
2024-05-15 14:10:52.789 | INFO     | __main__:train:123 - Epoch: [45][120/390]	 loss 3.98682	 cls_loss: 0.2904 cluster_loss: 1.0227 sup_con_loss: 0.7033 contrastive_loss: 4.5758 
2024-05-15 14:11:07.157 | INFO     | __main__:train:123 - Epoch: [45][140/390]	 loss 3.86202	 cls_loss: 0.2701 cluster_loss: 0.8353 sup_con_loss: 0.7300 contrastive_loss: 4.5678 
2024-05-15 14:11:21.665 | INFO     | __main__:train:123 - Epoch: [45][160/390]	 loss 3.93395	 cls_loss: 0.3324 cluster_loss: 0.8804 sup_con_loss: 0.7838 contrastive_loss: 4.5708 
2024-05-15 14:11:35.990 | INFO     | __main__:train:123 - Epoch: [45][180/390]	 loss 3.98179	 cls_loss: 0.3282 cluster_loss: 0.9149 sup_con_loss: 0.8636 contrastive_loss: 4.5692 
2024-05-15 14:11:50.323 | INFO     | __main__:train:123 - Epoch: [45][200/390]	 loss 3.99243	 cls_loss: 0.3910 cluster_loss: 0.9450 sup_con_loss: 0.7699 contrastive_loss: 4.5720 
2024-05-15 14:12:04.659 | INFO     | __main__:train:123 - Epoch: [45][220/390]	 loss 4.02671	 cls_loss: 0.3996 cluster_loss: 0.9331 sup_con_loss: 0.8861 contrastive_loss: 4.5695 
2024-05-15 14:12:19.008 | INFO     | __main__:train:123 - Epoch: [45][240/390]	 loss 3.97900	 cls_loss: 0.3523 cluster_loss: 0.9168 sup_con_loss: 0.8152 contrastive_loss: 4.5761 
2024-05-15 14:12:33.354 | INFO     | __main__:train:123 - Epoch: [45][260/390]	 loss 4.07403	 cls_loss: 0.3846 cluster_loss: 1.0535 sup_con_loss: 0.7936 contrastive_loss: 4.5798 
2024-05-15 14:12:47.729 | INFO     | __main__:train:123 - Epoch: [45][280/390]	 loss 3.92946	 cls_loss: 0.2690 cluster_loss: 0.8811 sup_con_loss: 0.8407 contrastive_loss: 4.5667 
2024-05-15 14:13:02.212 | INFO     | __main__:train:123 - Epoch: [45][300/390]	 loss 3.81251	 cls_loss: 0.2998 cluster_loss: 0.7742 sup_con_loss: 0.6721 contrastive_loss: 4.5679 
2024-05-15 14:13:16.779 | INFO     | __main__:train:123 - Epoch: [45][320/390]	 loss 3.98280	 cls_loss: 0.3692 cluster_loss: 0.9084 sup_con_loss: 0.8302 contrastive_loss: 4.5731 
2024-05-15 14:13:31.087 | INFO     | __main__:train:123 - Epoch: [45][340/390]	 loss 3.77917	 cls_loss: 0.2364 cluster_loss: 0.8388 sup_con_loss: 0.5253 contrastive_loss: 4.5652 
2024-05-15 14:13:45.612 | INFO     | __main__:train:123 - Epoch: [45][360/390]	 loss 3.89787	 cls_loss: 0.2870 cluster_loss: 0.8700 sup_con_loss: 0.7328 contrastive_loss: 4.5776 
2024-05-15 14:13:59.467 | INFO     | __main__:train:123 - Epoch: [45][380/390]	 loss 4.00213	 cls_loss: 0.3226 cluster_loss: 0.9604 sup_con_loss: 0.8378 contrastive_loss: 4.5719 
2024-05-15 14:14:05.614 | INFO     | __main__:train:126 - Train Epoch: 45 Avg Loss: 3.9532 
2024-05-15 14:14:05.615 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:14:33.563 | INFO     | __main__:train:135 - Train Accuracies: All 0.8064 | Old 0.8490 | New 0.6360
2024-05-15 14:14:37.498 | INFO     | __main__:train:123 - Epoch: [46][0/390]	 loss 3.93548	 cls_loss: 0.3274 cluster_loss: 0.9852 sup_con_loss: 0.5977 contrastive_loss: 4.5712 
2024-05-15 14:14:52.016 | INFO     | __main__:train:123 - Epoch: [46][20/390]	 loss 3.88713	 cls_loss: 0.3830 cluster_loss: 0.8635 sup_con_loss: 0.6401 contrastive_loss: 4.5658 
2024-05-15 14:15:06.338 | INFO     | __main__:train:123 - Epoch: [46][40/390]	 loss 4.06945	 cls_loss: 0.3808 cluster_loss: 1.0334 sup_con_loss: 0.8285 contrastive_loss: 4.5761 
2024-05-15 14:15:20.906 | INFO     | __main__:train:123 - Epoch: [46][60/390]	 loss 3.91919	 cls_loss: 0.3344 cluster_loss: 0.8294 sup_con_loss: 0.8427 contrastive_loss: 4.5663 
2024-05-15 14:15:35.357 | INFO     | __main__:train:123 - Epoch: [46][80/390]	 loss 3.90354	 cls_loss: 0.2744 cluster_loss: 0.9327 sup_con_loss: 0.6443 contrastive_loss: 4.5781 
2024-05-15 14:15:49.847 | INFO     | __main__:train:123 - Epoch: [46][100/390]	 loss 4.01120	 cls_loss: 0.4002 cluster_loss: 0.9969 sup_con_loss: 0.7115 contrastive_loss: 4.5755 
2024-05-15 14:16:04.423 | INFO     | __main__:train:123 - Epoch: [46][120/390]	 loss 3.93190	 cls_loss: 0.3879 cluster_loss: 0.9349 sup_con_loss: 0.6236 contrastive_loss: 4.5696 
2024-05-15 14:16:18.818 | INFO     | __main__:train:123 - Epoch: [46][140/390]	 loss 3.89636	 cls_loss: 0.3005 cluster_loss: 0.9066 sup_con_loss: 0.6631 contrastive_loss: 4.5690 
2024-05-15 14:16:33.261 | INFO     | __main__:train:123 - Epoch: [46][160/390]	 loss 4.01543	 cls_loss: 0.3867 cluster_loss: 0.9657 sup_con_loss: 0.7982 contrastive_loss: 4.5738 
2024-05-15 14:16:47.749 | INFO     | __main__:train:123 - Epoch: [46][180/390]	 loss 4.03702	 cls_loss: 0.4330 cluster_loss: 0.9610 sup_con_loss: 0.8247 contrastive_loss: 4.5726 
2024-05-15 14:17:02.147 | INFO     | __main__:train:123 - Epoch: [46][200/390]	 loss 4.05735	 cls_loss: 0.4106 cluster_loss: 0.9668 sup_con_loss: 0.8895 contrastive_loss: 4.5752 
2024-05-15 14:17:16.380 | INFO     | __main__:train:123 - Epoch: [46][220/390]	 loss 3.84490	 cls_loss: 0.3445 cluster_loss: 0.8420 sup_con_loss: 0.5957 contrastive_loss: 4.5670 
2024-05-15 14:17:30.953 | INFO     | __main__:train:123 - Epoch: [46][240/390]	 loss 3.93367	 cls_loss: 0.3767 cluster_loss: 0.8623 sup_con_loss: 0.7705 contrastive_loss: 4.5718 
2024-05-15 14:17:45.605 | INFO     | __main__:train:123 - Epoch: [46][260/390]	 loss 3.78375	 cls_loss: 0.2786 cluster_loss: 0.7770 sup_con_loss: 0.6128 contrastive_loss: 4.5642 
2024-05-15 14:18:00.099 | INFO     | __main__:train:123 - Epoch: [46][280/390]	 loss 3.92247	 cls_loss: 0.2290 cluster_loss: 0.9261 sup_con_loss: 0.7678 contrastive_loss: 4.5718 
2024-05-15 14:18:14.539 | INFO     | __main__:train:123 - Epoch: [46][300/390]	 loss 3.86839	 cls_loss: 0.3469 cluster_loss: 0.9235 sup_con_loss: 0.5061 contrastive_loss: 4.5685 
2024-05-15 14:18:29.025 | INFO     | __main__:train:123 - Epoch: [46][320/390]	 loss 4.01924	 cls_loss: 0.3420 cluster_loss: 1.0101 sup_con_loss: 0.7646 contrastive_loss: 4.5775 
2024-05-15 14:18:43.566 | INFO     | __main__:train:123 - Epoch: [46][340/390]	 loss 3.94303	 cls_loss: 0.3858 cluster_loss: 0.8695 sup_con_loss: 0.7850 contrastive_loss: 4.5663 
2024-05-15 14:18:58.090 | INFO     | __main__:train:123 - Epoch: [46][360/390]	 loss 3.83165	 cls_loss: 0.3785 cluster_loss: 0.8088 sup_con_loss: 0.5843 contrastive_loss: 4.5676 
2024-05-15 14:19:12.252 | INFO     | __main__:train:123 - Epoch: [46][380/390]	 loss 4.07050	 cls_loss: 0.3997 cluster_loss: 0.9261 sup_con_loss: 1.0339 contrastive_loss: 4.5643 
2024-05-15 14:19:18.601 | INFO     | __main__:train:126 - Train Epoch: 46 Avg Loss: 3.9388 
2024-05-15 14:19:18.602 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:19:46.407 | INFO     | __main__:train:135 - Train Accuracies: All 0.8038 | Old 0.8469 | New 0.6315
2024-05-15 14:19:50.926 | INFO     | __main__:train:123 - Epoch: [47][0/390]	 loss 4.06847	 cls_loss: 0.3905 cluster_loss: 0.9776 sup_con_loss: 0.9240 contrastive_loss: 4.5737 
2024-05-15 14:20:05.326 | INFO     | __main__:train:123 - Epoch: [47][20/390]	 loss 3.92610	 cls_loss: 0.2962 cluster_loss: 0.8662 sup_con_loss: 0.8209 contrastive_loss: 4.5725 
2024-05-15 14:20:19.660 | INFO     | __main__:train:123 - Epoch: [47][40/390]	 loss 3.96621	 cls_loss: 0.4444 cluster_loss: 0.8529 sup_con_loss: 0.8139 contrastive_loss: 4.5714 
2024-05-15 14:20:34.236 | INFO     | __main__:train:123 - Epoch: [47][60/390]	 loss 3.91813	 cls_loss: 0.3871 cluster_loss: 0.9801 sup_con_loss: 0.4966 contrastive_loss: 4.5720 
2024-05-15 14:20:48.763 | INFO     | __main__:train:123 - Epoch: [47][80/390]	 loss 3.81415	 cls_loss: 0.2537 cluster_loss: 0.8150 sup_con_loss: 0.6525 contrastive_loss: 4.5650 
2024-05-15 14:21:03.238 | INFO     | __main__:train:123 - Epoch: [47][100/390]	 loss 4.01476	 cls_loss: 0.3829 cluster_loss: 0.9696 sup_con_loss: 0.7998 contrastive_loss: 4.5701 
2024-05-15 14:21:17.747 | INFO     | __main__:train:123 - Epoch: [47][120/390]	 loss 4.03119	 cls_loss: 0.3958 cluster_loss: 0.8786 sup_con_loss: 0.9920 contrastive_loss: 4.5759 
2024-05-15 14:21:32.404 | INFO     | __main__:train:123 - Epoch: [47][140/390]	 loss 3.78615	 cls_loss: 0.3235 cluster_loss: 0.7696 sup_con_loss: 0.5906 contrastive_loss: 4.5631 
2024-05-15 14:21:46.818 | INFO     | __main__:train:123 - Epoch: [47][160/390]	 loss 3.98949	 cls_loss: 0.4702 cluster_loss: 0.9395 sup_con_loss: 0.6878 contrastive_loss: 4.5746 
2024-05-15 14:22:01.234 | INFO     | __main__:train:123 - Epoch: [47][180/390]	 loss 4.06061	 cls_loss: 0.4166 cluster_loss: 1.0580 sup_con_loss: 0.7224 contrastive_loss: 4.5758 
2024-05-15 14:22:15.660 | INFO     | __main__:train:123 - Epoch: [47][200/390]	 loss 3.85908	 cls_loss: 0.3021 cluster_loss: 0.8113 sup_con_loss: 0.7403 contrastive_loss: 4.5644 
2024-05-15 14:22:30.244 | INFO     | __main__:train:123 - Epoch: [47][220/390]	 loss 4.01625	 cls_loss: 0.2862 cluster_loss: 1.0009 sup_con_loss: 0.8380 contrastive_loss: 4.5726 
2024-05-15 14:22:44.834 | INFO     | __main__:train:123 - Epoch: [47][240/390]	 loss 3.85441	 cls_loss: 0.3180 cluster_loss: 0.9053 sup_con_loss: 0.5232 contrastive_loss: 4.5716 
2024-05-15 14:22:59.363 | INFO     | __main__:train:123 - Epoch: [47][260/390]	 loss 3.87271	 cls_loss: 0.4441 cluster_loss: 0.8232 sup_con_loss: 0.6099 contrastive_loss: 4.5673 
2024-05-15 14:23:13.914 | INFO     | __main__:train:123 - Epoch: [47][280/390]	 loss 3.99837	 cls_loss: 0.2735 cluster_loss: 0.9921 sup_con_loss: 0.8183 contrastive_loss: 4.5714 
2024-05-15 14:23:28.468 | INFO     | __main__:train:123 - Epoch: [47][300/390]	 loss 3.88263	 cls_loss: 0.3103 cluster_loss: 0.8578 sup_con_loss: 0.7071 contrastive_loss: 4.5677 
2024-05-15 14:23:42.963 | INFO     | __main__:train:123 - Epoch: [47][320/390]	 loss 3.81908	 cls_loss: 0.2723 cluster_loss: 0.8049 sup_con_loss: 0.6672 contrastive_loss: 4.5647 
2024-05-15 14:23:57.721 | INFO     | __main__:train:123 - Epoch: [47][340/390]	 loss 3.98802	 cls_loss: 0.4103 cluster_loss: 0.8756 sup_con_loss: 0.8717 contrastive_loss: 4.5695 
2024-05-15 14:24:12.279 | INFO     | __main__:train:123 - Epoch: [47][360/390]	 loss 3.93895	 cls_loss: 0.3072 cluster_loss: 0.8928 sup_con_loss: 0.7923 contrastive_loss: 4.5751 
2024-05-15 14:24:26.546 | INFO     | __main__:train:123 - Epoch: [47][380/390]	 loss 3.99918	 cls_loss: 0.4097 cluster_loss: 0.9008 sup_con_loss: 0.8594 contrastive_loss: 4.5684 
2024-05-15 14:24:32.853 | INFO     | __main__:train:126 - Train Epoch: 47 Avg Loss: 3.9375 
2024-05-15 14:24:32.854 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:25:00.610 | INFO     | __main__:train:135 - Train Accuracies: All 0.8049 | Old 0.8469 | New 0.6370
2024-05-15 14:25:04.175 | INFO     | __main__:train:123 - Epoch: [48][0/390]	 loss 3.90571	 cls_loss: 0.2806 cluster_loss: 0.8787 sup_con_loss: 0.7594 contrastive_loss: 4.5701 
2024-05-15 14:25:18.940 | INFO     | __main__:train:123 - Epoch: [48][20/390]	 loss 3.88520	 cls_loss: 0.3956 cluster_loss: 0.8971 sup_con_loss: 0.5538 contrastive_loss: 4.5690 
2024-05-15 14:25:33.487 | INFO     | __main__:train:123 - Epoch: [48][40/390]	 loss 3.75853	 cls_loss: 0.2960 cluster_loss: 0.7648 sup_con_loss: 0.5522 contrastive_loss: 4.5609 
2024-05-15 14:25:48.074 | INFO     | __main__:train:123 - Epoch: [48][60/390]	 loss 3.95127	 cls_loss: 0.2924 cluster_loss: 0.8653 sup_con_loss: 0.8964 contrastive_loss: 4.5734 
2024-05-15 14:26:02.649 | INFO     | __main__:train:123 - Epoch: [48][80/390]	 loss 4.04834	 cls_loss: 0.3140 cluster_loss: 0.9762 sup_con_loss: 0.9404 contrastive_loss: 4.5765 
2024-05-15 14:26:17.182 | INFO     | __main__:train:123 - Epoch: [48][100/390]	 loss 3.88803	 cls_loss: 0.4418 cluster_loss: 0.8665 sup_con_loss: 0.5742 contrastive_loss: 4.5680 
2024-05-15 14:26:31.498 | INFO     | __main__:train:123 - Epoch: [48][120/390]	 loss 4.03168	 cls_loss: 0.4146 cluster_loss: 0.8888 sup_con_loss: 0.9661 contrastive_loss: 4.5704 
2024-05-15 14:26:46.048 | INFO     | __main__:train:123 - Epoch: [48][140/390]	 loss 3.94237	 cls_loss: 0.2687 cluster_loss: 0.9479 sup_con_loss: 0.7522 contrastive_loss: 4.5676 
2024-05-15 14:27:00.638 | INFO     | __main__:train:123 - Epoch: [48][160/390]	 loss 3.91947	 cls_loss: 0.3936 cluster_loss: 0.8407 sup_con_loss: 0.7647 contrastive_loss: 4.5656 
2024-05-15 14:27:15.109 | INFO     | __main__:train:123 - Epoch: [48][180/390]	 loss 3.94733	 cls_loss: 0.4171 cluster_loss: 0.9216 sup_con_loss: 0.6573 contrastive_loss: 4.5727 
2024-05-15 14:27:29.667 | INFO     | __main__:train:123 - Epoch: [48][200/390]	 loss 3.84047	 cls_loss: 0.2190 cluster_loss: 0.8824 sup_con_loss: 0.6220 contrastive_loss: 4.5731 
2024-05-15 14:27:44.175 | INFO     | __main__:train:123 - Epoch: [48][220/390]	 loss 3.93508	 cls_loss: 0.3084 cluster_loss: 0.8610 sup_con_loss: 0.8486 contrastive_loss: 4.5700 
2024-05-15 14:27:58.518 | INFO     | __main__:train:123 - Epoch: [48][240/390]	 loss 4.01238	 cls_loss: 0.3139 cluster_loss: 0.9173 sup_con_loss: 0.9569 contrastive_loss: 4.5713 
2024-05-15 14:28:13.003 | INFO     | __main__:train:123 - Epoch: [48][260/390]	 loss 3.98823	 cls_loss: 0.2891 cluster_loss: 0.9190 sup_con_loss: 0.8983 contrastive_loss: 4.5774 
2024-05-15 14:28:27.537 | INFO     | __main__:train:123 - Epoch: [48][280/390]	 loss 4.09127	 cls_loss: 0.3731 cluster_loss: 0.9968 sup_con_loss: 0.9761 contrastive_loss: 4.5709 
2024-05-15 14:28:42.005 | INFO     | __main__:train:123 - Epoch: [48][300/390]	 loss 4.01252	 cls_loss: 0.3598 cluster_loss: 0.9481 sup_con_loss: 0.8603 contrastive_loss: 4.5680 
2024-05-15 14:28:56.461 | INFO     | __main__:train:123 - Epoch: [48][320/390]	 loss 4.00732	 cls_loss: 0.3387 cluster_loss: 0.9492 sup_con_loss: 0.8609 contrastive_loss: 4.5700 
2024-05-15 14:29:10.847 | INFO     | __main__:train:123 - Epoch: [48][340/390]	 loss 3.84330	 cls_loss: 0.3575 cluster_loss: 0.7796 sup_con_loss: 0.6855 contrastive_loss: 4.5715 
2024-05-15 14:29:25.142 | INFO     | __main__:train:123 - Epoch: [48][360/390]	 loss 3.88722	 cls_loss: 0.2830 cluster_loss: 0.8663 sup_con_loss: 0.7242 contrastive_loss: 4.5717 
2024-05-15 14:29:38.912 | INFO     | __main__:train:123 - Epoch: [48][380/390]	 loss 3.90561	 cls_loss: 0.3093 cluster_loss: 0.8840 sup_con_loss: 0.7258 contrastive_loss: 4.5673 
2024-05-15 14:29:45.174 | INFO     | __main__:train:126 - Train Epoch: 48 Avg Loss: 3.9388 
2024-05-15 14:29:45.175 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:30:12.736 | INFO     | __main__:train:135 - Train Accuracies: All 0.8047 | Old 0.8472 | New 0.6345
2024-05-15 14:30:15.636 | INFO     | __main__:train:123 - Epoch: [49][0/390]	 loss 3.89012	 cls_loss: 0.3956 cluster_loss: 0.8688 sup_con_loss: 0.6231 contrastive_loss: 4.5675 
2024-05-15 14:30:30.458 | INFO     | __main__:train:123 - Epoch: [49][20/390]	 loss 3.97661	 cls_loss: 0.2761 cluster_loss: 1.0048 sup_con_loss: 0.7284 contrastive_loss: 4.5722 
2024-05-15 14:30:44.964 | INFO     | __main__:train:123 - Epoch: [49][40/390]	 loss 3.88762	 cls_loss: 0.3519 cluster_loss: 0.9253 sup_con_loss: 0.5481 contrastive_loss: 4.5711 
2024-05-15 14:30:59.422 | INFO     | __main__:train:123 - Epoch: [49][60/390]	 loss 3.80234	 cls_loss: 0.2559 cluster_loss: 0.8389 sup_con_loss: 0.5650 contrastive_loss: 4.5689 
2024-05-15 14:31:13.826 | INFO     | __main__:train:123 - Epoch: [49][80/390]	 loss 3.93077	 cls_loss: 0.2517 cluster_loss: 0.9058 sup_con_loss: 0.8050 contrastive_loss: 4.5725 
2024-05-15 14:31:28.155 | INFO     | __main__:train:123 - Epoch: [49][100/390]	 loss 3.88640	 cls_loss: 0.3027 cluster_loss: 0.8814 sup_con_loss: 0.6830 contrastive_loss: 4.5669 
2024-05-15 14:31:42.720 | INFO     | __main__:train:123 - Epoch: [49][120/390]	 loss 3.89807	 cls_loss: 0.4329 cluster_loss: 0.9198 sup_con_loss: 0.5090 contrastive_loss: 4.5700 
2024-05-15 14:31:57.228 | INFO     | __main__:train:123 - Epoch: [49][140/390]	 loss 4.09639	 cls_loss: 0.3365 cluster_loss: 1.0135 sup_con_loss: 0.9788 contrastive_loss: 4.5804 
2024-05-15 14:32:11.386 | INFO     | __main__:train:123 - Epoch: [49][160/390]	 loss 3.89231	 cls_loss: 0.3449 cluster_loss: 0.8543 sup_con_loss: 0.6983 contrastive_loss: 4.5722 
2024-05-15 14:32:25.903 | INFO     | __main__:train:123 - Epoch: [49][180/390]	 loss 3.95722	 cls_loss: 0.4070 cluster_loss: 0.9035 sup_con_loss: 0.7397 contrastive_loss: 4.5671 
2024-05-15 14:32:40.148 | INFO     | __main__:train:123 - Epoch: [49][200/390]	 loss 3.88904	 cls_loss: 0.2960 cluster_loss: 0.8385 sup_con_loss: 0.7768 contrastive_loss: 4.5669 
2024-05-15 14:32:54.676 | INFO     | __main__:train:123 - Epoch: [49][220/390]	 loss 4.01365	 cls_loss: 0.3986 cluster_loss: 0.9940 sup_con_loss: 0.7398 contrastive_loss: 4.5678 
2024-05-15 14:33:09.148 | INFO     | __main__:train:123 - Epoch: [49][240/390]	 loss 3.90486	 cls_loss: 0.3394 cluster_loss: 0.9027 sup_con_loss: 0.6533 contrastive_loss: 4.5703 
2024-05-15 14:33:23.749 | INFO     | __main__:train:123 - Epoch: [49][260/390]	 loss 3.97307	 cls_loss: 0.2499 cluster_loss: 0.9344 sup_con_loss: 0.8795 contrastive_loss: 4.5699 
2024-05-15 14:33:38.016 | INFO     | __main__:train:123 - Epoch: [49][280/390]	 loss 3.86559	 cls_loss: 0.3442 cluster_loss: 0.8231 sup_con_loss: 0.6947 contrastive_loss: 4.5646 
2024-05-15 14:33:52.558 | INFO     | __main__:train:123 - Epoch: [49][300/390]	 loss 3.90105	 cls_loss: 0.3852 cluster_loss: 0.8824 sup_con_loss: 0.6386 contrastive_loss: 4.5679 
2024-05-15 14:34:06.944 | INFO     | __main__:train:123 - Epoch: [49][320/390]	 loss 4.03345	 cls_loss: 0.4679 cluster_loss: 1.0023 sup_con_loss: 0.6857 contrastive_loss: 4.5818 
2024-05-15 14:34:21.246 | INFO     | __main__:train:123 - Epoch: [49][340/390]	 loss 4.01984	 cls_loss: 0.4635 cluster_loss: 0.9271 sup_con_loss: 0.8112 contrastive_loss: 4.5708 
2024-05-15 14:34:35.765 | INFO     | __main__:train:123 - Epoch: [49][360/390]	 loss 3.92405	 cls_loss: 0.3438 cluster_loss: 0.8738 sup_con_loss: 0.7415 contrastive_loss: 4.5788 
2024-05-15 14:34:49.889 | INFO     | __main__:train:123 - Epoch: [49][380/390]	 loss 3.83836	 cls_loss: 0.2730 cluster_loss: 0.8239 sup_con_loss: 0.6792 contrastive_loss: 4.5685 
2024-05-15 14:34:56.125 | INFO     | __main__:train:126 - Train Epoch: 49 Avg Loss: 3.9391 
2024-05-15 14:34:56.126 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:35:23.518 | INFO     | __main__:train:135 - Train Accuracies: All 0.8067 | Old 0.8462 | New 0.6485
2024-05-15 14:35:27.885 | INFO     | __main__:train:123 - Epoch: [50][0/390]	 loss 4.07190	 cls_loss: 0.3014 cluster_loss: 0.9919 sup_con_loss: 0.9916 contrastive_loss: 4.5763 
2024-05-15 14:35:42.143 | INFO     | __main__:train:123 - Epoch: [50][20/390]	 loss 4.12955	 cls_loss: 0.4790 cluster_loss: 0.9831 sup_con_loss: 0.9917 contrastive_loss: 4.5781 
2024-05-15 14:35:56.438 | INFO     | __main__:train:123 - Epoch: [50][40/390]	 loss 3.86556	 cls_loss: 0.2621 cluster_loss: 0.9134 sup_con_loss: 0.5981 contrastive_loss: 4.5704 
2024-05-15 14:36:11.012 | INFO     | __main__:train:123 - Epoch: [50][60/390]	 loss 3.93039	 cls_loss: 0.3984 cluster_loss: 0.8775 sup_con_loss: 0.7148 contrastive_loss: 4.5698 
2024-05-15 14:36:25.455 | INFO     | __main__:train:123 - Epoch: [50][80/390]	 loss 3.99136	 cls_loss: 0.3355 cluster_loss: 0.9729 sup_con_loss: 0.7677 contrastive_loss: 4.5736 
2024-05-15 14:36:39.883 | INFO     | __main__:train:123 - Epoch: [50][100/390]	 loss 3.98261	 cls_loss: 0.4867 cluster_loss: 0.8630 sup_con_loss: 0.8014 contrastive_loss: 4.5705 
2024-05-15 14:36:54.494 | INFO     | __main__:train:123 - Epoch: [50][120/390]	 loss 3.86911	 cls_loss: 0.2205 cluster_loss: 0.9418 sup_con_loss: 0.5943 contrastive_loss: 4.5720 
2024-05-15 14:37:08.784 | INFO     | __main__:train:123 - Epoch: [50][140/390]	 loss 3.95054	 cls_loss: 0.3735 cluster_loss: 0.9625 sup_con_loss: 0.6332 contrastive_loss: 4.5732 
2024-05-15 14:37:23.248 | INFO     | __main__:train:123 - Epoch: [50][160/390]	 loss 3.87341	 cls_loss: 0.2379 cluster_loss: 0.8034 sup_con_loss: 0.8582 contrastive_loss: 4.5656 
2024-05-15 14:37:37.807 | INFO     | __main__:train:123 - Epoch: [50][180/390]	 loss 3.99208	 cls_loss: 0.2818 cluster_loss: 0.9877 sup_con_loss: 0.7953 contrastive_loss: 4.5740 
2024-05-15 14:37:52.259 | INFO     | __main__:train:123 - Epoch: [50][200/390]	 loss 3.91981	 cls_loss: 0.4302 cluster_loss: 0.8263 sup_con_loss: 0.7363 contrastive_loss: 4.5760 
2024-05-15 14:38:06.694 | INFO     | __main__:train:123 - Epoch: [50][220/390]	 loss 3.89224	 cls_loss: 0.3587 cluster_loss: 0.8389 sup_con_loss: 0.7184 contrastive_loss: 4.5692 
