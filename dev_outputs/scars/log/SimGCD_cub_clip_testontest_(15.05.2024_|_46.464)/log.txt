2024-05-15 09:42:46.481 | INFO     | __main__:<module>:262 - Using evaluation function v2 to print results
2024-05-15 09:42:49.312 | INFO     | __main__:<module>:320 - model build
2024-05-15 09:42:59.505 | INFO     | __main__:train:37 - [Parameter containing:
tensor([[-0.0146, -0.0039, -0.0066,  ..., -0.0070, -0.0130,  0.0021],
        [-0.0165,  0.0127,  0.0296,  ...,  0.0056,  0.0020, -0.0008],
        [ 0.0086, -0.0143, -0.0048,  ..., -0.0151, -0.0058, -0.0208],
        ...,
        [-0.0014, -0.0116, -0.0034,  ..., -0.0181,  0.0034, -0.0155],
        [ 0.0091,  0.0107, -0.0240,  ..., -0.0009,  0.0183,  0.0085],
        [ 0.0078,  0.0050, -0.0247,  ...,  0.0250,  0.0013, -0.0091]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([2.0889, 1.9712, 2.1220, 2.0694, 1.7119, 1.9933, 2.0799, 2.1845, 2.1324,
        1.9399, 1.9302, 2.1218, 2.0308, 2.0338, 2.1017, 2.0951, 2.0867, 2.0461,
        1.9855, 2.1302, 2.0887, 2.0182, 1.9796, 2.0423, 2.0565, 2.1084, 2.0156,
        2.0070, 1.9619, 1.9993, 1.9641, 2.1471, 2.1692, 2.0407, 2.0561, 1.7148,
        1.9077, 2.0416, 2.0109, 2.0489, 2.0492, 1.9716, 1.9944, 2.1945, 2.1414,
        2.0624, 2.0382, 1.7156, 2.0872, 2.1689, 1.9913, 1.9874, 2.0611, 2.0384,
        2.0404, 2.0571, 2.1749, 1.6695, 1.9704, 2.0106, 2.1707, 2.1583, 1.7716,
        2.0785, 2.2589, 1.5433, 2.0498, 2.1556, 2.2475, 1.6193, 2.0502, 1.9964,
        1.9409, 2.1102, 2.1142, 1.8969, 2.0679, 1.9978, 1.9989, 1.9519, 1.8782,
        2.0690, 1.9627, 2.0572, 2.0304, 1.9690, 2.0986, 2.1040, 2.1931, 2.0631,
        2.0917, 2.0014, 2.0774, 1.8498, 1.8968, 2.1439, 1.9327, 2.0869, 2.1567,
        1.9760, 2.1401, 1.9547, 2.2018, 1.9345, 1.9655, 1.9996, 1.9677, 1.8891,
        2.1570, 2.0854, 1.7238, 2.0024, 2.1032, 2.0581, 1.9916, 2.1147, 1.9997,
        1.9987, 2.1948, 2.0187, 2.0479, 2.0436, 2.1438, 2.0421, 2.0650, 2.0503,
        2.0932, 2.1495, 2.1429, 2.2573, 2.2058, 2.0425, 1.9905, 2.0903, 2.0920,
        2.0021, 1.9449, 1.8799, 1.9051, 2.0264, 2.0167, 2.0677, 2.1733, 2.1022,
        2.0488, 1.9244, 2.0255, 1.9785, 2.0561, 2.1360, 2.0916, 1.9735, 2.0471,
        2.0240, 2.0103, 2.0339, 2.0940, 2.1347, 1.8951, 1.9661, 2.0123, 1.6906,
        3.3178, 1.8554, 2.0427, 2.0867, 2.0731, 2.2051, 2.0521, 1.8706, 2.1517,
        2.0741, 1.9758, 2.0706, 2.1696, 2.1816, 1.6692, 1.9433, 2.1614, 1.9843,
        2.0820, 2.0234, 2.0356, 1.9761, 2.0929, 2.0717, 1.9660, 1.7830, 2.1155,
        1.8459, 2.1288, 2.1077, 1.9489, 1.8428, 2.0365, 2.0475, 1.9203, 2.0346,
        1.1667, 1.9564, 2.0408, 2.1060, 1.9239, 2.1521, 2.0335, 1.3680, 1.7952,
        2.1439, 2.1176, 1.9656, 2.1946, 2.0850, 1.9810, 2.1021, 2.0342, 2.0878,
        2.1313, 1.8232, 2.0304, 2.0742, 2.0921, 2.0573, 2.1955, 2.1340, 1.9932,
        2.0807, 2.0696, 2.0617, 2.0044, 2.0714, 2.1352, 2.1044, 1.9892, 1.9525,
        2.2479, 1.9878, 2.0778, 2.0956, 2.1284, 2.0560, 2.0330, 2.0734, 2.1860,
        1.5747, 2.0099, 2.1652, 2.0593, 1.9409, 2.1205, 2.0521, 2.1560, 2.0191,
        1.9861, 2.0404, 2.0371, 2.1400, 2.0281, 2.0016, 2.1504, 1.8761, 1.9561,
        2.0932, 2.0882, 1.9444, 1.9052, 2.0580, 1.9558, 2.0516, 1.9564, 2.0826,
        2.0919, 2.1240, 1.9847, 2.0485, 2.0422, 2.0356, 1.8621, 2.1438, 2.1475,
        1.9393, 2.0578, 2.0111, 2.2414, 2.0166, 2.0661, 2.1818, 2.1259, 2.1101,
        1.9721, 1.9622, 1.9813, 2.1706, 1.9655, 1.9623, 1.9839, 2.0574, 2.1466,
        2.1507, 2.1680, 1.4906, 2.1190, 1.9092, 2.1146, 2.0637, 2.1615, 1.9260,
        1.9775, 1.5948, 2.1446, 1.5686, 1.9936, 2.1131, 1.9389, 2.0891, 2.0374,
        2.1002, 2.0653, 1.9287, 2.0661, 1.9063, 1.8354, 2.0778, 2.1387, 2.0642,
        2.0024, 1.9499, 2.0517, 2.1462, 1.8669, 2.0599, 2.1175, 2.1816, 2.1317,
        2.0351, 2.0384, 2.2207, 2.0393, 1.9515, 2.0600, 2.0513, 2.0241, 1.4406,
        2.0965, 2.0465, 2.1819, 2.0059, 1.9621, 2.0900, 1.9422, 1.9949, 2.1607,
        2.0713, 2.0750, 1.9913, 1.9107, 2.0848, 2.0837, 2.2114, 2.1490, 2.0948,
        2.0114, 2.1234, 1.8489, 1.9168, 1.8525, 2.0741, 1.9893, 2.0585, 1.9955,
        2.1011, 1.9959, 1.7628, 2.1660, 2.0795, 2.1166, 2.0747, 1.9695, 2.0061,
        2.1132, 2.0464, 2.0033, 1.9074, 2.1169, 2.0778, 2.1615, 2.0775, 2.0127,
        2.1388, 2.0702, 1.9780, 1.9862, 2.0076, 1.9101, 2.1197, 2.0821, 2.1660,
        1.6451, 2.1807, 2.1510, 2.0006, 2.1235, 1.9724, 2.0940, 1.9664, 1.9653,
        2.0226, 2.1127, 1.6622, 2.0052, 2.0679, 2.0634, 2.0308, 1.9230, 2.3308,
        2.0401, 2.0522, 2.1555, 1.6648, 2.1525, 1.9948, 2.0326, 2.1897, 1.9837,
        2.1599, 2.0922, 2.1043, 2.1130, 2.1266, 2.0320, 1.7905, 2.1134, 1.9856,
        2.1800, 2.0651, 2.2059, 1.6809, 2.0747, 1.8652, 1.9064, 1.9484, 2.2097,
        2.1325, 1.9957, 2.1914, 2.0445, 2.0817, 2.0053, 1.9492, 1.9617, 1.7545,
        2.0156, 2.0356, 2.1489, 2.0271, 1.9697, 2.0002, 2.1147, 2.1199, 2.0303,
        1.8091, 1.9793, 2.0675, 1.9589, 1.9105, 2.1151, 2.0203, 2.0771, 1.9647,
        2.0655, 2.0748, 2.0341, 1.9724, 2.0934, 1.9961, 0.3405, 1.9650, 2.0412,
        2.1093, 2.0053, 2.0580, 2.1316, 2.1214, 1.8425, 2.1187, 1.7724, 2.0784,
        2.0464, 1.7946, 2.1334, 2.0780, 2.1812, 2.1858, 2.1440, 2.1193, 2.0504,
        1.9470, 1.9928, 1.9448, 2.1769, 2.0388, 2.1214, 2.0781, 1.8957, 1.9711,
        2.0179, 1.9501, 2.0254, 2.0303, 2.0969, 2.0911, 1.3677, 1.9383, 1.8941,
        1.9511, 1.9579, 1.9971, 2.0300, 2.0202, 1.3250, 1.6754, 2.1596, 2.0482,
        2.0071, 2.1047, 1.7998, 1.9864, 1.8247, 2.0342, 1.9502, 2.0478, 2.0405,
        1.8985, 2.0594, 2.0769, 1.8037, 2.0254, 2.1384, 2.1213, 2.1802, 2.1189,
        2.0923, 1.9126, 2.1282, 2.1499, 1.9930, 2.0616, 1.7846, 1.9661, 2.2175,
        2.1521, 2.0787, 2.1360, 1.9558, 1.9896, 2.0940, 1.9441, 2.0977, 1.9642,
        2.1160, 1.8274, 1.9144, 2.1407, 2.0104, 2.0967, 2.0736, 1.9755, 2.1098,
        1.9763, 2.0681, 2.0670, 2.1317, 2.0077, 2.0983, 2.1026, 2.0234, 2.0871,
        2.0295, 1.9842, 1.9524, 1.9992, 2.2744, 2.0831, 2.0668, 2.0192, 1.9225,
        1.9729, 1.9364, 2.1503, 2.0694, 2.0884, 2.0392, 1.9470, 2.0753, 1.9972,
        1.9704, 2.0931, 2.1665, 2.0933, 1.9537, 2.0864, 2.0178, 1.9763, 2.0968,
        1.9622, 1.9361, 2.1092, 1.9623, 2.1026, 1.9378, 2.1092, 2.0311, 1.8948,
        1.9659, 1.9941, 2.0935, 2.2296, 2.2099, 1.9437, 2.0376, 2.0139, 1.9951,
        1.9778, 2.0384, 2.0941, 2.0700, 2.0591, 2.1484, 2.0313, 2.1585, 1.9633,
        2.1104, 2.0213, 2.0351, 1.5722, 2.0605, 2.1667, 2.1725, 2.0506, 2.0942,
        2.1590, 1.8917, 2.0401, 2.0299, 2.0578, 2.0229, 2.0145, 2.1082, 2.1442,
        1.9256, 2.1155, 1.5651, 2.1935, 2.0792, 2.1289, 2.0517, 2.0612, 1.9459,
        2.2552, 1.9714, 2.0947, 2.0609, 2.0716, 1.4566, 1.9967, 2.0037, 1.8967,
        2.1645, 1.9572, 2.0952, 2.1335, 2.1983, 1.9248, 2.0946, 2.0167, 1.9967,
        2.0781, 2.0708, 2.2572, 2.0508, 2.1440, 2.1336, 2.1300, 1.9258, 2.0525,
        2.0640, 2.1309, 1.9936, 2.0384, 2.1007, 1.3213, 2.1270, 2.0090, 2.1278,
        2.0224, 2.0256, 2.1023, 1.8309, 1.9433, 1.9495, 2.0881, 2.1420, 2.1381,
        2.0138, 1.8643, 2.0748, 1.9934, 2.0834, 1.9823, 2.1178, 2.2130, 2.0325,
        1.9908, 2.1174, 2.2464, 2.0913, 2.0609, 2.0776, 2.1028, 2.0192, 2.0302,
        1.9206, 2.1787, 2.0304, 1.9062, 3.4342, 1.9552, 1.9041, 2.0857, 2.0320,
        1.9365, 1.9956, 1.9609, 1.9605, 1.6706, 2.1107, 1.9736, 2.0025, 2.0189,
        1.9959, 2.0646, 2.1581, 1.8589, 2.0009, 2.2918, 2.0199, 1.9572, 1.8224,
        1.8725, 2.0706, 2.1529, 2.1153, 2.0897, 1.9255, 2.1299, 2.1575, 2.0383,
        2.0991, 2.0648, 2.1076, 2.0169, 1.8750, 2.1080, 2.1821, 1.9574, 2.1477,
        2.0282, 1.6939, 2.0273], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 2.8504e-02, -6.6268e-02,  2.0469e-02, -2.4063e-01, -7.6139e-01,
         2.2901e-01, -1.9181e-02, -1.6229e-01,  1.4640e-01, -7.2078e-02,
        -6.4448e-02,  1.3004e-01, -2.8852e-02, -2.2445e-01, -1.5333e-01,
        -2.0017e-02, -1.4645e-02,  1.3456e-01,  1.1367e-01,  8.5705e-04,
        -1.9154e-01, -1.1060e-01,  7.7911e-02,  1.0782e-03,  1.8011e-01,
        -6.9190e-02,  8.1234e-02,  4.6244e-03,  2.0738e-01,  2.1546e-01,
        -1.2781e-01,  2.1756e-01,  1.9662e-01,  6.6177e-02, -9.6555e-02,
         3.6276e-02,  3.0880e-01, -4.2114e-02, -1.7260e-01, -3.2207e-02,
         8.0750e-02,  1.4804e-01,  1.5997e-01, -1.2746e-02, -2.0021e-01,
         2.4657e-01, -1.5625e-01,  4.1060e-01,  1.1039e-02,  1.7342e-03,
        -2.8080e-01,  7.2084e-02, -9.5679e-03,  2.8248e-01, -1.3109e-01,
        -4.9942e-02, -7.1812e-03,  2.3124e-01,  3.6284e-01,  2.3893e-01,
         4.1646e-02,  1.7018e-01,  1.5995e-01, -1.8938e-02, -1.4604e-01,
         3.8497e-01, -2.0005e-01,  1.1361e-01, -8.6606e-02,  8.1025e-02,
         1.0023e-01,  1.3133e-02,  1.4986e-01, -1.4484e-01, -1.8983e-01,
         1.2057e-01,  6.8089e-02,  1.7732e-02, -1.4275e-02,  4.1753e-01,
        -1.7650e-01, -1.2653e-01, -1.9440e-01,  3.2383e-02,  6.3769e-03,
        -6.3109e-02,  3.4128e-01,  1.1140e-01,  5.1841e-02, -7.8993e-02,
         3.1750e-01, -2.7008e-01,  2.6093e-01, -4.3071e-02, -1.6392e-01,
        -5.4121e-02, -2.5731e-01,  1.8952e-01,  8.5067e-02, -6.1931e-02,
         1.1788e-01,  3.1476e-01,  9.4151e-02, -1.6559e-01,  1.5590e-01,
         1.8307e-01,  3.4897e-01,  3.2071e-01, -9.9986e-02,  1.2352e-01,
        -3.6178e-02,  1.7537e-01, -8.1322e-02, -3.0396e-02,  5.5304e-02,
        -2.4548e-02,  8.7976e-02, -1.4368e-01,  2.1969e-02,  1.4105e-01,
        -2.4492e-02, -2.6660e-01,  1.4491e-01,  1.0095e-01,  1.1811e-02,
         7.9764e-02,  1.2176e-01, -5.9894e-02,  6.5163e-02,  2.0217e-02,
         7.2439e-02,  9.9869e-02,  2.5884e-01, -1.3904e-01, -7.1748e-02,
        -1.6325e-01, -3.4289e-01, -4.1664e-02, -2.6442e-01, -2.1141e-01,
        -3.2832e-01, -1.9760e-01,  1.9742e-01,  6.4940e-02,  1.1064e-01,
         9.8830e-02, -5.5894e-02, -7.5366e-02, -2.5354e-02,  1.8192e-01,
        -1.5770e-01, -9.7434e-02, -2.0390e-02,  1.2097e-01,  1.3304e-01,
        -2.2001e-01, -1.9000e-01,  1.0213e-01,  2.8652e-01,  7.8545e-02,
        -4.1900e-01, -1.5198e-01,  8.8589e-01,  6.4870e-02,  1.9507e-01,
         1.1244e-01,  1.3662e-01, -5.5619e-03, -1.8421e-01,  3.6384e-01,
         1.0898e-01,  1.7704e-02, -1.4382e-01,  3.1085e-02,  4.4398e-02,
         2.6900e-01,  2.5362e-01, -2.0190e-01, -2.3754e-01, -2.3838e-01,
         6.8463e-02, -5.0972e-02, -7.0668e-02, -1.4725e-01, -1.0647e-01,
        -2.4313e-01, -2.0898e-02, -6.2361e-03, -9.2734e-02,  1.2089e-01,
         1.3466e-01, -1.5528e-01,  9.1784e-02,  3.1565e-01, -8.8425e-02,
         1.1853e-01,  2.9177e-01,  4.9970e-02,  2.9435e-01, -2.6363e-02,
        -3.5723e-03,  1.1744e-02, -3.8822e-01, -6.3808e-02, -2.5130e-02,
        -2.9781e-01,  2.4423e-01,  1.2359e-01, -2.0310e-01,  1.1666e-01,
        -1.0383e-01, -2.4320e-01,  1.0476e-01, -6.4406e-02, -1.1334e-01,
        -1.6384e-01,  4.5622e-02,  4.6126e-02, -2.3101e-01, -4.2995e-02,
        -3.2435e-02,  1.8009e-01,  1.0649e-01,  1.6906e-01,  2.8517e-01,
         1.5568e-01,  1.0221e-01,  1.8800e-01,  1.2888e-01,  1.1685e-01,
        -1.8799e-02, -8.4322e-02, -2.2413e-01, -3.1196e-01,  2.1720e-01,
        -2.6198e-01, -2.5196e-02,  9.2941e-02,  6.2847e-02,  2.4234e-01,
        -3.4218e-01, -1.8627e-01,  1.3247e-01, -3.4954e-01, -3.0751e-01,
        -1.5220e-01,  1.3842e-01, -7.7362e-02, -8.6115e-02,  8.0118e-02,
        -1.3368e-01, -1.9051e-01,  1.4877e-01,  1.5263e-01,  6.0897e-02,
        -2.5285e-01,  2.9866e-01,  1.6658e-01,  1.3632e-01, -3.1249e-01,
         5.9045e-02,  1.4344e-01, -8.2676e-02,  3.4193e-01, -1.9562e-01,
        -8.8454e-03,  8.0644e-02, -1.5615e-02,  1.6132e-01,  9.9109e-03,
        -2.1831e-01,  2.0527e-04,  2.0767e-01, -1.9229e-01, -6.2333e-02,
         2.8975e-01,  2.2820e-01,  5.4081e-02, -2.8020e-02, -7.5214e-02,
         1.3673e-01,  2.2390e-01,  8.9420e-02,  2.9266e-01,  8.7919e-03,
        -5.7573e-02,  2.4258e-01,  3.9646e-02,  3.2063e-01,  2.2784e-01,
         9.1253e-02,  1.3482e-01,  5.0732e-02,  8.2332e-02, -2.5443e-01,
        -5.3945e-02,  3.5411e-02,  1.5352e-01, -3.5287e-02,  6.9929e-02,
         5.6327e-02,  2.1955e-01, -1.0062e-01, -2.5025e-02,  1.6035e-01,
        -1.7752e-01,  3.2281e-03,  3.3873e-01, -2.4688e-02, -3.2176e-01,
         1.9693e-01, -7.4028e-02, -4.5915e-02, -7.9669e-02, -1.4538e-01,
         8.4735e-02, -9.0724e-02, -1.1218e-02, -5.2998e-02, -2.8057e-01,
         2.5006e-01, -7.6606e-02,  8.9481e-02,  2.1017e-01, -1.4844e-01,
        -1.1927e-01, -1.7030e-01, -2.6984e-02, -1.2455e-01, -1.7030e-02,
        -2.3333e-03, -3.4794e-02,  1.1147e-01,  6.8249e-02,  1.1888e-01,
        -6.2072e-03, -5.5677e-02,  2.5506e-02,  1.4834e-01,  2.8043e-01,
         7.5211e-02,  3.2190e-01,  1.0136e-01,  1.3366e-01, -1.1125e-01,
        -2.2113e-02, -1.9412e-01,  5.0625e-02,  1.1823e-01, -1.0454e-01,
        -9.8780e-03,  7.1319e-02, -1.9997e-02, -1.6023e-02, -3.0358e-01,
         4.9813e-02, -3.7291e-02,  4.1195e-02,  4.0840e-02,  2.5038e-01,
        -1.3593e-01, -3.9288e-02,  1.4432e-01,  1.5827e-01,  3.7552e-01,
         5.9903e-02,  3.5905e-01,  2.4094e-01,  8.8207e-02, -4.8586e-02,
         8.4939e-03, -1.3572e-02,  1.3245e-01, -2.6827e-01,  2.1792e-01,
         1.3221e-02, -1.5648e-01,  1.1359e-02, -1.0186e-01,  4.2942e-03,
         1.7744e-01,  2.8789e-02,  4.6753e-02, -9.7045e-02,  1.3429e-01,
        -9.7212e-02, -7.1507e-02, -3.4633e-03,  1.3883e-01, -2.0727e-01,
         5.2512e-03,  9.8836e-02, -1.7465e-01, -4.8214e-02,  2.0891e-01,
        -1.7837e-01,  1.4867e-01, -6.8707e-02,  2.1542e-01,  2.3465e-02,
        -1.6120e-01, -7.1607e-03,  6.9563e-02,  2.0662e-01, -1.3214e-02,
        -3.6621e-03,  3.9815e-02,  1.1840e-01, -9.3086e-04,  1.6077e-01,
         2.5887e-01,  3.3082e-02,  9.1206e-02, -8.1345e-02,  1.5747e-01,
         2.2100e-01,  1.4447e-01,  1.3298e-01,  5.6416e-02,  7.1703e-02,
         2.7277e-02, -3.4643e-02,  1.1375e-02, -1.2775e-01,  1.4280e-01,
        -1.1996e-03,  1.4385e-01,  6.2964e-02,  7.8224e-04,  3.1198e-01,
        -8.2213e-02,  3.0901e-01, -1.8916e-01,  6.4224e-02, -6.0739e-02,
         3.6575e-01,  2.7061e-01, -4.9588e-01,  3.7487e-02,  1.9776e-01,
         9.6408e-02, -2.3510e-02, -2.2246e-01,  1.2251e-01,  6.9865e-02,
        -1.0218e-01, -1.2041e-01,  3.6202e-02,  1.3946e-01,  1.2657e-01,
         4.5796e-01,  1.1498e-01,  1.9502e-01,  7.9131e-02, -2.3154e-01,
        -1.8005e-01, -9.3380e-02, -7.7939e-02,  2.3785e-03, -1.6296e-02,
        -2.1455e-01,  1.5535e-01,  6.0736e-02, -2.5630e-01, -5.4613e-02,
        -1.0549e-01,  2.1634e-01,  1.6198e-01,  1.9333e-02, -1.3153e-01,
        -7.9205e-02,  1.5459e-01,  1.2753e-01,  3.5562e-01, -2.0996e+00,
         1.8678e-01,  1.3896e-01,  1.3344e-01,  1.9373e-01, -1.3733e-01,
        -4.8426e-02,  1.0204e-01, -5.3012e-02,  6.2561e-02,  2.0782e-01,
        -2.7372e-02,  1.2175e-01,  9.3349e-02, -1.4915e-02,  1.4731e-01,
        -4.1648e-02,  1.9402e-01, -4.0652e-02,  1.3574e-01,  2.3930e-01,
         2.1554e-01,  1.9693e-01,  3.4045e-01,  5.3095e-03, -1.1799e-01,
         2.0984e-01,  1.4215e-01,  1.6345e-01, -9.9329e-02,  2.8398e-01,
         1.3910e-01, -1.9960e-02, -4.4554e-02, -4.9254e-02,  1.2946e-01,
         1.6621e-02, -1.4321e-01, -3.1374e-01,  1.4049e-01, -1.4755e-01,
         2.1822e-01, -1.5111e-01,  9.3312e-02,  7.1476e-01, -4.9110e-01,
         1.7405e-01,  9.4529e-02, -1.6380e-01,  2.5346e-01, -1.6801e-01,
         1.5994e-01, -1.4535e-01, -1.8851e-02,  4.0526e-01,  9.0625e-02,
         1.1590e-03,  7.2827e-02, -1.7893e-01,  6.2739e-03, -2.0410e-02,
         3.5180e-01, -1.2868e-01, -8.4570e-02,  8.2254e-02,  9.5615e-02,
        -4.3183e-02, -1.3872e-01,  2.5896e-02, -2.4962e-01,  1.2058e-01,
         4.3327e-02,  2.0339e-01,  2.3865e-02,  9.1863e-02,  5.4388e-02,
         1.1565e-01, -9.1775e-02,  1.7982e-01, -6.4217e-04, -6.8558e-02,
         2.7526e-01,  1.1384e-01,  3.2608e-01, -2.5336e-01, -4.9204e-01,
        -1.1743e-01, -6.1600e-03,  1.7388e-02, -1.4776e-01,  2.1700e-02,
         5.5266e-02, -9.0456e-02, -3.9565e-01,  5.1087e-02, -9.5472e-03,
         1.0449e-01,  8.0698e-02, -1.3287e-01, -1.2388e-02, -1.5503e-01,
         1.4343e-01, -1.7563e-01,  1.2543e-01, -1.0902e-01,  3.5237e-01,
         1.3982e-02, -1.6298e-01,  7.5430e-02,  1.4752e-01, -2.1335e-01,
         2.0591e-02,  8.5383e-02, -1.4727e-01,  1.1335e-01,  1.0891e-01,
         8.6156e-02,  2.1769e-02, -9.0225e-02, -6.0408e-02,  1.7196e-01,
         1.7566e-01,  5.9273e-02, -2.7534e-01,  1.5645e-01,  2.8370e-02,
         9.7209e-02,  1.5087e-01, -1.5383e-01, -1.1292e-01,  1.7840e-01,
         1.4085e-01,  3.9992e-01,  3.1400e-02, -3.0622e-01,  4.0452e-02,
         1.0277e-01,  2.9261e-01,  7.9035e-02, -1.6428e-01,  1.3205e-01,
        -5.9142e-02,  6.1661e-02, -2.2273e-01, -1.1950e-01,  1.1244e-02,
        -7.5128e-04,  6.2115e-02, -7.5307e-02, -1.9999e-01, -5.0906e-02,
         1.5986e-01, -1.5384e-01,  1.8486e-02,  5.3391e-02, -2.2856e-02,
         1.6538e-02,  1.6673e-01,  1.6526e-01,  3.7644e-01,  2.2116e-02,
         1.6101e-02,  8.4632e-02,  1.6122e-01, -1.2911e-01, -4.2537e-03,
         4.3188e-01, -1.6758e-01,  3.1329e-01, -3.9120e-02,  2.3113e-01,
        -4.7350e-02, -1.7430e-03, -4.6825e-03,  9.6474e-02, -1.3785e-02,
         3.6050e-01, -2.3145e-01,  9.9413e-02, -7.9733e-02,  2.7168e-01,
        -7.9808e-02, -9.3619e-02,  2.1798e-02, -7.2180e-02, -6.3289e-02,
         1.9584e-01, -4.4046e-02,  4.8561e-01,  1.6550e-01,  2.8046e-01,
        -2.8508e-01, -1.3410e-01, -3.0835e-01, -8.8017e-02, -8.2227e-02,
        -2.3335e-01, -3.8007e-01, -1.8516e-01, -8.0878e-02,  6.9986e-02,
         3.1223e-03, -1.6395e-01,  6.1631e-02,  1.0742e-01,  1.4890e-01,
         3.1640e-01,  3.0610e-01, -1.3100e-01, -2.2691e-01,  2.3236e-01,
         2.3160e-02, -2.4449e-02,  2.5452e-01,  9.4654e-02, -2.5230e-01,
         1.0485e-01, -3.9629e-02,  2.7753e-01, -8.5534e-02, -2.8560e-01,
        -1.0625e-01, -1.9122e-01, -7.7833e-02,  1.7479e-02,  1.1107e-01,
         1.9104e-01, -5.3806e-02,  1.4329e-01,  7.4348e-02, -3.7891e-02,
        -1.6470e-01,  2.7303e-01,  1.9698e-01, -3.4510e-01, -3.8307e-02,
         1.8732e-01, -7.2547e-02, -1.5918e-01, -1.2051e-01,  1.5882e-01,
        -2.4738e-01, -1.3463e-02,  2.5781e-01, -1.8179e-01,  3.0621e-01,
         3.7397e-02, -6.3673e-02, -6.0211e-02,  3.0543e-01,  1.8674e+00,
         2.9141e-01, -2.1947e-01, -4.4114e-02, -1.2505e-01,  3.7997e-01,
        -1.4247e-02, -3.6082e-01,  1.8770e-01,  9.6182e-03,  1.2768e-01,
        -1.4299e-01,  2.9161e-01, -3.4586e-01, -4.6834e-02,  1.1860e-01,
         5.2585e-03,  1.3938e-01, -3.3451e-01,  9.1791e-02,  2.3278e-01,
         4.8837e-03, -3.0325e-01,  3.3830e-01,  4.9341e-02,  1.1152e-02,
         9.8717e-02,  3.3592e-02,  4.5998e-01,  1.2436e-02,  1.7771e-01,
         8.9274e-02, -1.4505e-01, -4.3361e-03, -6.2916e-02, -2.6059e-01,
         3.9149e-01, -7.0545e-02,  7.1020e-02,  1.5872e-01,  1.6568e-01,
         1.2057e-01, -2.5532e-01,  1.2026e-01], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 0.0032,  0.0204, -0.0020,  ..., -0.0134,  0.0182,  0.0079],
        [ 0.0039,  0.0227,  0.0147,  ...,  0.0069, -0.0088,  0.0002],
        [ 0.0234,  0.0145,  0.0353,  ..., -0.0183, -0.0043, -0.0039],
        ...,
        [-0.0105,  0.0073,  0.0207,  ...,  0.0153, -0.0183, -0.0241],
        [ 0.0283,  0.0126, -0.0240,  ...,  0.0002, -0.0133,  0.0178],
        [-0.0102,  0.0041,  0.0092,  ...,  0.0073,  0.0087, -0.0314]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.0820,  0.2947, -0.0189,  ...,  0.0406, -0.0679, -0.0443],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0014, -0.0327, -0.0172,  ...,  0.0096, -0.0359,  0.0276],
        [-0.0273, -0.0061, -0.0044,  ..., -0.0093, -0.0338, -0.0004],
        [ 0.0194,  0.0110,  0.0128,  ...,  0.0142,  0.0174, -0.0026],
        ...,
        [-0.0471, -0.0134, -0.0056,  ...,  0.0004,  0.0095,  0.0284],
        [ 0.0096, -0.0149, -0.0111,  ..., -0.0030, -0.0090, -0.0091],
        [-0.0043, -0.0149, -0.0157,  ...,  0.0204,  0.0081,  0.0181]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-5.8228e-02,  9.1492e-02,  1.6858e-01,  2.8473e-02,  4.3750e-01,
        -7.2021e-02, -2.1652e-02, -9.1125e-02, -6.7215e-03,  4.0222e-02,
         4.4312e-02, -9.1797e-02, -6.2561e-02, -9.0454e-02,  1.1432e-01,
        -1.3196e-01,  5.2002e-02, -4.8706e-02, -9.7198e-03, -3.0615e-01,
         6.1760e-03,  2.2324e-02,  2.8467e-01, -2.8275e-02, -7.9956e-02,
        -5.9326e-02,  2.8210e-03, -1.2201e-01,  6.4514e-02, -5.7251e-02,
        -1.1969e-01,  1.6772e-01, -1.1429e-02, -2.2766e-01, -1.7120e-02,
         9.6497e-02, -9.4482e-02, -6.5796e-02,  8.2825e-02,  1.0254e-01,
         1.3342e-01,  1.1084e-01,  3.7720e-02,  1.1597e-01,  1.7407e-01,
         1.3196e-01, -5.4245e-03,  2.9572e-02,  1.1803e-02,  8.5220e-03,
         8.0383e-02, -2.5684e-01,  1.3562e-01, -4.3610e-02,  1.2646e-01,
         1.2489e-02, -5.5237e-02, -1.2744e-01,  8.9478e-02, -1.6980e-01,
         1.3574e-01, -4.9866e-02, -1.4001e-01, -2.0660e-02,  6.9214e-02,
        -6.4697e-02,  3.5547e-01, -3.0365e-02, -5.8868e-02,  1.6205e-02,
         1.5617e-02, -9.3933e-02,  1.4429e-01,  1.6769e-02, -2.1301e-02,
         1.3391e-01,  1.0559e-01, -1.5356e-01,  4.4189e-02, -2.2449e-03,
         9.0759e-02,  1.7395e-01, -4.8218e-03, -2.0233e-02, -4.1809e-02,
        -5.4810e-02,  1.4270e-01,  5.6488e-02, -9.1492e-02, -4.0222e-02,
         2.7740e-02, -4.9652e-02,  1.6858e-01,  1.0675e-01, -4.3945e-02,
         2.2293e-02,  1.4795e-01,  2.4377e-01, -1.6150e-01,  5.3650e-02,
         5.6976e-02, -1.5576e-01,  3.2959e-02,  1.8567e-01,  1.5454e-01,
         1.0437e-01, -2.0410e-01, -7.5798e-03,  1.5210e-01, -2.3022e-01,
         1.0480e-01,  5.8289e-02, -2.3041e-02,  6.0944e-02, -3.7323e-02,
        -2.0605e-01,  1.6260e-01,  7.9468e-02, -1.2671e-01,  8.2397e-02,
        -2.4463e-01,  1.0339e-01,  7.5562e-02,  1.0004e-01,  2.4573e-01,
        -2.6108e-02, -1.3513e-01,  1.2445e-01, -1.6321e-01,  2.8882e-01,
         5.3619e-02,  7.2571e-02, -1.2964e-01, -5.5237e-02,  2.8782e-03,
         2.3651e-03,  2.6764e-02,  3.5286e-03, -6.9580e-02, -1.9470e-02,
         7.2021e-02,  1.6602e-02, -7.2365e-03,  1.0645e-01, -1.3831e-01,
        -5.7892e-02,  1.5576e-01, -1.9287e-01,  6.3782e-03, -1.9714e-01,
        -1.0419e-01, -1.8665e-01,  1.1884e-01, -4.6173e-02, -8.6121e-02,
        -1.4722e-01, -3.5675e-02,  3.3374e-01, -7.7209e-02, -1.1346e-01,
        -9.3018e-02, -7.9468e-02,  2.8535e+00,  3.7811e-02, -9.5398e-02,
         8.0261e-02,  2.8591e-03, -2.5757e-02,  2.4185e-02,  1.0553e-01,
         6.7505e-02, -1.7297e-01,  1.6626e-01, -1.7505e-01, -2.6581e-02,
         1.6931e-01, -6.7566e-02,  2.5543e-02, -7.9163e-02, -9.2468e-02,
        -2.1130e-01, -1.6833e-01,  3.4149e-02,  1.6858e-01, -1.4880e-01,
        -9.3933e-02,  2.8000e-02,  6.6643e-03,  9.3323e-02, -4.2480e-02,
        -8.1787e-02,  2.1350e-01, -1.1328e-01, -1.4856e-01,  3.6011e-01,
         1.2314e-02, -1.8286e-01,  9.1797e-02, -1.3260e-02, -1.6101e-01,
         1.8054e-01, -1.7990e-02, -8.9539e-02, -4.3274e-02,  1.7993e-01,
        -5.6519e-02,  4.4495e-02, -6.6223e-02,  1.9678e-01,  2.0523e-02,
         1.8774e-01, -2.0374e-01,  5.2551e-02, -2.1033e-01, -2.0740e-01,
         1.9202e-01,  8.7952e-02,  1.6663e-02,  6.9397e-02,  2.4512e-01,
        -2.5681e-02,  8.7402e-02, -1.1890e-01, -1.2939e-01,  3.6621e-02,
        -6.2134e-02,  2.7115e-02,  8.1238e-02, -1.6418e-01, -1.2463e-01,
         1.0876e-01,  1.0577e-01,  1.5613e-01,  1.6516e-01, -8.8501e-03,
        -1.0059e-01,  9.1492e-02, -1.0480e-01,  3.3905e-02, -9.6054e-03,
         5.0262e-02, -7.0496e-02, -2.2437e-01,  1.1566e-01, -4.0649e-02,
         1.2708e-01,  5.4413e-02, -3.6011e-02,  1.9592e-02,  1.8945e-01,
         7.6965e-02,  2.3938e-01,  7.8735e-02,  2.8491e-01, -1.3708e-01,
        -5.0507e-02, -5.0171e-02, -3.5309e-02, -1.9373e-01,  9.1309e-02,
         4.2084e-02,  1.0658e-02, -4.9255e-02, -2.0422e-01,  1.0266e-01,
         9.4971e-02,  6.9641e-02,  6.3477e-02, -6.7322e-02, -5.6580e-02,
         3.9673e-02,  1.8860e-02,  1.3806e-01,  5.4626e-02, -2.3145e-01,
        -2.0032e-01,  1.3086e-01,  7.6233e-02, -2.4536e-02, -2.3453e-02,
        -1.0394e-01,  1.5649e-01, -7.0740e-02, -9.3811e-02, -1.2732e-01,
        -1.7175e-01, -1.4087e-01, -7.5623e-02, -6.1462e-02, -3.4058e-01,
        -1.1115e-01,  1.1787e-02, -3.9246e-02,  2.5269e-02,  8.7524e-02,
        -9.3842e-03,  1.1627e-01, -1.1115e-01,  4.2755e-02,  1.8188e-01,
        -5.9540e-02,  1.7908e-01,  4.2725e-02,  6.0760e-02, -5.7922e-02,
        -2.4011e-01, -1.2184e-02, -1.0999e-01, -5.3345e-02,  7.1533e-02,
         5.6244e-02, -4.5898e-02,  2.4487e-01,  7.8964e-03,  1.2585e-01,
        -9.2285e-02,  1.8652e-01, -5.9387e-02,  2.8076e-02, -1.2856e-02,
         1.7444e-01,  3.6373e-03,  3.7750e-02,  6.0272e-02,  8.0017e-02,
        -5.8075e-02,  1.0547e-01, -4.0588e-02, -1.3220e-01,  1.0645e-01,
         1.3599e-01,  5.1056e-02, -9.2529e-02,  1.2915e-01, -1.7993e-01,
        -6.0577e-02, -4.3488e-02, -2.8824e-02,  1.1823e-01, -1.9882e-02,
        -7.2449e-02, -1.6785e-02,  6.5247e-02, -4.7073e-03,  8.0444e-02,
        -6.8909e-02, -7.3486e-02,  4.6204e-02,  3.7598e-02, -5.6580e-02,
        -9.2102e-02, -1.3403e-01,  8.9905e-02, -4.8950e-02, -5.6854e-02,
         1.6617e-02,  5.0232e-02,  5.3741e-02,  6.7017e-02, -1.7114e-01,
         1.2390e-01, -6.1584e-02, -4.9225e-02,  7.4341e-02, -7.8430e-02,
         1.1859e-01, -1.8127e-02,  9.7942e-04,  1.1499e-01, -8.2947e-02,
         2.5043e-03, -7.2510e-02,  4.4373e-02,  2.0361e-01,  9.3567e-02,
         1.2164e-01,  9.6130e-02,  1.3733e-01,  8.3618e-02, -1.2573e-01,
        -5.1346e-03,  7.1350e-02, -5.2673e-02,  1.3770e-01, -7.1289e-02,
         2.0294e-02, -8.5754e-02,  3.6713e-02, -4.5197e-02, -4.0161e-01,
         1.0187e-01, -1.6760e-01,  5.4840e-02,  5.2338e-02, -1.0120e-01,
        -6.1707e-02,  8.3984e-02, -1.0864e-01,  3.5492e-02,  1.5149e-01,
         4.5410e-02,  2.1240e-02, -1.2988e-01, -1.4722e-01, -6.7406e-03,
        -1.7258e-02, -6.3477e-02,  6.5430e-02, -9.7290e-02, -1.2115e-01,
        -1.1426e-01,  1.0071e-01,  5.2490e-02,  4.4785e-03,  6.3538e-02,
        -1.0834e-01, -1.5417e-01,  1.1505e-01,  6.0822e-02,  2.7664e-02,
         5.8823e-03, -1.2610e-01,  6.7505e-02,  1.6823e-03, -6.2256e-02,
         1.6064e-01, -8.3923e-03,  8.4595e-02,  1.4200e-03,  7.7148e-02,
         8.1055e-02,  1.6068e-02, -5.9998e-02, -4.0588e-03,  2.3987e-01,
        -1.4062e-01, -3.8544e-02, -1.5723e-01,  1.7532e-02,  1.0724e-01,
        -1.0303e-01, -2.9022e-02, -3.3417e-02, -6.9763e-02, -4.3457e-02,
         1.1212e-01,  1.0406e-01,  1.4978e-01,  1.7517e-01, -1.1664e-01,
        -5.3375e-02, -1.3794e-02, -8.0139e-02, -8.2153e-02,  1.7322e-01,
        -1.9226e-02,  5.3589e-02,  7.4890e-02,  6.2805e-02,  2.8290e-02,
         8.4473e-02,  7.5073e-03, -4.3518e-02,  2.6270e-01, -3.5980e-02,
         2.6636e-01,  1.1414e-01,  1.8091e-01,  1.1169e-02, -4.5105e-02,
        -3.8788e-02,  8.1116e-02,  1.2131e-02, -1.3477e-01, -7.1875e+00,
        -6.6948e-04,  6.2073e-02, -1.0217e-01,  5.1819e-02, -1.5869e-01,
        -8.2336e-02, -8.9233e-02,  1.2091e-01,  8.8440e-02, -5.6244e-02,
        -1.1438e-01, -4.9561e-02, -1.1359e-01, -1.4734e-01, -1.4368e-01,
         6.4636e-02, -6.9702e-02,  3.9093e-02,  2.3727e-02,  2.1576e-02,
        -3.1647e-02, -3.7384e-02, -1.6968e-01, -1.0895e-01,  7.3425e-02,
         1.0376e-01, -2.7954e-02,  7.8430e-02,  1.5466e-01,  8.2520e-02,
         1.8262e-01, -1.7676e-01, -9.2346e-02, -3.9124e-02, -9.7168e-02,
        -9.1797e-02, -1.9019e-01,  3.2471e-02, -1.5845e-01,  8.7559e-05,
         1.1029e-01, -5.4283e-03, -1.0956e-01,  1.0370e-01, -6.5430e-02,
        -9.5642e-02,  2.7481e-02, -1.2177e-01, -7.0923e-02, -7.9834e-02,
        -2.0538e-02,  7.6904e-02,  7.3059e-02, -2.0508e-01,  1.0223e-01,
         2.0459e-01,  1.5503e-01,  1.3893e-02, -7.2449e-02, -1.5030e-02,
        -1.5747e-01, -1.8713e-01,  3.1036e-02,  8.1543e-02, -1.2535e-02,
         2.3605e-02, -6.7406e-03, -1.7578e-01,  1.0181e-01,  1.6003e-01,
        -2.9556e-02, -5.6877e-03,  5.2856e-02,  1.7859e-01, -2.2717e-01,
        -4.3396e-02,  4.4128e-02,  9.1980e-02, -6.7810e-02, -3.9795e-02,
         2.7649e-02,  5.9021e-02,  1.1407e-01, -5.8350e-02,  2.4109e-01,
         1.3379e-01, -1.2671e-01, -8.6060e-02, -8.9188e-03,  1.5100e-01,
        -1.6919e-01, -7.2144e-02,  3.1372e-02,  1.4722e-01,  2.0889e-02,
         7.8796e-02, -1.1115e-01, -1.5640e-02, -1.8872e-01,  6.1798e-02,
         9.5215e-03, -8.1787e-02, -3.3741e-03,  6.9275e-02, -5.0995e-02,
        -4.7882e-02,  1.1646e-01,  1.3098e-01,  4.6875e-02,  1.1987e-01,
        -7.9163e-02, -1.6675e-01, -9.0027e-02, -7.5150e-03, -8.6426e-02,
        -4.0009e-02, -4.3274e-02, -8.6792e-02,  1.4563e-01, -1.9653e-02,
        -3.6201e-03,  6.0425e-02,  1.8652e-01,  1.4319e-01, -2.4048e-02,
        -1.0248e-01, -1.7126e-01,  9.4238e-02,  1.2769e-01,  6.8481e-02,
        -7.2250e-03, -2.3804e-02,  4.5746e-02, -4.7974e-02,  4.1107e-02,
        -2.5925e-02, -1.0913e-01,  2.2995e-02, -1.8433e-01, -4.4678e-02,
        -4.7913e-02,  1.2396e-01,  2.5732e-01, -3.2654e-02,  2.1130e-01,
        -4.9438e-02,  1.1487e-01, -2.1240e-01, -5.9692e-02,  3.6224e-02,
        -9.7900e-02,  1.1742e-02, -1.3220e-01,  1.2268e-01,  1.2964e-01,
         1.8579e-01, -3.6278e-03,  1.5778e-02,  2.1326e-01,  3.6426e-01,
        -5.0415e-02,  3.1128e-02, -2.0715e-01,  9.7656e-02,  5.9631e-02,
         1.8799e-01,  1.7859e-01, -1.3708e-01,  8.4167e-02,  5.9967e-02,
         2.7075e-01,  1.2421e-02,  8.4778e-02, -2.0642e-01, -1.2195e-01,
        -1.5100e-01, -1.1253e-02,  1.1139e-01, -2.0068e-01,  2.1915e-03,
        -7.4890e-02,  5.7434e-02,  1.8335e-01,  1.2036e-01,  1.4185e-01,
         8.3801e-02,  7.1716e-02, -4.1931e-02,  1.2561e-01, -9.9060e-02,
         6.2141e-03, -1.6968e-02,  7.0923e-02,  9.8206e-02, -7.6538e-02,
         1.5515e-01, -6.4880e-02, -1.1786e-01, -1.8347e-01, -5.0468e-03,
        -3.6377e-02, -8.3862e-02, -8.6731e-02,  4.1168e-02,  1.9547e-02,
         1.6602e-01, -1.2854e-01, -9.2590e-02, -2.1729e-01,  2.9507e-03,
         4.5074e-02, -2.8076e-02,  1.9189e-01,  3.4698e-02, -9.9182e-02,
        -6.1096e-02,  9.2102e-02, -7.0129e-02,  1.2903e-01, -8.4473e-02,
        -3.5648e-03, -1.9250e-01,  1.5979e-01,  9.2407e-02, -2.2266e-01,
         2.2839e-01, -2.3155e-03,  7.7759e-02, -2.7481e-02, -1.5442e-01,
        -1.3928e-01,  2.9648e-02,  1.9943e-02, -1.4435e-02,  1.0114e-01,
        -6.4941e-02, -1.0944e-01, -6.1157e-02,  8.1253e-03,  8.9661e-02,
         2.9190e-02,  6.3293e-02, -2.1326e-01, -2.2949e-01, -1.9250e-01,
        -2.0825e-01,  9.8694e-02, -6.9153e-02,  2.5464e-01, -2.1606e-01,
         1.5637e-01, -6.4819e-02, -4.6539e-02,  3.7262e-02, -1.1328e-01,
         6.9275e-02, -3.6346e-02,  1.9379e-02,  1.6556e-03,  8.8074e-02,
         7.0923e-02,  8.2642e-02,  6.3086e-04, -1.6983e-02, -1.0858e-01,
         3.4644e-01,  1.2134e-01,  3.4973e-02,  4.4128e-02, -1.4160e-01,
        -1.7761e-01,  3.7384e-02,  1.3477e-01, -9.5398e-02, -7.1899e-02,
        -8.4900e-02, -3.9825e-02,  1.1810e-02, -4.6326e-02,  1.3867e-01,
         9.7107e-02, -1.0150e-01, -3.7441e-03,  1.1786e-01,  2.5803e-02,
        -1.5845e-01, -2.1252e-01,  8.8135e-02, -1.6309e-01,  3.8879e-02,
         2.0004e-02, -8.9111e-03, -2.5162e-02], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([1.8728, 1.7547, 1.8036, 1.8577, 2.1824, 1.6302, 1.7899, 1.8454, 1.8129,
        1.7273, 1.7220, 1.6906, 1.7052, 1.8640, 1.8814, 1.7893, 1.8372, 1.7668,
        1.8278, 1.8139, 1.6810, 1.7612, 1.9072, 1.9237, 1.7676, 1.7158, 1.7243,
        1.8889, 1.9533, 1.7293, 1.8389, 1.7399, 1.7465, 1.8322, 1.7277, 1.7053,
        1.8235, 1.7012, 1.8006, 1.9238, 1.7329, 1.9680, 1.7680, 1.6244, 1.6845,
        1.7426, 2.0288, 1.8557, 1.8263, 1.7483, 1.8057, 1.7374, 1.6137, 1.7142,
        1.8097, 1.7203, 1.6717, 2.2543, 1.7730, 1.9483, 1.7105, 1.6814, 2.1237,
        1.8089, 1.8255, 2.6967, 1.8107, 1.6820, 1.7113, 1.7730, 1.9706, 1.9000,
        1.8153, 1.8284, 1.6183, 1.8163, 1.7406, 1.7113, 2.0869, 1.7986, 1.9555,
        1.8223, 1.8276, 1.7193, 1.8223, 1.8769, 1.7694, 1.7360, 1.7613, 1.6688,
        1.7022, 1.7667, 1.7065, 1.6603, 1.8120, 1.7049, 1.9174, 1.6893, 1.7328,
        1.7920, 1.7548, 1.7317, 1.6880, 1.7914, 1.9288, 1.8672, 1.7392, 1.7381,
        1.7374, 1.6784, 1.6162, 1.7353, 1.8124, 1.8877, 1.6735, 1.8267, 1.7956,
        1.7927, 1.7485, 1.7837, 1.8291, 1.6297, 1.7247, 1.8656, 1.9152, 1.6948,
        1.8756, 1.7589, 1.7512, 1.7557, 1.6466, 1.7673, 1.6632, 1.8049, 1.8177,
        1.7658, 1.7416, 1.9632, 1.7444, 1.6705, 1.7454, 1.7714, 1.6047, 1.8853,
        1.8237, 1.7208, 1.8655, 1.7895, 1.6101, 1.8483, 1.8135, 1.9494, 1.7945,
        1.7738, 1.8106, 1.7011, 1.8042, 1.7543, 1.6437, 1.7617, 1.7073, 1.9108,
        2.2943, 1.7387, 1.8584, 1.7676, 1.8051, 1.6746, 1.8105, 1.8633, 1.7904,
        1.7631, 1.6917, 1.6902, 1.8191, 1.8009, 2.6139, 2.0224, 1.6584, 1.7414,
        1.8323, 1.8182, 1.6671, 1.7693, 1.8126, 1.9716, 1.7960, 1.8354, 1.7195,
        1.7754, 1.6959, 1.6688, 1.7538, 1.8228, 1.9487, 1.7090, 1.7782, 1.7023,
        2.8828, 1.8100, 1.6670, 1.7395, 1.7515, 1.8846, 1.7747, 1.7251, 1.9862,
        1.7202, 1.6708, 1.7447, 1.5925, 1.8382, 1.7598, 1.6398, 1.8064, 1.8713,
        1.7965, 1.8117, 1.7469, 1.7719, 1.6719, 1.7656, 1.7959, 1.7350, 1.9117,
        1.7505, 1.8743, 1.8043, 1.7795, 1.5903, 1.7346, 1.8705, 1.7257, 1.7646,
        1.7982, 1.7138, 1.7988, 1.6006, 1.7668, 1.8458, 1.8039, 1.7732, 1.7253,
        1.9010, 1.6667, 1.6973, 1.7982, 1.9759, 1.7445, 1.8074, 1.8802, 1.7144,
        1.8232, 1.8887, 1.8002, 1.8160, 1.7317, 1.9969, 1.5780, 1.8576, 1.7781,
        1.8187, 1.8604, 1.8072, 1.7960, 1.7139, 1.7889, 1.7381, 2.0270, 1.7207,
        1.7016, 1.8712, 1.9156, 1.7318, 1.8476, 1.9266, 1.8543, 1.7598, 1.7594,
        1.6991, 1.7971, 1.7460, 1.7098, 1.6603, 1.7289, 1.6441, 1.5901, 1.7972,
        1.8199, 1.8494, 2.0410, 1.7342, 1.9012, 2.0701, 1.7688, 1.8882, 1.8565,
        1.6923, 1.7557, 1.8527, 1.7336, 1.8296, 1.6045, 1.8099, 1.6580, 1.8173,
        1.9574, 2.4231, 1.6354, 2.2620, 1.8750, 1.7571, 1.8287, 1.7256, 1.9842,
        1.8685, 1.7660, 1.8027, 1.7770, 1.8104, 1.8142, 1.7283, 1.6328, 1.6241,
        1.8117, 1.7310, 1.6545, 1.6059, 1.8148, 1.8364, 1.8917, 1.6462, 1.8115,
        1.8170, 1.7312, 1.7646, 1.6505, 1.7751, 1.7892, 1.8509, 1.9477, 2.3132,
        1.8190, 1.9001, 1.8442, 1.6871, 1.8139, 1.8468, 2.0367, 1.7056, 1.7112,
        1.7670, 1.8652, 1.7420, 1.8898, 1.7194, 1.7252, 1.7193, 1.7331, 1.7921,
        1.8597, 1.6955, 1.8549, 1.8233, 1.9745, 1.8662, 1.7944, 1.7510, 1.7609,
        1.8620, 1.7772, 1.7749, 1.8237, 1.6827, 1.7449, 1.8918, 2.0390, 1.6744,
        1.6268, 1.9001, 1.7299, 1.8229, 1.7977, 1.6669, 1.6024, 1.8188, 1.7334,
        1.8002, 1.7300, 2.1726, 1.6700, 1.7431, 1.8026, 1.7612, 1.7199, 1.6923,
        1.6644, 1.7084, 1.9433, 1.7660, 1.6032, 1.7658, 1.7543, 1.7204, 1.8941,
        1.9148, 1.6203, 1.9567, 1.7905, 1.6558, 1.8582, 1.8381, 2.0396, 1.9045,
        1.7705, 1.7511, 1.9576, 1.7374, 1.7310, 1.8140, 1.7889, 1.6220, 1.7523,
        1.7130, 1.7670, 1.9267, 1.6834, 1.7794, 1.6861, 1.8663, 1.8656, 1.8087,
        1.7948, 1.7197, 1.5690, 2.2319, 1.8988, 1.7588, 1.8252, 1.7361, 1.7862,
        1.7354, 1.7093, 1.6775, 1.8468, 1.7330, 1.8936, 1.7130, 1.9545, 1.7298,
        1.6729, 1.7887, 1.6690, 1.8468, 1.7840, 1.6812, 1.7597, 1.7837, 1.6500,
        1.6694, 1.9539, 2.0421, 1.8921, 1.6931, 1.9127, 1.6547, 1.7876, 1.8291,
        1.7312, 1.7571, 1.8522, 1.6939, 1.7974, 1.7912, 0.8882, 1.6792, 1.9011,
        1.9224, 1.9471, 1.8223, 1.7650, 1.6055, 1.8410, 1.8062, 1.8044, 1.6980,
        1.6566, 1.8625, 1.6706, 1.8128, 1.6267, 1.6477, 1.7553, 1.9766, 1.7803,
        1.8333, 1.8784, 1.8143, 1.6159, 1.7287, 1.7773, 1.8109, 1.7313, 1.7073,
        1.7380, 1.8799, 1.9860, 1.7834, 1.7660, 1.7020, 1.7776, 1.7250, 1.7347,
        1.8215, 1.8359, 1.6774, 1.6558, 1.9027, 1.8354, 1.8742, 1.9091, 1.5760,
        1.7863, 1.7307, 1.7591, 1.8141, 1.7780, 1.6756, 1.8921, 1.7581, 1.7023,
        2.0466, 1.8074, 1.7839, 1.8754, 1.7286, 1.7876, 1.7002, 2.0919, 1.8398,
        1.8331, 1.8582, 1.6425, 1.7268, 1.7142, 1.6585, 1.9305, 1.7850, 1.8508,
        1.8482, 1.7189, 1.7981, 1.7123, 1.9352, 1.6617, 1.8423, 1.6198, 1.8271,
        1.5744, 1.5694, 1.9952, 1.7246, 1.7511, 1.7516, 1.8460, 1.7108, 1.6408,
        1.7795, 1.7720, 1.7731, 1.8246, 1.7610, 1.7569, 1.7468, 1.6235, 1.6202,
        1.8685, 1.9336, 1.8680, 1.6949, 1.6995, 1.7635, 1.9551, 1.7691, 1.8124,
        1.6820, 1.7588, 1.7007, 1.8804, 1.7494, 1.7261, 1.6887, 1.7230, 1.6999,
        1.8258, 1.8233, 1.7310, 1.7570, 1.8468, 1.8310, 1.8870, 1.8488, 1.6868,
        1.6953, 1.8011, 1.7912, 1.8304, 1.7616, 1.7405, 1.7687, 1.6651, 1.8245,
        1.9685, 1.7588, 1.9942, 1.7595, 1.7126, 1.8003, 1.7433, 1.8008, 1.6225,
        1.7806, 1.9816, 1.6298, 1.8152, 1.7408, 1.6533, 1.8026, 1.6771, 1.8380,
        1.6243, 1.6354, 1.7953, 1.6672, 1.8827, 1.7984, 1.7944, 1.8477, 1.7569,
        1.7022, 1.7916, 1.7132, 1.8886, 1.8595, 1.8478, 1.6918, 1.7125, 1.8510,
        1.7128, 1.6492, 2.3644, 1.7261, 1.7210, 1.7260, 1.6130, 1.7513, 1.8390,
        1.5612, 1.7037, 1.7211, 2.0017, 1.8191, 1.6772, 1.9493, 1.7455, 1.7580,
        1.8318, 1.8643, 1.8332, 1.6268, 1.7308, 1.7625, 1.8703, 1.9427, 1.6668,
        1.7555, 1.9277, 1.8045, 1.8333, 1.8089, 1.8974, 1.8570, 1.6222, 1.6393,
        1.8872, 1.7288, 1.7060, 1.8189, 1.8870, 1.5363, 1.7967, 1.8584, 1.8575,
        1.8540, 1.9067, 1.7273, 1.8267, 1.6695, 1.7934, 2.0291, 1.8780, 1.8184,
        1.7510, 2.0461, 1.7423, 1.9446, 1.7160, 1.8476, 1.7205, 1.5758, 1.7424,
        1.8978, 1.6879, 1.7293, 1.7713, 2.0305, 1.7728, 1.7149, 1.7348, 1.8070,
        1.7017, 1.7348, 1.7224, 1.6461, 3.0367, 2.0496, 1.9265, 1.7293, 1.8574,
        1.7300, 1.9038, 1.9109, 1.7025, 1.8784, 1.7179, 1.7195, 1.8131, 1.7751,
        1.5399, 1.6682, 1.6308, 1.8071, 1.6887, 1.8143, 1.6503, 1.7686, 2.0083,
        1.8872, 1.8092, 1.8261, 1.8082, 1.7445, 1.7919, 1.7134, 1.7019, 1.8723,
        1.7027, 1.6786, 1.9378, 1.6835, 1.7235, 1.7465, 1.7253, 1.5730, 1.7733,
        1.7328, 1.8270, 1.6885], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 5.2365e-01, -7.1796e-01, -5.0786e-01, -4.9917e-01, -7.3772e-01,
         6.9723e-01, -3.5357e-01,  1.9300e-01,  8.1534e-01, -4.2375e-01,
        -5.5178e-01,  1.4156e-01,  4.5392e-01, -3.9982e-01,  9.1892e-02,
         2.9702e-01, -1.6993e-01,  5.2459e-01,  3.2687e-01,  8.4204e-01,
        -3.4065e-01, -8.2934e-02, -9.8496e-02, -1.5335e-01,  1.2601e-01,
        -2.3323e-01,  4.7130e-02,  1.0745e-01,  5.4473e-01,  1.7098e-01,
        -2.7386e-02, -1.1410e-01,  6.8870e-01,  3.2801e-01,  4.2133e-02,
        -7.9934e-01,  7.5729e-01,  3.4921e-01, -6.0004e-01,  1.1502e-01,
        -1.5464e-01,  3.9600e-02, -1.5119e-01, -6.2914e-01, -4.5401e-01,
        -3.7856e-01, -9.9999e-01,  6.7442e-01, -3.6019e-01, -7.1740e-02,
        -1.2775e+00,  9.6797e-01,  1.9182e-01,  7.7241e-01, -3.6558e-01,
        -3.3722e-01,  6.6506e-01, -4.6364e-01,  5.5047e-01,  7.0849e-01,
        -4.1890e-01,  1.7221e-01, -5.1826e-01, -2.4845e-01, -3.9541e-01,
        -3.9335e-01, -1.0862e+00,  1.6473e-01,  1.7781e-01, -3.8569e-02,
        -3.6542e-01,  5.1494e-01, -8.3685e-01, -4.0427e-02, -4.5675e-01,
        -1.7859e-01, -3.8725e-01,  8.1883e-01,  7.5297e-01,  9.6626e-01,
         6.9364e-01, -5.0789e-01, -2.9852e-01,  1.2151e-01,  5.9669e-01,
        -3.1539e-01,  2.4300e-01, -9.6667e-02,  2.1359e-01,  6.6510e-02,
         1.2985e-01, -6.6999e-01, -1.6000e-01, -7.6950e-01,  3.1830e-01,
         2.8169e-01, -1.0485e+00, -1.2472e-01,  4.7124e-01, -4.2533e-02,
         4.8433e-01,  6.8543e-01,  3.5366e-01, -7.9901e-01,  1.6506e-01,
        -4.2159e-01,  9.2392e-01,  9.5828e-01, -5.4434e-01,  1.0712e+00,
        -2.4472e-01,  7.0082e-01,  1.6221e-01,  1.4343e-01,  1.1916e-01,
         6.4597e-01, -1.1901e+00, -3.2111e-01,  3.1017e-01,  2.6418e-02,
         7.9504e-01, -3.1324e-01,  1.2178e-01,  1.0480e-01, -4.9646e-01,
         2.0638e-01,  8.8050e-01, -6.3201e-01,  6.5160e-01, -9.2663e-01,
        -2.6332e-01, -2.4718e-01,  7.2113e-01, -7.5739e-02, -7.9578e-01,
         2.6718e-02, -8.4464e-01,  9.4241e-01, -3.8431e-01, -2.1190e-01,
        -8.9685e-01, -8.7849e-01,  3.6954e-01,  1.9354e-01,  5.9779e-01,
         5.4826e-01, -1.2341e-01,  5.3911e-01, -1.6569e-01,  8.4252e-01,
         2.7934e-01,  1.2656e-02, -5.4159e-01,  6.6726e-01,  4.8667e-01,
         6.7404e-01,  1.9638e-01, -9.7165e-01,  5.7017e-01,  4.4719e-01,
        -4.4060e-01,  4.2154e-02,  5.2086e-01,  5.6147e-01,  3.0391e-01,
         7.2406e-02,  7.2969e-01,  1.9448e-01, -5.1772e-01,  7.0601e-01,
        -6.6572e-02,  3.0350e-01, -6.1288e-01,  1.8081e-01, -2.8082e-01,
         4.0170e-01, -7.3233e-01,  7.8688e-01,  4.8578e-01,  5.3802e-02,
         7.2736e-01, -1.2462e-01, -4.1762e-01, -5.2198e-01,  7.9471e-01,
        -2.1844e-01, -3.9000e-01, -7.0649e-02,  1.5447e-01,  3.3449e-01,
        -1.9374e-01, -1.0065e+00,  5.1472e-01,  4.9429e-01, -1.0720e+00,
         7.3440e-02,  7.5565e-01, -3.9948e-01, -2.7195e-01,  1.0148e+00,
        -6.9053e-01, -3.2311e-01, -9.6137e-02, -1.9534e-01, -3.0459e-01,
         1.9061e-01,  1.8854e-01,  1.7647e-01, -8.0750e-01,  6.3126e-03,
        -3.9351e-01,  8.9944e-03, -4.0237e-01,  7.4425e-01,  4.2765e-01,
         7.7177e-02, -3.1802e-01, -4.0009e-01, -9.7929e-01, -7.0733e-01,
        -4.4645e-02,  1.8130e-01,  7.0742e-01,  6.4393e-01,  7.3340e-01,
         5.9089e-01,  2.3528e-01, -1.2541e-01,  6.3674e-01,  5.6344e-01,
        -5.0353e-01,  9.3068e-02, -1.0097e+00, -8.5259e-01,  5.3163e-01,
        -2.0954e-01, -6.7231e-01,  5.8950e-01, -1.1848e-01,  5.2578e-01,
        -1.2783e+00,  1.8730e-02,  1.1769e+00,  6.8309e-01, -4.2683e-01,
        -3.9154e-01, -3.4926e-01, -5.6174e-01, -3.3042e-01,  8.3069e-02,
        -8.3279e-01, -1.1924e+00,  2.7345e-01, -1.9008e-01,  6.7653e-01,
         5.9039e-01,  8.5830e-01,  9.9681e-01,  7.1562e-01, -3.2135e-01,
        -1.6521e-01,  3.2021e-01,  4.2866e-01,  7.8429e-01, -7.3655e-01,
        -5.8396e-01, -1.3525e-01,  4.8509e-01,  5.6491e-02, -1.0362e-01,
        -6.2418e-01,  5.4077e-01, -1.6874e-01, -6.9453e-01,  7.5331e-01,
         7.8803e-01,  4.4891e-01, -1.9796e-01,  2.5425e-01,  3.6405e-01,
         3.2291e-01, -1.2160e-01,  6.6354e-01,  8.8945e-01,  8.0281e-01,
         2.7114e-01,  8.3829e-01,  3.3341e-01,  5.7132e-01,  1.4304e+00,
        -2.1592e-01,  9.8776e-02,  1.9750e-01, -9.7931e-01, -2.5027e-01,
        -7.6657e-02,  2.8210e-02,  5.7256e-01, -3.3332e-01, -3.7285e-01,
        -5.8753e-02, -3.2304e-03, -7.2074e-01, -3.4369e-01,  3.6055e-01,
         6.7007e-01, -6.9041e-01, -8.7381e-01,  5.3381e-01,  5.5394e-02,
        -1.1908e-01,  2.9258e-01, -2.1546e-01, -8.9148e-02, -1.7491e-01,
         9.0487e-01, -5.3882e-01,  8.9418e-01, -1.2240e-01, -1.8015e-01,
         2.9930e-01, -3.0655e-02,  4.2033e-01,  3.7214e-01, -4.8523e-01,
        -3.4483e-03, -5.0961e-01,  7.6495e-02,  5.2234e-01,  3.0821e-01,
        -1.6111e-02, -1.8429e-01,  2.2425e-01, -1.6561e-01,  5.7110e-01,
        -5.3549e-01,  1.0812e-01, -2.6734e-01, -1.0017e-03,  2.8036e-01,
         4.8367e-02,  6.0758e-03, -1.0951e-01,  1.6691e-01,  1.4140e-01,
         8.8960e-02, -1.9603e-01, -2.5611e-01,  1.3564e-01,  4.0519e-01,
         3.0434e-01,  2.0968e-01,  3.4171e-01,  1.7606e-01, -6.2441e-01,
         9.6200e-02, -6.4161e-01,  1.8472e-02, -4.7658e-01,  8.7947e-01,
        -4.6602e-01,  4.7910e-02,  5.2541e-01, -5.8759e-02, -1.3030e-01,
         3.8045e-01,  2.3253e-01,  4.5987e-01, -3.4145e-01,  2.9264e-01,
        -2.1473e-01, -4.5932e-01,  3.1802e-01, -9.6810e-01, -2.1568e-01,
        -7.9277e-01,  2.5702e-01, -6.7151e-01, -4.9960e-01,  2.7052e-01,
         6.7502e-01, -3.3173e-01,  9.9372e-01, -4.2096e-01,  2.1906e-01,
         3.3975e-02,  4.1775e-01, -2.8452e-01,  8.6553e-01,  3.9230e-01,
         1.1056e-01,  5.2030e-01, -2.8196e-01, -2.2177e-01,  7.0446e-01,
        -8.6578e-02,  1.3139e-01, -8.7306e-02,  1.1427e-01,  8.5759e-02,
        -1.4070e-01, -2.9741e-01,  8.9071e-01,  4.8202e-01, -5.9489e-02,
         8.9789e-01,  5.3881e-01,  6.0686e-01, -2.2308e-01,  8.6642e-01,
         4.9077e-01, -5.5091e-02, -4.3449e-01, -6.7774e-01,  3.7506e-02,
         2.6588e-01,  1.5247e-01, -2.2813e-01,  9.3671e-02,  3.6551e-01,
         5.5127e-01,  1.6033e-01, -2.5638e-01, -4.1804e-02, -3.0825e-02,
         2.9872e-01,  6.0427e-03, -1.8824e-01, -1.0125e-02,  8.9432e-01,
        -5.2097e-01,  8.5012e-01, -2.2281e-01, -4.3597e-01, -8.0840e-01,
        -9.1855e-01,  3.0273e-01,  1.0803e-01,  1.1152e+00, -1.5719e-01,
         1.1975e-02,  3.6843e-01, -2.9271e-01,  4.6732e-01,  1.7604e-01,
        -3.3923e-01, -2.9831e-01, -4.8610e-01, -7.5712e-02,  4.3407e-01,
         1.1111e+00,  5.1410e-01,  6.9860e-01,  4.0695e-01, -1.0682e+00,
         1.8455e-01, -4.9250e-01, -3.1826e-01, -2.9163e-01, -2.3546e-01,
        -7.8442e-01,  9.2442e-01,  1.6520e-01, -1.2188e+00,  5.7878e-01,
        -8.5155e-01,  6.7063e-02,  2.6550e-02, -4.3127e-01, -2.7951e-01,
         1.8323e-02,  1.9658e-02,  4.7443e-01,  6.1131e-01, -2.3093e+00,
         3.5181e-01,  2.5726e-01,  4.7805e-01, -1.7803e-01, -1.2482e-01,
         4.9293e-01,  5.0750e-01, -3.7749e-01, -2.6515e-01,  2.8590e-01,
         5.1095e-01,  1.3173e-01, -8.0175e-03,  1.1047e-01,  8.3509e-01,
        -4.4624e-01,  4.7691e-01,  1.7419e-01, -4.7241e-01,  3.1154e-01,
         2.9991e-01,  4.3542e-01,  1.2321e+00,  2.3803e-01, -4.3119e-01,
         3.7148e-01,  4.9344e-01, -3.0547e-01, -5.3341e-01, -3.1853e-02,
        -2.7490e-01,  3.2135e-01,  2.6234e-01, -1.9682e-02,  7.0717e-01,
        -4.2983e-01,  4.6524e-01, -5.3194e-01,  9.8069e-01,  5.8870e-01,
         2.1646e-01, -3.4338e-01,  4.2862e-02,  2.7347e-01, -2.2594e-01,
         1.0674e+00,  4.2078e-01,  4.8330e-01,  6.1738e-01,  3.2063e-01,
         1.7535e-01,  9.7011e-02, -1.8358e-01,  9.3179e-01, -2.2381e-02,
        -9.4270e-01,  1.1792e-01, -2.6643e-01,  2.1232e-01,  4.9830e-01,
         7.6591e-01,  2.9870e-02, -4.3988e-01,  4.0557e-01,  2.6872e-01,
        -4.1879e-02,  7.5626e-02,  6.6198e-01, -8.3230e-01, -3.0211e-01,
        -2.0827e-01, -7.0032e-01, -1.4148e-01, -1.6967e-01,  7.0183e-01,
         8.5013e-02, -4.1698e-01, -6.0489e-02,  3.0557e-01,  3.9021e-02,
         6.1819e-01, -6.1305e-01,  3.1326e-01, -3.0995e-01, -1.5488e+00,
        -3.0525e-01,  8.8306e-02,  9.2133e-01, -6.2372e-01, -1.9223e-01,
         3.8362e-01,  5.9477e-01, -6.0004e-01,  1.2034e-01, -2.1234e-01,
         5.9456e-02,  6.9879e-01, -3.6176e-01,  6.5457e-01, -4.7436e-01,
         1.7996e-01,  3.0792e-01,  7.9678e-02, -6.7011e-01,  5.5580e-01,
         1.0828e-01, -1.7653e-01,  4.6241e-02,  2.5056e-01, -8.9252e-01,
         3.1166e-01,  7.0509e-01,  6.8284e-01, -6.6525e-03,  6.1851e-01,
        -5.9312e-02,  8.9731e-02,  2.2366e-01, -5.5600e-01,  1.0849e+00,
         2.4813e-01,  1.1594e-01, -1.2167e+00,  1.5649e-01, -8.0943e-02,
         1.0084e+00,  5.9600e-01, -7.4801e-01, -6.5032e-01,  2.3075e-01,
         3.7942e-01,  4.2537e-01,  1.2587e-01, -4.7321e-01,  3.1443e-01,
         5.0104e-01,  9.8989e-01,  8.0250e-01,  3.8222e-01,  7.4728e-01,
         5.6268e-01, -4.8270e-02, -1.1665e+00,  3.3048e-02, -7.2375e-01,
         2.0525e-01, -2.7832e-01,  8.6653e-02, -2.8562e-02,  3.1306e-01,
         7.6600e-01, -8.0847e-01,  5.4784e-01, -2.8227e-01, -6.6010e-01,
        -9.8385e-01,  2.1724e-01,  2.2076e-02,  7.8854e-01, -9.0521e-01,
         3.7701e-01,  1.6502e-01,  4.0696e-01, -7.0733e-01,  3.6993e-01,
         5.1732e-01, -8.8974e-01,  2.7491e-01, -6.1666e-01,  5.1971e-01,
        -7.9852e-01, -2.4639e-01, -7.7815e-02,  7.8357e-01,  8.9507e-01,
        -6.5967e-01, -5.7339e-01, -3.1713e-01,  4.5455e-01,  1.1265e-01,
         2.2680e-01, -1.7445e-01, -3.9281e-01, -2.5807e-01, -3.0213e-01,
         3.4656e-01, -7.4623e-01, -2.8558e-01,  1.7893e-01,  1.1342e-01,
        -8.0159e-01, -2.1829e-01, -9.9254e-01, -1.5305e-01, -1.0703e-03,
        -1.1041e+00, -4.0161e-01, -3.3409e-02,  1.1676e-02, -2.7472e-01,
         1.2675e-01, -4.5706e-01,  4.1187e-01, -7.0135e-01,  7.1590e-01,
         5.4015e-01,  1.0804e+00,  1.8346e-01,  6.5775e-01,  4.7076e-01,
        -1.9371e-01, -7.5276e-02,  4.3526e-01,  3.9449e-01,  4.0887e-01,
        -5.3782e-02, -4.4384e-01,  1.9879e-02, -9.1176e-02,  2.1702e-01,
         1.1800e-01,  5.0274e-01, -7.4890e-01, -4.1458e-01,  4.5222e-01,
        -3.2062e-01, -7.9956e-02,  6.9394e-01, -1.0899e+00,  3.4254e-01,
        -1.7928e-01,  6.2240e-01,  6.2699e-02, -1.3474e-01, -5.6386e-01,
         3.9947e-01,  1.0371e+00,  3.3629e-01, -4.3833e-01,  1.2022e-01,
        -5.9730e-01, -4.6715e-01,  1.3945e+00,  7.4473e-01,  8.4637e-01,
         6.8066e-01, -1.5404e-01,  3.6263e-01, -7.1079e-02, -2.0128e-01,
         1.5455e-02, -5.0949e-01,  1.2212e-01, -5.4024e-01,  9.5051e-01,
        -6.7772e-01, -5.8960e-01, -3.9299e-01,  3.8189e-01, -9.1321e-02,
        -8.0553e-01,  5.3645e-01, -1.0406e+00,  2.0083e-01,  1.0773e+00,
        -1.1015e+00, -3.1549e-01, -3.2300e-01,  2.5178e-01,  6.0958e-01,
         4.2411e-01,  1.3246e+00, -6.4755e-02,  4.4368e-02,  5.3248e-01,
         4.2212e-01, -2.6460e-01,  8.3685e-01, -5.3068e-01, -1.3308e-01,
        -1.4837e-01,  3.9950e-02,  5.2396e-02,  4.1660e-01, -4.7404e-01,
         9.1058e-01,  4.1668e-01, -2.8680e-02,  3.0489e-01, -5.8090e-02,
         3.7067e-01,  4.8497e-02,  4.4988e-01], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[-0.0061,  0.0216,  0.0378,  ...,  0.0087, -0.0147, -0.0091],
        [-0.0168, -0.0002,  0.0211,  ..., -0.0099,  0.0221, -0.0031],
        [-0.0080,  0.0057,  0.0215,  ..., -0.0246, -0.0132, -0.0026],
        ...,
        [-0.0203, -0.0034, -0.0117,  ...,  0.0065, -0.0158, -0.0084],
        [ 0.0024,  0.0246, -0.0107,  ...,  0.0023, -0.0117, -0.0079],
        [-0.0057, -0.0211, -0.0082,  ..., -0.0046,  0.0054, -0.0058]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.5361, -0.4248, -0.5415,  ..., -0.5884, -0.7373, -0.2561],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0039, -0.0219, -0.0056,  ...,  0.0122, -0.0174,  0.0043],
        [ 0.0055,  0.0005,  0.0034,  ...,  0.0172, -0.0199,  0.0144],
        [-0.0199, -0.0112, -0.0061,  ...,  0.0151, -0.0054,  0.0031],
        ...,
        [-0.0059,  0.0005,  0.0246,  ..., -0.0153,  0.0092,  0.0027],
        [ 0.0113,  0.0209,  0.0061,  ...,  0.0087,  0.0100, -0.0050],
        [-0.0006,  0.0108,  0.0045,  ...,  0.0284, -0.0053,  0.0162]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-2.9590e-01, -4.0253e-02, -1.9165e-01, -4.5410e-01, -2.3712e-02,
         4.4067e-02,  7.4768e-02,  4.4703e-04, -1.4130e-02,  4.3945e-02,
        -6.3904e-02, -1.5918e-01, -2.5528e-02, -1.0309e-01, -8.4351e-02,
         3.9581e-02, -1.4819e-01,  3.3569e-02,  7.2998e-02, -1.4124e-01,
        -2.0667e-01, -1.0364e-01,  3.0542e-01, -2.5732e-01, -4.1077e-02,
        -8.7158e-02,  9.7717e-02, -1.2891e-01,  1.2901e-02, -4.3030e-02,
        -2.5903e-01,  7.9117e-03, -1.3269e-01, -1.2366e-01, -3.4546e-02,
         1.7041e-01, -2.7969e-02, -5.4169e-02,  7.0862e-02, -2.1399e-01,
         8.4991e-03, -7.7248e-03,  3.8574e-02,  1.6992e-01, -9.3384e-02,
         2.0813e-01,  3.1281e-02,  1.2964e-01,  2.5574e-02, -4.7272e-02,
         1.1328e-01,  3.9856e-02,  1.1298e-01, -4.8553e-02,  3.2776e-02,
         3.4351e-03, -3.6011e-02,  1.0506e-02, -3.4790e-02,  2.0508e-01,
        -5.3040e-02,  4.5258e-02,  1.4977e-02,  3.6804e-02,  2.9343e-02,
         1.5283e-01,  3.9978e-02,  3.1952e-02,  1.9760e-02,  1.4709e-01,
        -1.2500e-01, -7.8857e-02,  1.5894e-01, -7.4463e-02, -6.4331e-02,
         2.6636e-01,  2.1851e-01, -2.4927e-01, -5.5908e-02, -1.9379e-02,
        -1.1237e-01,  3.1555e-02,  6.7291e-03,  1.0150e-01, -1.0187e-01,
         2.2903e-02,  2.2144e-01,  8.0200e-02,  1.8555e-01, -7.0839e-03,
         7.8552e-02, -8.8196e-02,  8.1360e-02,  6.9946e-02, -1.7773e-01,
        -2.0695e-03, -3.5156e-01,  2.8809e-01, -2.0264e-02, -8.1238e-02,
        -2.9739e-02,  1.1401e-01, -2.1606e-02,  1.4795e-01,  8.8072e-04,
        -8.4381e-03,  2.5864e-02,  4.0771e-02, -1.0229e-01, -1.2225e-01,
        -6.2561e-02, -1.0016e-01, -2.7368e-01, -8.6060e-02, -3.1433e-02,
         3.0670e-02,  6.1798e-02,  1.9455e-02, -1.5533e-02,  2.2266e-01,
        -1.4453e-01,  1.1023e-01,  5.7007e-02, -5.2917e-02,  2.1851e-02,
        -2.9083e-02, -2.3743e-02,  1.5674e-01, -1.9775e-01, -2.3056e-02,
        -9.3460e-03,  1.0193e-01, -1.8726e-01, -6.4392e-02, -6.2805e-02,
         7.5562e-02, -1.6760e-01, -1.1688e-01,  2.8015e-02, -1.0919e-01,
        -1.1879e-02, -5.0049e-02,  1.8845e-02, -7.2205e-02,  2.4365e-01,
         2.6855e-02, -1.5388e-02,  6.9763e-02,  5.0232e-02,  1.3477e-01,
        -9.5459e-02, -7.9224e-02,  2.4048e-01, -8.8501e-02, -1.0388e-01,
        -7.4463e-02,  8.4229e-02, -5.0476e-02,  2.0111e-02, -1.8762e-01,
         7.7209e-02,  1.3318e-01,  1.2168e+00, -1.1208e-02,  6.1646e-02,
        -5.8319e-02, -2.2003e-02,  1.2573e-01, -7.0679e-02, -1.4001e-01,
        -1.2064e-03,  2.1530e-02, -2.8473e-02,  2.4551e-02,  3.5461e-02,
         5.5206e-02,  5.6122e-02, -1.0205e-01,  3.4088e-02, -1.1230e-01,
        -2.6147e-01, -4.8431e-02,  1.2146e-01, -6.3721e-02, -4.0527e-02,
         2.0642e-01,  9.1553e-02, -2.6779e-02,  1.3525e-01, -7.9712e-02,
         1.5480e-02,  9.6680e-02, -6.1005e-02, -5.5237e-02,  1.9275e-01,
        -1.2561e-01, -7.4707e-02,  2.5269e-01,  8.6792e-02, -1.7395e-02,
        -5.9387e-02,  6.9580e-02, -7.6782e-02,  2.5879e-02,  3.7842e-02,
        -1.1726e-02,  6.5063e-02,  1.3573e-02,  7.0129e-02,  9.0714e-03,
         4.5776e-02, -6.6040e-02,  6.6650e-02, -4.7852e-02, -1.3025e-01,
         1.4587e-01,  2.0972e-01,  7.6599e-02, -1.8701e-01,  8.8928e-02,
         1.6003e-01, -7.3814e-03, -1.5283e-01, -2.7252e-02,  4.1168e-02,
         3.6621e-02, -6.9092e-02,  3.0716e-02, -6.1432e-02, -3.8330e-02,
        -1.2793e-01, -1.3354e-01,  5.9082e-02, -4.7180e-02, -8.1360e-02,
        -7.0572e-03, -3.7537e-02, -1.4343e-01,  3.2959e-02, -2.9236e-02,
         1.8384e-01,  1.3403e-01, -2.1606e-01,  5.8136e-02, -1.0608e-01,
         2.0752e-01,  1.9458e-01, -2.6337e-02,  2.3779e-01,  2.1301e-02,
        -3.8422e-02,  2.4426e-01, -6.0394e-02, -6.2561e-02, -8.0750e-02,
         4.0680e-02, -7.0810e-04, -1.9019e-01, -1.3342e-01,  4.2023e-02,
         3.3245e-03,  1.8262e-01, -3.8971e-02, -7.0007e-02,  7.7896e-03,
        -1.2781e-01,  2.5903e-01,  7.0557e-02,  1.2024e-01,  1.7273e-01,
         8.8562e-02,  2.0825e-01, -9.3231e-03, -9.8389e-02, -6.7017e-02,
        -8.6426e-02, -1.3452e-01,  2.9614e-01,  1.4542e-02, -1.7590e-01,
        -2.1387e-01,  1.6223e-01, -1.8713e-01, -8.6243e-02, -7.2510e-02,
        -4.6120e-03, -2.8290e-02,  3.9307e-02,  3.2471e-01, -6.1157e-02,
         3.3646e-03, -4.1382e-02, -1.2463e-01,  1.7383e-01, -5.4131e-03,
        -1.9263e-01, -1.0979e-02, -2.4951e-01,  1.4343e-01,  1.7053e-01,
         6.4697e-02,  1.6575e-03, -1.6586e-02,  1.5839e-02,  3.9246e-02,
        -1.6687e-01,  1.7468e-01,  1.8457e-01, -1.1737e-01,  2.4216e-02,
         1.3708e-01,  1.4595e-02,  2.1631e-01, -2.5742e-02,  1.6064e-01,
        -3.2129e-01,  5.4016e-02, -1.0266e-01,  4.7264e-03, -1.2238e-01,
         3.3765e-01, -9.9640e-03, -1.0114e-01,  5.9326e-02, -8.7036e-02,
        -1.0773e-01, -3.5187e-02, -3.4332e-02,  4.6295e-02,  5.4688e-02,
         8.1177e-02,  4.6448e-02,  5.7190e-02, -1.2164e-01, -2.7481e-02,
         1.1151e-01, -2.0630e-01,  1.9028e-02,  1.1517e-01, -3.5431e-02,
        -4.4769e-02, -1.0028e-01,  2.7954e-01,  1.3000e-01,  2.0691e-01,
        -3.9124e-02, -1.0669e-01, -4.2480e-02,  1.7346e-01, -3.8528e-03,
        -1.4026e-01, -1.1115e-01,  5.2338e-03,  7.7087e-02, -3.5645e-02,
         2.6489e-02,  1.3062e-02,  5.5756e-02,  1.2769e-01,  4.7455e-02,
        -8.1726e-02,  3.9520e-02, -2.2339e-02,  1.7590e-01, -4.4365e-03,
        -3.5919e-02,  8.0322e-02,  2.7393e-01, -7.9895e-02, -1.2390e-01,
        -6.1310e-02, -1.6711e-01, -1.7410e-02,  2.0178e-01, -8.9417e-02,
         8.0017e-02, -1.4490e-01,  1.3135e-01, -1.1688e-01, -7.2693e-02,
         4.4037e-02, -2.5314e-02, -1.9446e-01,  9.2590e-02,  9.1858e-03,
        -2.0493e-02,  1.7712e-01,  5.5511e-02, -1.1908e-01,  9.3872e-02,
         2.5749e-03, -7.6721e-02,  3.7098e-03, -9.8648e-03,  9.6680e-02,
        -6.0059e-02, -1.2744e-01,  2.9354e-03, -4.1656e-02,  3.8544e-02,
        -8.4106e-02,  1.1084e-01, -8.6182e-02,  7.4219e-02,  2.6749e-02,
         5.2795e-02, -2.4509e-03, -1.1700e-01, -3.4393e-02,  2.2781e-02,
        -2.9037e-02, -1.1212e-01,  3.5522e-02,  7.7332e-02,  5.1392e-02,
        -3.5065e-02,  5.5115e-02,  6.6452e-03,  8.0078e-02, -1.5393e-01,
        -2.7930e-01,  9.9640e-03, -4.9438e-02, -3.7628e-02, -1.9458e-01,
         5.6793e-02, -1.0315e-01,  4.5074e-02,  1.7365e-02,  2.3651e-02,
         2.1408e-02,  8.1787e-03, -1.7480e-01,  6.2927e-02,  8.3801e-02,
         5.3558e-02,  7.6180e-03, -1.1334e-01, -7.8247e-02,  2.8667e-03,
        -6.3416e-02,  6.2988e-02,  2.1643e-01,  6.2561e-03, -2.1362e-01,
        -5.3986e-02,  1.7725e-01,  1.1670e-01, -1.8994e-01,  1.9653e-01,
         1.9019e-01, -9.4652e-04,  3.6835e-02, -4.3762e-02,  3.2684e-02,
        -1.6992e-01, -1.6174e-01, -1.2720e-01,  4.3610e-02, -1.9141e-01,
         1.6809e-01, -3.8696e-02, -1.2769e-01, -1.7896e-01, -7.1228e-02,
        -1.0086e-02,  1.3794e-01, -1.3940e-01, -2.0279e-02, -8.5999e-02,
         1.5839e-02,  1.9910e-01, -1.0004e-01,  7.7438e-03, -2.9102e+00,
         2.0081e-01,  3.2074e-02, -1.2115e-01,  4.0863e-02, -3.6865e-02,
         2.2018e-02,  6.1249e-02, -9.7168e-02,  5.5695e-02,  1.5930e-01,
        -1.0162e-01,  7.9712e-02,  3.5217e-02,  5.8472e-02,  8.0444e-02,
         6.3599e-02, -1.4801e-02,  1.7357e-03,  3.2153e-01, -5.4321e-02,
         6.0883e-02, -2.1484e-01,  1.0399e-02, -7.3181e-02,  7.9468e-02,
        -6.1401e-02, -3.4302e-01,  4.6875e-02,  7.9468e-02,  8.6914e-02,
         3.0899e-02, -1.9702e-01,  2.4084e-01,  1.0040e-01,  1.2244e-01,
        -1.7847e-01, -1.3257e-01,  9.2712e-02, -6.2286e-02, -8.8318e-02,
         1.8164e-01, -4.6082e-02, -8.9233e-02,  2.8412e-02, -9.6893e-03,
        -1.6504e-01,  6.8970e-02,  3.8116e-02,  2.6367e-01, -2.3645e-01,
         3.8208e-02, -2.3556e-03, -5.4932e-03, -3.3539e-02,  7.7307e-05,
        -1.7676e-01, -1.7505e-01,  2.1347e-02, -1.4832e-02, -1.3879e-01,
        -7.8796e-02, -7.0862e-02, -6.1096e-02, -1.3818e-01,  6.1859e-02,
         3.8086e-02, -1.9897e-01,  1.4941e-01,  1.1169e-01,  1.1578e-01,
        -6.3110e-02,  1.6968e-01,  8.6594e-03, -4.9194e-02, -2.4329e-01,
         1.7261e-01,  4.1504e-02,  1.1957e-01, -1.5649e-01, -1.4307e-01,
        -5.1544e-02,  6.3660e-02,  1.9409e-02,  1.9608e-02,  1.0785e-01,
        -1.7151e-01, -5.6610e-02, -1.0071e-03, -2.2839e-01, -1.1896e-01,
         1.8127e-01, -1.7071e-03, -7.4158e-02, -1.1340e-01,  1.3538e-01,
        -1.1639e-01, -1.1559e-02,  6.2500e-02, -4.6631e-02, -6.3171e-02,
         4.8599e-03, -1.6675e-01, -3.7994e-02,  3.4637e-02,  1.6577e-01,
         9.7595e-02, -2.4826e-02, -3.0766e-03,  1.1060e-01,  1.0114e-01,
         1.2231e-01,  1.2878e-02, -2.4268e-01,  6.8045e-04,  4.6021e-02,
         1.0437e-02, -1.8762e-01, -1.3147e-01,  1.5393e-01,  2.9388e-02,
         2.2205e-01,  2.8610e-02,  3.0365e-02, -3.2257e-02,  8.6731e-02,
        -7.7820e-02, -6.4392e-02, -1.0144e-01, -4.7302e-02, -7.9529e-02,
        -6.4468e-03,  2.5317e-01,  1.1414e-01,  1.2793e-01,  9.6893e-03,
        -1.3931e-02,  4.8950e-02,  6.9275e-02, -4.1779e-02,  1.1096e-01,
         6.3057e-03,  1.3818e-01, -1.8625e-03, -1.6809e-01,  2.0935e-01,
         1.3208e-01, -1.3184e-01, -7.0984e-02,  1.1829e-01,  1.5942e-01,
        -1.6711e-01,  5.1697e-02, -2.3438e-02, -8.4106e-02,  7.5867e-02,
         6.1127e-02,  1.7639e-01,  7.8630e-04, -1.8591e-01, -5.0842e-02,
        -2.4402e-01, -4.5258e-02, -8.4000e-03,  5.9204e-02, -1.2611e-02,
        -2.6184e-02,  5.6488e-02,  1.5857e-01,  3.0322e-01, -1.6931e-01,
         6.1432e-02,  2.2424e-01, -3.1952e-02, -1.4270e-01, -1.4087e-01,
        -1.9455e-02,  9.7778e-02, -2.0703e-01, -2.0337e-01, -1.9699e-02,
        -2.3120e-01, -3.6133e-02,  6.3538e-02,  3.3569e-02,  3.3203e-02,
         4.7455e-02, -7.8613e-02,  2.0178e-01, -2.6016e-02,  1.5186e-01,
        -7.0740e-02,  2.3071e-02,  6.3171e-02,  7.5302e-03, -5.4550e-03,
         1.4429e-03, -1.9873e-01,  8.9844e-02,  3.3966e-02,  1.4978e-01,
        -1.0706e-01, -9.5642e-02, -1.9055e-01, -7.9651e-02, -2.3010e-01,
        -4.8218e-02,  1.1768e-01, -2.6993e-02,  2.4216e-02,  1.2805e-01,
         1.0254e-01, -6.7017e-02,  2.0532e-01, -1.9141e-01, -2.3706e-01,
        -1.5503e-01, -6.0272e-02,  7.2083e-02,  3.3169e-03, -2.0618e-01,
         1.2245e-02, -7.1289e-02,  8.6792e-02,  5.0232e-02, -3.9307e-02,
        -2.7512e-02,  8.1848e-02,  3.9368e-02,  9.4788e-02, -1.5373e-02,
         1.3367e-01, -4.4785e-03,  9.5398e-02, -2.1851e-02,  1.7017e-01,
         6.8481e-02, -3.4454e-02, -8.7036e-02, -1.4661e-01, -8.5938e-02,
        -2.7563e-01,  5.7190e-02, -1.0846e-01, -1.9312e-01, -1.0974e-01,
        -1.2012e-01,  3.4973e-02, -6.6650e-02,  1.8726e-01,  1.7651e-01,
        -4.8431e-02,  7.8247e-02,  3.9001e-02, -9.8755e-02,  9.0149e-02,
         2.0471e-01, -2.8613e-01,  1.0962e-01, -6.2256e-02,  2.3242e-01,
         4.7760e-02, -4.9957e-02, -1.1151e-01,  9.3384e-02,  6.3354e-02,
         1.0455e-01,  1.4001e-01,  1.1639e-01,  3.6774e-02, -1.3770e-01,
        -3.3325e-01, -1.2366e-01, -1.2402e-01,  2.1672e-04, -5.4688e-02,
        -3.9948e-02,  1.3342e-01, -1.2469e-01, -6.0196e-03,  7.2571e-02,
         8.7769e-02, -7.4463e-02, -2.3181e-01, -1.7639e-01, -1.3623e-01,
         8.8013e-02, -1.0242e-01,  5.3772e-02,  3.7933e-02, -1.6785e-01,
         4.8492e-02, -8.1421e-02, -1.2634e-01], device='cuda:0',
       requires_grad=True)]
2024-05-15 09:42:59.581 | INFO     | __main__:train:38 - [Parameter containing:
tensor([[ 0.0140,  0.0086, -0.0101,  ..., -0.0117, -0.0243,  0.0044],
        [-0.0255,  0.0537,  0.0109,  ...,  0.0122, -0.0141, -0.0050],
        [ 0.0052,  0.0088, -0.0233,  ..., -0.0006, -0.0374, -0.0270],
        ...,
        [-0.0082,  0.0195,  0.0208,  ...,  0.0177,  0.0125,  0.0179],
        [ 0.0060,  0.0263,  0.0043,  ...,  0.0071,  0.0258,  0.0259],
        [ 0.0220,  0.0209,  0.0165,  ..., -0.0156,  0.0295, -0.0047]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0138, -0.0109, -0.0047,  ...,  0.0097, -0.0180,  0.0049],
        [-0.0048,  0.0361, -0.0159,  ..., -0.0111,  0.0125,  0.0179],
        [-0.0059,  0.0111, -0.0034,  ..., -0.0119,  0.0029,  0.0209],
        ...,
        [-0.0116,  0.0152, -0.0191,  ...,  0.0415,  0.0013,  0.0277],
        [ 0.0257, -0.0091, -0.0276,  ...,  0.0088,  0.0040,  0.0081],
        [-0.0384,  0.0135, -0.0108,  ..., -0.0070, -0.0051,  0.0077]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0228, -0.0118, -0.0041,  ..., -0.0367,  0.0029,  0.0461],
        [-0.0385,  0.0101,  0.0005,  ..., -0.0171, -0.0031, -0.0187],
        [ 0.0123, -0.0247, -0.0075,  ...,  0.0023,  0.0374, -0.0162],
        ...,
        [ 0.0168, -0.0095, -0.0204,  ...,  0.0013,  0.0051,  0.0040],
        [ 0.0025, -0.0468, -0.0601,  ...,  0.0210, -0.0471,  0.0403],
        [-0.0046,  0.0167, -0.0252,  ...,  0.0092, -0.0288, -0.0052]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0270, -0.0419, -0.0426,  ...,  0.0132, -0.0078,  0.0168],
        [ 0.0090,  0.0196, -0.0313,  ..., -0.0302,  0.0011, -0.0198],
        [ 0.0317, -0.0116, -0.0089,  ...,  0.0257,  0.0005, -0.0264],
        ...,
        [-0.0119,  0.0238,  0.0203,  ..., -0.0065, -0.0003,  0.0055],
        [-0.0034, -0.0175,  0.0290,  ...,  0.0279, -0.0421, -0.0272],
        [ 0.0003, -0.0086,  0.0238,  ...,  0.0145, -0.0440,  0.0074]],
       device='cuda:0', requires_grad=True)]
2024-05-15 09:43:07.962 | INFO     | __main__:train:123 - Epoch: [0][0/46]	 loss 9.66506	 cls_loss: 5.4741 cluster_loss: 5.2685 sup_con_loss: 2.5100 contrastive_loss: 5.3017 
2024-05-15 09:43:25.773 | INFO     | __main__:train:123 - Epoch: [0][20/46]	 loss 7.62927	 cls_loss: 2.7763 cluster_loss: 4.3749 sup_con_loss: 2.0999 contrastive_loss: 4.7368 
2024-05-15 09:43:40.642 | INFO     | __main__:train:123 - Epoch: [0][40/46]	 loss 6.03531	 cls_loss: 1.1487 cluster_loss: 3.2774 sup_con_loss: 1.2834 contrastive_loss: 4.6980 
2024-05-15 09:43:44.491 | INFO     | __main__:train:126 - Train Epoch: 0 Avg Loss: 7.4799 
2024-05-15 09:43:44.492 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:44:06.814 | INFO     | __main__:train:135 - Train Accuracies: All 0.3963 | Old 0.7458 | New 0.0546
2024-05-15 09:44:12.298 | INFO     | __main__:train:123 - Epoch: [1][0/46]	 loss 5.71574	 cls_loss: 1.1623 cluster_loss: 2.8585 sup_con_loss: 1.2212 contrastive_loss: 4.6515 
2024-05-15 09:44:28.786 | INFO     | __main__:train:123 - Epoch: [1][20/46]	 loss 5.69471	 cls_loss: 1.0105 cluster_loss: 2.7135 sup_con_loss: 1.6032 contrastive_loss: 4.6402 
2024-05-15 09:44:43.825 | INFO     | __main__:train:123 - Epoch: [1][40/46]	 loss 5.46670	 cls_loss: 0.8989 cluster_loss: 2.6933 sup_con_loss: 1.0823 contrastive_loss: 4.6503 
2024-05-15 09:44:47.752 | INFO     | __main__:train:126 - Train Epoch: 1 Avg Loss: 5.6282 
2024-05-15 09:44:47.752 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:45:08.210 | INFO     | __main__:train:135 - Train Accuracies: All 0.4441 | Old 0.7929 | New 0.1031
2024-05-15 09:45:13.937 | INFO     | __main__:train:123 - Epoch: [2][0/46]	 loss 5.59989	 cls_loss: 0.9835 cluster_loss: 2.6987 sup_con_loss: 1.3488 contrastive_loss: 4.6606 
2024-05-15 09:45:30.591 | INFO     | __main__:train:123 - Epoch: [2][20/46]	 loss 5.41521	 cls_loss: 0.8035 cluster_loss: 2.6347 sup_con_loss: 1.1266 contrastive_loss: 4.6571 
2024-05-15 09:45:45.904 | INFO     | __main__:train:123 - Epoch: [2][40/46]	 loss 5.53880	 cls_loss: 0.7661 cluster_loss: 2.4758 sup_con_loss: 1.8305 contrastive_loss: 4.6473 
2024-05-15 09:45:49.910 | INFO     | __main__:train:126 - Train Epoch: 2 Avg Loss: 5.4297 
2024-05-15 09:45:49.910 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:46:11.592 | INFO     | __main__:train:135 - Train Accuracies: All 0.4727 | Old 0.8174 | New 0.1358
2024-05-15 09:46:16.714 | INFO     | __main__:train:123 - Epoch: [3][0/46]	 loss 5.30768	 cls_loss: 0.8877 cluster_loss: 2.4436 sup_con_loss: 1.1050 contrastive_loss: 4.6491 
2024-05-15 09:46:33.186 | INFO     | __main__:train:123 - Epoch: [3][20/46]	 loss 5.19456	 cls_loss: 0.6703 cluster_loss: 2.4471 sup_con_loss: 0.9829 contrastive_loss: 4.6543 
2024-05-15 09:46:48.835 | INFO     | __main__:train:123 - Epoch: [3][40/46]	 loss 5.17291	 cls_loss: 0.6555 cluster_loss: 2.3330 sup_con_loss: 1.1668 contrastive_loss: 4.6441 
2024-05-15 09:46:52.795 | INFO     | __main__:train:126 - Train Epoch: 3 Avg Loss: 5.2515 
2024-05-15 09:46:52.796 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:47:15.251 | INFO     | __main__:train:135 - Train Accuracies: All 0.4993 | Old 0.8272 | New 0.1788
2024-05-15 09:47:20.697 | INFO     | __main__:train:123 - Epoch: [4][0/46]	 loss 5.22525	 cls_loss: 0.7853 cluster_loss: 2.4348 sup_con_loss: 0.9686 contrastive_loss: 4.6597 
2024-05-15 09:47:37.946 | INFO     | __main__:train:123 - Epoch: [4][20/46]	 loss 5.06373	 cls_loss: 0.7090 cluster_loss: 2.1876 sup_con_loss: 1.0846 contrastive_loss: 4.6370 
2024-05-15 09:47:54.095 | INFO     | __main__:train:123 - Epoch: [4][40/46]	 loss 5.32414	 cls_loss: 0.7926 cluster_loss: 2.4517 sup_con_loss: 1.2132 contrastive_loss: 4.6593 
2024-05-15 09:47:58.130 | INFO     | __main__:train:126 - Train Epoch: 4 Avg Loss: 5.1836 
2024-05-15 09:47:58.130 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:48:20.707 | INFO     | __main__:train:135 - Train Accuracies: All 0.5290 | Old 0.8247 | New 0.2399
2024-05-15 09:48:26.426 | INFO     | __main__:train:123 - Epoch: [5][0/46]	 loss 5.22307	 cls_loss: 0.7602 cluster_loss: 2.2752 sup_con_loss: 1.3133 contrastive_loss: 4.6438 
2024-05-15 09:48:43.565 | INFO     | __main__:train:123 - Epoch: [5][20/46]	 loss 5.16503	 cls_loss: 0.6324 cluster_loss: 2.3204 sup_con_loss: 1.1621 contrastive_loss: 4.6595 
2024-05-15 09:48:59.383 | INFO     | __main__:train:123 - Epoch: [5][40/46]	 loss 5.14146	 cls_loss: 0.7050 cluster_loss: 2.1096 sup_con_loss: 1.4455 contrastive_loss: 4.6424 
2024-05-15 09:49:03.381 | INFO     | __main__:train:126 - Train Epoch: 5 Avg Loss: 5.0928 
2024-05-15 09:49:03.381 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:49:25.747 | INFO     | __main__:train:135 - Train Accuracies: All 0.5506 | Old 0.8310 | New 0.2765
2024-05-15 09:49:31.835 | INFO     | __main__:train:123 - Epoch: [6][0/46]	 loss 5.07636	 cls_loss: 0.7498 cluster_loss: 2.1223 sup_con_loss: 1.1897 contrastive_loss: 4.6431 
2024-05-15 09:49:49.149 | INFO     | __main__:train:123 - Epoch: [6][20/46]	 loss 5.03815	 cls_loss: 0.6735 cluster_loss: 2.1375 sup_con_loss: 1.1192 contrastive_loss: 4.6482 
2024-05-15 09:50:05.098 | INFO     | __main__:train:123 - Epoch: [6][40/46]	 loss 4.97779	 cls_loss: 0.6181 cluster_loss: 2.0832 sup_con_loss: 1.1083 contrastive_loss: 4.6454 
2024-05-15 09:50:09.081 | INFO     | __main__:train:126 - Train Epoch: 6 Avg Loss: 5.0317 
2024-05-15 09:50:09.082 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:50:31.217 | INFO     | __main__:train:135 - Train Accuracies: All 0.5740 | Old 0.8324 | New 0.3215
2024-05-15 09:50:36.676 | INFO     | __main__:train:123 - Epoch: [7][0/46]	 loss 5.11325	 cls_loss: 0.6373 cluster_loss: 2.1010 sup_con_loss: 1.4696 contrastive_loss: 4.6310 
2024-05-15 09:50:54.055 | INFO     | __main__:train:123 - Epoch: [7][20/46]	 loss 5.02640	 cls_loss: 0.6775 cluster_loss: 2.1690 sup_con_loss: 1.0259 contrastive_loss: 4.6468 
2024-05-15 09:51:09.972 | INFO     | __main__:train:123 - Epoch: [7][40/46]	 loss 5.01410	 cls_loss: 0.7228 cluster_loss: 1.9608 sup_con_loss: 1.3409 contrastive_loss: 4.6420 
2024-05-15 09:51:14.028 | INFO     | __main__:train:126 - Train Epoch: 7 Avg Loss: 4.9626 
2024-05-15 09:51:14.029 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:51:36.144 | INFO     | __main__:train:135 - Train Accuracies: All 0.5910 | Old 0.8376 | New 0.3498
2024-05-15 09:51:42.208 | INFO     | __main__:train:123 - Epoch: [8][0/46]	 loss 4.98728	 cls_loss: 0.6682 cluster_loss: 2.0393 sup_con_loss: 1.1671 contrastive_loss: 4.6452 
2024-05-15 09:51:59.498 | INFO     | __main__:train:123 - Epoch: [8][20/46]	 loss 4.91968	 cls_loss: 0.7537 cluster_loss: 2.0845 sup_con_loss: 0.8153 contrastive_loss: 4.6394 
2024-05-15 09:52:15.160 | INFO     | __main__:train:123 - Epoch: [8][40/46]	 loss 4.96808	 cls_loss: 0.6623 cluster_loss: 1.9742 sup_con_loss: 1.2238 contrastive_loss: 4.6534 
2024-05-15 09:52:19.135 | INFO     | __main__:train:126 - Train Epoch: 8 Avg Loss: 4.8976 
2024-05-15 09:52:19.135 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:52:41.273 | INFO     | __main__:train:135 - Train Accuracies: All 0.6093 | Old 0.8279 | New 0.3956
2024-05-15 09:52:46.605 | INFO     | __main__:train:123 - Epoch: [9][0/46]	 loss 4.70732	 cls_loss: 0.5331 cluster_loss: 1.8261 sup_con_loss: 0.9190 contrastive_loss: 4.6340 
2024-05-15 09:53:03.858 | INFO     | __main__:train:123 - Epoch: [9][20/46]	 loss 4.83748	 cls_loss: 0.6763 cluster_loss: 1.9710 sup_con_loss: 0.8342 contrastive_loss: 4.6579 
2024-05-15 09:53:19.682 | INFO     | __main__:train:123 - Epoch: [9][40/46]	 loss 4.66718	 cls_loss: 0.6575 cluster_loss: 1.8325 sup_con_loss: 0.6648 contrastive_loss: 4.6357 
2024-05-15 09:53:23.867 | INFO     | __main__:train:126 - Train Epoch: 9 Avg Loss: 4.8389 
2024-05-15 09:53:23.869 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:53:45.667 | INFO     | __main__:train:135 - Train Accuracies: All 0.6155 | Old 0.8307 | New 0.4051
2024-05-15 09:53:50.805 | INFO     | __main__:train:123 - Epoch: [10][0/46]	 loss 4.67309	 cls_loss: 0.6521 cluster_loss: 1.7623 sup_con_loss: 0.8312 contrastive_loss: 4.6283 
2024-05-15 09:54:08.340 | INFO     | __main__:train:123 - Epoch: [10][20/46]	 loss 4.87745	 cls_loss: 0.7079 cluster_loss: 1.9961 sup_con_loss: 0.8923 contrastive_loss: 4.6460 
2024-05-15 09:54:24.330 | INFO     | __main__:train:123 - Epoch: [10][40/46]	 loss 4.80946	 cls_loss: 0.5950 cluster_loss: 1.8808 sup_con_loss: 1.0299 contrastive_loss: 4.6434 
2024-05-15 09:54:28.338 | INFO     | __main__:train:126 - Train Epoch: 10 Avg Loss: 4.7836 
2024-05-15 09:54:28.339 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:54:50.357 | INFO     | __main__:train:135 - Train Accuracies: All 0.6224 | Old 0.8314 | New 0.4181
2024-05-15 09:54:56.922 | INFO     | __main__:train:123 - Epoch: [11][0/46]	 loss 4.72031	 cls_loss: 0.6156 cluster_loss: 1.7885 sup_con_loss: 0.9525 contrastive_loss: 4.6291 
2024-05-15 09:55:13.512 | INFO     | __main__:train:123 - Epoch: [11][20/46]	 loss 4.67338	 cls_loss: 0.4940 cluster_loss: 1.8066 sup_con_loss: 0.8822 contrastive_loss: 4.6423 
2024-05-15 09:55:29.257 | INFO     | __main__:train:123 - Epoch: [11][40/46]	 loss 4.69111	 cls_loss: 0.6833 cluster_loss: 1.8078 sup_con_loss: 0.7478 contrastive_loss: 4.6387 
2024-05-15 09:55:33.255 | INFO     | __main__:train:126 - Train Epoch: 11 Avg Loss: 4.7475 
2024-05-15 09:55:33.256 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:55:55.647 | INFO     | __main__:train:135 - Train Accuracies: All 0.6210 | Old 0.8286 | New 0.4181
2024-05-15 09:56:01.491 | INFO     | __main__:train:123 - Epoch: [12][0/46]	 loss 4.78449	 cls_loss: 0.6159 cluster_loss: 1.8951 sup_con_loss: 0.9164 contrastive_loss: 4.6405 
2024-05-15 09:56:19.274 | INFO     | __main__:train:123 - Epoch: [12][20/46]	 loss 4.73559	 cls_loss: 0.6202 cluster_loss: 1.7996 sup_con_loss: 0.9548 contrastive_loss: 4.6378 
2024-05-15 09:56:35.146 | INFO     | __main__:train:123 - Epoch: [12][40/46]	 loss 4.68555	 cls_loss: 0.6312 cluster_loss: 1.7104 sup_con_loss: 0.9889 contrastive_loss: 4.6258 
2024-05-15 09:56:39.143 | INFO     | __main__:train:126 - Train Epoch: 12 Avg Loss: 4.7104 
2024-05-15 09:56:39.144 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:57:01.609 | INFO     | __main__:train:135 - Train Accuracies: All 0.6332 | Old 0.8237 | New 0.4471
2024-05-15 09:57:09.337 | INFO     | __main__:train:123 - Epoch: [13][0/46]	 loss 4.76568	 cls_loss: 0.5621 cluster_loss: 1.7556 sup_con_loss: 1.1865 contrastive_loss: 4.6347 
2024-05-15 09:57:27.022 | INFO     | __main__:train:123 - Epoch: [13][20/46]	 loss 4.69509	 cls_loss: 0.5835 cluster_loss: 1.7031 sup_con_loss: 1.0451 contrastive_loss: 4.6432 
2024-05-15 09:57:42.868 | INFO     | __main__:train:123 - Epoch: [13][40/46]	 loss 4.59559	 cls_loss: 0.5876 cluster_loss: 1.6263 sup_con_loss: 0.9196 contrastive_loss: 4.6322 
2024-05-15 09:57:46.873 | INFO     | __main__:train:126 - Train Epoch: 13 Avg Loss: 4.6651 
2024-05-15 09:57:46.873 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:58:08.949 | INFO     | __main__:train:135 - Train Accuracies: All 0.6464 | Old 0.8251 | New 0.4717
2024-05-15 09:58:14.513 | INFO     | __main__:train:123 - Epoch: [14][0/46]	 loss 4.70218	 cls_loss: 0.5833 cluster_loss: 1.7156 sup_con_loss: 1.0445 contrastive_loss: 4.6420 
2024-05-15 09:58:32.117 | INFO     | __main__:train:123 - Epoch: [14][20/46]	 loss 4.57503	 cls_loss: 0.5564 cluster_loss: 1.7161 sup_con_loss: 0.7177 contrastive_loss: 4.6364 
2024-05-15 09:58:47.843 | INFO     | __main__:train:123 - Epoch: [14][40/46]	 loss 4.66942	 cls_loss: 0.6938 cluster_loss: 1.6921 sup_con_loss: 0.8942 contrastive_loss: 4.6366 
2024-05-15 09:58:51.939 | INFO     | __main__:train:126 - Train Epoch: 14 Avg Loss: 4.6551 
2024-05-15 09:58:51.940 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:59:13.550 | INFO     | __main__:train:135 - Train Accuracies: All 0.6522 | Old 0.8282 | New 0.4802
2024-05-15 09:59:19.202 | INFO     | __main__:train:123 - Epoch: [15][0/46]	 loss 4.66426	 cls_loss: 0.5931 cluster_loss: 1.6999 sup_con_loss: 0.9648 contrastive_loss: 4.6370 
2024-05-15 09:59:35.907 | INFO     | __main__:train:123 - Epoch: [15][20/46]	 loss 4.49547	 cls_loss: 0.5243 cluster_loss: 1.5586 sup_con_loss: 0.8368 contrastive_loss: 4.6246 
2024-05-15 09:59:51.403 | INFO     | __main__:train:123 - Epoch: [15][40/46]	 loss 4.55068	 cls_loss: 0.6052 cluster_loss: 1.5540 sup_con_loss: 0.9263 contrastive_loss: 4.6224 
2024-05-15 09:59:55.340 | INFO     | __main__:train:126 - Train Epoch: 15 Avg Loss: 4.6029 
2024-05-15 09:59:55.341 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:00:16.571 | INFO     | __main__:train:135 - Train Accuracies: All 0.6543 | Old 0.8275 | New 0.4850
2024-05-15 10:00:22.669 | INFO     | __main__:train:123 - Epoch: [16][0/46]	 loss 4.44654	 cls_loss: 0.4115 cluster_loss: 1.5880 sup_con_loss: 0.7453 contrastive_loss: 4.6300 
2024-05-15 10:00:39.201 | INFO     | __main__:train:123 - Epoch: [16][20/46]	 loss 4.70396	 cls_loss: 0.6570 cluster_loss: 1.7497 sup_con_loss: 0.9134 contrastive_loss: 4.6415 
2024-05-15 10:00:54.674 | INFO     | __main__:train:123 - Epoch: [16][40/46]	 loss 4.40238	 cls_loss: 0.5408 cluster_loss: 1.5094 sup_con_loss: 0.6258 contrastive_loss: 4.6353 
2024-05-15 10:00:58.597 | INFO     | __main__:train:126 - Train Epoch: 16 Avg Loss: 4.5725 
2024-05-15 10:00:58.598 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:01:21.801 | INFO     | __main__:train:135 - Train Accuracies: All 0.6634 | Old 0.8275 | New 0.5031
2024-05-15 10:01:27.511 | INFO     | __main__:train:123 - Epoch: [17][0/46]	 loss 4.58042	 cls_loss: 0.5664 cluster_loss: 1.6224 sup_con_loss: 0.9116 contrastive_loss: 4.6285 
2024-05-15 10:01:44.190 | INFO     | __main__:train:123 - Epoch: [17][20/46]	 loss 4.54966	 cls_loss: 0.5359 cluster_loss: 1.6056 sup_con_loss: 0.8675 contrastive_loss: 4.6382 
2024-05-15 10:01:59.426 | INFO     | __main__:train:123 - Epoch: [17][40/46]	 loss 4.49785	 cls_loss: 0.5235 cluster_loss: 1.5557 sup_con_loss: 0.8378 contrastive_loss: 4.6311 
2024-05-15 10:02:03.312 | INFO     | __main__:train:126 - Train Epoch: 17 Avg Loss: 4.5529 
2024-05-15 10:02:03.313 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:02:24.710 | INFO     | __main__:train:135 - Train Accuracies: All 0.6695 | Old 0.8324 | New 0.5102
2024-05-15 10:02:30.804 | INFO     | __main__:train:123 - Epoch: [18][0/46]	 loss 4.52228	 cls_loss: 0.5179 cluster_loss: 1.5514 sup_con_loss: 0.9190 contrastive_loss: 4.6322 
2024-05-15 10:02:47.067 | INFO     | __main__:train:123 - Epoch: [18][20/46]	 loss 4.49227	 cls_loss: 0.5347 cluster_loss: 1.4971 sup_con_loss: 0.9351 contrastive_loss: 4.6227 
2024-05-15 10:03:02.363 | INFO     | __main__:train:123 - Epoch: [18][40/46]	 loss 4.54493	 cls_loss: 0.5230 cluster_loss: 1.6578 sup_con_loss: 0.7844 contrastive_loss: 4.6304 
2024-05-15 10:03:06.227 | INFO     | __main__:train:126 - Train Epoch: 18 Avg Loss: 4.5315 
2024-05-15 10:03:06.228 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:03:27.308 | INFO     | __main__:train:135 - Train Accuracies: All 0.6712 | Old 0.8328 | New 0.5133
2024-05-15 10:03:32.760 | INFO     | __main__:train:123 - Epoch: [19][0/46]	 loss 4.61779	 cls_loss: 0.5737 cluster_loss: 1.5726 sup_con_loss: 1.0935 contrastive_loss: 4.6340 
2024-05-15 10:03:49.115 | INFO     | __main__:train:123 - Epoch: [19][20/46]	 loss 4.59369	 cls_loss: 0.5935 cluster_loss: 1.5452 sup_con_loss: 1.0727 contrastive_loss: 4.6248 
2024-05-15 10:04:04.557 | INFO     | __main__:train:123 - Epoch: [19][40/46]	 loss 4.48383	 cls_loss: 0.5946 cluster_loss: 1.4654 sup_con_loss: 0.8949 contrastive_loss: 4.6307 
2024-05-15 10:04:08.362 | INFO     | __main__:train:126 - Train Epoch: 19 Avg Loss: 4.5026 
2024-05-15 10:04:08.362 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:04:29.728 | INFO     | __main__:train:135 - Train Accuracies: All 0.6747 | Old 0.8321 | New 0.5208
2024-05-15 10:04:35.555 | INFO     | __main__:train:123 - Epoch: [20][0/46]	 loss 4.44326	 cls_loss: 0.5040 cluster_loss: 1.5385 sup_con_loss: 0.7228 contrastive_loss: 4.6367 
2024-05-15 10:04:51.966 | INFO     | __main__:train:123 - Epoch: [20][20/46]	 loss 4.45261	 cls_loss: 0.5138 cluster_loss: 1.4803 sup_con_loss: 0.8393 contrastive_loss: 4.6413 
2024-05-15 10:05:07.262 | INFO     | __main__:train:123 - Epoch: [20][40/46]	 loss 4.42962	 cls_loss: 0.4654 cluster_loss: 1.5013 sup_con_loss: 0.7962 contrastive_loss: 4.6342 
2024-05-15 10:05:11.237 | INFO     | __main__:train:126 - Train Epoch: 20 Avg Loss: 4.4731 
2024-05-15 10:05:11.238 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:05:32.233 | INFO     | __main__:train:135 - Train Accuracies: All 0.6802 | Old 0.8303 | New 0.5334
2024-05-15 10:05:38.138 | INFO     | __main__:train:123 - Epoch: [21][0/46]	 loss 4.42857	 cls_loss: 0.4790 cluster_loss: 1.5015 sup_con_loss: 0.7951 contrastive_loss: 4.6256 
2024-05-15 10:05:54.668 | INFO     | __main__:train:123 - Epoch: [21][20/46]	 loss 4.42920	 cls_loss: 0.5253 cluster_loss: 1.4719 sup_con_loss: 0.8049 contrastive_loss: 4.6260 
2024-05-15 10:06:10.154 | INFO     | __main__:train:123 - Epoch: [21][40/46]	 loss 4.55365	 cls_loss: 0.5671 cluster_loss: 1.5763 sup_con_loss: 0.9112 contrastive_loss: 4.6333 
2024-05-15 10:06:14.072 | INFO     | __main__:train:126 - Train Epoch: 21 Avg Loss: 4.4631 
2024-05-15 10:06:14.073 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:06:34.884 | INFO     | __main__:train:135 - Train Accuracies: All 0.6804 | Old 0.8328 | New 0.5314
2024-05-15 10:06:41.436 | INFO     | __main__:train:123 - Epoch: [22][0/46]	 loss 4.26843	 cls_loss: 0.4536 cluster_loss: 1.3922 sup_con_loss: 0.5701 contrastive_loss: 4.6233 
2024-05-15 10:06:57.641 | INFO     | __main__:train:123 - Epoch: [22][20/46]	 loss 4.42710	 cls_loss: 0.4886 cluster_loss: 1.4757 sup_con_loss: 0.8114 contrastive_loss: 4.6353 
2024-05-15 10:07:13.043 | INFO     | __main__:train:123 - Epoch: [22][40/46]	 loss 4.44139	 cls_loss: 0.4591 cluster_loss: 1.4791 sup_con_loss: 0.8692 contrastive_loss: 4.6385 
2024-05-15 10:07:16.937 | INFO     | __main__:train:126 - Train Epoch: 22 Avg Loss: 4.4273 
2024-05-15 10:07:16.938 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:07:38.325 | INFO     | __main__:train:135 - Train Accuracies: All 0.6829 | Old 0.8317 | New 0.5375
2024-05-15 10:07:44.231 | INFO     | __main__:train:123 - Epoch: [23][0/46]	 loss 4.50200	 cls_loss: 0.4568 cluster_loss: 1.4877 sup_con_loss: 1.0633 contrastive_loss: 4.6200 
2024-05-15 10:08:00.768 | INFO     | __main__:train:123 - Epoch: [23][20/46]	 loss 4.41790	 cls_loss: 0.4961 cluster_loss: 1.4474 sup_con_loss: 0.8446 contrastive_loss: 4.6274 
2024-05-15 10:08:16.106 | INFO     | __main__:train:123 - Epoch: [23][40/46]	 loss 4.31237	 cls_loss: 0.5069 cluster_loss: 1.3698 sup_con_loss: 0.6687 contrastive_loss: 4.6317 
2024-05-15 10:08:20.053 | INFO     | __main__:train:126 - Train Epoch: 23 Avg Loss: 4.4267 
2024-05-15 10:08:20.053 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:08:42.043 | INFO     | __main__:train:135 - Train Accuracies: All 0.6912 | Old 0.8373 | New 0.5485
2024-05-15 10:08:48.556 | INFO     | __main__:train:123 - Epoch: [24][0/46]	 loss 4.48101	 cls_loss: 0.6232 cluster_loss: 1.4550 sup_con_loss: 0.8749 contrastive_loss: 4.6323 
2024-05-15 10:09:05.274 | INFO     | __main__:train:123 - Epoch: [24][20/46]	 loss 4.51555	 cls_loss: 0.5012 cluster_loss: 1.4814 sup_con_loss: 1.0440 contrastive_loss: 4.6336 
2024-05-15 10:09:21.147 | INFO     | __main__:train:123 - Epoch: [24][40/46]	 loss 4.22602	 cls_loss: 0.4764 cluster_loss: 1.2849 sup_con_loss: 0.6234 contrastive_loss: 4.6245 
2024-05-15 10:09:25.047 | INFO     | __main__:train:126 - Train Epoch: 24 Avg Loss: 4.4037 
2024-05-15 10:09:25.047 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:09:47.214 | INFO     | __main__:train:135 - Train Accuracies: All 0.6950 | Old 0.8328 | New 0.5604
2024-05-15 10:09:52.678 | INFO     | __main__:train:123 - Epoch: [25][0/46]	 loss 4.35583	 cls_loss: 0.4831 cluster_loss: 1.4014 sup_con_loss: 0.7644 contrastive_loss: 4.6281 
2024-05-15 10:10:10.413 | INFO     | __main__:train:123 - Epoch: [25][20/46]	 loss 4.43794	 cls_loss: 0.5510 cluster_loss: 1.4286 sup_con_loss: 0.8713 contrastive_loss: 4.6332 
2024-05-15 10:10:26.386 | INFO     | __main__:train:123 - Epoch: [25][40/46]	 loss 4.39128	 cls_loss: 0.4768 cluster_loss: 1.4107 sup_con_loss: 0.8601 contrastive_loss: 4.6252 
2024-05-15 10:10:30.460 | INFO     | __main__:train:126 - Train Epoch: 25 Avg Loss: 4.3751 
2024-05-15 10:10:30.461 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:10:52.775 | INFO     | __main__:train:135 - Train Accuracies: All 0.7006 | Old 0.8334 | New 0.5706
2024-05-15 10:10:58.929 | INFO     | __main__:train:123 - Epoch: [26][0/46]	 loss 4.37745	 cls_loss: 0.4790 cluster_loss: 1.3770 sup_con_loss: 0.8965 contrastive_loss: 4.6169 
2024-05-15 10:11:16.262 | INFO     | __main__:train:123 - Epoch: [26][20/46]	 loss 4.34304	 cls_loss: 0.5072 cluster_loss: 1.3399 sup_con_loss: 0.8340 contrastive_loss: 4.6195 
2024-05-15 10:11:32.001 | INFO     | __main__:train:123 - Epoch: [26][40/46]	 loss 4.30585	 cls_loss: 0.4428 cluster_loss: 1.3363 sup_con_loss: 0.7842 contrastive_loss: 4.6273 
2024-05-15 10:11:35.966 | INFO     | __main__:train:126 - Train Epoch: 26 Avg Loss: 4.3579 
2024-05-15 10:11:35.967 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:11:58.454 | INFO     | __main__:train:135 - Train Accuracies: All 0.7009 | Old 0.8324 | New 0.5724
2024-05-15 10:12:05.926 | INFO     | __main__:train:123 - Epoch: [27][0/46]	 loss 4.41485	 cls_loss: 0.4775 cluster_loss: 1.4490 sup_con_loss: 0.8600 contrastive_loss: 4.6230 
2024-05-15 10:12:23.288 | INFO     | __main__:train:123 - Epoch: [27][20/46]	 loss 4.28562	 cls_loss: 0.4608 cluster_loss: 1.3574 sup_con_loss: 0.6677 contrastive_loss: 4.6282 
2024-05-15 10:12:39.107 | INFO     | __main__:train:123 - Epoch: [27][40/46]	 loss 4.34161	 cls_loss: 0.5018 cluster_loss: 1.3617 sup_con_loss: 0.7736 contrastive_loss: 4.6310 
2024-05-15 10:12:43.131 | INFO     | __main__:train:126 - Train Epoch: 27 Avg Loss: 4.3341 
2024-05-15 10:12:43.132 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:13:04.938 | INFO     | __main__:train:135 - Train Accuracies: All 0.7031 | Old 0.8279 | New 0.5812
2024-05-15 10:13:11.266 | INFO     | __main__:train:123 - Epoch: [28][0/46]	 loss 4.41925	 cls_loss: 0.4725 cluster_loss: 1.4177 sup_con_loss: 0.9364 contrastive_loss: 4.6225 
2024-05-15 10:13:28.322 | INFO     | __main__:train:123 - Epoch: [28][20/46]	 loss 4.36270	 cls_loss: 0.4686 cluster_loss: 1.3423 sup_con_loss: 0.9062 contrastive_loss: 4.6293 
2024-05-15 10:13:43.847 | INFO     | __main__:train:123 - Epoch: [28][40/46]	 loss 4.33978	 cls_loss: 0.4995 cluster_loss: 1.3579 sup_con_loss: 0.7766 contrastive_loss: 4.6316 
2024-05-15 10:13:47.711 | INFO     | __main__:train:126 - Train Epoch: 28 Avg Loss: 4.3484 
2024-05-15 10:13:47.712 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:14:09.691 | INFO     | __main__:train:135 - Train Accuracies: All 0.7018 | Old 0.8307 | New 0.5758
2024-05-15 10:14:14.767 | INFO     | __main__:train:123 - Epoch: [29][0/46]	 loss 4.31303	 cls_loss: 0.5476 cluster_loss: 1.3318 sup_con_loss: 0.7135 contrastive_loss: 4.6246 
2024-05-15 10:14:32.204 | INFO     | __main__:train:123 - Epoch: [29][20/46]	 loss 4.32294	 cls_loss: 0.4714 cluster_loss: 1.3409 sup_con_loss: 0.7823 contrastive_loss: 4.6347 
2024-05-15 10:14:47.853 | INFO     | __main__:train:123 - Epoch: [29][40/46]	 loss 4.27134	 cls_loss: 0.4593 cluster_loss: 1.3512 sup_con_loss: 0.6355 contrastive_loss: 4.6306 
2024-05-15 10:14:51.817 | INFO     | __main__:train:126 - Train Epoch: 29 Avg Loss: 4.3056 
2024-05-15 10:14:51.818 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:15:14.464 | INFO     | __main__:train:135 - Train Accuracies: All 0.7069 | Old 0.8328 | New 0.5840
2024-05-15 10:15:20.732 | INFO     | __main__:train:123 - Epoch: [30][0/46]	 loss 4.39922	 cls_loss: 0.5045 cluster_loss: 1.3782 sup_con_loss: 0.8974 contrastive_loss: 4.6350 
2024-05-15 10:15:37.608 | INFO     | __main__:train:123 - Epoch: [30][20/46]	 loss 4.29944	 cls_loss: 0.4396 cluster_loss: 1.3454 sup_con_loss: 0.7481 contrastive_loss: 4.6296 
2024-05-15 10:15:53.328 | INFO     | __main__:train:123 - Epoch: [30][40/46]	 loss 4.27287	 cls_loss: 0.4338 cluster_loss: 1.3156 sup_con_loss: 0.7495 contrastive_loss: 4.6209 
2024-05-15 10:15:57.248 | INFO     | __main__:train:126 - Train Epoch: 30 Avg Loss: 4.3123 
2024-05-15 10:15:57.250 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:16:18.686 | INFO     | __main__:train:135 - Train Accuracies: All 0.7049 | Old 0.8310 | New 0.5816
2024-05-15 10:16:24.783 | INFO     | __main__:train:123 - Epoch: [31][0/46]	 loss 4.23689	 cls_loss: 0.4923 cluster_loss: 1.3067 sup_con_loss: 0.6126 contrastive_loss: 4.6167 
2024-05-15 10:16:41.786 | INFO     | __main__:train:123 - Epoch: [31][20/46]	 loss 4.29027	 cls_loss: 0.4494 cluster_loss: 1.2969 sup_con_loss: 0.8184 contrastive_loss: 4.6208 
2024-05-15 10:16:57.286 | INFO     | __main__:train:123 - Epoch: [31][40/46]	 loss 4.31790	 cls_loss: 0.4518 cluster_loss: 1.3502 sup_con_loss: 0.7720 contrastive_loss: 4.6337 
2024-05-15 10:17:01.167 | INFO     | __main__:train:126 - Train Epoch: 31 Avg Loss: 4.2972 
2024-05-15 10:17:01.168 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:17:23.148 | INFO     | __main__:train:135 - Train Accuracies: All 0.7043 | Old 0.8272 | New 0.5843
2024-05-15 10:17:29.795 | INFO     | __main__:train:123 - Epoch: [32][0/46]	 loss 4.37036	 cls_loss: 0.4393 cluster_loss: 1.4557 sup_con_loss: 0.7491 contrastive_loss: 4.6280 
2024-05-15 10:17:46.403 | INFO     | __main__:train:123 - Epoch: [32][20/46]	 loss 4.27770	 cls_loss: 0.4472 cluster_loss: 1.3092 sup_con_loss: 0.7552 contrastive_loss: 4.6244 
2024-05-15 10:18:02.056 | INFO     | __main__:train:123 - Epoch: [32][40/46]	 loss 4.46660	 cls_loss: 0.4287 cluster_loss: 1.4892 sup_con_loss: 0.9612 contrastive_loss: 4.6341 
2024-05-15 10:18:06.060 | INFO     | __main__:train:126 - Train Epoch: 32 Avg Loss: 4.2884 
2024-05-15 10:18:06.060 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:18:28.102 | INFO     | __main__:train:135 - Train Accuracies: All 0.7057 | Old 0.8268 | New 0.5874
2024-05-15 10:18:33.289 | INFO     | __main__:train:123 - Epoch: [33][0/46]	 loss 4.22404	 cls_loss: 0.4283 cluster_loss: 1.2898 sup_con_loss: 0.6680 contrastive_loss: 4.6183 
2024-05-15 10:18:51.059 | INFO     | __main__:train:123 - Epoch: [33][20/46]	 loss 4.20903	 cls_loss: 0.4403 cluster_loss: 1.2946 sup_con_loss: 0.5991 contrastive_loss: 4.6212 
2024-05-15 10:19:06.898 | INFO     | __main__:train:123 - Epoch: [33][40/46]	 loss 4.24857	 cls_loss: 0.4340 cluster_loss: 1.3154 sup_con_loss: 0.6652 contrastive_loss: 4.6289 
2024-05-15 10:19:10.780 | INFO     | __main__:train:126 - Train Epoch: 33 Avg Loss: 4.2760 
2024-05-15 10:19:10.780 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:19:33.306 | INFO     | __main__:train:135 - Train Accuracies: All 0.7095 | Old 0.8265 | New 0.5952
2024-05-15 10:19:39.430 | INFO     | __main__:train:123 - Epoch: [34][0/46]	 loss 4.35801	 cls_loss: 0.4617 cluster_loss: 1.4250 sup_con_loss: 0.7502 contrastive_loss: 4.6270 
2024-05-15 10:19:56.705 | INFO     | __main__:train:123 - Epoch: [34][20/46]	 loss 4.34693	 cls_loss: 0.3999 cluster_loss: 1.3763 sup_con_loss: 0.8657 contrastive_loss: 4.6298 
2024-05-15 10:20:12.422 | INFO     | __main__:train:123 - Epoch: [34][40/46]	 loss 4.28628	 cls_loss: 0.4261 cluster_loss: 1.3582 sup_con_loss: 0.7088 contrastive_loss: 4.6250 
2024-05-15 10:20:16.362 | INFO     | __main__:train:126 - Train Epoch: 34 Avg Loss: 4.2776 
2024-05-15 10:20:16.363 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:20:38.752 | INFO     | __main__:train:135 - Train Accuracies: All 0.7078 | Old 0.8275 | New 0.5908
2024-05-15 10:20:44.025 | INFO     | __main__:train:123 - Epoch: [35][0/46]	 loss 4.26876	 cls_loss: 0.4213 cluster_loss: 1.3010 sup_con_loss: 0.7532 contrastive_loss: 4.6339 
2024-05-15 10:21:01.739 | INFO     | __main__:train:123 - Epoch: [35][20/46]	 loss 4.39054	 cls_loss: 0.5431 cluster_loss: 1.4064 sup_con_loss: 0.8029 contrastive_loss: 4.6235 
2024-05-15 10:21:17.523 | INFO     | __main__:train:123 - Epoch: [35][40/46]	 loss 4.23366	 cls_loss: 0.4464 cluster_loss: 1.2968 sup_con_loss: 0.6710 contrastive_loss: 4.6149 
2024-05-15 10:21:21.621 | INFO     | __main__:train:126 - Train Epoch: 35 Avg Loss: 4.2714 
2024-05-15 10:21:21.621 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:21:44.092 | INFO     | __main__:train:135 - Train Accuracies: All 0.7097 | Old 0.8369 | New 0.5853
2024-05-15 10:21:49.412 | INFO     | __main__:train:123 - Epoch: [36][0/46]	 loss 4.24896	 cls_loss: 0.4646 cluster_loss: 1.3037 sup_con_loss: 0.6866 contrastive_loss: 4.6133 
2024-05-15 10:22:07.072 | INFO     | __main__:train:123 - Epoch: [36][20/46]	 loss 4.33515	 cls_loss: 0.4074 cluster_loss: 1.4044 sup_con_loss: 0.7713 contrastive_loss: 4.6304 
2024-05-15 10:22:23.060 | INFO     | __main__:train:123 - Epoch: [36][40/46]	 loss 4.28033	 cls_loss: 0.4358 cluster_loss: 1.3798 sup_con_loss: 0.6272 contrastive_loss: 4.6329 
2024-05-15 10:22:27.004 | INFO     | __main__:train:126 - Train Epoch: 36 Avg Loss: 4.2430 
2024-05-15 10:22:27.005 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:22:49.309 | INFO     | __main__:train:135 - Train Accuracies: All 0.7078 | Old 0.8321 | New 0.5863
2024-05-15 10:22:55.823 | INFO     | __main__:train:123 - Epoch: [37][0/46]	 loss 4.19382	 cls_loss: 0.4129 cluster_loss: 1.2564 sup_con_loss: 0.6591 contrastive_loss: 4.6185 
2024-05-15 10:23:12.475 | INFO     | __main__:train:123 - Epoch: [37][20/46]	 loss 4.33162	 cls_loss: 0.4529 cluster_loss: 1.3469 sup_con_loss: 0.8447 contrastive_loss: 4.6184 
2024-05-15 10:23:28.161 | INFO     | __main__:train:123 - Epoch: [37][40/46]	 loss 4.32782	 cls_loss: 0.4780 cluster_loss: 1.3746 sup_con_loss: 0.7277 contrastive_loss: 4.6344 
2024-05-15 10:23:32.152 | INFO     | __main__:train:126 - Train Epoch: 37 Avg Loss: 4.2804 
2024-05-15 10:23:32.153 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:23:54.296 | INFO     | __main__:train:135 - Train Accuracies: All 0.7107 | Old 0.8303 | New 0.5939
2024-05-15 10:23:59.389 | INFO     | __main__:train:123 - Epoch: [38][0/46]	 loss 4.22929	 cls_loss: 0.4393 cluster_loss: 1.2932 sup_con_loss: 0.6761 contrastive_loss: 4.6128 
2024-05-15 10:24:17.097 | INFO     | __main__:train:123 - Epoch: [38][20/46]	 loss 4.25115	 cls_loss: 0.4322 cluster_loss: 1.3570 sup_con_loss: 0.6137 contrastive_loss: 4.6201 
2024-05-15 10:24:32.692 | INFO     | __main__:train:123 - Epoch: [38][40/46]	 loss 4.12751	 cls_loss: 0.4195 cluster_loss: 1.2166 sup_con_loss: 0.5418 contrastive_loss: 4.6158 
2024-05-15 10:24:36.636 | INFO     | __main__:train:126 - Train Epoch: 38 Avg Loss: 4.2574 
2024-05-15 10:24:36.637 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:24:59.230 | INFO     | __main__:train:135 - Train Accuracies: All 0.7069 | Old 0.8265 | New 0.5901
2024-05-15 10:25:04.547 | INFO     | __main__:train:123 - Epoch: [39][0/46]	 loss 4.21450	 cls_loss: 0.3903 cluster_loss: 1.3531 sup_con_loss: 0.5494 contrastive_loss: 4.6248 
2024-05-15 10:25:21.854 | INFO     | __main__:train:123 - Epoch: [39][20/46]	 loss 4.12223	 cls_loss: 0.3809 cluster_loss: 1.2602 sup_con_loss: 0.4649 contrastive_loss: 4.6262 
2024-05-15 10:25:37.309 | INFO     | __main__:train:123 - Epoch: [39][40/46]	 loss 4.14440	 cls_loss: 0.4123 cluster_loss: 1.2453 sup_con_loss: 0.5431 contrastive_loss: 4.6162 
2024-05-15 10:25:41.243 | INFO     | __main__:train:126 - Train Epoch: 39 Avg Loss: 4.2421 
2024-05-15 10:25:41.243 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:26:03.386 | INFO     | __main__:train:135 - Train Accuracies: All 0.7125 | Old 0.8279 | New 0.5997
2024-05-15 10:26:09.241 | INFO     | __main__:train:123 - Epoch: [40][0/46]	 loss 4.35343	 cls_loss: 0.4457 cluster_loss: 1.3740 sup_con_loss: 0.8620 contrastive_loss: 4.6195 
2024-05-15 10:26:26.820 | INFO     | __main__:train:123 - Epoch: [40][20/46]	 loss 4.25251	 cls_loss: 0.4342 cluster_loss: 1.2930 sup_con_loss: 0.7429 contrastive_loss: 4.6155 
2024-05-15 10:26:42.504 | INFO     | __main__:train:123 - Epoch: [40][40/46]	 loss 4.19490	 cls_loss: 0.4499 cluster_loss: 1.3034 sup_con_loss: 0.5367 contrastive_loss: 4.6191 
2024-05-15 10:26:46.502 | INFO     | __main__:train:126 - Train Epoch: 40 Avg Loss: 4.2443 
2024-05-15 10:26:46.502 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:27:08.398 | INFO     | __main__:train:135 - Train Accuracies: All 0.7121 | Old 0.8317 | New 0.5952
2024-05-15 10:27:13.853 | INFO     | __main__:train:123 - Epoch: [41][0/46]	 loss 4.28039	 cls_loss: 0.4255 cluster_loss: 1.3674 sup_con_loss: 0.7008 contrastive_loss: 4.6113 
2024-05-15 10:27:31.184 | INFO     | __main__:train:123 - Epoch: [41][20/46]	 loss 4.24158	 cls_loss: 0.3881 cluster_loss: 1.3209 sup_con_loss: 0.6955 contrastive_loss: 4.6211 
2024-05-15 10:27:47.001 | INFO     | __main__:train:123 - Epoch: [41][40/46]	 loss 4.19410	 cls_loss: 0.4042 cluster_loss: 1.3142 sup_con_loss: 0.5546 contrastive_loss: 4.6220 
2024-05-15 10:27:51.002 | INFO     | __main__:train:126 - Train Epoch: 41 Avg Loss: 4.2249 
2024-05-15 10:27:51.003 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:28:13.156 | INFO     | __main__:train:135 - Train Accuracies: All 0.7145 | Old 0.8307 | New 0.6010
2024-05-15 10:28:18.162 | INFO     | __main__:train:123 - Epoch: [42][0/46]	 loss 4.28334	 cls_loss: 0.3958 cluster_loss: 1.2677 sup_con_loss: 0.9144 contrastive_loss: 4.6166 
2024-05-15 10:28:35.786 | INFO     | __main__:train:123 - Epoch: [42][20/46]	 loss 4.21444	 cls_loss: 0.4381 cluster_loss: 1.2535 sup_con_loss: 0.7104 contrastive_loss: 4.6119 
2024-05-15 10:28:51.624 | INFO     | __main__:train:123 - Epoch: [42][40/46]	 loss 4.25356	 cls_loss: 0.4054 cluster_loss: 1.3332 sup_con_loss: 0.6952 contrastive_loss: 4.6181 
2024-05-15 10:28:55.588 | INFO     | __main__:train:126 - Train Epoch: 42 Avg Loss: 4.2295 
2024-05-15 10:28:55.588 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:29:17.735 | INFO     | __main__:train:135 - Train Accuracies: All 0.7154 | Old 0.8324 | New 0.6010
2024-05-15 10:29:23.109 | INFO     | __main__:train:123 - Epoch: [43][0/46]	 loss 4.26730	 cls_loss: 0.4411 cluster_loss: 1.2883 sup_con_loss: 0.7776 contrastive_loss: 4.6206 
2024-05-15 10:29:40.623 | INFO     | __main__:train:123 - Epoch: [43][20/46]	 loss 4.25695	 cls_loss: 0.3988 cluster_loss: 1.2354 sup_con_loss: 0.9179 contrastive_loss: 4.6048 
2024-05-15 10:29:56.144 | INFO     | __main__:train:123 - Epoch: [43][40/46]	 loss 4.26799	 cls_loss: 0.3833 cluster_loss: 1.3023 sup_con_loss: 0.8077 contrastive_loss: 4.6225 
2024-05-15 10:30:00.165 | INFO     | __main__:train:126 - Train Epoch: 43 Avg Loss: 4.2204 
2024-05-15 10:30:00.166 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:30:22.404 | INFO     | __main__:train:135 - Train Accuracies: All 0.7163 | Old 0.8314 | New 0.6038
2024-05-15 10:30:27.789 | INFO     | __main__:train:123 - Epoch: [44][0/46]	 loss 4.32324	 cls_loss: 0.4031 cluster_loss: 1.3477 sup_con_loss: 0.8635 contrastive_loss: 4.6214 
2024-05-15 10:30:44.932 | INFO     | __main__:train:123 - Epoch: [44][20/46]	 loss 4.19888	 cls_loss: 0.3903 cluster_loss: 1.2404 sup_con_loss: 0.7519 contrastive_loss: 4.6044 
2024-05-15 10:31:00.689 | INFO     | __main__:train:123 - Epoch: [44][40/46]	 loss 4.17595	 cls_loss: 0.4163 cluster_loss: 1.2997 sup_con_loss: 0.5304 contrastive_loss: 4.6151 
2024-05-15 10:31:04.619 | INFO     | __main__:train:126 - Train Epoch: 44 Avg Loss: 4.2304 
2024-05-15 10:31:04.619 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:31:26.877 | INFO     | __main__:train:135 - Train Accuracies: All 0.7145 | Old 0.8321 | New 0.5997
2024-05-15 10:31:33.201 | INFO     | __main__:train:123 - Epoch: [45][0/46]	 loss 4.25177	 cls_loss: 0.4237 cluster_loss: 1.3261 sup_con_loss: 0.6977 contrastive_loss: 4.6112 
2024-05-15 10:31:50.204 | INFO     | __main__:train:123 - Epoch: [45][20/46]	 loss 4.32193	 cls_loss: 0.4115 cluster_loss: 1.3395 sup_con_loss: 0.8795 contrastive_loss: 4.6145 
2024-05-15 10:32:05.796 | INFO     | __main__:train:123 - Epoch: [45][40/46]	 loss 4.30245	 cls_loss: 0.4342 cluster_loss: 1.3876 sup_con_loss: 0.6986 contrastive_loss: 4.6216 
2024-05-15 10:32:09.813 | INFO     | __main__:train:126 - Train Epoch: 45 Avg Loss: 4.2236 
2024-05-15 10:32:09.813 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:32:31.717 | INFO     | __main__:train:135 - Train Accuracies: All 0.7132 | Old 0.8369 | New 0.5922
2024-05-15 10:32:37.142 | INFO     | __main__:train:123 - Epoch: [46][0/46]	 loss 4.22563	 cls_loss: 0.4019 cluster_loss: 1.3298 sup_con_loss: 0.6293 contrastive_loss: 4.6159 
2024-05-15 10:32:54.719 | INFO     | __main__:train:123 - Epoch: [46][20/46]	 loss 4.24867	 cls_loss: 0.4123 cluster_loss: 1.3736 sup_con_loss: 0.5990 contrastive_loss: 4.6183 
2024-05-15 10:33:10.340 | INFO     | __main__:train:123 - Epoch: [46][40/46]	 loss 4.22990	 cls_loss: 0.3732 cluster_loss: 1.3336 sup_con_loss: 0.6489 contrastive_loss: 4.6236 
2024-05-15 10:33:14.371 | INFO     | __main__:train:126 - Train Epoch: 46 Avg Loss: 4.2188 
2024-05-15 10:33:14.373 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:33:36.726 | INFO     | __main__:train:135 - Train Accuracies: All 0.7149 | Old 0.8415 | New 0.5911
2024-05-15 10:33:42.488 | INFO     | __main__:train:123 - Epoch: [47][0/46]	 loss 4.22159	 cls_loss: 0.4116 cluster_loss: 1.3313 sup_con_loss: 0.5943 contrastive_loss: 4.6218 
2024-05-15 10:33:59.481 | INFO     | __main__:train:123 - Epoch: [47][20/46]	 loss 4.15790	 cls_loss: 0.3641 cluster_loss: 1.2147 sup_con_loss: 0.6982 contrastive_loss: 4.6100 
2024-05-15 10:34:15.393 | INFO     | __main__:train:123 - Epoch: [47][40/46]	 loss 4.16848	 cls_loss: 0.4077 cluster_loss: 1.2302 sup_con_loss: 0.6503 contrastive_loss: 4.6132 
2024-05-15 10:34:19.278 | INFO     | __main__:train:126 - Train Epoch: 47 Avg Loss: 4.2124 
2024-05-15 10:34:19.278 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:34:40.988 | INFO     | __main__:train:135 - Train Accuracies: All 0.7171 | Old 0.8411 | New 0.5959
2024-05-15 10:34:46.462 | INFO     | __main__:train:123 - Epoch: [48][0/46]	 loss 4.08723	 cls_loss: 0.3511 cluster_loss: 1.2079 sup_con_loss: 0.5193 contrastive_loss: 4.6116 
2024-05-15 10:35:03.888 | INFO     | __main__:train:123 - Epoch: [48][20/46]	 loss 4.21432	 cls_loss: 0.3675 cluster_loss: 1.3246 sup_con_loss: 0.6439 contrastive_loss: 4.6144 
2024-05-15 10:35:19.398 | INFO     | __main__:train:123 - Epoch: [48][40/46]	 loss 4.14399	 cls_loss: 0.3540 cluster_loss: 1.2638 sup_con_loss: 0.5719 contrastive_loss: 4.6131 
2024-05-15 10:35:23.361 | INFO     | __main__:train:126 - Train Epoch: 48 Avg Loss: 4.1993 
2024-05-15 10:35:23.361 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:35:45.367 | INFO     | __main__:train:135 - Train Accuracies: All 0.7140 | Old 0.8348 | New 0.5959
2024-05-15 10:35:52.043 | INFO     | __main__:train:123 - Epoch: [49][0/46]	 loss 4.24301	 cls_loss: 0.4121 cluster_loss: 1.2994 sup_con_loss: 0.7101 contrastive_loss: 4.6241 
2024-05-15 10:36:08.871 | INFO     | __main__:train:123 - Epoch: [49][20/46]	 loss 4.11536	 cls_loss: 0.3976 cluster_loss: 1.2871 sup_con_loss: 0.3868 contrastive_loss: 4.6219 
2024-05-15 10:36:24.408 | INFO     | __main__:train:123 - Epoch: [49][40/46]	 loss 4.18325	 cls_loss: 0.4441 cluster_loss: 1.2595 sup_con_loss: 0.5829 contrastive_loss: 4.6233 
2024-05-15 10:36:28.375 | INFO     | __main__:train:126 - Train Epoch: 49 Avg Loss: 4.2088 
2024-05-15 10:36:28.376 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:36:50.098 | INFO     | __main__:train:135 - Train Accuracies: All 0.7182 | Old 0.8401 | New 0.5990
2024-05-15 10:36:56.533 | INFO     | __main__:train:123 - Epoch: [50][0/46]	 loss 4.15387	 cls_loss: 0.3663 cluster_loss: 1.2590 sup_con_loss: 0.5979 contrastive_loss: 4.6124 
2024-05-15 10:37:13.590 | INFO     | __main__:train:123 - Epoch: [50][20/46]	 loss 4.25019	 cls_loss: 0.3871 cluster_loss: 1.3040 sup_con_loss: 0.7704 contrastive_loss: 4.6115 
2024-05-15 10:37:29.272 | INFO     | __main__:train:123 - Epoch: [50][40/46]	 loss 4.06612	 cls_loss: 0.3752 cluster_loss: 1.2089 sup_con_loss: 0.4248 contrastive_loss: 4.6159 
2024-05-15 10:37:33.184 | INFO     | __main__:train:126 - Train Epoch: 50 Avg Loss: 4.1932 
2024-05-15 10:37:33.185 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:37:55.247 | INFO     | __main__:train:135 - Train Accuracies: All 0.7194 | Old 0.8404 | New 0.6010
2024-05-15 10:38:01.328 | INFO     | __main__:train:123 - Epoch: [51][0/46]	 loss 3.98713	 cls_loss: 0.3208 cluster_loss: 1.1823 sup_con_loss: 0.3173 contrastive_loss: 4.6081 
2024-05-15 10:38:19.170 | INFO     | __main__:train:123 - Epoch: [51][20/46]	 loss 4.16637	 cls_loss: 0.4112 cluster_loss: 1.2699 sup_con_loss: 0.5437 contrastive_loss: 4.6257 
2024-05-15 10:38:35.067 | INFO     | __main__:train:123 - Epoch: [51][40/46]	 loss 4.18089	 cls_loss: 0.3802 cluster_loss: 1.2501 sup_con_loss: 0.6750 contrastive_loss: 4.6139 
2024-05-15 10:38:39.172 | INFO     | __main__:train:126 - Train Epoch: 51 Avg Loss: 4.1876 
2024-05-15 10:38:39.172 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:39:01.578 | INFO     | __main__:train:135 - Train Accuracies: All 0.7173 | Old 0.8366 | New 0.6007
2024-05-15 10:39:08.385 | INFO     | __main__:train:123 - Epoch: [52][0/46]	 loss 4.26278	 cls_loss: 0.3705 cluster_loss: 1.3072 sup_con_loss: 0.8237 contrastive_loss: 4.6079 
2024-05-15 10:39:25.574 | INFO     | __main__:train:123 - Epoch: [52][20/46]	 loss 4.28197	 cls_loss: 0.3555 cluster_loss: 1.3536 sup_con_loss: 0.7783 contrastive_loss: 4.6236 
2024-05-15 10:39:41.294 | INFO     | __main__:train:123 - Epoch: [52][40/46]	 loss 4.20706	 cls_loss: 0.3576 cluster_loss: 1.2687 sup_con_loss: 0.7467 contrastive_loss: 4.6090 
2024-05-15 10:39:45.306 | INFO     | __main__:train:126 - Train Epoch: 52 Avg Loss: 4.1884 
2024-05-15 10:39:45.307 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:40:07.390 | INFO     | __main__:train:135 - Train Accuracies: All 0.7199 | Old 0.8355 | New 0.6068
2024-05-15 10:40:13.498 | INFO     | __main__:train:123 - Epoch: [53][0/46]	 loss 4.21609	 cls_loss: 0.3750 cluster_loss: 1.3036 sup_con_loss: 0.6818 contrastive_loss: 4.6137 
2024-05-15 10:40:30.794 | INFO     | __main__:train:123 - Epoch: [53][20/46]	 loss 4.34579	 cls_loss: 0.3421 cluster_loss: 1.3446 sup_con_loss: 1.0103 contrastive_loss: 4.6130 
2024-05-15 10:40:46.635 | INFO     | __main__:train:123 - Epoch: [53][40/46]	 loss 4.05034	 cls_loss: 0.3944 cluster_loss: 1.1674 sup_con_loss: 0.4366 contrastive_loss: 4.6164 
2024-05-15 10:40:50.592 | INFO     | __main__:train:126 - Train Epoch: 53 Avg Loss: 4.1892 
2024-05-15 10:40:50.593 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:41:12.840 | INFO     | __main__:train:135 - Train Accuracies: All 0.7213 | Old 0.8355 | New 0.6096
2024-05-15 10:41:18.230 | INFO     | __main__:train:123 - Epoch: [54][0/46]	 loss 4.21842	 cls_loss: 0.3537 cluster_loss: 1.3066 sup_con_loss: 0.7034 contrastive_loss: 4.6141 
2024-05-15 10:41:35.546 | INFO     | __main__:train:123 - Epoch: [54][20/46]	 loss 4.13778	 cls_loss: 0.3726 cluster_loss: 1.2528 sup_con_loss: 0.5595 contrastive_loss: 4.6111 
2024-05-15 10:41:50.929 | INFO     | __main__:train:123 - Epoch: [54][40/46]	 loss 4.13877	 cls_loss: 0.3665 cluster_loss: 1.2596 sup_con_loss: 0.5621 contrastive_loss: 4.6078 
2024-05-15 10:41:54.841 | INFO     | __main__:train:126 - Train Epoch: 54 Avg Loss: 4.1950 
2024-05-15 10:41:54.842 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:42:16.672 | INFO     | __main__:train:135 - Train Accuracies: All 0.7213 | Old 0.8373 | New 0.6078
2024-05-15 10:42:23.378 | INFO     | __main__:train:123 - Epoch: [55][0/46]	 loss 4.12434	 cls_loss: 0.3727 cluster_loss: 1.2396 sup_con_loss: 0.5610 contrastive_loss: 4.6027 
2024-05-15 10:42:40.084 | INFO     | __main__:train:123 - Epoch: [55][20/46]	 loss 4.28181	 cls_loss: 0.3827 cluster_loss: 1.3078 sup_con_loss: 0.8523 contrastive_loss: 4.6146 
2024-05-15 10:42:55.620 | INFO     | __main__:train:123 - Epoch: [55][40/46]	 loss 4.18339	 cls_loss: 0.3320 cluster_loss: 1.2840 sup_con_loss: 0.6648 contrastive_loss: 4.6152 
2024-05-15 10:42:59.638 | INFO     | __main__:train:126 - Train Epoch: 55 Avg Loss: 4.1964 
2024-05-15 10:42:59.639 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:43:22.079 | INFO     | __main__:train:135 - Train Accuracies: All 0.7206 | Old 0.8390 | New 0.6048
2024-05-15 10:43:27.442 | INFO     | __main__:train:123 - Epoch: [56][0/46]	 loss 4.14839	 cls_loss: 0.3726 cluster_loss: 1.2167 sup_con_loss: 0.6732 contrastive_loss: 4.6023 
2024-05-15 10:43:44.872 | INFO     | __main__:train:123 - Epoch: [56][20/46]	 loss 4.25291	 cls_loss: 0.3627 cluster_loss: 1.3020 sup_con_loss: 0.8226 contrastive_loss: 4.6027 
2024-05-15 10:44:00.559 | INFO     | __main__:train:123 - Epoch: [56][40/46]	 loss 4.16815	 cls_loss: 0.3776 cluster_loss: 1.3051 sup_con_loss: 0.5475 contrastive_loss: 4.6093 
2024-05-15 10:44:04.596 | INFO     | __main__:train:126 - Train Epoch: 56 Avg Loss: 4.1755 
2024-05-15 10:44:04.596 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:44:26.657 | INFO     | __main__:train:135 - Train Accuracies: All 0.7188 | Old 0.8401 | New 0.6003
2024-05-15 10:44:33.095 | INFO     | __main__:train:123 - Epoch: [57][0/46]	 loss 4.18501	 cls_loss: 0.3617 cluster_loss: 1.2825 sup_con_loss: 0.6454 contrastive_loss: 4.6137 
2024-05-15 10:44:49.851 | INFO     | __main__:train:123 - Epoch: [57][20/46]	 loss 4.16278	 cls_loss: 0.3634 cluster_loss: 1.2299 sup_con_loss: 0.6961 contrastive_loss: 4.6039 
2024-05-15 10:45:05.478 | INFO     | __main__:train:123 - Epoch: [57][40/46]	 loss 4.10006	 cls_loss: 0.3425 cluster_loss: 1.2013 sup_con_loss: 0.5884 contrastive_loss: 4.6052 
2024-05-15 10:45:09.466 | INFO     | __main__:train:126 - Train Epoch: 57 Avg Loss: 4.1687 
2024-05-15 10:45:09.467 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:45:30.849 | INFO     | __main__:train:135 - Train Accuracies: All 0.7176 | Old 0.8341 | New 0.6038
2024-05-15 10:45:36.693 | INFO     | __main__:train:123 - Epoch: [58][0/46]	 loss 4.25000	 cls_loss: 0.3728 cluster_loss: 1.3753 sup_con_loss: 0.6256 contrastive_loss: 4.6255 
2024-05-15 10:45:53.884 | INFO     | __main__:train:123 - Epoch: [58][20/46]	 loss 4.34839	 cls_loss: 0.4208 cluster_loss: 1.3951 sup_con_loss: 0.8497 contrastive_loss: 4.6106 
2024-05-15 10:46:09.406 | INFO     | __main__:train:123 - Epoch: [58][40/46]	 loss 4.05106	 cls_loss: 0.3407 cluster_loss: 1.1358 sup_con_loss: 0.5886 contrastive_loss: 4.5962 
2024-05-15 10:46:13.386 | INFO     | __main__:train:126 - Train Epoch: 58 Avg Loss: 4.1751 
2024-05-15 10:46:13.387 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:46:35.406 | INFO     | __main__:train:135 - Train Accuracies: All 0.7194 | Old 0.8394 | New 0.6020
2024-05-15 10:46:41.249 | INFO     | __main__:train:123 - Epoch: [59][0/46]	 loss 4.08173	 cls_loss: 0.3754 cluster_loss: 1.2079 sup_con_loss: 0.4915 contrastive_loss: 4.6049 
2024-05-15 10:46:58.684 | INFO     | __main__:train:123 - Epoch: [59][20/46]	 loss 4.07197	 cls_loss: 0.4047 cluster_loss: 1.1830 sup_con_loss: 0.4808 contrastive_loss: 4.6048 
2024-05-15 10:47:14.285 | INFO     | __main__:train:123 - Epoch: [59][40/46]	 loss 4.09697	 cls_loss: 0.3173 cluster_loss: 1.1882 sup_con_loss: 0.6322 contrastive_loss: 4.6035 
2024-05-15 10:47:18.306 | INFO     | __main__:train:126 - Train Epoch: 59 Avg Loss: 4.1655 
2024-05-15 10:47:18.306 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:47:40.165 | INFO     | __main__:train:135 - Train Accuracies: All 0.7209 | Old 0.8425 | New 0.6020
2024-05-15 10:47:46.607 | INFO     | __main__:train:123 - Epoch: [60][0/46]	 loss 4.18189	 cls_loss: 0.3776 cluster_loss: 1.2481 sup_con_loss: 0.6762 contrastive_loss: 4.6182 
2024-05-15 10:48:03.639 | INFO     | __main__:train:123 - Epoch: [60][20/46]	 loss 4.22991	 cls_loss: 0.3368 cluster_loss: 1.2716 sup_con_loss: 0.8345 contrastive_loss: 4.6052 
2024-05-15 10:48:19.145 | INFO     | __main__:train:123 - Epoch: [60][40/46]	 loss 3.96255	 cls_loss: 0.3272 cluster_loss: 1.1080 sup_con_loss: 0.3934 contrastive_loss: 4.6002 
2024-05-15 10:48:23.260 | INFO     | __main__:train:126 - Train Epoch: 60 Avg Loss: 4.1761 
2024-05-15 10:48:23.261 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:48:45.079 | INFO     | __main__:train:135 - Train Accuracies: All 0.7202 | Old 0.8394 | New 0.6038
2024-05-15 10:48:50.948 | INFO     | __main__:train:123 - Epoch: [61][0/46]	 loss 4.20464	 cls_loss: 0.3737 cluster_loss: 1.3290 sup_con_loss: 0.6018 contrastive_loss: 4.6144 
2024-05-15 10:49:08.191 | INFO     | __main__:train:123 - Epoch: [61][20/46]	 loss 4.12524	 cls_loss: 0.3650 cluster_loss: 1.2451 sup_con_loss: 0.5439 contrastive_loss: 4.6120 
2024-05-15 10:49:23.906 | INFO     | __main__:train:123 - Epoch: [61][40/46]	 loss 4.13896	 cls_loss: 0.3735 cluster_loss: 1.2417 sup_con_loss: 0.5691 contrastive_loss: 4.6184 
2024-05-15 10:49:27.878 | INFO     | __main__:train:126 - Train Epoch: 61 Avg Loss: 4.1714 
2024-05-15 10:49:27.878 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:49:50.053 | INFO     | __main__:train:135 - Train Accuracies: All 0.7218 | Old 0.8450 | New 0.6014
2024-05-15 10:49:56.649 | INFO     | __main__:train:123 - Epoch: [62][0/46]	 loss 4.17172	 cls_loss: 0.3231 cluster_loss: 1.2682 sup_con_loss: 0.6804 contrastive_loss: 4.6094 
2024-05-15 10:50:13.399 | INFO     | __main__:train:123 - Epoch: [62][20/46]	 loss 4.20217	 cls_loss: 0.3443 cluster_loss: 1.2527 sup_con_loss: 0.7841 contrastive_loss: 4.6046 
2024-05-15 10:50:29.119 | INFO     | __main__:train:123 - Epoch: [62][40/46]	 loss 4.22845	 cls_loss: 0.3451 cluster_loss: 1.2220 sup_con_loss: 0.9164 contrastive_loss: 4.6041 
2024-05-15 10:50:33.012 | INFO     | __main__:train:126 - Train Epoch: 62 Avg Loss: 4.1636 
2024-05-15 10:50:33.013 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:50:54.635 | INFO     | __main__:train:135 - Train Accuracies: All 0.7213 | Old 0.8373 | New 0.6078
2024-05-15 10:51:00.756 | INFO     | __main__:train:123 - Epoch: [63][0/46]	 loss 4.27483	 cls_loss: 0.4140 cluster_loss: 1.3426 sup_con_loss: 0.7411 contrastive_loss: 4.6121 
2024-05-15 10:51:17.634 | INFO     | __main__:train:123 - Epoch: [63][20/46]	 loss 4.15788	 cls_loss: 0.3335 cluster_loss: 1.2451 sup_con_loss: 0.6804 contrastive_loss: 4.6057 
2024-05-15 10:51:33.205 | INFO     | __main__:train:123 - Epoch: [63][40/46]	 loss 4.18435	 cls_loss: 0.3570 cluster_loss: 1.2938 sup_con_loss: 0.6400 contrastive_loss: 4.6068 
2024-05-15 10:51:37.093 | INFO     | __main__:train:126 - Train Epoch: 63 Avg Loss: 4.1688 
2024-05-15 10:51:37.094 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:51:59.331 | INFO     | __main__:train:135 - Train Accuracies: All 0.7239 | Old 0.8471 | New 0.6034
2024-05-15 10:52:05.760 | INFO     | __main__:train:123 - Epoch: [64][0/46]	 loss 4.29581	 cls_loss: 0.3626 cluster_loss: 1.3608 sup_con_loss: 0.8233 contrastive_loss: 4.6096 
2024-05-15 10:52:23.020 | INFO     | __main__:train:123 - Epoch: [64][20/46]	 loss 4.25235	 cls_loss: 0.3218 cluster_loss: 1.3514 sup_con_loss: 0.7481 contrastive_loss: 4.6146 
2024-05-15 10:52:38.565 | INFO     | __main__:train:123 - Epoch: [64][40/46]	 loss 4.14900	 cls_loss: 0.3673 cluster_loss: 1.2490 sup_con_loss: 0.6233 contrastive_loss: 4.6006 
2024-05-15 10:52:42.557 | INFO     | __main__:train:126 - Train Epoch: 64 Avg Loss: 4.1802 
2024-05-15 10:52:42.557 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:53:04.152 | INFO     | __main__:train:135 - Train Accuracies: All 0.7239 | Old 0.8418 | New 0.6085
2024-05-15 10:53:09.291 | INFO     | __main__:train:123 - Epoch: [65][0/46]	 loss 4.26431	 cls_loss: 0.3934 cluster_loss: 1.3640 sup_con_loss: 0.6936 contrastive_loss: 4.6112 
2024-05-15 10:53:27.338 | INFO     | __main__:train:123 - Epoch: [65][20/46]	 loss 4.15101	 cls_loss: 0.3187 cluster_loss: 1.2447 sup_con_loss: 0.6855 contrastive_loss: 4.6007 
2024-05-15 10:53:42.931 | INFO     | __main__:train:123 - Epoch: [65][40/46]	 loss 4.15479	 cls_loss: 0.3379 cluster_loss: 1.2310 sup_con_loss: 0.6924 contrastive_loss: 4.6062 
2024-05-15 10:53:46.900 | INFO     | __main__:train:126 - Train Epoch: 65 Avg Loss: 4.1630 
2024-05-15 10:53:46.900 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:54:08.534 | INFO     | __main__:train:135 - Train Accuracies: All 0.7201 | Old 0.8331 | New 0.6096
2024-05-15 10:54:14.937 | INFO     | __main__:train:123 - Epoch: [66][0/46]	 loss 4.18960	 cls_loss: 0.3505 cluster_loss: 1.2597 sup_con_loss: 0.7262 contrastive_loss: 4.6061 
2024-05-15 10:54:32.031 | INFO     | __main__:train:123 - Epoch: [66][20/46]	 loss 4.12851	 cls_loss: 0.3573 cluster_loss: 1.2454 sup_con_loss: 0.5569 contrastive_loss: 4.6139 
2024-05-15 10:54:47.773 | INFO     | __main__:train:123 - Epoch: [66][40/46]	 loss 4.25121	 cls_loss: 0.3450 cluster_loss: 1.3169 sup_con_loss: 0.7984 contrastive_loss: 4.6077 
2024-05-15 10:54:51.776 | INFO     | __main__:train:126 - Train Epoch: 66 Avg Loss: 4.1628 
2024-05-15 10:54:51.777 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:55:13.560 | INFO     | __main__:train:135 - Train Accuracies: All 0.7228 | Old 0.8397 | New 0.6085
2024-05-15 10:55:19.769 | INFO     | __main__:train:123 - Epoch: [67][0/46]	 loss 4.18072	 cls_loss: 0.3372 cluster_loss: 1.2567 sup_con_loss: 0.7324 contrastive_loss: 4.5992 
2024-05-15 10:55:36.594 | INFO     | __main__:train:123 - Epoch: [67][20/46]	 loss 4.25958	 cls_loss: 0.3433 cluster_loss: 1.3028 sup_con_loss: 0.8521 contrastive_loss: 4.6067 
2024-05-15 10:55:52.512 | INFO     | __main__:train:123 - Epoch: [67][40/46]	 loss 4.13153	 cls_loss: 0.3362 cluster_loss: 1.2648 sup_con_loss: 0.5669 contrastive_loss: 4.6051 
2024-05-15 10:55:56.494 | INFO     | __main__:train:126 - Train Epoch: 67 Avg Loss: 4.1605 
2024-05-15 10:55:56.495 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:56:18.269 | INFO     | __main__:train:135 - Train Accuracies: All 0.7228 | Old 0.8450 | New 0.6034
2024-05-15 10:56:23.514 | INFO     | __main__:train:123 - Epoch: [68][0/46]	 loss 4.24609	 cls_loss: 0.3739 cluster_loss: 1.3671 sup_con_loss: 0.6656 contrastive_loss: 4.6056 
2024-05-15 10:56:41.472 | INFO     | __main__:train:123 - Epoch: [68][20/46]	 loss 4.11699	 cls_loss: 0.3170 cluster_loss: 1.2462 sup_con_loss: 0.5718 contrastive_loss: 4.6090 
2024-05-15 10:56:57.105 | INFO     | __main__:train:123 - Epoch: [68][40/46]	 loss 4.20317	 cls_loss: 0.3790 cluster_loss: 1.3482 sup_con_loss: 0.5574 contrastive_loss: 4.6140 
2024-05-15 10:57:01.059 | INFO     | __main__:train:126 - Train Epoch: 68 Avg Loss: 4.1771 
2024-05-15 10:57:01.060 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:57:23.351 | INFO     | __main__:train:135 - Train Accuracies: All 0.7230 | Old 0.8425 | New 0.6061
2024-05-15 10:57:29.636 | INFO     | __main__:train:123 - Epoch: [69][0/46]	 loss 4.02458	 cls_loss: 0.3355 cluster_loss: 1.1305 sup_con_loss: 0.5315 contrastive_loss: 4.5944 
2024-05-15 10:57:46.750 | INFO     | __main__:train:123 - Epoch: [69][20/46]	 loss 4.22635	 cls_loss: 0.3856 cluster_loss: 1.2446 sup_con_loss: 0.8309 contrastive_loss: 4.6024 
2024-05-15 10:58:02.637 | INFO     | __main__:train:123 - Epoch: [69][40/46]	 loss 4.19917	 cls_loss: 0.3507 cluster_loss: 1.3060 sup_con_loss: 0.6690 contrastive_loss: 4.6052 
2024-05-15 10:58:06.743 | INFO     | __main__:train:126 - Train Epoch: 69 Avg Loss: 4.1363 
2024-05-15 10:58:06.744 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:58:29.384 | INFO     | __main__:train:135 - Train Accuracies: All 0.7232 | Old 0.8394 | New 0.6096
2024-05-15 10:58:35.825 | INFO     | __main__:train:123 - Epoch: [70][0/46]	 loss 4.21525	 cls_loss: 0.3184 cluster_loss: 1.3471 sup_con_loss: 0.6501 contrastive_loss: 4.6164 
2024-05-15 10:58:52.827 | INFO     | __main__:train:123 - Epoch: [70][20/46]	 loss 4.14098	 cls_loss: 0.3520 cluster_loss: 1.2451 sup_con_loss: 0.6119 contrastive_loss: 4.6066 
2024-05-15 10:59:08.823 | INFO     | __main__:train:123 - Epoch: [70][40/46]	 loss 4.13895	 cls_loss: 0.3248 cluster_loss: 1.2093 sup_con_loss: 0.7170 contrastive_loss: 4.5974 
2024-05-15 10:59:12.778 | INFO     | __main__:train:126 - Train Epoch: 70 Avg Loss: 4.1611 
2024-05-15 10:59:12.779 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:59:34.085 | INFO     | __main__:train:135 - Train Accuracies: All 0.7239 | Old 0.8387 | New 0.6116
2024-05-15 10:59:40.471 | INFO     | __main__:train:123 - Epoch: [71][0/46]	 loss 4.19624	 cls_loss: 0.3305 cluster_loss: 1.2855 sup_con_loss: 0.7082 contrastive_loss: 4.6110 
2024-05-15 10:59:57.084 | INFO     | __main__:train:123 - Epoch: [71][20/46]	 loss 4.25101	 cls_loss: 0.3288 cluster_loss: 1.3318 sup_con_loss: 0.7923 contrastive_loss: 4.6046 
2024-05-15 11:00:12.638 | INFO     | __main__:train:123 - Epoch: [71][40/46]	 loss 4.07521	 cls_loss: 0.3497 cluster_loss: 1.2077 sup_con_loss: 0.5005 contrastive_loss: 4.6041 
2024-05-15 11:00:16.523 | INFO     | __main__:train:126 - Train Epoch: 71 Avg Loss: 4.1583 
2024-05-15 11:00:16.524 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:00:38.053 | INFO     | __main__:train:135 - Train Accuracies: All 0.7221 | Old 0.8366 | New 0.6102
2024-05-15 11:00:44.627 | INFO     | __main__:train:123 - Epoch: [72][0/46]	 loss 4.21553	 cls_loss: 0.3583 cluster_loss: 1.2800 sup_con_loss: 0.7627 contrastive_loss: 4.6018 
2024-05-15 11:01:01.652 | INFO     | __main__:train:123 - Epoch: [72][20/46]	 loss 4.09220	 cls_loss: 0.3386 cluster_loss: 1.2165 sup_con_loss: 0.5374 contrastive_loss: 4.6075 
2024-05-15 11:01:17.352 | INFO     | __main__:train:123 - Epoch: [72][40/46]	 loss 4.01156	 cls_loss: 0.3224 cluster_loss: 1.1428 sup_con_loss: 0.4673 contrastive_loss: 4.6036 
2024-05-15 11:01:21.320 | INFO     | __main__:train:126 - Train Epoch: 72 Avg Loss: 4.1509 
2024-05-15 11:01:21.320 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:01:43.629 | INFO     | __main__:train:135 - Train Accuracies: All 0.7232 | Old 0.8432 | New 0.6058
2024-05-15 11:01:50.075 | INFO     | __main__:train:123 - Epoch: [73][0/46]	 loss 4.14805	 cls_loss: 0.3275 cluster_loss: 1.2471 sup_con_loss: 0.6703 contrastive_loss: 4.5972 
2024-05-15 11:02:07.043 | INFO     | __main__:train:123 - Epoch: [73][20/46]	 loss 4.13265	 cls_loss: 0.3108 cluster_loss: 1.2474 sup_con_loss: 0.6150 contrastive_loss: 4.6120 
2024-05-15 11:02:22.896 | INFO     | __main__:train:123 - Epoch: [73][40/46]	 loss 4.12705	 cls_loss: 0.3258 cluster_loss: 1.2414 sup_con_loss: 0.6196 contrastive_loss: 4.5989 
2024-05-15 11:02:26.857 | INFO     | __main__:train:126 - Train Epoch: 73 Avg Loss: 4.1421 
2024-05-15 11:02:26.858 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:02:49.009 | INFO     | __main__:train:135 - Train Accuracies: All 0.7228 | Old 0.8380 | New 0.6102
2024-05-15 11:02:54.881 | INFO     | __main__:train:123 - Epoch: [74][0/46]	 loss 4.16676	 cls_loss: 0.3504 cluster_loss: 1.3096 sup_con_loss: 0.5746 contrastive_loss: 4.6027 
2024-05-15 11:03:11.884 | INFO     | __main__:train:123 - Epoch: [74][20/46]	 loss 4.13256	 cls_loss: 0.2999 cluster_loss: 1.2683 sup_con_loss: 0.6051 contrastive_loss: 4.6021 
2024-05-15 11:03:27.438 | INFO     | __main__:train:123 - Epoch: [74][40/46]	 loss 4.19054	 cls_loss: 0.3486 cluster_loss: 1.3107 sup_con_loss: 0.6383 contrastive_loss: 4.6049 
2024-05-15 11:03:31.375 | INFO     | __main__:train:126 - Train Epoch: 74 Avg Loss: 4.1484 
2024-05-15 11:03:31.376 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:03:53.458 | INFO     | __main__:train:135 - Train Accuracies: All 0.7239 | Old 0.8443 | New 0.6061
2024-05-15 11:04:00.017 | INFO     | __main__:train:123 - Epoch: [75][0/46]	 loss 4.07073	 cls_loss: 0.3154 cluster_loss: 1.1888 sup_con_loss: 0.5686 contrastive_loss: 4.5979 
2024-05-15 11:04:16.840 | INFO     | __main__:train:123 - Epoch: [75][20/46]	 loss 4.11849	 cls_loss: 0.3718 cluster_loss: 1.2600 sup_con_loss: 0.5134 contrastive_loss: 4.5994 
2024-05-15 11:04:32.563 | INFO     | __main__:train:123 - Epoch: [75][40/46]	 loss 4.19877	 cls_loss: 0.3359 cluster_loss: 1.2983 sup_con_loss: 0.6910 contrastive_loss: 4.6084 
2024-05-15 11:04:36.507 | INFO     | __main__:train:126 - Train Epoch: 75 Avg Loss: 4.1409 
2024-05-15 11:04:36.507 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:04:58.639 | INFO     | __main__:train:135 - Train Accuracies: All 0.7192 | Old 0.8425 | New 0.5986
2024-05-15 11:05:03.629 | INFO     | __main__:train:123 - Epoch: [76][0/46]	 loss 4.16657	 cls_loss: 0.3264 cluster_loss: 1.2452 sup_con_loss: 0.7150 contrastive_loss: 4.6042 
2024-05-15 11:05:21.987 | INFO     | __main__:train:123 - Epoch: [76][20/46]	 loss 4.15200	 cls_loss: 0.3785 cluster_loss: 1.3288 sup_con_loss: 0.4536 contrastive_loss: 4.6108 
2024-05-15 11:05:37.675 | INFO     | __main__:train:123 - Epoch: [76][40/46]	 loss 4.14698	 cls_loss: 0.3268 cluster_loss: 1.2799 sup_con_loss: 0.5905 contrastive_loss: 4.6062 
2024-05-15 11:05:41.645 | INFO     | __main__:train:126 - Train Epoch: 76 Avg Loss: 4.1345 
2024-05-15 11:05:41.645 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:06:03.503 | INFO     | __main__:train:135 - Train Accuracies: All 0.7230 | Old 0.8362 | New 0.6123
2024-05-15 11:06:10.022 | INFO     | __main__:train:123 - Epoch: [77][0/46]	 loss 4.12097	 cls_loss: 0.3631 cluster_loss: 1.2413 sup_con_loss: 0.5512 contrastive_loss: 4.6063 
2024-05-15 11:06:26.870 | INFO     | __main__:train:123 - Epoch: [77][20/46]	 loss 4.12501	 cls_loss: 0.3324 cluster_loss: 1.2728 sup_con_loss: 0.5385 contrastive_loss: 4.6044 
2024-05-15 11:06:42.620 | INFO     | __main__:train:123 - Epoch: [77][40/46]	 loss 4.22490	 cls_loss: 0.3190 cluster_loss: 1.3072 sup_con_loss: 0.7753 contrastive_loss: 4.6035 
2024-05-15 11:06:46.680 | INFO     | __main__:train:126 - Train Epoch: 77 Avg Loss: 4.1320 
2024-05-15 11:06:46.681 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:07:08.715 | INFO     | __main__:train:135 - Train Accuracies: All 0.7220 | Old 0.8373 | New 0.6092
2024-05-15 11:07:14.562 | INFO     | __main__:train:123 - Epoch: [78][0/46]	 loss 4.12043	 cls_loss: 0.3639 cluster_loss: 1.2100 sup_con_loss: 0.5966 contrastive_loss: 4.6119 
2024-05-15 11:07:31.767 | INFO     | __main__:train:123 - Epoch: [78][20/46]	 loss 4.12407	 cls_loss: 0.2878 cluster_loss: 1.2401 sup_con_loss: 0.6470 contrastive_loss: 4.6012 
2024-05-15 11:07:47.458 | INFO     | __main__:train:123 - Epoch: [78][40/46]	 loss 4.08390	 cls_loss: 0.3220 cluster_loss: 1.2271 sup_con_loss: 0.5124 contrastive_loss: 4.6065 
2024-05-15 11:07:51.374 | INFO     | __main__:train:126 - Train Epoch: 78 Avg Loss: 4.1316 
2024-05-15 11:07:51.374 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:08:13.954 | INFO     | __main__:train:135 - Train Accuracies: All 0.7213 | Old 0.8310 | New 0.6140
2024-05-15 11:08:19.757 | INFO     | __main__:train:123 - Epoch: [79][0/46]	 loss 4.17517	 cls_loss: 0.3280 cluster_loss: 1.2555 sup_con_loss: 0.7308 contrastive_loss: 4.5977 
2024-05-15 11:08:36.719 | INFO     | __main__:train:123 - Epoch: [79][20/46]	 loss 4.24880	 cls_loss: 0.3281 cluster_loss: 1.3353 sup_con_loss: 0.7896 contrastive_loss: 4.5995 
2024-05-15 11:08:52.578 | INFO     | __main__:train:123 - Epoch: [79][40/46]	 loss 4.15212	 cls_loss: 0.3110 cluster_loss: 1.2722 sup_con_loss: 0.6460 contrastive_loss: 4.6004 
2024-05-15 11:08:56.474 | INFO     | __main__:train:126 - Train Epoch: 79 Avg Loss: 4.1430 
2024-05-15 11:08:56.475 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:09:17.766 | INFO     | __main__:train:135 - Train Accuracies: All 0.7233 | Old 0.8376 | New 0.6116
2024-05-15 11:09:24.208 | INFO     | __main__:train:123 - Epoch: [80][0/46]	 loss 4.18153	 cls_loss: 0.2901 cluster_loss: 1.3262 sup_con_loss: 0.6400 contrastive_loss: 4.6061 
2024-05-15 11:09:41.368 | INFO     | __main__:train:123 - Epoch: [80][20/46]	 loss 4.17057	 cls_loss: 0.3148 cluster_loss: 1.3124 sup_con_loss: 0.6063 contrastive_loss: 4.6078 
2024-05-15 11:09:56.912 | INFO     | __main__:train:123 - Epoch: [80][40/46]	 loss 4.07507	 cls_loss: 0.3058 cluster_loss: 1.2129 sup_con_loss: 0.5364 contrastive_loss: 4.6030 
2024-05-15 11:10:00.861 | INFO     | __main__:train:126 - Train Epoch: 80 Avg Loss: 4.1338 
2024-05-15 11:10:00.861 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:10:23.084 | INFO     | __main__:train:135 - Train Accuracies: All 0.7228 | Old 0.8359 | New 0.6123
2024-05-15 11:10:28.439 | INFO     | __main__:train:123 - Epoch: [81][0/46]	 loss 4.09313	 cls_loss: 0.3156 cluster_loss: 1.1579 sup_con_loss: 0.6987 contrastive_loss: 4.5931 
2024-05-15 11:10:45.890 | INFO     | __main__:train:123 - Epoch: [81][20/46]	 loss 4.18495	 cls_loss: 0.3156 cluster_loss: 1.2695 sup_con_loss: 0.7337 contrastive_loss: 4.6039 
2024-05-15 11:11:01.532 | INFO     | __main__:train:123 - Epoch: [81][40/46]	 loss 4.14464	 cls_loss: 0.3047 cluster_loss: 1.2809 sup_con_loss: 0.5994 contrastive_loss: 4.6086 
2024-05-15 11:11:05.492 | INFO     | __main__:train:126 - Train Epoch: 81 Avg Loss: 4.1211 
2024-05-15 11:11:05.493 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:11:28.118 | INFO     | __main__:train:135 - Train Accuracies: All 0.7225 | Old 0.8369 | New 0.6106
2024-05-15 11:11:33.417 | INFO     | __main__:train:123 - Epoch: [82][0/46]	 loss 4.09265	 cls_loss: 0.3118 cluster_loss: 1.2061 sup_con_loss: 0.6120 contrastive_loss: 4.5928 
2024-05-15 11:11:50.778 | INFO     | __main__:train:123 - Epoch: [82][20/46]	 loss 4.09423	 cls_loss: 0.3219 cluster_loss: 1.1922 sup_con_loss: 0.5994 contrastive_loss: 4.6106 
2024-05-15 11:12:06.565 | INFO     | __main__:train:123 - Epoch: [82][40/46]	 loss 4.11327	 cls_loss: 0.3417 cluster_loss: 1.2091 sup_con_loss: 0.6170 contrastive_loss: 4.6028 
2024-05-15 11:12:10.509 | INFO     | __main__:train:126 - Train Epoch: 82 Avg Loss: 4.1328 
2024-05-15 11:12:10.510 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:12:32.706 | INFO     | __main__:train:135 - Train Accuracies: All 0.7237 | Old 0.8387 | New 0.6113
2024-05-15 11:12:38.386 | INFO     | __main__:train:123 - Epoch: [83][0/46]	 loss 4.28258	 cls_loss: 0.3140 cluster_loss: 1.3470 sup_con_loss: 0.8655 contrastive_loss: 4.6065 
2024-05-15 11:12:55.708 | INFO     | __main__:train:123 - Epoch: [83][20/46]	 loss 3.99082	 cls_loss: 0.2938 cluster_loss: 1.1350 sup_con_loss: 0.4597 contrastive_loss: 4.5990 
2024-05-15 11:13:11.226 | INFO     | __main__:train:123 - Epoch: [83][40/46]	 loss 4.22035	 cls_loss: 0.3402 cluster_loss: 1.3266 sup_con_loss: 0.7020 contrastive_loss: 4.6051 
2024-05-15 11:13:15.130 | INFO     | __main__:train:126 - Train Epoch: 83 Avg Loss: 4.1258 
2024-05-15 11:13:15.130 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:13:37.531 | INFO     | __main__:train:135 - Train Accuracies: All 0.7214 | Old 0.8401 | New 0.6055
2024-05-15 11:13:42.804 | INFO     | __main__:train:123 - Epoch: [84][0/46]	 loss 4.19945	 cls_loss: 0.3163 cluster_loss: 1.3432 sup_con_loss: 0.6311 contrastive_loss: 4.6073 
2024-05-15 11:14:00.629 | INFO     | __main__:train:123 - Epoch: [84][20/46]	 loss 4.16948	 cls_loss: 0.3297 cluster_loss: 1.2870 sup_con_loss: 0.6254 contrastive_loss: 4.6132 
2024-05-15 11:14:16.350 | INFO     | __main__:train:123 - Epoch: [84][40/46]	 loss 4.11738	 cls_loss: 0.3016 cluster_loss: 1.2558 sup_con_loss: 0.5782 contrastive_loss: 4.6049 
2024-05-15 11:14:20.423 | INFO     | __main__:train:126 - Train Epoch: 84 Avg Loss: 4.1163 
2024-05-15 11:14:20.424 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:14:42.389 | INFO     | __main__:train:135 - Train Accuracies: All 0.7214 | Old 0.8352 | New 0.6102
2024-05-15 11:14:48.499 | INFO     | __main__:train:123 - Epoch: [85][0/46]	 loss 4.08053	 cls_loss: 0.3224 cluster_loss: 1.1739 sup_con_loss: 0.6233 contrastive_loss: 4.5947 
2024-05-15 11:15:05.469 | INFO     | __main__:train:123 - Epoch: [85][20/46]	 loss 4.13069	 cls_loss: 0.3348 cluster_loss: 1.2440 sup_con_loss: 0.6160 contrastive_loss: 4.5989 
2024-05-15 11:15:21.281 | INFO     | __main__:train:123 - Epoch: [85][40/46]	 loss 4.22863	 cls_loss: 0.3286 cluster_loss: 1.2723 sup_con_loss: 0.8526 contrastive_loss: 4.5972 
2024-05-15 11:15:25.213 | INFO     | __main__:train:126 - Train Epoch: 85 Avg Loss: 4.1451 
2024-05-15 11:15:25.213 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:15:47.233 | INFO     | __main__:train:135 - Train Accuracies: All 0.7213 | Old 0.8397 | New 0.6055
2024-05-15 11:15:53.130 | INFO     | __main__:train:123 - Epoch: [86][0/46]	 loss 4.10690	 cls_loss: 0.3150 cluster_loss: 1.2705 sup_con_loss: 0.5121 contrastive_loss: 4.6024 
2024-05-15 11:16:10.680 | INFO     | __main__:train:123 - Epoch: [86][20/46]	 loss 4.14856	 cls_loss: 0.2930 cluster_loss: 1.2618 sup_con_loss: 0.6815 contrastive_loss: 4.5959 
2024-05-15 11:16:26.384 | INFO     | __main__:train:123 - Epoch: [86][40/46]	 loss 4.05148	 cls_loss: 0.3337 cluster_loss: 1.2104 sup_con_loss: 0.4518 contrastive_loss: 4.5998 
2024-05-15 11:16:30.386 | INFO     | __main__:train:126 - Train Epoch: 86 Avg Loss: 4.1333 
2024-05-15 11:16:30.387 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:16:53.263 | INFO     | __main__:train:135 - Train Accuracies: All 0.7249 | Old 0.8422 | New 0.6102
2024-05-15 11:16:59.074 | INFO     | __main__:train:123 - Epoch: [87][0/46]	 loss 4.22000	 cls_loss: 0.3132 cluster_loss: 1.3599 sup_con_loss: 0.6639 contrastive_loss: 4.6063 
2024-05-15 11:17:16.391 | INFO     | __main__:train:123 - Epoch: [87][20/46]	 loss 4.08639	 cls_loss: 0.2920 cluster_loss: 1.2249 sup_con_loss: 0.5804 contrastive_loss: 4.5921 
2024-05-15 11:17:32.159 | INFO     | __main__:train:123 - Epoch: [87][40/46]	 loss 4.01432	 cls_loss: 0.3089 cluster_loss: 1.1716 sup_con_loss: 0.4527 contrastive_loss: 4.5943 
2024-05-15 11:17:36.145 | INFO     | __main__:train:126 - Train Epoch: 87 Avg Loss: 4.1341 
2024-05-15 11:17:36.146 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:17:58.742 | INFO     | __main__:train:135 - Train Accuracies: All 0.7221 | Old 0.8373 | New 0.6096
2024-05-15 11:18:05.313 | INFO     | __main__:train:123 - Epoch: [88][0/46]	 loss 4.12920	 cls_loss: 0.3004 cluster_loss: 1.2418 sup_con_loss: 0.6551 contrastive_loss: 4.5963 
2024-05-15 11:18:22.380 | INFO     | __main__:train:123 - Epoch: [88][20/46]	 loss 4.02913	 cls_loss: 0.2830 cluster_loss: 1.1376 sup_con_loss: 0.5750 contrastive_loss: 4.5991 
2024-05-15 11:18:37.791 | INFO     | __main__:train:123 - Epoch: [88][40/46]	 loss 4.17391	 cls_loss: 0.3054 cluster_loss: 1.2822 sup_con_loss: 0.6994 contrastive_loss: 4.5982 
2024-05-15 11:18:41.689 | INFO     | __main__:train:126 - Train Epoch: 88 Avg Loss: 4.1175 
2024-05-15 11:18:41.690 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:19:03.750 | INFO     | __main__:train:135 - Train Accuracies: All 0.7218 | Old 0.8362 | New 0.6099
2024-05-15 11:19:09.779 | INFO     | __main__:train:123 - Epoch: [89][0/46]	 loss 4.13960	 cls_loss: 0.3362 cluster_loss: 1.2616 sup_con_loss: 0.6043 contrastive_loss: 4.6005 
2024-05-15 11:19:26.972 | INFO     | __main__:train:123 - Epoch: [89][20/46]	 loss 4.04578	 cls_loss: 0.2918 cluster_loss: 1.2373 sup_con_loss: 0.4235 contrastive_loss: 4.6018 
2024-05-15 11:19:42.509 | INFO     | __main__:train:123 - Epoch: [89][40/46]	 loss 4.03049	 cls_loss: 0.3204 cluster_loss: 1.1602 sup_con_loss: 0.4952 contrastive_loss: 4.6014 
2024-05-15 11:19:46.474 | INFO     | __main__:train:126 - Train Epoch: 89 Avg Loss: 4.0920 
2024-05-15 11:19:46.474 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:20:08.617 | INFO     | __main__:train:135 - Train Accuracies: All 0.7232 | Old 0.8373 | New 0.6116
2024-05-15 11:20:14.239 | INFO     | __main__:train:123 - Epoch: [90][0/46]	 loss 4.13742	 cls_loss: 0.2908 cluster_loss: 1.2747 sup_con_loss: 0.6169 contrastive_loss: 4.6018 
2024-05-15 11:20:31.532 | INFO     | __main__:train:123 - Epoch: [90][20/46]	 loss 4.07807	 cls_loss: 0.3170 cluster_loss: 1.2030 sup_con_loss: 0.5526 contrastive_loss: 4.6027 
2024-05-15 11:20:47.170 | INFO     | __main__:train:123 - Epoch: [90][40/46]	 loss 4.10082	 cls_loss: 0.3163 cluster_loss: 1.1861 sup_con_loss: 0.6622 contrastive_loss: 4.5960 
2024-05-15 11:20:51.139 | INFO     | __main__:train:126 - Train Epoch: 90 Avg Loss: 4.1263 
2024-05-15 11:20:51.140 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:21:13.419 | INFO     | __main__:train:135 - Train Accuracies: All 0.7214 | Old 0.8408 | New 0.6048
2024-05-15 11:21:19.344 | INFO     | __main__:train:123 - Epoch: [91][0/46]	 loss 4.10299	 cls_loss: 0.2998 cluster_loss: 1.2719 sup_con_loss: 0.5132 contrastive_loss: 4.6026 
2024-05-15 11:21:36.544 | INFO     | __main__:train:123 - Epoch: [91][20/46]	 loss 4.27076	 cls_loss: 0.2909 cluster_loss: 1.3994 sup_con_loss: 0.7665 contrastive_loss: 4.6016 
2024-05-15 11:21:52.224 | INFO     | __main__:train:123 - Epoch: [91][40/46]	 loss 4.07491	 cls_loss: 0.2980 cluster_loss: 1.2236 sup_con_loss: 0.5314 contrastive_loss: 4.5989 
2024-05-15 11:21:56.213 | INFO     | __main__:train:126 - Train Epoch: 91 Avg Loss: 4.1134 
2024-05-15 11:21:56.213 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:22:17.635 | INFO     | __main__:train:135 - Train Accuracies: All 0.7223 | Old 0.8359 | New 0.6113
2024-05-15 11:22:23.364 | INFO     | __main__:train:123 - Epoch: [92][0/46]	 loss 4.17189	 cls_loss: 0.3485 cluster_loss: 1.2401 sup_con_loss: 0.7217 contrastive_loss: 4.6019 
2024-05-15 11:22:40.968 | INFO     | __main__:train:123 - Epoch: [92][20/46]	 loss 4.12439	 cls_loss: 0.2967 cluster_loss: 1.2181 sup_con_loss: 0.6912 contrastive_loss: 4.5951 
2024-05-15 11:22:56.653 | INFO     | __main__:train:123 - Epoch: [92][40/46]	 loss 4.00941	 cls_loss: 0.3091 cluster_loss: 1.1591 sup_con_loss: 0.4624 contrastive_loss: 4.5938 
2024-05-15 11:23:00.756 | INFO     | __main__:train:126 - Train Epoch: 92 Avg Loss: 4.1160 
2024-05-15 11:23:00.757 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:23:23.379 | INFO     | __main__:train:135 - Train Accuracies: All 0.7239 | Old 0.8328 | New 0.6174
2024-05-15 11:23:28.672 | INFO     | __main__:train:123 - Epoch: [93][0/46]	 loss 4.15486	 cls_loss: 0.2896 cluster_loss: 1.3419 sup_con_loss: 0.5561 contrastive_loss: 4.5948 
2024-05-15 11:23:46.538 | INFO     | __main__:train:123 - Epoch: [93][20/46]	 loss 4.09529	 cls_loss: 0.2817 cluster_loss: 1.2275 sup_con_loss: 0.6085 contrastive_loss: 4.5935 
2024-05-15 11:24:02.358 | INFO     | __main__:train:123 - Epoch: [93][40/46]	 loss 4.22217	 cls_loss: 0.3297 cluster_loss: 1.2847 sup_con_loss: 0.8063 contrastive_loss: 4.5992 
2024-05-15 11:24:06.417 | INFO     | __main__:train:126 - Train Epoch: 93 Avg Loss: 4.1296 
2024-05-15 11:24:06.418 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:24:27.926 | INFO     | __main__:train:135 - Train Accuracies: All 0.7249 | Old 0.8394 | New 0.6130
2024-05-15 11:24:33.245 | INFO     | __main__:train:123 - Epoch: [94][0/46]	 loss 4.04647	 cls_loss: 0.3118 cluster_loss: 1.1857 sup_con_loss: 0.5067 contrastive_loss: 4.5989 
2024-05-15 11:24:50.486 | INFO     | __main__:train:123 - Epoch: [94][20/46]	 loss 4.07178	 cls_loss: 0.3001 cluster_loss: 1.2211 sup_con_loss: 0.5115 contrastive_loss: 4.6061 
2024-05-15 11:25:06.131 | INFO     | __main__:train:123 - Epoch: [94][40/46]	 loss 4.05232	 cls_loss: 0.3264 cluster_loss: 1.2356 sup_con_loss: 0.4202 contrastive_loss: 4.5967 
2024-05-15 11:25:10.024 | INFO     | __main__:train:126 - Train Epoch: 94 Avg Loss: 4.1151 
2024-05-15 11:25:10.024 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:25:32.117 | INFO     | __main__:train:135 - Train Accuracies: All 0.7256 | Old 0.8355 | New 0.6181
2024-05-15 11:25:37.714 | INFO     | __main__:train:123 - Epoch: [95][0/46]	 loss 4.26176	 cls_loss: 0.2938 cluster_loss: 1.3164 sup_con_loss: 0.8958 contrastive_loss: 4.5997 
2024-05-15 11:25:55.153 | INFO     | __main__:train:123 - Epoch: [95][20/46]	 loss 4.14808	 cls_loss: 0.3397 cluster_loss: 1.2450 sup_con_loss: 0.6625 contrastive_loss: 4.5970 
2024-05-15 11:26:10.711 | INFO     | __main__:train:123 - Epoch: [95][40/46]	 loss 4.22503	 cls_loss: 0.3031 cluster_loss: 1.3460 sup_con_loss: 0.7255 contrastive_loss: 4.6002 
2024-05-15 11:26:14.696 | INFO     | __main__:train:126 - Train Epoch: 95 Avg Loss: 4.1268 
2024-05-15 11:26:14.697 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:26:37.037 | INFO     | __main__:train:135 - Train Accuracies: All 0.7226 | Old 0.8387 | New 0.6092
2024-05-15 11:26:43.220 | INFO     | __main__:train:123 - Epoch: [96][0/46]	 loss 4.04700	 cls_loss: 0.2891 cluster_loss: 1.1960 sup_con_loss: 0.5153 contrastive_loss: 4.5971 
2024-05-15 11:27:00.104 | INFO     | __main__:train:123 - Epoch: [96][20/46]	 loss 4.25839	 cls_loss: 0.3298 cluster_loss: 1.3268 sup_con_loss: 0.8315 contrastive_loss: 4.5993 
2024-05-15 11:27:15.751 | INFO     | __main__:train:123 - Epoch: [96][40/46]	 loss 4.07212	 cls_loss: 0.3283 cluster_loss: 1.2080 sup_con_loss: 0.5267 contrastive_loss: 4.5965 
2024-05-15 11:27:19.701 | INFO     | __main__:train:126 - Train Epoch: 96 Avg Loss: 4.1394 
2024-05-15 11:27:19.701 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:27:41.914 | INFO     | __main__:train:135 - Train Accuracies: All 0.7226 | Old 0.8390 | New 0.6089
2024-05-15 11:27:47.866 | INFO     | __main__:train:123 - Epoch: [97][0/46]	 loss 4.10312	 cls_loss: 0.2916 cluster_loss: 1.2669 sup_con_loss: 0.5465 contrastive_loss: 4.5944 
2024-05-15 11:28:04.821 | INFO     | __main__:train:123 - Epoch: [97][20/46]	 loss 4.21470	 cls_loss: 0.3257 cluster_loss: 1.2888 sup_con_loss: 0.7753 contrastive_loss: 4.6025 
2024-05-15 11:28:20.390 | INFO     | __main__:train:123 - Epoch: [97][40/46]	 loss 4.04175	 cls_loss: 0.3091 cluster_loss: 1.2043 sup_con_loss: 0.4610 contrastive_loss: 4.5991 
2024-05-15 11:28:24.295 | INFO     | __main__:train:126 - Train Epoch: 97 Avg Loss: 4.1072 
2024-05-15 11:28:24.296 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:28:46.392 | INFO     | __main__:train:135 - Train Accuracies: All 0.7237 | Old 0.8415 | New 0.6085
2024-05-15 11:28:52.366 | INFO     | __main__:train:123 - Epoch: [98][0/46]	 loss 4.21842	 cls_loss: 0.2838 cluster_loss: 1.3262 sup_con_loss: 0.7702 contrastive_loss: 4.5961 
2024-05-15 11:29:09.513 | INFO     | __main__:train:123 - Epoch: [98][20/46]	 loss 4.20049	 cls_loss: 0.3223 cluster_loss: 1.2755 sup_con_loss: 0.7630 contrastive_loss: 4.6024 
2024-05-15 11:29:25.018 | INFO     | __main__:train:123 - Epoch: [98][40/46]	 loss 4.01486	 cls_loss: 0.3078 cluster_loss: 1.1804 sup_con_loss: 0.4332 contrastive_loss: 4.5974 
2024-05-15 11:29:29.019 | INFO     | __main__:train:126 - Train Epoch: 98 Avg Loss: 4.1277 
2024-05-15 11:29:29.020 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:29:51.184 | INFO     | __main__:train:135 - Train Accuracies: All 0.7261 | Old 0.8369 | New 0.6177
2024-05-15 11:29:57.021 | INFO     | __main__:train:123 - Epoch: [99][0/46]	 loss 4.17310	 cls_loss: 0.3516 cluster_loss: 1.2740 sup_con_loss: 0.6493 contrastive_loss: 4.6072 
2024-05-15 11:30:13.461 | INFO     | __main__:train:123 - Epoch: [99][20/46]	 loss 4.09067	 cls_loss: 0.3191 cluster_loss: 1.2189 sup_con_loss: 0.5689 contrastive_loss: 4.5963 
2024-05-15 11:30:28.799 | INFO     | __main__:train:123 - Epoch: [99][40/46]	 loss 4.07050	 cls_loss: 0.3048 cluster_loss: 1.1933 sup_con_loss: 0.5751 contrastive_loss: 4.5952 
2024-05-15 11:30:32.743 | INFO     | __main__:train:126 - Train Epoch: 99 Avg Loss: 4.1235 
2024-05-15 11:30:32.743 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:30:54.325 | INFO     | __main__:train:135 - Train Accuracies: All 0.7221 | Old 0.8376 | New 0.6092
2024-05-15 11:31:00.786 | INFO     | __main__:train:123 - Epoch: [100][0/46]	 loss 4.12815	 cls_loss: 0.3022 cluster_loss: 1.2417 sup_con_loss: 0.6543 contrastive_loss: 4.5943 
2024-05-15 11:31:17.874 | INFO     | __main__:train:123 - Epoch: [100][20/46]	 loss 4.10810	 cls_loss: 0.2932 cluster_loss: 1.2627 sup_con_loss: 0.5581 contrastive_loss: 4.5990 
2024-05-15 11:31:33.497 | INFO     | __main__:train:123 - Epoch: [100][40/46]	 loss 4.07455	 cls_loss: 0.3244 cluster_loss: 1.2072 sup_con_loss: 0.5321 contrastive_loss: 4.6001 
2024-05-15 11:31:37.550 | INFO     | __main__:train:126 - Train Epoch: 100 Avg Loss: 4.1174 
2024-05-15 11:31:37.551 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:31:59.902 | INFO     | __main__:train:135 - Train Accuracies: All 0.7237 | Old 0.8408 | New 0.6092
2024-05-15 11:32:06.336 | INFO     | __main__:train:123 - Epoch: [101][0/46]	 loss 4.27990	 cls_loss: 0.3037 cluster_loss: 1.3263 sup_con_loss: 0.9188 contrastive_loss: 4.5999 
2024-05-15 11:32:23.711 | INFO     | __main__:train:123 - Epoch: [101][20/46]	 loss 4.10167	 cls_loss: 0.2972 cluster_loss: 1.2434 sup_con_loss: 0.5784 contrastive_loss: 4.5953 
2024-05-15 11:32:39.182 | INFO     | __main__:train:123 - Epoch: [101][40/46]	 loss 4.11106	 cls_loss: 0.2919 cluster_loss: 1.2209 sup_con_loss: 0.6521 contrastive_loss: 4.5955 
2024-05-15 11:32:43.235 | INFO     | __main__:train:126 - Train Epoch: 101 Avg Loss: 4.1113 
2024-05-15 11:32:43.236 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:33:05.690 | INFO     | __main__:train:135 - Train Accuracies: All 0.7256 | Old 0.8422 | New 0.6116
2024-05-15 11:33:11.058 | INFO     | __main__:train:123 - Epoch: [102][0/46]	 loss 4.11042	 cls_loss: 0.2944 cluster_loss: 1.2506 sup_con_loss: 0.5976 contrastive_loss: 4.5928 
2024-05-15 11:33:28.552 | INFO     | __main__:train:123 - Epoch: [102][20/46]	 loss 4.12616	 cls_loss: 0.2957 cluster_loss: 1.2855 sup_con_loss: 0.5571 contrastive_loss: 4.6032 
2024-05-15 11:33:44.301 | INFO     | __main__:train:123 - Epoch: [102][40/46]	 loss 3.95296	 cls_loss: 0.2974 cluster_loss: 1.1751 sup_con_loss: 0.2809 contrastive_loss: 4.5950 
2024-05-15 11:33:48.286 | INFO     | __main__:train:126 - Train Epoch: 102 Avg Loss: 4.1111 
2024-05-15 11:33:48.286 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:34:10.533 | INFO     | __main__:train:135 - Train Accuracies: All 0.7216 | Old 0.8387 | New 0.6072
2024-05-15 11:34:16.470 | INFO     | __main__:train:123 - Epoch: [103][0/46]	 loss 4.10916	 cls_loss: 0.3082 cluster_loss: 1.2388 sup_con_loss: 0.6030 contrastive_loss: 4.5924 
2024-05-15 11:34:33.810 | INFO     | __main__:train:123 - Epoch: [103][20/46]	 loss 4.13095	 cls_loss: 0.2973 cluster_loss: 1.2192 sup_con_loss: 0.7054 contrastive_loss: 4.5962 
2024-05-15 11:34:49.562 | INFO     | __main__:train:123 - Epoch: [103][40/46]	 loss 4.10183	 cls_loss: 0.3041 cluster_loss: 1.1711 sup_con_loss: 0.7079 contrastive_loss: 4.5945 
2024-05-15 11:34:53.489 | INFO     | __main__:train:126 - Train Epoch: 103 Avg Loss: 4.1101 
2024-05-15 11:34:53.490 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:35:16.167 | INFO     | __main__:train:135 - Train Accuracies: All 0.7230 | Old 0.8387 | New 0.6099
2024-05-15 11:35:21.864 | INFO     | __main__:train:123 - Epoch: [104][0/46]	 loss 4.11792	 cls_loss: 0.2985 cluster_loss: 1.2904 sup_con_loss: 0.5192 contrastive_loss: 4.6045 
2024-05-15 11:35:39.618 | INFO     | __main__:train:123 - Epoch: [104][20/46]	 loss 4.10805	 cls_loss: 0.2917 cluster_loss: 1.2498 sup_con_loss: 0.5746 contrastive_loss: 4.6038 
2024-05-15 11:35:55.324 | INFO     | __main__:train:123 - Epoch: [104][40/46]	 loss 4.24981	 cls_loss: 0.3132 cluster_loss: 1.3175 sup_con_loss: 0.8404 contrastive_loss: 4.5995 
2024-05-15 11:35:59.269 | INFO     | __main__:train:126 - Train Epoch: 104 Avg Loss: 4.0981 
2024-05-15 11:35:59.269 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:36:21.372 | INFO     | __main__:train:135 - Train Accuracies: All 0.7240 | Old 0.8404 | New 0.6102
2024-05-15 11:36:26.986 | INFO     | __main__:train:123 - Epoch: [105][0/46]	 loss 4.05380	 cls_loss: 0.3030 cluster_loss: 1.1935 sup_con_loss: 0.5428 contrastive_loss: 4.5877 
2024-05-15 11:36:44.450 | INFO     | __main__:train:123 - Epoch: [105][20/46]	 loss 4.04623	 cls_loss: 0.2872 cluster_loss: 1.1440 sup_con_loss: 0.6122 contrastive_loss: 4.5967 
2024-05-15 11:37:00.305 | INFO     | __main__:train:123 - Epoch: [105][40/46]	 loss 4.21018	 cls_loss: 0.3020 cluster_loss: 1.3046 sup_con_loss: 0.7456 contrastive_loss: 4.6085 
2024-05-15 11:37:04.352 | INFO     | __main__:train:126 - Train Epoch: 105 Avg Loss: 4.0988 
2024-05-15 11:37:04.353 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:37:26.489 | INFO     | __main__:train:135 - Train Accuracies: All 0.7239 | Old 0.8418 | New 0.6085
2024-05-15 11:37:32.786 | INFO     | __main__:train:123 - Epoch: [106][0/46]	 loss 4.01362	 cls_loss: 0.2898 cluster_loss: 1.1346 sup_con_loss: 0.5228 contrastive_loss: 4.6027 
2024-05-15 11:37:49.729 | INFO     | __main__:train:123 - Epoch: [106][20/46]	 loss 4.11793	 cls_loss: 0.2920 cluster_loss: 1.2309 sup_con_loss: 0.6367 contrastive_loss: 4.6043 
2024-05-15 11:38:05.459 | INFO     | __main__:train:123 - Epoch: [106][40/46]	 loss 4.18023	 cls_loss: 0.3102 cluster_loss: 1.3219 sup_con_loss: 0.6385 contrastive_loss: 4.5984 
2024-05-15 11:38:09.460 | INFO     | __main__:train:126 - Train Epoch: 106 Avg Loss: 4.1192 
2024-05-15 11:38:09.460 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:38:31.261 | INFO     | __main__:train:135 - Train Accuracies: All 0.7221 | Old 0.8394 | New 0.6075
2024-05-15 11:38:37.571 | INFO     | __main__:train:123 - Epoch: [107][0/46]	 loss 4.27190	 cls_loss: 0.2778 cluster_loss: 1.3384 sup_con_loss: 0.8869 contrastive_loss: 4.6066 
2024-05-15 11:38:54.448 | INFO     | __main__:train:123 - Epoch: [107][20/46]	 loss 4.04595	 cls_loss: 0.2958 cluster_loss: 1.1715 sup_con_loss: 0.5551 contrastive_loss: 4.5949 
2024-05-15 11:39:10.166 | INFO     | __main__:train:123 - Epoch: [107][40/46]	 loss 4.13409	 cls_loss: 0.2980 cluster_loss: 1.2543 sup_con_loss: 0.6499 contrastive_loss: 4.5955 
2024-05-15 11:39:14.156 | INFO     | __main__:train:126 - Train Epoch: 107 Avg Loss: 4.1060 
2024-05-15 11:39:14.156 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:39:36.309 | INFO     | __main__:train:135 - Train Accuracies: All 0.7258 | Old 0.8436 | New 0.6106
2024-05-15 11:39:42.354 | INFO     | __main__:train:123 - Epoch: [108][0/46]	 loss 4.05663	 cls_loss: 0.2935 cluster_loss: 1.2002 sup_con_loss: 0.5387 contrastive_loss: 4.5927 
2024-05-15 11:39:59.632 | INFO     | __main__:train:123 - Epoch: [108][20/46]	 loss 4.17861	 cls_loss: 0.3133 cluster_loss: 1.2835 sup_con_loss: 0.6912 contrastive_loss: 4.6043 
2024-05-15 11:40:15.300 | INFO     | __main__:train:123 - Epoch: [108][40/46]	 loss 4.15294	 cls_loss: 0.2932 cluster_loss: 1.2764 sup_con_loss: 0.6759 contrastive_loss: 4.5909 
2024-05-15 11:40:19.226 | INFO     | __main__:train:126 - Train Epoch: 108 Avg Loss: 4.1110 
2024-05-15 11:40:19.227 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:40:41.290 | INFO     | __main__:train:135 - Train Accuracies: All 0.7232 | Old 0.8404 | New 0.6085
2024-05-15 11:40:46.964 | INFO     | __main__:train:123 - Epoch: [109][0/46]	 loss 4.29407	 cls_loss: 0.2938 cluster_loss: 1.4162 sup_con_loss: 0.7960 contrastive_loss: 4.6033 
2024-05-15 11:41:04.457 | INFO     | __main__:train:123 - Epoch: [109][20/46]	 loss 4.02911	 cls_loss: 0.2741 cluster_loss: 1.1449 sup_con_loss: 0.5862 contrastive_loss: 4.5905 
2024-05-15 11:41:20.038 | INFO     | __main__:train:123 - Epoch: [109][40/46]	 loss 4.05778	 cls_loss: 0.2967 cluster_loss: 1.2065 sup_con_loss: 0.5135 contrastive_loss: 4.6000 
2024-05-15 11:41:24.007 | INFO     | __main__:train:126 - Train Epoch: 109 Avg Loss: 4.1144 
2024-05-15 11:41:24.008 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:41:46.026 | INFO     | __main__:train:135 - Train Accuracies: All 0.7244 | Old 0.8422 | New 0.6092
2024-05-15 11:41:51.717 | INFO     | __main__:train:123 - Epoch: [110][0/46]	 loss 4.09848	 cls_loss: 0.3059 cluster_loss: 1.2104 sup_con_loss: 0.6187 contrastive_loss: 4.5970 
2024-05-15 11:42:08.990 | INFO     | __main__:train:123 - Epoch: [110][20/46]	 loss 4.02516	 cls_loss: 0.3011 cluster_loss: 1.1298 sup_con_loss: 0.5763 contrastive_loss: 4.5903 
2024-05-15 11:42:24.650 | INFO     | __main__:train:123 - Epoch: [110][40/46]	 loss 4.10532	 cls_loss: 0.2854 cluster_loss: 1.2014 sup_con_loss: 0.6806 contrastive_loss: 4.5943 
2024-05-15 11:42:28.645 | INFO     | __main__:train:126 - Train Epoch: 110 Avg Loss: 4.1126 
2024-05-15 11:42:28.645 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:42:50.860 | INFO     | __main__:train:135 - Train Accuracies: All 0.7240 | Old 0.8422 | New 0.6085
2024-05-15 11:42:57.262 | INFO     | __main__:train:123 - Epoch: [111][0/46]	 loss 4.09303	 cls_loss: 0.3344 cluster_loss: 1.1672 sup_con_loss: 0.6523 contrastive_loss: 4.5985 
2024-05-15 11:43:14.367 | INFO     | __main__:train:123 - Epoch: [111][20/46]	 loss 4.06385	 cls_loss: 0.2515 cluster_loss: 1.2101 sup_con_loss: 0.5798 contrastive_loss: 4.5943 
2024-05-15 11:43:30.121 | INFO     | __main__:train:123 - Epoch: [111][40/46]	 loss 4.09723	 cls_loss: 0.3055 cluster_loss: 1.2198 sup_con_loss: 0.5985 contrastive_loss: 4.5969 
2024-05-15 11:43:34.100 | INFO     | __main__:train:126 - Train Epoch: 111 Avg Loss: 4.0988 
2024-05-15 11:43:34.101 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:43:56.030 | INFO     | __main__:train:135 - Train Accuracies: All 0.7239 | Old 0.8432 | New 0.6072
2024-05-15 11:44:02.376 | INFO     | __main__:train:123 - Epoch: [112][0/46]	 loss 4.03143	 cls_loss: 0.2937 cluster_loss: 1.1280 sup_con_loss: 0.6189 contrastive_loss: 4.5829 
2024-05-15 11:44:19.426 | INFO     | __main__:train:123 - Epoch: [112][20/46]	 loss 4.19656	 cls_loss: 0.2778 cluster_loss: 1.3206 sup_con_loss: 0.7261 contrastive_loss: 4.5951 
2024-05-15 11:44:35.024 | INFO     | __main__:train:123 - Epoch: [112][40/46]	 loss 4.09924	 cls_loss: 0.3030 cluster_loss: 1.2451 sup_con_loss: 0.5450 contrastive_loss: 4.6048 
2024-05-15 11:44:38.929 | INFO     | __main__:train:126 - Train Epoch: 112 Avg Loss: 4.1143 
2024-05-15 11:44:38.930 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:45:00.660 | INFO     | __main__:train:135 - Train Accuracies: All 0.7254 | Old 0.8436 | New 0.6099
2024-05-15 11:45:07.172 | INFO     | __main__:train:123 - Epoch: [113][0/46]	 loss 4.11754	 cls_loss: 0.2809 cluster_loss: 1.2312 sup_con_loss: 0.6706 contrastive_loss: 4.5911 
2024-05-15 11:45:24.323 | INFO     | __main__:train:123 - Epoch: [113][20/46]	 loss 4.17091	 cls_loss: 0.2886 cluster_loss: 1.2600 sup_con_loss: 0.7568 contrastive_loss: 4.5939 
2024-05-15 11:45:40.038 | INFO     | __main__:train:123 - Epoch: [113][40/46]	 loss 4.17975	 cls_loss: 0.2872 cluster_loss: 1.3263 sup_con_loss: 0.6511 contrastive_loss: 4.5989 
2024-05-15 11:45:44.092 | INFO     | __main__:train:126 - Train Epoch: 113 Avg Loss: 4.1289 
2024-05-15 11:45:44.093 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:46:06.061 | INFO     | __main__:train:135 - Train Accuracies: All 0.7239 | Old 0.8408 | New 0.6096
2024-05-15 11:46:11.779 | INFO     | __main__:train:123 - Epoch: [114][0/46]	 loss 4.06653	 cls_loss: 0.3042 cluster_loss: 1.2133 sup_con_loss: 0.5199 contrastive_loss: 4.5991 
2024-05-15 11:46:29.031 | INFO     | __main__:train:123 - Epoch: [114][20/46]	 loss 4.03573	 cls_loss: 0.2923 cluster_loss: 1.1935 sup_con_loss: 0.4892 contrastive_loss: 4.5945 
2024-05-15 11:46:44.552 | INFO     | __main__:train:123 - Epoch: [114][40/46]	 loss 4.15446	 cls_loss: 0.2804 cluster_loss: 1.2862 sup_con_loss: 0.6684 contrastive_loss: 4.5944 
2024-05-15 11:46:48.526 | INFO     | __main__:train:126 - Train Epoch: 114 Avg Loss: 4.1204 
2024-05-15 11:46:48.526 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:47:10.282 | INFO     | __main__:train:135 - Train Accuracies: All 0.7235 | Old 0.8436 | New 0.6061
2024-05-15 11:47:16.235 | INFO     | __main__:train:123 - Epoch: [115][0/46]	 loss 4.25648	 cls_loss: 0.3125 cluster_loss: 1.3090 sup_con_loss: 0.8774 contrastive_loss: 4.5987 
2024-05-15 11:47:33.275 | INFO     | __main__:train:123 - Epoch: [115][20/46]	 loss 4.15680	 cls_loss: 0.2825 cluster_loss: 1.3020 sup_con_loss: 0.6326 contrastive_loss: 4.6004 
2024-05-15 11:47:48.893 | INFO     | __main__:train:123 - Epoch: [115][40/46]	 loss 4.09363	 cls_loss: 0.3058 cluster_loss: 1.1844 sup_con_loss: 0.6615 contrastive_loss: 4.5927 
2024-05-15 11:47:52.857 | INFO     | __main__:train:126 - Train Epoch: 115 Avg Loss: 4.0977 
2024-05-15 11:47:52.857 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:48:15.069 | INFO     | __main__:train:135 - Train Accuracies: All 0.7223 | Old 0.8404 | New 0.6068
2024-05-15 11:48:21.115 | INFO     | __main__:train:123 - Epoch: [116][0/46]	 loss 4.06992	 cls_loss: 0.3027 cluster_loss: 1.1858 sup_con_loss: 0.5883 contrastive_loss: 4.5958 
2024-05-15 11:48:38.210 | INFO     | __main__:train:123 - Epoch: [116][20/46]	 loss 4.13466	 cls_loss: 0.2814 cluster_loss: 1.2374 sup_con_loss: 0.7059 contrastive_loss: 4.5919 
2024-05-15 11:48:54.023 | INFO     | __main__:train:123 - Epoch: [116][40/46]	 loss 4.18090	 cls_loss: 0.2737 cluster_loss: 1.2710 sup_con_loss: 0.7706 contrastive_loss: 4.5988 
2024-05-15 11:48:57.996 | INFO     | __main__:train:126 - Train Epoch: 116 Avg Loss: 4.0847 
2024-05-15 11:48:57.997 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:49:19.866 | INFO     | __main__:train:135 - Train Accuracies: All 0.7233 | Old 0.8418 | New 0.6075
2024-05-15 11:49:26.480 | INFO     | __main__:train:123 - Epoch: [117][0/46]	 loss 4.15003	 cls_loss: 0.3124 cluster_loss: 1.2804 sup_con_loss: 0.6102 contrastive_loss: 4.6074 
2024-05-15 11:49:43.489 | INFO     | __main__:train:123 - Epoch: [117][20/46]	 loss 3.98783	 cls_loss: 0.3032 cluster_loss: 1.1094 sup_con_loss: 0.5020 contrastive_loss: 4.5921 
2024-05-15 11:49:59.231 | INFO     | __main__:train:123 - Epoch: [117][40/46]	 loss 4.17945	 cls_loss: 0.3055 cluster_loss: 1.3008 sup_con_loss: 0.6782 contrastive_loss: 4.5994 
2024-05-15 11:50:03.167 | INFO     | __main__:train:126 - Train Epoch: 117 Avg Loss: 4.0987 
2024-05-15 11:50:03.168 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:50:25.448 | INFO     | __main__:train:135 - Train Accuracies: All 0.7245 | Old 0.8404 | New 0.6113
2024-05-15 11:50:31.144 | INFO     | __main__:train:123 - Epoch: [118][0/46]	 loss 4.14010	 cls_loss: 0.3031 cluster_loss: 1.2571 sup_con_loss: 0.6490 contrastive_loss: 4.5997 
2024-05-15 11:50:48.186 | INFO     | __main__:train:123 - Epoch: [118][20/46]	 loss 4.07897	 cls_loss: 0.2919 cluster_loss: 1.1912 sup_con_loss: 0.6167 contrastive_loss: 4.5949 
2024-05-15 11:51:03.817 | INFO     | __main__:train:123 - Epoch: [118][40/46]	 loss 4.03743	 cls_loss: 0.2862 cluster_loss: 1.1956 sup_con_loss: 0.5052 contrastive_loss: 4.5897 
2024-05-15 11:51:07.739 | INFO     | __main__:train:126 - Train Epoch: 118 Avg Loss: 4.0963 
2024-05-15 11:51:07.739 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:51:29.722 | INFO     | __main__:train:135 - Train Accuracies: All 0.7240 | Old 0.8443 | New 0.6065
2024-05-15 11:51:35.407 | INFO     | __main__:train:123 - Epoch: [119][0/46]	 loss 4.20063	 cls_loss: 0.2834 cluster_loss: 1.2496 sup_con_loss: 0.8703 contrastive_loss: 4.5917 
2024-05-15 11:51:52.880 | INFO     | __main__:train:123 - Epoch: [119][20/46]	 loss 4.13971	 cls_loss: 0.3025 cluster_loss: 1.2671 sup_con_loss: 0.6441 contrastive_loss: 4.5919 
2024-05-15 11:52:08.720 | INFO     | __main__:train:123 - Epoch: [119][40/46]	 loss 4.08168	 cls_loss: 0.2946 cluster_loss: 1.2531 sup_con_loss: 0.5007 contrastive_loss: 4.5981 
2024-05-15 11:52:12.720 | INFO     | __main__:train:126 - Train Epoch: 119 Avg Loss: 4.1109 
2024-05-15 11:52:12.721 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:52:34.636 | INFO     | __main__:train:135 - Train Accuracies: All 0.7235 | Old 0.8411 | New 0.6085
2024-05-15 11:52:40.927 | INFO     | __main__:train:123 - Epoch: [120][0/46]	 loss 4.12075	 cls_loss: 0.2842 cluster_loss: 1.2109 sup_con_loss: 0.7021 contrastive_loss: 4.5976 
2024-05-15 11:52:58.293 | INFO     | __main__:train:123 - Epoch: [120][20/46]	 loss 4.23181	 cls_loss: 0.2900 cluster_loss: 1.3400 sup_con_loss: 0.7817 contrastive_loss: 4.5934 
2024-05-15 11:53:13.963 | INFO     | __main__:train:123 - Epoch: [120][40/46]	 loss 3.99182	 cls_loss: 0.2827 cluster_loss: 1.1629 sup_con_loss: 0.4328 contrastive_loss: 4.5931 
2024-05-15 11:53:17.918 | INFO     | __main__:train:126 - Train Epoch: 120 Avg Loss: 4.0838 
2024-05-15 11:53:17.919 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:53:39.845 | INFO     | __main__:train:135 - Train Accuracies: All 0.7242 | Old 0.8411 | New 0.6099
2024-05-15 11:53:46.128 | INFO     | __main__:train:123 - Epoch: [121][0/46]	 loss 4.09047	 cls_loss: 0.3103 cluster_loss: 1.1818 sup_con_loss: 0.6575 contrastive_loss: 4.5901 
2024-05-15 11:54:03.209 | INFO     | __main__:train:123 - Epoch: [121][20/46]	 loss 4.10986	 cls_loss: 0.2880 cluster_loss: 1.2242 sup_con_loss: 0.6439 contrastive_loss: 4.5969 
2024-05-15 11:54:18.796 | INFO     | __main__:train:123 - Epoch: [121][40/46]	 loss 4.12579	 cls_loss: 0.2778 cluster_loss: 1.2619 sup_con_loss: 0.6287 contrastive_loss: 4.5973 
2024-05-15 11:54:22.782 | INFO     | __main__:train:126 - Train Epoch: 121 Avg Loss: 4.0986 
2024-05-15 11:54:22.783 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:54:45.011 | INFO     | __main__:train:135 - Train Accuracies: All 0.7237 | Old 0.8411 | New 0.6089
2024-05-15 11:54:50.595 | INFO     | __main__:train:123 - Epoch: [122][0/46]	 loss 4.01446	 cls_loss: 0.2811 cluster_loss: 1.1017 sup_con_loss: 0.6134 contrastive_loss: 4.5927 
2024-05-15 11:55:07.715 | INFO     | __main__:train:123 - Epoch: [122][20/46]	 loss 3.97034	 cls_loss: 0.2862 cluster_loss: 1.1221 sup_con_loss: 0.4484 contrastive_loss: 4.5905 
2024-05-15 11:55:23.502 | INFO     | __main__:train:123 - Epoch: [122][40/46]	 loss 4.17615	 cls_loss: 0.2713 cluster_loss: 1.3002 sup_con_loss: 0.7076 contrastive_loss: 4.5975 
2024-05-15 11:55:27.472 | INFO     | __main__:train:126 - Train Epoch: 122 Avg Loss: 4.1045 
2024-05-15 11:55:27.472 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:55:49.163 | INFO     | __main__:train:135 - Train Accuracies: All 0.7249 | Old 0.8450 | New 0.6075
2024-05-15 11:55:55.439 | INFO     | __main__:train:123 - Epoch: [123][0/46]	 loss 4.10714	 cls_loss: 0.2909 cluster_loss: 1.2553 sup_con_loss: 0.5698 contrastive_loss: 4.6000 
2024-05-15 11:56:12.317 | INFO     | __main__:train:123 - Epoch: [123][20/46]	 loss 4.23742	 cls_loss: 0.2989 cluster_loss: 1.3832 sup_con_loss: 0.7007 contrastive_loss: 4.5977 
2024-05-15 11:56:28.229 | INFO     | __main__:train:123 - Epoch: [123][40/46]	 loss 4.15016	 cls_loss: 0.2852 cluster_loss: 1.2360 sup_con_loss: 0.7535 contrastive_loss: 4.5895 
2024-05-15 11:56:32.135 | INFO     | __main__:train:126 - Train Epoch: 123 Avg Loss: 4.1207 
2024-05-15 11:56:32.136 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:56:54.270 | INFO     | __main__:train:135 - Train Accuracies: All 0.7273 | Old 0.8464 | New 0.6109
2024-05-15 11:56:59.731 | INFO     | __main__:train:123 - Epoch: [124][0/46]	 loss 4.00346	 cls_loss: 0.2924 cluster_loss: 1.1400 sup_con_loss: 0.4990 contrastive_loss: 4.5930 
2024-05-15 11:57:17.038 | INFO     | __main__:train:123 - Epoch: [124][20/46]	 loss 4.00856	 cls_loss: 0.2982 cluster_loss: 1.1705 sup_con_loss: 0.4572 contrastive_loss: 4.5898 
2024-05-15 11:57:33.018 | INFO     | __main__:train:123 - Epoch: [124][40/46]	 loss 4.06110	 cls_loss: 0.3072 cluster_loss: 1.1630 sup_con_loss: 0.6105 contrastive_loss: 4.5907 
2024-05-15 11:57:37.018 | INFO     | __main__:train:126 - Train Epoch: 124 Avg Loss: 4.1022 
2024-05-15 11:57:37.019 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:57:59.211 | INFO     | __main__:train:135 - Train Accuracies: All 0.7254 | Old 0.8362 | New 0.6171
2024-05-15 11:58:05.212 | INFO     | __main__:train:123 - Epoch: [125][0/46]	 loss 4.09243	 cls_loss: 0.2573 cluster_loss: 1.2163 sup_con_loss: 0.6412 contrastive_loss: 4.5959 
2024-05-15 11:58:22.563 | INFO     | __main__:train:123 - Epoch: [125][20/46]	 loss 4.01902	 cls_loss: 0.2910 cluster_loss: 1.1987 sup_con_loss: 0.4352 contrastive_loss: 4.5934 
2024-05-15 11:58:38.351 | INFO     | __main__:train:123 - Epoch: [125][40/46]	 loss 4.10594	 cls_loss: 0.2703 cluster_loss: 1.2431 sup_con_loss: 0.6116 contrastive_loss: 4.5988 
2024-05-15 11:58:42.430 | INFO     | __main__:train:126 - Train Epoch: 125 Avg Loss: 4.0770 
2024-05-15 11:58:42.431 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:59:04.924 | INFO     | __main__:train:135 - Train Accuracies: All 0.7268 | Old 0.8429 | New 0.6133
2024-05-15 11:59:10.635 | INFO     | __main__:train:123 - Epoch: [126][0/46]	 loss 3.92636	 cls_loss: 0.2695 cluster_loss: 1.0982 sup_con_loss: 0.3796 contrastive_loss: 4.5928 
2024-05-15 11:59:28.127 | INFO     | __main__:train:123 - Epoch: [126][20/46]	 loss 4.13925	 cls_loss: 0.2952 cluster_loss: 1.2380 sup_con_loss: 0.7046 contrastive_loss: 4.5917 
2024-05-15 11:59:43.878 | INFO     | __main__:train:123 - Epoch: [126][40/46]	 loss 4.17459	 cls_loss: 0.2957 cluster_loss: 1.2870 sup_con_loss: 0.7044 contrastive_loss: 4.5969 
2024-05-15 11:59:47.865 | INFO     | __main__:train:126 - Train Epoch: 126 Avg Loss: 4.0943 
2024-05-15 11:59:47.866 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:00:09.548 | INFO     | __main__:train:135 - Train Accuracies: All 0.7251 | Old 0.8359 | New 0.6167
2024-05-15 12:00:15.789 | INFO     | __main__:train:123 - Epoch: [127][0/46]	 loss 4.03293	 cls_loss: 0.2898 cluster_loss: 1.2028 sup_con_loss: 0.4624 contrastive_loss: 4.5966 
2024-05-15 12:00:32.717 | INFO     | __main__:train:123 - Epoch: [127][20/46]	 loss 4.13509	 cls_loss: 0.2811 cluster_loss: 1.2700 sup_con_loss: 0.6363 contrastive_loss: 4.5977 
2024-05-15 12:00:48.516 | INFO     | __main__:train:123 - Epoch: [127][40/46]	 loss 4.10060	 cls_loss: 0.3035 cluster_loss: 1.2171 sup_con_loss: 0.6314 contrastive_loss: 4.5881 
2024-05-15 12:00:52.588 | INFO     | __main__:train:126 - Train Epoch: 127 Avg Loss: 4.0999 
2024-05-15 12:00:52.588 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:01:14.753 | INFO     | __main__:train:135 - Train Accuracies: All 0.7276 | Old 0.8408 | New 0.6171
2024-05-15 12:01:20.207 | INFO     | __main__:train:123 - Epoch: [128][0/46]	 loss 4.04636	 cls_loss: 0.2845 cluster_loss: 1.1902 sup_con_loss: 0.5349 contrastive_loss: 4.5938 
2024-05-15 12:01:37.513 | INFO     | __main__:train:123 - Epoch: [128][20/46]	 loss 4.03302	 cls_loss: 0.2798 cluster_loss: 1.1806 sup_con_loss: 0.5249 contrastive_loss: 4.5908 
2024-05-15 12:01:53.232 | INFO     | __main__:train:123 - Epoch: [128][40/46]	 loss 4.02039	 cls_loss: 0.2794 cluster_loss: 1.1898 sup_con_loss: 0.4791 contrastive_loss: 4.5870 
2024-05-15 12:01:57.199 | INFO     | __main__:train:126 - Train Epoch: 128 Avg Loss: 4.0985 
2024-05-15 12:01:57.200 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:02:18.811 | INFO     | __main__:train:135 - Train Accuracies: All 0.7256 | Old 0.8390 | New 0.6147
2024-05-15 12:02:24.387 | INFO     | __main__:train:123 - Epoch: [129][0/46]	 loss 4.03569	 cls_loss: 0.2714 cluster_loss: 1.1704 sup_con_loss: 0.5596 contrastive_loss: 4.5909 
2024-05-15 12:02:41.665 | INFO     | __main__:train:123 - Epoch: [129][20/46]	 loss 4.12431	 cls_loss: 0.2979 cluster_loss: 1.2216 sup_con_loss: 0.6766 contrastive_loss: 4.5988 
2024-05-15 12:02:57.441 | INFO     | __main__:train:123 - Epoch: [129][40/46]	 loss 4.07818	 cls_loss: 0.2986 cluster_loss: 1.1619 sup_con_loss: 0.6626 contrastive_loss: 4.5947 
2024-05-15 12:03:01.384 | INFO     | __main__:train:126 - Train Epoch: 129 Avg Loss: 4.0881 
2024-05-15 12:03:01.385 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:03:23.919 | INFO     | __main__:train:135 - Train Accuracies: All 0.7251 | Old 0.8408 | New 0.6119
2024-05-15 12:03:30.020 | INFO     | __main__:train:123 - Epoch: [130][0/46]	 loss 4.08164	 cls_loss: 0.2757 cluster_loss: 1.2350 sup_con_loss: 0.5579 contrastive_loss: 4.5956 
2024-05-15 12:03:46.718 | INFO     | __main__:train:123 - Epoch: [130][20/46]	 loss 3.97392	 cls_loss: 0.2792 cluster_loss: 1.1346 sup_con_loss: 0.4444 contrastive_loss: 4.5895 
2024-05-15 12:04:02.368 | INFO     | __main__:train:123 - Epoch: [130][40/46]	 loss 4.09199	 cls_loss: 0.2959 cluster_loss: 1.2153 sup_con_loss: 0.6083 contrastive_loss: 4.5931 
2024-05-15 12:04:06.310 | INFO     | __main__:train:126 - Train Epoch: 130 Avg Loss: 4.1005 
2024-05-15 12:04:06.311 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:04:28.777 | INFO     | __main__:train:135 - Train Accuracies: All 0.7235 | Old 0.8404 | New 0.6092
2024-05-15 12:04:34.577 | INFO     | __main__:train:123 - Epoch: [131][0/46]	 loss 4.06300	 cls_loss: 0.2858 cluster_loss: 1.2185 sup_con_loss: 0.5218 contrastive_loss: 4.5974 
2024-05-15 12:04:51.573 | INFO     | __main__:train:123 - Epoch: [131][20/46]	 loss 4.08892	 cls_loss: 0.2735 cluster_loss: 1.1807 sup_con_loss: 0.6876 contrastive_loss: 4.5924 
2024-05-15 12:05:07.288 | INFO     | __main__:train:123 - Epoch: [131][40/46]	 loss 4.18258	 cls_loss: 0.2902 cluster_loss: 1.2745 sup_con_loss: 0.7660 contrastive_loss: 4.5915 
2024-05-15 12:05:11.161 | INFO     | __main__:train:126 - Train Epoch: 131 Avg Loss: 4.0908 
2024-05-15 12:05:11.161 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:05:32.872 | INFO     | __main__:train:135 - Train Accuracies: All 0.7280 | Old 0.8436 | New 0.6150
2024-05-15 12:05:38.171 | INFO     | __main__:train:123 - Epoch: [132][0/46]	 loss 4.15867	 cls_loss: 0.2968 cluster_loss: 1.2551 sup_con_loss: 0.7111 contrastive_loss: 4.6002 
2024-05-15 12:05:55.699 | INFO     | __main__:train:123 - Epoch: [132][20/46]	 loss 4.08761	 cls_loss: 0.2561 cluster_loss: 1.1997 sup_con_loss: 0.6739 contrastive_loss: 4.5882 
2024-05-15 12:06:11.235 | INFO     | __main__:train:123 - Epoch: [132][40/46]	 loss 4.00609	 cls_loss: 0.2528 cluster_loss: 1.1852 sup_con_loss: 0.4489 contrastive_loss: 4.6002 
2024-05-15 12:06:15.111 | INFO     | __main__:train:126 - Train Epoch: 132 Avg Loss: 4.0887 
2024-05-15 12:06:15.112 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:06:36.766 | INFO     | __main__:train:135 - Train Accuracies: All 0.7259 | Old 0.8362 | New 0.6181
2024-05-15 12:06:42.907 | INFO     | __main__:train:123 - Epoch: [133][0/46]	 loss 4.03465	 cls_loss: 0.2757 cluster_loss: 1.1932 sup_con_loss: 0.5097 contrastive_loss: 4.5911 
2024-05-15 12:06:59.814 | INFO     | __main__:train:123 - Epoch: [133][20/46]	 loss 3.99746	 cls_loss: 0.2684 cluster_loss: 1.1372 sup_con_loss: 0.5115 contrastive_loss: 4.5928 
2024-05-15 12:07:15.373 | INFO     | __main__:train:123 - Epoch: [133][40/46]	 loss 4.21063	 cls_loss: 0.3225 cluster_loss: 1.2797 sup_con_loss: 0.8092 contrastive_loss: 4.5889 
2024-05-15 12:07:19.417 | INFO     | __main__:train:126 - Train Epoch: 133 Avg Loss: 4.0848 
2024-05-15 12:07:19.417 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:07:41.076 | INFO     | __main__:train:135 - Train Accuracies: All 0.7276 | Old 0.8425 | New 0.6154
2024-05-15 12:07:46.711 | INFO     | __main__:train:123 - Epoch: [134][0/46]	 loss 4.22504	 cls_loss: 0.2692 cluster_loss: 1.2988 sup_con_loss: 0.8739 contrastive_loss: 4.5858 
2024-05-15 12:08:04.028 | INFO     | __main__:train:123 - Epoch: [134][20/46]	 loss 4.18024	 cls_loss: 0.2678 cluster_loss: 1.3336 sup_con_loss: 0.6748 contrastive_loss: 4.5900 
2024-05-15 12:08:19.836 | INFO     | __main__:train:123 - Epoch: [134][40/46]	 loss 4.13942	 cls_loss: 0.2739 cluster_loss: 1.2429 sup_con_loss: 0.7074 contrastive_loss: 4.5970 
2024-05-15 12:08:23.872 | INFO     | __main__:train:126 - Train Epoch: 134 Avg Loss: 4.0949 
2024-05-15 12:08:23.872 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:08:46.045 | INFO     | __main__:train:135 - Train Accuracies: All 0.7287 | Old 0.8460 | New 0.6140
2024-05-15 12:08:52.331 | INFO     | __main__:train:123 - Epoch: [135][0/46]	 loss 4.10905	 cls_loss: 0.2805 cluster_loss: 1.2339 sup_con_loss: 0.6224 contrastive_loss: 4.6016 
2024-05-15 12:09:09.676 | INFO     | __main__:train:123 - Epoch: [135][20/46]	 loss 4.13772	 cls_loss: 0.2951 cluster_loss: 1.2350 sup_con_loss: 0.7027 contrastive_loss: 4.5935 
2024-05-15 12:09:25.403 | INFO     | __main__:train:123 - Epoch: [135][40/46]	 loss 4.07183	 cls_loss: 0.2949 cluster_loss: 1.2221 sup_con_loss: 0.5295 contrastive_loss: 4.5984 
2024-05-15 12:09:29.283 | INFO     | __main__:train:126 - Train Epoch: 135 Avg Loss: 4.0854 
2024-05-15 12:09:29.284 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:09:51.395 | INFO     | __main__:train:135 - Train Accuracies: All 0.7275 | Old 0.8373 | New 0.6201
2024-05-15 12:09:57.337 | INFO     | __main__:train:123 - Epoch: [136][0/46]	 loss 4.06465	 cls_loss: 0.2920 cluster_loss: 1.2097 sup_con_loss: 0.5525 contrastive_loss: 4.5889 
2024-05-15 12:10:14.328 | INFO     | __main__:train:123 - Epoch: [136][20/46]	 loss 4.08806	 cls_loss: 0.2834 cluster_loss: 1.2310 sup_con_loss: 0.5779 contrastive_loss: 4.5945 
2024-05-15 12:10:30.227 | INFO     | __main__:train:123 - Epoch: [136][40/46]	 loss 4.09551	 cls_loss: 0.2875 cluster_loss: 1.2400 sup_con_loss: 0.5844 contrastive_loss: 4.5913 
2024-05-15 12:10:34.183 | INFO     | __main__:train:126 - Train Epoch: 136 Avg Loss: 4.0847 
2024-05-15 12:10:34.184 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:10:55.976 | INFO     | __main__:train:135 - Train Accuracies: All 0.7258 | Old 0.8411 | New 0.6130
2024-05-15 12:11:01.213 | INFO     | __main__:train:123 - Epoch: [137][0/46]	 loss 4.14698	 cls_loss: 0.2912 cluster_loss: 1.2682 sup_con_loss: 0.6702 contrastive_loss: 4.5942 
2024-05-15 12:11:18.653 | INFO     | __main__:train:123 - Epoch: [137][20/46]	 loss 4.12987	 cls_loss: 0.2764 cluster_loss: 1.2370 sup_con_loss: 0.6943 contrastive_loss: 4.5940 
2024-05-15 12:11:34.409 | INFO     | __main__:train:123 - Epoch: [137][40/46]	 loss 4.05031	 cls_loss: 0.2766 cluster_loss: 1.1906 sup_con_loss: 0.5563 contrastive_loss: 4.5922 
2024-05-15 12:11:38.407 | INFO     | __main__:train:126 - Train Epoch: 137 Avg Loss: 4.0983 
2024-05-15 12:11:38.408 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:12:00.230 | INFO     | __main__:train:135 - Train Accuracies: All 0.7245 | Old 0.8397 | New 0.6119
2024-05-15 12:12:06.656 | INFO     | __main__:train:123 - Epoch: [138][0/46]	 loss 4.09670	 cls_loss: 0.2879 cluster_loss: 1.2071 sup_con_loss: 0.6537 contrastive_loss: 4.5885 
2024-05-15 12:12:23.931 | INFO     | __main__:train:123 - Epoch: [138][20/46]	 loss 4.02155	 cls_loss: 0.2546 cluster_loss: 1.1976 sup_con_loss: 0.4633 contrastive_loss: 4.6029 
2024-05-15 12:12:39.391 | INFO     | __main__:train:123 - Epoch: [138][40/46]	 loss 4.10340	 cls_loss: 0.2744 cluster_loss: 1.2465 sup_con_loss: 0.5955 contrastive_loss: 4.5980 
2024-05-15 12:12:43.249 | INFO     | __main__:train:126 - Train Epoch: 138 Avg Loss: 4.0927 
2024-05-15 12:12:43.249 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:13:05.602 | INFO     | __main__:train:135 - Train Accuracies: All 0.7245 | Old 0.8408 | New 0.6109
2024-05-15 12:13:12.207 | INFO     | __main__:train:123 - Epoch: [139][0/46]	 loss 4.10995	 cls_loss: 0.2690 cluster_loss: 1.2474 sup_con_loss: 0.6370 contrastive_loss: 4.5877 
2024-05-15 12:13:29.078 | INFO     | __main__:train:123 - Epoch: [139][20/46]	 loss 3.96910	 cls_loss: 0.2792 cluster_loss: 1.0777 sup_con_loss: 0.5467 contrastive_loss: 4.5839 
2024-05-15 12:13:44.748 | INFO     | __main__:train:123 - Epoch: [139][40/46]	 loss 4.01694	 cls_loss: 0.2740 cluster_loss: 1.1529 sup_con_loss: 0.5249 contrastive_loss: 4.5969 
2024-05-15 12:13:48.716 | INFO     | __main__:train:126 - Train Epoch: 139 Avg Loss: 4.0908 
2024-05-15 12:13:48.717 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:14:10.749 | INFO     | __main__:train:135 - Train Accuracies: All 0.7233 | Old 0.8397 | New 0.6096
2024-05-15 12:14:16.028 | INFO     | __main__:train:123 - Epoch: [140][0/46]	 loss 4.00427	 cls_loss: 0.2663 cluster_loss: 1.1443 sup_con_loss: 0.5254 contrastive_loss: 4.5898 
2024-05-15 12:14:33.632 | INFO     | __main__:train:123 - Epoch: [140][20/46]	 loss 4.07390	 cls_loss: 0.2700 cluster_loss: 1.1893 sup_con_loss: 0.6371 contrastive_loss: 4.5898 
2024-05-15 12:14:49.313 | INFO     | __main__:train:123 - Epoch: [140][40/46]	 loss 4.13616	 cls_loss: 0.3370 cluster_loss: 1.2306 sup_con_loss: 0.6595 contrastive_loss: 4.5961 
2024-05-15 12:14:53.221 | INFO     | __main__:train:126 - Train Epoch: 140 Avg Loss: 4.0859 
2024-05-15 12:14:53.221 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:15:14.898 | INFO     | __main__:train:135 - Train Accuracies: All 0.7244 | Old 0.8415 | New 0.6099
2024-05-15 12:15:21.032 | INFO     | __main__:train:123 - Epoch: [141][0/46]	 loss 4.07458	 cls_loss: 0.2754 cluster_loss: 1.2596 sup_con_loss: 0.4924 contrastive_loss: 4.5955 
2024-05-15 12:15:38.014 | INFO     | __main__:train:123 - Epoch: [141][20/46]	 loss 4.04890	 cls_loss: 0.2958 cluster_loss: 1.1665 sup_con_loss: 0.5787 contrastive_loss: 4.5918 
2024-05-15 12:15:53.821 | INFO     | __main__:train:123 - Epoch: [141][40/46]	 loss 4.12721	 cls_loss: 0.2674 cluster_loss: 1.2253 sup_con_loss: 0.7178 contrastive_loss: 4.5938 
2024-05-15 12:15:57.786 | INFO     | __main__:train:126 - Train Epoch: 141 Avg Loss: 4.0953 
2024-05-15 12:15:57.786 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:16:19.968 | INFO     | __main__:train:135 - Train Accuracies: All 0.7244 | Old 0.8404 | New 0.6109
2024-05-15 12:16:25.961 | INFO     | __main__:train:123 - Epoch: [142][0/46]	 loss 4.23349	 cls_loss: 0.2913 cluster_loss: 1.2730 sup_con_loss: 0.9109 contrastive_loss: 4.5927 
2024-05-15 12:16:42.988 | INFO     | __main__:train:123 - Epoch: [142][20/46]	 loss 4.18648	 cls_loss: 0.2669 cluster_loss: 1.2758 sup_con_loss: 0.7837 contrastive_loss: 4.5993 
2024-05-15 12:16:58.763 | INFO     | __main__:train:123 - Epoch: [142][40/46]	 loss 4.09002	 cls_loss: 0.2789 cluster_loss: 1.2215 sup_con_loss: 0.6076 contrastive_loss: 4.5935 
2024-05-15 12:17:02.690 | INFO     | __main__:train:126 - Train Epoch: 142 Avg Loss: 4.1166 
2024-05-15 12:17:02.690 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:17:25.253 | INFO     | __main__:train:135 - Train Accuracies: All 0.7251 | Old 0.8429 | New 0.6099
2024-05-15 12:17:31.580 | INFO     | __main__:train:123 - Epoch: [143][0/46]	 loss 4.12657	 cls_loss: 0.3072 cluster_loss: 1.2701 sup_con_loss: 0.5926 contrastive_loss: 4.5940 
2024-05-15 12:17:48.536 | INFO     | __main__:train:123 - Epoch: [143][20/46]	 loss 4.07634	 cls_loss: 0.2955 cluster_loss: 1.2217 sup_con_loss: 0.5537 contrastive_loss: 4.5923 
2024-05-15 12:18:04.170 | INFO     | __main__:train:123 - Epoch: [143][40/46]	 loss 4.18599	 cls_loss: 0.2905 cluster_loss: 1.3065 sup_con_loss: 0.7081 contrastive_loss: 4.5957 
2024-05-15 12:18:08.066 | INFO     | __main__:train:126 - Train Epoch: 143 Avg Loss: 4.0756 
2024-05-15 12:18:08.067 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:18:29.892 | INFO     | __main__:train:135 - Train Accuracies: All 0.7247 | Old 0.8411 | New 0.6109
2024-05-15 12:18:35.792 | INFO     | __main__:train:123 - Epoch: [144][0/46]	 loss 4.04578	 cls_loss: 0.3223 cluster_loss: 1.2450 sup_con_loss: 0.3855 contrastive_loss: 4.5981 
2024-05-15 12:18:52.888 | INFO     | __main__:train:123 - Epoch: [144][20/46]	 loss 4.05384	 cls_loss: 0.2718 cluster_loss: 1.1905 sup_con_loss: 0.5762 contrastive_loss: 4.5895 
2024-05-15 12:19:08.523 | INFO     | __main__:train:123 - Epoch: [144][40/46]	 loss 4.13560	 cls_loss: 0.2829 cluster_loss: 1.2268 sup_con_loss: 0.7358 contrastive_loss: 4.5872 
2024-05-15 12:19:12.559 | INFO     | __main__:train:126 - Train Epoch: 144 Avg Loss: 4.1000 
2024-05-15 12:19:12.559 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:19:34.727 | INFO     | __main__:train:135 - Train Accuracies: All 0.7261 | Old 0.8411 | New 0.6137
2024-05-15 12:19:40.443 | INFO     | __main__:train:123 - Epoch: [145][0/46]	 loss 4.19518	 cls_loss: 0.2824 cluster_loss: 1.2878 sup_con_loss: 0.7749 contrastive_loss: 4.5970 
2024-05-15 12:19:57.816 | INFO     | __main__:train:123 - Epoch: [145][20/46]	 loss 4.17399	 cls_loss: 0.2815 cluster_loss: 1.3080 sup_con_loss: 0.6636 contrastive_loss: 4.6047 
2024-05-15 12:20:13.423 | INFO     | __main__:train:123 - Epoch: [145][40/46]	 loss 4.12560	 cls_loss: 0.2767 cluster_loss: 1.2573 sup_con_loss: 0.6493 contrastive_loss: 4.5911 
2024-05-15 12:20:17.429 | INFO     | __main__:train:126 - Train Epoch: 145 Avg Loss: 4.1055 
2024-05-15 12:20:17.430 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:20:39.121 | INFO     | __main__:train:135 - Train Accuracies: All 0.7258 | Old 0.8422 | New 0.6119
2024-05-15 12:20:44.851 | INFO     | __main__:train:123 - Epoch: [146][0/46]	 loss 4.19358	 cls_loss: 0.2853 cluster_loss: 1.3123 sup_con_loss: 0.7254 contrastive_loss: 4.5951 
2024-05-15 12:21:02.104 | INFO     | __main__:train:123 - Epoch: [146][20/46]	 loss 4.12818	 cls_loss: 0.2873 cluster_loss: 1.2718 sup_con_loss: 0.6046 contrastive_loss: 4.5990 
2024-05-15 12:21:17.748 | INFO     | __main__:train:123 - Epoch: [146][40/46]	 loss 3.98834	 cls_loss: 0.3012 cluster_loss: 1.1463 sup_con_loss: 0.4405 contrastive_loss: 4.5902 
2024-05-15 12:21:21.793 | INFO     | __main__:train:126 - Train Epoch: 146 Avg Loss: 4.1001 
2024-05-15 12:21:21.793 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:21:43.675 | INFO     | __main__:train:135 - Train Accuracies: All 0.7252 | Old 0.8439 | New 0.6092
2024-05-15 12:21:49.389 | INFO     | __main__:train:123 - Epoch: [147][0/46]	 loss 4.15873	 cls_loss: 0.2705 cluster_loss: 1.2601 sup_con_loss: 0.7479 contrastive_loss: 4.5897 
2024-05-15 12:22:06.801 | INFO     | __main__:train:123 - Epoch: [147][20/46]	 loss 3.97177	 cls_loss: 0.3037 cluster_loss: 1.1222 sup_con_loss: 0.4400 contrastive_loss: 4.5878 
2024-05-15 12:22:22.216 | INFO     | __main__:train:123 - Epoch: [147][40/46]	 loss 4.06008	 cls_loss: 0.3050 cluster_loss: 1.1617 sup_con_loss: 0.6024 contrastive_loss: 4.5960 
2024-05-15 12:22:26.187 | INFO     | __main__:train:126 - Train Epoch: 147 Avg Loss: 4.0909 
2024-05-15 12:22:26.188 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:22:48.084 | INFO     | __main__:train:135 - Train Accuracies: All 0.7271 | Old 0.8446 | New 0.6123
2024-05-15 12:22:53.581 | INFO     | __main__:train:123 - Epoch: [148][0/46]	 loss 4.03630	 cls_loss: 0.2955 cluster_loss: 1.1321 sup_con_loss: 0.6171 contrastive_loss: 4.5862 
2024-05-15 12:23:10.504 | INFO     | __main__:train:123 - Epoch: [148][20/46]	 loss 4.06854	 cls_loss: 0.2898 cluster_loss: 1.1614 sup_con_loss: 0.6596 contrastive_loss: 4.5866 
2024-05-15 12:23:26.129 | INFO     | __main__:train:123 - Epoch: [148][40/46]	 loss 4.08315	 cls_loss: 0.2966 cluster_loss: 1.2025 sup_con_loss: 0.5984 contrastive_loss: 4.5974 
2024-05-15 12:23:30.251 | INFO     | __main__:train:126 - Train Epoch: 148 Avg Loss: 4.0992 
2024-05-15 12:23:30.251 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:23:52.677 | INFO     | __main__:train:135 - Train Accuracies: All 0.7266 | Old 0.8439 | New 0.6119
2024-05-15 12:23:58.054 | INFO     | __main__:train:123 - Epoch: [149][0/46]	 loss 4.11134	 cls_loss: 0.2800 cluster_loss: 1.2202 sup_con_loss: 0.6603 contrastive_loss: 4.5986 
2024-05-15 12:24:15.601 | INFO     | __main__:train:123 - Epoch: [149][20/46]	 loss 4.02197	 cls_loss: 0.3112 cluster_loss: 1.1649 sup_con_loss: 0.5021 contrastive_loss: 4.5849 
2024-05-15 12:24:31.197 | INFO     | __main__:train:123 - Epoch: [149][40/46]	 loss 4.12321	 cls_loss: 0.2701 cluster_loss: 1.2437 sup_con_loss: 0.6700 contrastive_loss: 4.5935 
2024-05-15 12:24:35.228 | INFO     | __main__:train:126 - Train Epoch: 149 Avg Loss: 4.0866 
2024-05-15 12:24:35.229 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:24:57.620 | INFO     | __main__:train:135 - Train Accuracies: All 0.7278 | Old 0.8432 | New 0.6150
2024-05-15 12:25:03.098 | INFO     | __main__:train:123 - Epoch: [150][0/46]	 loss 4.02076	 cls_loss: 0.2648 cluster_loss: 1.1510 sup_con_loss: 0.5572 contrastive_loss: 4.5922 
2024-05-15 12:25:20.416 | INFO     | __main__:train:123 - Epoch: [150][20/46]	 loss 4.07933	 cls_loss: 0.2994 cluster_loss: 1.2070 sup_con_loss: 0.5784 contrastive_loss: 4.5962 
2024-05-15 12:25:36.193 | INFO     | __main__:train:123 - Epoch: [150][40/46]	 loss 4.15580	 cls_loss: 0.2674 cluster_loss: 1.2797 sup_con_loss: 0.7076 contrastive_loss: 4.5888 
2024-05-15 12:25:40.201 | INFO     | __main__:train:126 - Train Epoch: 150 Avg Loss: 4.0865 
2024-05-15 12:25:40.202 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:26:02.282 | INFO     | __main__:train:135 - Train Accuracies: All 0.7259 | Old 0.8436 | New 0.6109
2024-05-15 12:26:07.895 | INFO     | __main__:train:123 - Epoch: [151][0/46]	 loss 4.19839	 cls_loss: 0.2824 cluster_loss: 1.2733 sup_con_loss: 0.8261 contrastive_loss: 4.5889 
2024-05-15 12:26:25.559 | INFO     | __main__:train:123 - Epoch: [151][20/46]	 loss 4.09415	 cls_loss: 0.2582 cluster_loss: 1.2431 sup_con_loss: 0.5975 contrastive_loss: 4.5948 
2024-05-15 12:26:41.124 | INFO     | __main__:train:123 - Epoch: [151][40/46]	 loss 4.10407	 cls_loss: 0.2771 cluster_loss: 1.1940 sup_con_loss: 0.7043 contrastive_loss: 4.5915 
2024-05-15 12:26:45.133 | INFO     | __main__:train:126 - Train Epoch: 151 Avg Loss: 4.0813 
2024-05-15 12:26:45.134 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:27:07.894 | INFO     | __main__:train:135 - Train Accuracies: All 0.7254 | Old 0.8425 | New 0.6109
2024-05-15 12:27:13.314 | INFO     | __main__:train:123 - Epoch: [152][0/46]	 loss 4.15993	 cls_loss: 0.2849 cluster_loss: 1.2243 sup_con_loss: 0.8098 contrastive_loss: 4.5862 
2024-05-15 12:27:30.745 | INFO     | __main__:train:123 - Epoch: [152][20/46]	 loss 4.14616	 cls_loss: 0.2840 cluster_loss: 1.2394 sup_con_loss: 0.7387 contrastive_loss: 4.5886 
2024-05-15 12:27:46.318 | INFO     | __main__:train:123 - Epoch: [152][40/46]	 loss 4.15833	 cls_loss: 0.2761 cluster_loss: 1.2617 sup_con_loss: 0.7210 contrastive_loss: 4.5988 
2024-05-15 12:27:50.271 | INFO     | __main__:train:126 - Train Epoch: 152 Avg Loss: 4.0799 
2024-05-15 12:27:50.272 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:28:12.573 | INFO     | __main__:train:135 - Train Accuracies: All 0.7273 | Old 0.8453 | New 0.6119
2024-05-15 12:28:18.155 | INFO     | __main__:train:123 - Epoch: [153][0/46]	 loss 4.14234	 cls_loss: 0.2648 cluster_loss: 1.2502 sup_con_loss: 0.7238 contrastive_loss: 4.5903 
2024-05-15 12:28:35.638 | INFO     | __main__:train:123 - Epoch: [153][20/46]	 loss 4.05863	 cls_loss: 0.2825 cluster_loss: 1.1680 sup_con_loss: 0.6101 contrastive_loss: 4.5955 
2024-05-15 12:28:51.314 | INFO     | __main__:train:123 - Epoch: [153][40/46]	 loss 4.08185	 cls_loss: 0.2512 cluster_loss: 1.2151 sup_con_loss: 0.6310 contrastive_loss: 4.5897 
2024-05-15 12:28:55.303 | INFO     | __main__:train:126 - Train Epoch: 153 Avg Loss: 4.0986 
2024-05-15 12:28:55.304 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:29:16.966 | INFO     | __main__:train:135 - Train Accuracies: All 0.7244 | Old 0.8418 | New 0.6096
2024-05-15 12:29:23.200 | INFO     | __main__:train:123 - Epoch: [154][0/46]	 loss 4.06305	 cls_loss: 0.2675 cluster_loss: 1.1995 sup_con_loss: 0.5893 contrastive_loss: 4.5900 
2024-05-15 12:29:40.214 | INFO     | __main__:train:123 - Epoch: [154][20/46]	 loss 4.18451	 cls_loss: 0.2778 cluster_loss: 1.3179 sup_con_loss: 0.6851 contrastive_loss: 4.6013 
2024-05-15 12:29:56.101 | INFO     | __main__:train:123 - Epoch: [154][40/46]	 loss 4.21360	 cls_loss: 0.2818 cluster_loss: 1.2986 sup_con_loss: 0.8090 contrastive_loss: 4.5965 
2024-05-15 12:30:00.139 | INFO     | __main__:train:126 - Train Epoch: 154 Avg Loss: 4.1121 
2024-05-15 12:30:00.140 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:30:22.169 | INFO     | __main__:train:135 - Train Accuracies: All 0.7233 | Old 0.8418 | New 0.6075
2024-05-15 12:30:28.610 | INFO     | __main__:train:123 - Epoch: [155][0/46]	 loss 3.99196	 cls_loss: 0.2823 cluster_loss: 1.1994 sup_con_loss: 0.3607 contrastive_loss: 4.5958 
2024-05-15 12:30:45.524 | INFO     | __main__:train:123 - Epoch: [155][20/46]	 loss 3.95266	 cls_loss: 0.2713 cluster_loss: 1.1368 sup_con_loss: 0.3879 contrastive_loss: 4.5892 
2024-05-15 12:31:00.881 | INFO     | __main__:train:123 - Epoch: [155][40/46]	 loss 4.00338	 cls_loss: 0.2832 cluster_loss: 1.1578 sup_con_loss: 0.4739 contrastive_loss: 4.5936 
2024-05-15 12:31:04.833 | INFO     | __main__:train:126 - Train Epoch: 155 Avg Loss: 4.0768 
2024-05-15 12:31:04.834 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:31:26.911 | INFO     | __main__:train:135 - Train Accuracies: All 0.7244 | Old 0.8404 | New 0.6109
2024-05-15 12:31:32.785 | INFO     | __main__:train:123 - Epoch: [156][0/46]	 loss 4.10314	 cls_loss: 0.2797 cluster_loss: 1.2152 sup_con_loss: 0.6584 contrastive_loss: 4.5921 
2024-05-15 12:31:49.880 | INFO     | __main__:train:123 - Epoch: [156][20/46]	 loss 4.06089	 cls_loss: 0.2707 cluster_loss: 1.1841 sup_con_loss: 0.6086 contrastive_loss: 4.5900 
2024-05-15 12:32:05.674 | INFO     | __main__:train:123 - Epoch: [156][40/46]	 loss 4.15113	 cls_loss: 0.2951 cluster_loss: 1.2433 sup_con_loss: 0.7209 contrastive_loss: 4.5960 
2024-05-15 12:32:09.688 | INFO     | __main__:train:126 - Train Epoch: 156 Avg Loss: 4.0962 
2024-05-15 12:32:09.688 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:32:31.603 | INFO     | __main__:train:135 - Train Accuracies: All 0.7254 | Old 0.8429 | New 0.6106
2024-05-15 12:32:37.757 | INFO     | __main__:train:123 - Epoch: [157][0/46]	 loss 4.03746	 cls_loss: 0.2718 cluster_loss: 1.1767 sup_con_loss: 0.5569 contrastive_loss: 4.5886 
2024-05-15 12:32:54.480 | INFO     | __main__:train:123 - Epoch: [157][20/46]	 loss 4.18342	 cls_loss: 0.2586 cluster_loss: 1.2915 sup_con_loss: 0.7636 contrastive_loss: 4.5941 
2024-05-15 12:33:10.249 | INFO     | __main__:train:123 - Epoch: [157][40/46]	 loss 4.14565	 cls_loss: 0.2802 cluster_loss: 1.2310 sup_con_loss: 0.7626 contrastive_loss: 4.5854 
2024-05-15 12:33:14.213 | INFO     | __main__:train:126 - Train Epoch: 157 Avg Loss: 4.0814 
2024-05-15 12:33:14.213 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:33:36.055 | INFO     | __main__:train:135 - Train Accuracies: All 0.7264 | Old 0.8425 | New 0.6130
2024-05-15 12:33:41.395 | INFO     | __main__:train:123 - Epoch: [158][0/46]	 loss 4.07211	 cls_loss: 0.2897 cluster_loss: 1.2088 sup_con_loss: 0.5673 contrastive_loss: 4.5945 
2024-05-15 12:33:59.509 | INFO     | __main__:train:123 - Epoch: [158][20/46]	 loss 4.04743	 cls_loss: 0.2795 cluster_loss: 1.1822 sup_con_loss: 0.5588 contrastive_loss: 4.5933 
2024-05-15 12:34:15.229 | INFO     | __main__:train:123 - Epoch: [158][40/46]	 loss 4.07994	 cls_loss: 0.2757 cluster_loss: 1.2022 sup_con_loss: 0.6287 contrastive_loss: 4.5876 
2024-05-15 12:34:19.147 | INFO     | __main__:train:126 - Train Epoch: 158 Avg Loss: 4.0698 
2024-05-15 12:34:19.147 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:34:40.776 | INFO     | __main__:train:135 - Train Accuracies: All 0.7266 | Old 0.8425 | New 0.6133
2024-05-15 12:34:46.581 | INFO     | __main__:train:123 - Epoch: [159][0/46]	 loss 4.09116	 cls_loss: 0.2821 cluster_loss: 1.2591 sup_con_loss: 0.5344 contrastive_loss: 4.5953 
2024-05-15 12:35:04.226 | INFO     | __main__:train:123 - Epoch: [159][20/46]	 loss 3.99671	 cls_loss: 0.2889 cluster_loss: 1.1541 sup_con_loss: 0.4659 contrastive_loss: 4.5883 
2024-05-15 12:35:19.915 | INFO     | __main__:train:123 - Epoch: [159][40/46]	 loss 3.95708	 cls_loss: 0.2783 cluster_loss: 1.1148 sup_con_loss: 0.4486 contrastive_loss: 4.5817 
2024-05-15 12:35:23.874 | INFO     | __main__:train:126 - Train Epoch: 159 Avg Loss: 4.0815 
2024-05-15 12:35:23.874 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:35:45.865 | INFO     | __main__:train:135 - Train Accuracies: All 0.7254 | Old 0.8415 | New 0.6119
2024-05-15 12:35:52.761 | INFO     | __main__:train:123 - Epoch: [160][0/46]	 loss 4.08907	 cls_loss: 0.2746 cluster_loss: 1.2220 sup_con_loss: 0.6055 contrastive_loss: 4.5949 
2024-05-15 12:36:09.648 | INFO     | __main__:train:123 - Epoch: [160][20/46]	 loss 4.04835	 cls_loss: 0.2819 cluster_loss: 1.1495 sup_con_loss: 0.6288 contrastive_loss: 4.5884 
2024-05-15 12:36:25.198 | INFO     | __main__:train:123 - Epoch: [160][40/46]	 loss 4.22130	 cls_loss: 0.2518 cluster_loss: 1.3559 sup_con_loss: 0.7487 contrastive_loss: 4.5996 
2024-05-15 12:36:29.229 | INFO     | __main__:train:126 - Train Epoch: 160 Avg Loss: 4.1000 
2024-05-15 12:36:29.230 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:36:50.593 | INFO     | __main__:train:135 - Train Accuracies: All 0.7254 | Old 0.8432 | New 0.6102
2024-05-15 12:36:56.618 | INFO     | __main__:train:123 - Epoch: [161][0/46]	 loss 4.15545	 cls_loss: 0.2871 cluster_loss: 1.2186 sup_con_loss: 0.8009 contrastive_loss: 4.5886 
2024-05-15 12:37:14.092 | INFO     | __main__:train:123 - Epoch: [161][20/46]	 loss 4.14906	 cls_loss: 0.2821 cluster_loss: 1.2759 sup_con_loss: 0.6810 contrastive_loss: 4.5887 
2024-05-15 12:37:29.593 | INFO     | __main__:train:123 - Epoch: [161][40/46]	 loss 3.91421	 cls_loss: 0.2728 cluster_loss: 1.0898 sup_con_loss: 0.3720 contrastive_loss: 4.5849 
2024-05-15 12:37:33.619 | INFO     | __main__:train:126 - Train Epoch: 161 Avg Loss: 4.0949 
2024-05-15 12:37:33.619 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:37:56.198 | INFO     | __main__:train:135 - Train Accuracies: All 0.7266 | Old 0.8422 | New 0.6137
2024-05-15 12:38:02.680 | INFO     | __main__:train:123 - Epoch: [162][0/46]	 loss 4.07342	 cls_loss: 0.2936 cluster_loss: 1.1795 sup_con_loss: 0.6241 contrastive_loss: 4.5932 
2024-05-15 12:38:19.834 | INFO     | __main__:train:123 - Epoch: [162][20/46]	 loss 4.11521	 cls_loss: 0.2737 cluster_loss: 1.2248 sup_con_loss: 0.6861 contrastive_loss: 4.5895 
2024-05-15 12:38:35.423 | INFO     | __main__:train:123 - Epoch: [162][40/46]	 loss 4.14241	 cls_loss: 0.3022 cluster_loss: 1.2243 sup_con_loss: 0.7323 contrastive_loss: 4.5916 
2024-05-15 12:38:39.360 | INFO     | __main__:train:126 - Train Epoch: 162 Avg Loss: 4.0948 
2024-05-15 12:38:39.360 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:39:01.690 | INFO     | __main__:train:135 - Train Accuracies: All 0.7254 | Old 0.8439 | New 0.6096
2024-05-15 12:39:07.979 | INFO     | __main__:train:123 - Epoch: [163][0/46]	 loss 4.09963	 cls_loss: 0.2678 cluster_loss: 1.2264 sup_con_loss: 0.6410 contrastive_loss: 4.5914 
2024-05-15 12:39:25.457 | INFO     | __main__:train:123 - Epoch: [163][20/46]	 loss 4.07927	 cls_loss: 0.2649 cluster_loss: 1.2155 sup_con_loss: 0.6134 contrastive_loss: 4.5874 
2024-05-15 12:39:41.224 | INFO     | __main__:train:123 - Epoch: [163][40/46]	 loss 4.10160	 cls_loss: 0.2734 cluster_loss: 1.2526 sup_con_loss: 0.5896 contrastive_loss: 4.5929 
2024-05-15 12:39:45.218 | INFO     | __main__:train:126 - Train Epoch: 163 Avg Loss: 4.0779 
2024-05-15 12:39:45.218 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:40:06.832 | INFO     | __main__:train:135 - Train Accuracies: All 0.7266 | Old 0.8453 | New 0.6106
2024-05-15 12:40:14.374 | INFO     | __main__:train:123 - Epoch: [164][0/46]	 loss 4.15130	 cls_loss: 0.2725 cluster_loss: 1.2601 sup_con_loss: 0.7217 contrastive_loss: 4.5912 
2024-05-15 12:40:31.253 | INFO     | __main__:train:123 - Epoch: [164][20/46]	 loss 4.22716	 cls_loss: 0.2969 cluster_loss: 1.3290 sup_con_loss: 0.7572 contrastive_loss: 4.6068 
2024-05-15 12:40:46.502 | INFO     | __main__:train:123 - Epoch: [164][40/46]	 loss 4.06992	 cls_loss: 0.2663 cluster_loss: 1.2035 sup_con_loss: 0.5909 contrastive_loss: 4.5963 
2024-05-15 12:40:50.374 | INFO     | __main__:train:126 - Train Epoch: 164 Avg Loss: 4.1021 
2024-05-15 12:40:50.374 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:41:12.981 | INFO     | __main__:train:135 - Train Accuracies: All 0.7247 | Old 0.8411 | New 0.6109
2024-05-15 12:41:19.293 | INFO     | __main__:train:123 - Epoch: [165][0/46]	 loss 4.09598	 cls_loss: 0.2840 cluster_loss: 1.2431 sup_con_loss: 0.5786 contrastive_loss: 4.5940 
2024-05-15 12:41:36.109 | INFO     | __main__:train:123 - Epoch: [165][20/46]	 loss 4.01141	 cls_loss: 0.2579 cluster_loss: 1.2198 sup_con_loss: 0.4094 contrastive_loss: 4.5923 
2024-05-15 12:41:51.600 | INFO     | __main__:train:123 - Epoch: [165][40/46]	 loss 4.11285	 cls_loss: 0.2564 cluster_loss: 1.2197 sup_con_loss: 0.6985 contrastive_loss: 4.5936 
2024-05-15 12:41:55.599 | INFO     | __main__:train:126 - Train Epoch: 165 Avg Loss: 4.0820 
2024-05-15 12:41:55.599 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:42:17.556 | INFO     | __main__:train:135 - Train Accuracies: All 0.7256 | Old 0.8429 | New 0.6109
2024-05-15 12:42:23.879 | INFO     | __main__:train:123 - Epoch: [166][0/46]	 loss 4.09902	 cls_loss: 0.2912 cluster_loss: 1.1768 sup_con_loss: 0.7013 contrastive_loss: 4.5949 
2024-05-15 12:42:41.047 | INFO     | __main__:train:123 - Epoch: [166][20/46]	 loss 3.91214	 cls_loss: 0.2984 cluster_loss: 1.0680 sup_con_loss: 0.3868 contrastive_loss: 4.5817 
2024-05-15 12:42:56.655 | INFO     | __main__:train:123 - Epoch: [166][40/46]	 loss 4.02520	 cls_loss: 0.2808 cluster_loss: 1.2074 sup_con_loss: 0.4457 contrastive_loss: 4.5940 
2024-05-15 12:43:00.726 | INFO     | __main__:train:126 - Train Epoch: 166 Avg Loss: 4.0817 
2024-05-15 12:43:00.726 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:43:22.534 | INFO     | __main__:train:135 - Train Accuracies: All 0.7266 | Old 0.8432 | New 0.6126
2024-05-15 12:43:27.741 | INFO     | __main__:train:123 - Epoch: [167][0/46]	 loss 3.99726	 cls_loss: 0.2812 cluster_loss: 1.1467 sup_con_loss: 0.4858 contrastive_loss: 4.5899 
2024-05-15 12:43:45.858 | INFO     | __main__:train:123 - Epoch: [167][20/46]	 loss 4.11146	 cls_loss: 0.2565 cluster_loss: 1.2620 sup_con_loss: 0.6224 contrastive_loss: 4.5900 
2024-05-15 12:44:01.564 | INFO     | __main__:train:123 - Epoch: [167][40/46]	 loss 3.92728	 cls_loss: 0.2660 cluster_loss: 1.1142 sup_con_loss: 0.3720 contrastive_loss: 4.5843 
2024-05-15 12:44:05.560 | INFO     | __main__:train:126 - Train Epoch: 167 Avg Loss: 4.0768 
2024-05-15 12:44:05.561 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:44:27.825 | INFO     | __main__:train:135 - Train Accuracies: All 0.7258 | Old 0.8432 | New 0.6109
2024-05-15 12:44:34.196 | INFO     | __main__:train:123 - Epoch: [168][0/46]	 loss 4.11739	 cls_loss: 0.2894 cluster_loss: 1.2596 sup_con_loss: 0.5987 contrastive_loss: 4.5967 
2024-05-15 12:44:50.946 | INFO     | __main__:train:123 - Epoch: [168][20/46]	 loss 4.16884	 cls_loss: 0.2637 cluster_loss: 1.2860 sup_con_loss: 0.7321 contrastive_loss: 4.5914 
2024-05-15 12:45:06.684 | INFO     | __main__:train:123 - Epoch: [168][40/46]	 loss 4.11441	 cls_loss: 0.2639 cluster_loss: 1.2316 sup_con_loss: 0.6910 contrastive_loss: 4.5841 
2024-05-15 12:45:10.609 | INFO     | __main__:train:126 - Train Epoch: 168 Avg Loss: 4.0909 
2024-05-15 12:45:10.610 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:45:32.630 | INFO     | __main__:train:135 - Train Accuracies: All 0.7280 | Old 0.8457 | New 0.6130
2024-05-15 12:45:38.315 | INFO     | __main__:train:123 - Epoch: [169][0/46]	 loss 4.19884	 cls_loss: 0.2781 cluster_loss: 1.3136 sup_con_loss: 0.7481 contrastive_loss: 4.5936 
2024-05-15 12:45:55.494 | INFO     | __main__:train:123 - Epoch: [169][20/46]	 loss 4.08082	 cls_loss: 0.2639 cluster_loss: 1.2511 sup_con_loss: 0.5385 contrastive_loss: 4.5951 
2024-05-15 12:46:11.257 | INFO     | __main__:train:123 - Epoch: [169][40/46]	 loss 4.03430	 cls_loss: 0.2664 cluster_loss: 1.1653 sup_con_loss: 0.5713 contrastive_loss: 4.5903 
2024-05-15 12:46:15.345 | INFO     | __main__:train:126 - Train Epoch: 169 Avg Loss: 4.0844 
2024-05-15 12:46:15.346 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:46:36.872 | INFO     | __main__:train:135 - Train Accuracies: All 0.7290 | Old 0.8478 | New 0.6130
2024-05-15 12:46:42.337 | INFO     | __main__:train:123 - Epoch: [170][0/46]	 loss 4.16463	 cls_loss: 0.2702 cluster_loss: 1.2707 sup_con_loss: 0.7431 contrastive_loss: 4.5908 
2024-05-15 12:46:59.762 | INFO     | __main__:train:123 - Epoch: [170][20/46]	 loss 4.08244	 cls_loss: 0.2800 cluster_loss: 1.1658 sup_con_loss: 0.7011 contrastive_loss: 4.5866 
2024-05-15 12:47:15.342 | INFO     | __main__:train:123 - Epoch: [170][40/46]	 loss 4.02551	 cls_loss: 0.2685 cluster_loss: 1.1861 sup_con_loss: 0.5046 contrastive_loss: 4.5907 
2024-05-15 12:47:19.337 | INFO     | __main__:train:126 - Train Epoch: 170 Avg Loss: 4.0768 
2024-05-15 12:47:19.338 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:47:40.919 | INFO     | __main__:train:135 - Train Accuracies: All 0.7264 | Old 0.8432 | New 0.6123
2024-05-15 12:47:47.042 | INFO     | __main__:train:123 - Epoch: [171][0/46]	 loss 4.07078	 cls_loss: 0.2909 cluster_loss: 1.1887 sup_con_loss: 0.6095 contrastive_loss: 4.5893 
2024-05-15 12:48:03.820 | INFO     | __main__:train:123 - Epoch: [171][20/46]	 loss 4.05026	 cls_loss: 0.2803 cluster_loss: 1.1585 sup_con_loss: 0.6135 contrastive_loss: 4.5914 
2024-05-15 12:48:19.567 | INFO     | __main__:train:123 - Epoch: [171][40/46]	 loss 4.17827	 cls_loss: 0.2714 cluster_loss: 1.2822 sup_con_loss: 0.7594 contrastive_loss: 4.5909 
2024-05-15 12:48:23.579 | INFO     | __main__:train:126 - Train Epoch: 171 Avg Loss: 4.1019 
2024-05-15 12:48:23.579 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:48:45.797 | INFO     | __main__:train:135 - Train Accuracies: All 0.7273 | Old 0.8418 | New 0.6154
2024-05-15 12:48:52.379 | INFO     | __main__:train:123 - Epoch: [172][0/46]	 loss 3.99919	 cls_loss: 0.2813 cluster_loss: 1.1655 sup_con_loss: 0.4503 contrastive_loss: 4.5931 
2024-05-15 12:49:09.447 | INFO     | __main__:train:123 - Epoch: [172][20/46]	 loss 4.13192	 cls_loss: 0.2988 cluster_loss: 1.2290 sup_con_loss: 0.6936 contrastive_loss: 4.5935 
2024-05-15 12:49:25.444 | INFO     | __main__:train:123 - Epoch: [172][40/46]	 loss 4.06026	 cls_loss: 0.2860 cluster_loss: 1.1951 sup_con_loss: 0.5600 contrastive_loss: 4.5959 
2024-05-15 12:49:29.312 | INFO     | __main__:train:126 - Train Epoch: 172 Avg Loss: 4.0905 
2024-05-15 12:49:29.312 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:49:50.420 | INFO     | __main__:train:135 - Train Accuracies: All 0.7242 | Old 0.8408 | New 0.6102
2024-05-15 12:49:55.991 | INFO     | __main__:train:123 - Epoch: [173][0/46]	 loss 4.05665	 cls_loss: 0.2817 cluster_loss: 1.1913 sup_con_loss: 0.5770 contrastive_loss: 4.5873 
2024-05-15 12:50:13.063 | INFO     | __main__:train:123 - Epoch: [173][20/46]	 loss 4.04278	 cls_loss: 0.2874 cluster_loss: 1.1476 sup_con_loss: 0.6045 contrastive_loss: 4.5918 
2024-05-15 12:50:28.539 | INFO     | __main__:train:123 - Epoch: [173][40/46]	 loss 4.13861	 cls_loss: 0.2647 cluster_loss: 1.2503 sup_con_loss: 0.7013 contrastive_loss: 4.5966 
2024-05-15 12:50:32.416 | INFO     | __main__:train:126 - Train Epoch: 173 Avg Loss: 4.0831 
2024-05-15 12:50:32.417 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:50:53.896 | INFO     | __main__:train:135 - Train Accuracies: All 0.7276 | Old 0.8446 | New 0.6133
2024-05-15 12:50:59.374 | INFO     | __main__:train:123 - Epoch: [174][0/46]	 loss 4.04185	 cls_loss: 0.2793 cluster_loss: 1.1784 sup_con_loss: 0.5660 contrastive_loss: 4.5847 
2024-05-15 12:51:16.540 | INFO     | __main__:train:123 - Epoch: [174][20/46]	 loss 4.09248	 cls_loss: 0.2684 cluster_loss: 1.2298 sup_con_loss: 0.6171 contrastive_loss: 4.5895 
2024-05-15 12:51:32.227 | INFO     | __main__:train:123 - Epoch: [174][40/46]	 loss 4.02067	 cls_loss: 0.3106 cluster_loss: 1.1178 sup_con_loss: 0.5774 contrastive_loss: 4.5897 
2024-05-15 12:51:36.238 | INFO     | __main__:train:126 - Train Epoch: 174 Avg Loss: 4.0688 
2024-05-15 12:51:36.239 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:51:58.035 | INFO     | __main__:train:135 - Train Accuracies: All 0.7263 | Old 0.8415 | New 0.6137
2024-05-15 12:52:03.180 | INFO     | __main__:train:123 - Epoch: [175][0/46]	 loss 4.09803	 cls_loss: 0.2797 cluster_loss: 1.2565 sup_con_loss: 0.5585 contrastive_loss: 4.5968 
2024-05-15 12:52:20.904 | INFO     | __main__:train:123 - Epoch: [175][20/46]	 loss 4.05990	 cls_loss: 0.2873 cluster_loss: 1.1675 sup_con_loss: 0.6246 contrastive_loss: 4.5875 
2024-05-15 12:52:36.722 | INFO     | __main__:train:123 - Epoch: [175][40/46]	 loss 4.13063	 cls_loss: 0.2705 cluster_loss: 1.2291 sup_con_loss: 0.7243 contrastive_loss: 4.5901 
2024-05-15 12:52:40.594 | INFO     | __main__:train:126 - Train Epoch: 175 Avg Loss: 4.0632 
2024-05-15 12:52:40.595 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:53:02.583 | INFO     | __main__:train:135 - Train Accuracies: All 0.7258 | Old 0.8432 | New 0.6109
2024-05-15 12:53:07.930 | INFO     | __main__:train:123 - Epoch: [176][0/46]	 loss 4.09987	 cls_loss: 0.2836 cluster_loss: 1.2627 sup_con_loss: 0.5449 contrastive_loss: 4.5987 
2024-05-15 12:53:25.552 | INFO     | __main__:train:123 - Epoch: [176][20/46]	 loss 4.10080	 cls_loss: 0.2827 cluster_loss: 1.1968 sup_con_loss: 0.6827 contrastive_loss: 4.5923 
2024-05-15 12:53:41.389 | INFO     | __main__:train:123 - Epoch: [176][40/46]	 loss 4.09447	 cls_loss: 0.2752 cluster_loss: 1.1961 sup_con_loss: 0.6676 contrastive_loss: 4.5954 
2024-05-15 12:53:45.409 | INFO     | __main__:train:126 - Train Epoch: 176 Avg Loss: 4.0638 
2024-05-15 12:53:45.409 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:54:07.835 | INFO     | __main__:train:135 - Train Accuracies: All 0.7270 | Old 0.8436 | New 0.6130
2024-05-15 12:54:14.470 | INFO     | __main__:train:123 - Epoch: [177][0/46]	 loss 3.91061	 cls_loss: 0.2650 cluster_loss: 1.0893 sup_con_loss: 0.3737 contrastive_loss: 4.5831 
2024-05-15 12:54:31.769 | INFO     | __main__:train:123 - Epoch: [177][20/46]	 loss 4.14783	 cls_loss: 0.2561 cluster_loss: 1.2644 sup_con_loss: 0.7176 contrastive_loss: 4.5926 
2024-05-15 12:54:47.518 | INFO     | __main__:train:123 - Epoch: [177][40/46]	 loss 4.12773	 cls_loss: 0.2782 cluster_loss: 1.2424 sup_con_loss: 0.6553 contrastive_loss: 4.6052 
2024-05-15 12:54:51.549 | INFO     | __main__:train:126 - Train Epoch: 177 Avg Loss: 4.0705 
2024-05-15 12:54:51.550 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:55:13.303 | INFO     | __main__:train:135 - Train Accuracies: All 0.7247 | Old 0.8432 | New 0.6089
2024-05-15 12:55:19.426 | INFO     | __main__:train:123 - Epoch: [178][0/46]	 loss 4.01929	 cls_loss: 0.2917 cluster_loss: 1.1654 sup_con_loss: 0.4980 contrastive_loss: 4.5929 
2024-05-15 12:55:36.695 | INFO     | __main__:train:123 - Epoch: [178][20/46]	 loss 4.06947	 cls_loss: 0.2772 cluster_loss: 1.1615 sup_con_loss: 0.6686 contrastive_loss: 4.5899 
2024-05-15 12:55:52.327 | INFO     | __main__:train:123 - Epoch: [178][40/46]	 loss 4.11374	 cls_loss: 0.2764 cluster_loss: 1.2485 sup_con_loss: 0.6197 contrastive_loss: 4.5979 
2024-05-15 12:55:56.158 | INFO     | __main__:train:126 - Train Epoch: 178 Avg Loss: 4.0831 
2024-05-15 12:55:56.158 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:56:17.965 | INFO     | __main__:train:135 - Train Accuracies: All 0.7249 | Old 0.8390 | New 0.6133
2024-05-15 12:56:23.330 | INFO     | __main__:train:123 - Epoch: [179][0/46]	 loss 4.09171	 cls_loss: 0.2778 cluster_loss: 1.1938 sup_con_loss: 0.6709 contrastive_loss: 4.5902 
2024-05-15 12:56:40.589 | INFO     | __main__:train:123 - Epoch: [179][20/46]	 loss 4.05123	 cls_loss: 0.2764 cluster_loss: 1.1846 sup_con_loss: 0.5676 contrastive_loss: 4.5936 
2024-05-15 12:56:56.149 | INFO     | __main__:train:123 - Epoch: [179][40/46]	 loss 3.98153	 cls_loss: 0.2759 cluster_loss: 1.1792 sup_con_loss: 0.3877 contrastive_loss: 4.5889 
2024-05-15 12:57:00.098 | INFO     | __main__:train:126 - Train Epoch: 179 Avg Loss: 4.0741 
2024-05-15 12:57:00.099 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:57:22.000 | INFO     | __main__:train:135 - Train Accuracies: All 0.7276 | Old 0.8425 | New 0.6154
2024-05-15 12:57:28.146 | INFO     | __main__:train:123 - Epoch: [180][0/46]	 loss 3.98209	 cls_loss: 0.2686 cluster_loss: 1.1562 sup_con_loss: 0.4481 contrastive_loss: 4.5842 
2024-05-15 12:57:45.312 | INFO     | __main__:train:123 - Epoch: [180][20/46]	 loss 3.98719	 cls_loss: 0.2623 cluster_loss: 1.1540 sup_con_loss: 0.4647 contrastive_loss: 4.5887 
2024-05-15 12:58:00.964 | INFO     | __main__:train:123 - Epoch: [180][40/46]	 loss 4.16329	 cls_loss: 0.2789 cluster_loss: 1.2582 sup_con_loss: 0.7532 contrastive_loss: 4.5911 
2024-05-15 12:58:04.969 | INFO     | __main__:train:126 - Train Epoch: 180 Avg Loss: 4.0737 
2024-05-15 12:58:04.970 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:58:26.554 | INFO     | __main__:train:135 - Train Accuracies: All 0.7245 | Old 0.8408 | New 0.6109
2024-05-15 12:58:32.599 | INFO     | __main__:train:123 - Epoch: [181][0/46]	 loss 4.05748	 cls_loss: 0.2679 cluster_loss: 1.1840 sup_con_loss: 0.6074 contrastive_loss: 4.5870 
2024-05-15 12:58:50.242 | INFO     | __main__:train:123 - Epoch: [181][20/46]	 loss 4.11198	 cls_loss: 0.2892 cluster_loss: 1.2329 sup_con_loss: 0.6543 contrastive_loss: 4.5852 
2024-05-15 12:59:05.882 | INFO     | __main__:train:123 - Epoch: [181][40/46]	 loss 4.07646	 cls_loss: 0.2722 cluster_loss: 1.1657 sup_con_loss: 0.6975 contrastive_loss: 4.5836 
2024-05-15 12:59:09.777 | INFO     | __main__:train:126 - Train Epoch: 181 Avg Loss: 4.0743 
2024-05-15 12:59:09.777 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:59:31.829 | INFO     | __main__:train:135 - Train Accuracies: All 0.7256 | Old 0.8425 | New 0.6113
2024-05-15 12:59:38.024 | INFO     | __main__:train:123 - Epoch: [182][0/46]	 loss 4.18240	 cls_loss: 0.2619 cluster_loss: 1.2732 sup_con_loss: 0.7920 contrastive_loss: 4.5938 
2024-05-15 12:59:55.397 | INFO     | __main__:train:123 - Epoch: [182][20/46]	 loss 4.09961	 cls_loss: 0.2709 cluster_loss: 1.2375 sup_con_loss: 0.6267 contrastive_loss: 4.5862 
2024-05-15 13:00:11.080 | INFO     | __main__:train:123 - Epoch: [182][40/46]	 loss 4.07244	 cls_loss: 0.2786 cluster_loss: 1.1938 sup_con_loss: 0.6187 contrastive_loss: 4.5883 
2024-05-15 13:00:15.053 | INFO     | __main__:train:126 - Train Epoch: 182 Avg Loss: 4.0653 
2024-05-15 13:00:15.053 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:00:37.466 | INFO     | __main__:train:135 - Train Accuracies: All 0.7256 | Old 0.8418 | New 0.6119
2024-05-15 13:00:43.206 | INFO     | __main__:train:123 - Epoch: [183][0/46]	 loss 4.02655	 cls_loss: 0.2707 cluster_loss: 1.1790 sup_con_loss: 0.5180 contrastive_loss: 4.5910 
2024-05-15 13:01:00.517 | INFO     | __main__:train:123 - Epoch: [183][20/46]	 loss 4.10675	 cls_loss: 0.2901 cluster_loss: 1.2548 sup_con_loss: 0.5768 contrastive_loss: 4.5965 
2024-05-15 13:01:16.134 | INFO     | __main__:train:123 - Epoch: [183][40/46]	 loss 4.10897	 cls_loss: 0.2776 cluster_loss: 1.2105 sup_con_loss: 0.6952 contrastive_loss: 4.5872 
2024-05-15 13:01:20.075 | INFO     | __main__:train:126 - Train Epoch: 183 Avg Loss: 4.0837 
2024-05-15 13:01:20.076 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:01:42.110 | INFO     | __main__:train:135 - Train Accuracies: All 0.7254 | Old 0.8439 | New 0.6096
2024-05-15 13:01:47.387 | INFO     | __main__:train:123 - Epoch: [184][0/46]	 loss 4.08928	 cls_loss: 0.2734 cluster_loss: 1.2259 sup_con_loss: 0.6117 contrastive_loss: 4.5888 
2024-05-15 13:02:04.740 | INFO     | __main__:train:123 - Epoch: [184][20/46]	 loss 4.19811	 cls_loss: 0.2677 cluster_loss: 1.2947 sup_con_loss: 0.7887 contrastive_loss: 4.5952 
2024-05-15 13:02:20.371 | INFO     | __main__:train:123 - Epoch: [184][40/46]	 loss 4.04204	 cls_loss: 0.2624 cluster_loss: 1.1628 sup_con_loss: 0.6043 contrastive_loss: 4.5890 
2024-05-15 13:02:24.272 | INFO     | __main__:train:126 - Train Epoch: 184 Avg Loss: 4.0832 
2024-05-15 13:02:24.273 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:02:46.673 | INFO     | __main__:train:135 - Train Accuracies: All 0.7268 | Old 0.8446 | New 0.6116
2024-05-15 13:02:52.864 | INFO     | __main__:train:123 - Epoch: [185][0/46]	 loss 3.95444	 cls_loss: 0.2818 cluster_loss: 1.1259 sup_con_loss: 0.4104 contrastive_loss: 4.5851 
2024-05-15 13:03:09.985 | INFO     | __main__:train:123 - Epoch: [185][20/46]	 loss 4.05128	 cls_loss: 0.2508 cluster_loss: 1.1896 sup_con_loss: 0.5902 contrastive_loss: 4.5902 
2024-05-15 13:03:25.508 | INFO     | __main__:train:123 - Epoch: [185][40/46]	 loss 4.06577	 cls_loss: 0.2645 cluster_loss: 1.1978 sup_con_loss: 0.6040 contrastive_loss: 4.5896 
2024-05-15 13:03:29.537 | INFO     | __main__:train:126 - Train Epoch: 185 Avg Loss: 4.0881 
2024-05-15 13:03:29.538 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:03:51.975 | INFO     | __main__:train:135 - Train Accuracies: All 0.7273 | Old 0.8457 | New 0.6116
2024-05-15 13:03:56.969 | INFO     | __main__:train:123 - Epoch: [186][0/46]	 loss 4.16021	 cls_loss: 0.3002 cluster_loss: 1.2390 sup_con_loss: 0.7425 contrastive_loss: 4.5999 
2024-05-15 13:04:14.406 | INFO     | __main__:train:123 - Epoch: [186][20/46]	 loss 4.01751	 cls_loss: 0.2777 cluster_loss: 1.1394 sup_con_loss: 0.5611 contrastive_loss: 4.5898 
2024-05-15 13:04:30.055 | INFO     | __main__:train:123 - Epoch: [186][40/46]	 loss 4.09894	 cls_loss: 0.2715 cluster_loss: 1.2641 sup_con_loss: 0.5576 contrastive_loss: 4.5956 
2024-05-15 13:04:33.978 | INFO     | __main__:train:126 - Train Epoch: 186 Avg Loss: 4.0882 
2024-05-15 13:04:33.979 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:04:55.596 | INFO     | __main__:train:135 - Train Accuracies: All 0.7259 | Old 0.8443 | New 0.6102
2024-05-15 13:05:00.577 | INFO     | __main__:train:123 - Epoch: [187][0/46]	 loss 4.05761	 cls_loss: 0.2491 cluster_loss: 1.2376 sup_con_loss: 0.5184 contrastive_loss: 4.5916 
2024-05-15 13:05:18.785 | INFO     | __main__:train:123 - Epoch: [187][20/46]	 loss 4.26543	 cls_loss: 0.2650 cluster_loss: 1.3255 sup_con_loss: 0.9227 contrastive_loss: 4.5972 
2024-05-15 13:05:34.303 | INFO     | __main__:train:123 - Epoch: [187][40/46]	 loss 4.14329	 cls_loss: 0.2868 cluster_loss: 1.2696 sup_con_loss: 0.6521 contrastive_loss: 4.5992 
2024-05-15 13:05:38.221 | INFO     | __main__:train:126 - Train Epoch: 187 Avg Loss: 4.0841 
2024-05-15 13:05:38.222 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:06:00.968 | INFO     | __main__:train:135 - Train Accuracies: All 0.7263 | Old 0.8443 | New 0.6109
2024-05-15 13:06:06.585 | INFO     | __main__:train:123 - Epoch: [188][0/46]	 loss 4.13836	 cls_loss: 0.2615 cluster_loss: 1.3600 sup_con_loss: 0.4849 contrastive_loss: 4.6048 
2024-05-15 13:06:23.427 | INFO     | __main__:train:123 - Epoch: [188][20/46]	 loss 4.18705	 cls_loss: 0.2682 cluster_loss: 1.2873 sup_con_loss: 0.7801 contrastive_loss: 4.5898 
2024-05-15 13:06:38.944 | INFO     | __main__:train:123 - Epoch: [188][40/46]	 loss 4.04981	 cls_loss: 0.2483 cluster_loss: 1.2029 sup_con_loss: 0.5577 contrastive_loss: 4.5936 
2024-05-15 13:06:42.864 | INFO     | __main__:train:126 - Train Epoch: 188 Avg Loss: 4.0877 
2024-05-15 13:06:42.865 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:07:04.811 | INFO     | __main__:train:135 - Train Accuracies: All 0.7251 | Old 0.8429 | New 0.6099
2024-05-15 13:07:10.505 | INFO     | __main__:train:123 - Epoch: [189][0/46]	 loss 4.10480	 cls_loss: 0.2472 cluster_loss: 1.2446 sup_con_loss: 0.6416 contrastive_loss: 4.5920 
2024-05-15 13:07:27.651 | INFO     | __main__:train:123 - Epoch: [189][20/46]	 loss 3.97612	 cls_loss: 0.2630 cluster_loss: 1.1371 sup_con_loss: 0.4540 contrastive_loss: 4.5940 
2024-05-15 13:07:43.415 | INFO     | __main__:train:123 - Epoch: [189][40/46]	 loss 4.12405	 cls_loss: 0.2574 cluster_loss: 1.2512 sup_con_loss: 0.6795 contrastive_loss: 4.5890 
2024-05-15 13:07:47.403 | INFO     | __main__:train:126 - Train Epoch: 189 Avg Loss: 4.0690 
2024-05-15 13:07:47.404 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:08:09.149 | INFO     | __main__:train:135 - Train Accuracies: All 0.7256 | Old 0.8432 | New 0.6106
2024-05-15 13:08:16.173 | INFO     | __main__:train:123 - Epoch: [190][0/46]	 loss 4.04255	 cls_loss: 0.2675 cluster_loss: 1.1991 sup_con_loss: 0.5294 contrastive_loss: 4.5911 
2024-05-15 13:08:33.098 | INFO     | __main__:train:123 - Epoch: [190][20/46]	 loss 4.10525	 cls_loss: 0.2821 cluster_loss: 1.1856 sup_con_loss: 0.7237 contrastive_loss: 4.5886 
2024-05-15 13:08:48.563 | INFO     | __main__:train:123 - Epoch: [190][40/46]	 loss 4.21333	 cls_loss: 0.2662 cluster_loss: 1.3264 sup_con_loss: 0.7641 contrastive_loss: 4.6009 
2024-05-15 13:08:52.489 | INFO     | __main__:train:126 - Train Epoch: 190 Avg Loss: 4.0758 
2024-05-15 13:08:52.490 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:09:14.401 | INFO     | __main__:train:135 - Train Accuracies: All 0.7261 | Old 0.8432 | New 0.6116
2024-05-15 13:09:21.081 | INFO     | __main__:train:123 - Epoch: [191][0/46]	 loss 3.99652	 cls_loss: 0.2666 cluster_loss: 1.1416 sup_con_loss: 0.5088 contrastive_loss: 4.5894 
2024-05-15 13:09:37.941 | INFO     | __main__:train:123 - Epoch: [191][20/46]	 loss 4.00404	 cls_loss: 0.2846 cluster_loss: 1.1092 sup_con_loss: 0.5681 contrastive_loss: 4.5917 
2024-05-15 13:09:53.654 | INFO     | __main__:train:123 - Epoch: [191][40/46]	 loss 4.01940	 cls_loss: 0.2975 cluster_loss: 1.1445 sup_con_loss: 0.5282 contrastive_loss: 4.5945 
2024-05-15 13:09:57.588 | INFO     | __main__:train:126 - Train Epoch: 191 Avg Loss: 4.0697 
2024-05-15 13:09:57.589 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:10:20.087 | INFO     | __main__:train:135 - Train Accuracies: All 0.7261 | Old 0.8450 | New 0.6099
2024-05-15 13:10:25.532 | INFO     | __main__:train:123 - Epoch: [192][0/46]	 loss 3.98950	 cls_loss: 0.2769 cluster_loss: 1.1171 sup_con_loss: 0.5408 contrastive_loss: 4.5803 
2024-05-15 13:10:42.690 | INFO     | __main__:train:123 - Epoch: [192][20/46]	 loss 4.06779	 cls_loss: 0.2855 cluster_loss: 1.1727 sup_con_loss: 0.6430 contrastive_loss: 4.5855 
2024-05-15 13:10:58.427 | INFO     | __main__:train:123 - Epoch: [192][40/46]	 loss 4.09423	 cls_loss: 0.2539 cluster_loss: 1.2118 sup_con_loss: 0.6631 contrastive_loss: 4.5932 
2024-05-15 13:11:02.436 | INFO     | __main__:train:126 - Train Epoch: 192 Avg Loss: 4.0696 
2024-05-15 13:11:02.436 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:11:24.616 | INFO     | __main__:train:135 - Train Accuracies: All 0.7270 | Old 0.8450 | New 0.6116
2024-05-15 13:11:30.688 | INFO     | __main__:train:123 - Epoch: [193][0/46]	 loss 3.93958	 cls_loss: 0.2507 cluster_loss: 1.0983 sup_con_loss: 0.4547 contrastive_loss: 4.5827 
2024-05-15 13:11:47.978 | INFO     | __main__:train:123 - Epoch: [193][20/46]	 loss 4.09594	 cls_loss: 0.2570 cluster_loss: 1.2123 sup_con_loss: 0.6499 contrastive_loss: 4.6008 
2024-05-15 13:12:03.367 | INFO     | __main__:train:123 - Epoch: [193][40/46]	 loss 3.98588	 cls_loss: 0.2638 cluster_loss: 1.1479 sup_con_loss: 0.4779 contrastive_loss: 4.5849 
2024-05-15 13:12:07.348 | INFO     | __main__:train:126 - Train Epoch: 193 Avg Loss: 4.0565 
2024-05-15 13:12:07.349 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:12:29.911 | INFO     | __main__:train:135 - Train Accuracies: All 0.7278 | Old 0.8439 | New 0.6143
2024-05-15 13:12:36.117 | INFO     | __main__:train:123 - Epoch: [194][0/46]	 loss 4.15898	 cls_loss: 0.2596 cluster_loss: 1.2770 sup_con_loss: 0.7247 contrastive_loss: 4.5915 
2024-05-15 13:12:53.214 | INFO     | __main__:train:123 - Epoch: [194][20/46]	 loss 4.12700	 cls_loss: 0.2729 cluster_loss: 1.2430 sup_con_loss: 0.6783 contrastive_loss: 4.5940 
2024-05-15 13:13:09.101 | INFO     | __main__:train:123 - Epoch: [194][40/46]	 loss 4.03003	 cls_loss: 0.2706 cluster_loss: 1.1856 sup_con_loss: 0.5219 contrastive_loss: 4.5877 
2024-05-15 13:13:13.296 | INFO     | __main__:train:126 - Train Epoch: 194 Avg Loss: 4.0943 
2024-05-15 13:13:13.297 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:13:35.626 | INFO     | __main__:train:135 - Train Accuracies: All 0.7252 | Old 0.8450 | New 0.6082
2024-05-15 13:13:41.997 | INFO     | __main__:train:123 - Epoch: [195][0/46]	 loss 4.08837	 cls_loss: 0.2465 cluster_loss: 1.2453 sup_con_loss: 0.6000 contrastive_loss: 4.5888 
2024-05-15 13:13:59.171 | INFO     | __main__:train:123 - Epoch: [195][20/46]	 loss 3.98584	 cls_loss: 0.2988 cluster_loss: 1.1333 sup_con_loss: 0.4635 contrastive_loss: 4.5883 
2024-05-15 13:14:14.787 | INFO     | __main__:train:123 - Epoch: [195][40/46]	 loss 3.99156	 cls_loss: 0.2849 cluster_loss: 1.1922 sup_con_loss: 0.3625 contrastive_loss: 4.6001 
2024-05-15 13:14:18.847 | INFO     | __main__:train:126 - Train Epoch: 195 Avg Loss: 4.0851 
2024-05-15 13:14:18.847 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:14:40.775 | INFO     | __main__:train:135 - Train Accuracies: All 0.7240 | Old 0.8422 | New 0.6085
2024-05-15 13:14:47.601 | INFO     | __main__:train:123 - Epoch: [196][0/46]	 loss 3.97296	 cls_loss: 0.2743 cluster_loss: 1.1374 sup_con_loss: 0.4427 contrastive_loss: 4.5888 
2024-05-15 13:15:04.637 | INFO     | __main__:train:123 - Epoch: [196][20/46]	 loss 4.13418	 cls_loss: 0.2772 cluster_loss: 1.2776 sup_con_loss: 0.6376 contrastive_loss: 4.5900 
2024-05-15 13:15:20.336 | INFO     | __main__:train:123 - Epoch: [196][40/46]	 loss 3.94554	 cls_loss: 0.2904 cluster_loss: 1.0922 sup_con_loss: 0.4375 contrastive_loss: 4.5859 
2024-05-15 13:15:24.342 | INFO     | __main__:train:126 - Train Epoch: 196 Avg Loss: 4.0794 
2024-05-15 13:15:24.343 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:15:46.528 | INFO     | __main__:train:135 - Train Accuracies: All 0.7264 | Old 0.8436 | New 0.6119
2024-05-15 13:15:51.517 | INFO     | __main__:train:123 - Epoch: [197][0/46]	 loss 4.04332	 cls_loss: 0.2625 cluster_loss: 1.1496 sup_con_loss: 0.6406 contrastive_loss: 4.5846 
2024-05-15 13:16:09.703 | INFO     | __main__:train:123 - Epoch: [197][20/46]	 loss 4.07878	 cls_loss: 0.2556 cluster_loss: 1.1758 sup_con_loss: 0.6991 contrastive_loss: 4.5851 
2024-05-15 13:16:25.403 | INFO     | __main__:train:123 - Epoch: [197][40/46]	 loss 4.07260	 cls_loss: 0.2555 cluster_loss: 1.2153 sup_con_loss: 0.5929 contrastive_loss: 4.5935 
2024-05-15 13:16:29.448 | INFO     | __main__:train:126 - Train Epoch: 197 Avg Loss: 4.0645 
2024-05-15 13:16:29.449 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:16:51.506 | INFO     | __main__:train:135 - Train Accuracies: All 0.7259 | Old 0.8436 | New 0.6109
2024-05-15 13:16:57.392 | INFO     | __main__:train:123 - Epoch: [198][0/46]	 loss 4.19989	 cls_loss: 0.2683 cluster_loss: 1.2754 sup_con_loss: 0.8427 contrastive_loss: 4.5878 
2024-05-15 13:17:14.417 | INFO     | __main__:train:123 - Epoch: [198][20/46]	 loss 4.12404	 cls_loss: 0.2763 cluster_loss: 1.2163 sup_con_loss: 0.7222 contrastive_loss: 4.5907 
2024-05-15 13:17:30.030 | INFO     | __main__:train:123 - Epoch: [198][40/46]	 loss 4.26028	 cls_loss: 0.2839 cluster_loss: 1.3608 sup_con_loss: 0.8270 contrastive_loss: 4.5953 
2024-05-15 13:17:34.077 | INFO     | __main__:train:126 - Train Epoch: 198 Avg Loss: 4.1022 
2024-05-15 13:17:34.078 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:17:55.667 | INFO     | __main__:train:135 - Train Accuracies: All 0.7249 | Old 0.8446 | New 0.6078
2024-05-15 13:18:01.494 | INFO     | __main__:train:123 - Epoch: [199][0/46]	 loss 4.05020	 cls_loss: 0.2793 cluster_loss: 1.1837 sup_con_loss: 0.5700 contrastive_loss: 4.5901 
2024-05-15 13:18:18.621 | INFO     | __main__:train:123 - Epoch: [199][20/46]	 loss 4.07755	 cls_loss: 0.2712 cluster_loss: 1.1887 sup_con_loss: 0.6491 contrastive_loss: 4.5889 
2024-05-15 13:18:33.895 | INFO     | __main__:train:123 - Epoch: [199][40/46]	 loss 4.08839	 cls_loss: 0.2703 cluster_loss: 1.2065 sup_con_loss: 0.6436 contrastive_loss: 4.5913 
2024-05-15 13:18:37.885 | INFO     | __main__:train:126 - Train Epoch: 199 Avg Loss: 4.0935 
2024-05-15 13:18:37.885 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:18:59.406 | INFO     | __main__:train:135 - Train Accuracies: All 0.7261 | Old 0.8439 | New 0.6109
