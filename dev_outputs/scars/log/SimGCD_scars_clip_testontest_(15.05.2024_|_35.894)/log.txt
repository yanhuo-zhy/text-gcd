2024-05-15 09:45:35.911 | INFO     | __main__:<module>:262 - Using evaluation function v2 to print results
2024-05-15 09:45:39.120 | INFO     | __main__:<module>:320 - model build
2024-05-15 09:45:44.646 | INFO     | __main__:train:37 - [Parameter containing:
tensor([[-0.0146, -0.0039, -0.0066,  ..., -0.0070, -0.0130,  0.0021],
        [-0.0165,  0.0127,  0.0296,  ...,  0.0056,  0.0020, -0.0008],
        [ 0.0086, -0.0143, -0.0048,  ..., -0.0151, -0.0058, -0.0208],
        ...,
        [-0.0014, -0.0116, -0.0034,  ..., -0.0181,  0.0034, -0.0155],
        [ 0.0091,  0.0107, -0.0240,  ..., -0.0009,  0.0183,  0.0085],
        [ 0.0078,  0.0050, -0.0247,  ...,  0.0250,  0.0013, -0.0091]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([2.0889, 1.9712, 2.1220, 2.0694, 1.7119, 1.9933, 2.0799, 2.1845, 2.1324,
        1.9399, 1.9302, 2.1218, 2.0308, 2.0338, 2.1017, 2.0951, 2.0867, 2.0461,
        1.9855, 2.1302, 2.0887, 2.0182, 1.9796, 2.0423, 2.0565, 2.1084, 2.0156,
        2.0070, 1.9619, 1.9993, 1.9641, 2.1471, 2.1692, 2.0407, 2.0561, 1.7148,
        1.9077, 2.0416, 2.0109, 2.0489, 2.0492, 1.9716, 1.9944, 2.1945, 2.1414,
        2.0624, 2.0382, 1.7156, 2.0872, 2.1689, 1.9913, 1.9874, 2.0611, 2.0384,
        2.0404, 2.0571, 2.1749, 1.6695, 1.9704, 2.0106, 2.1707, 2.1583, 1.7716,
        2.0785, 2.2589, 1.5433, 2.0498, 2.1556, 2.2475, 1.6193, 2.0502, 1.9964,
        1.9409, 2.1102, 2.1142, 1.8969, 2.0679, 1.9978, 1.9989, 1.9519, 1.8782,
        2.0690, 1.9627, 2.0572, 2.0304, 1.9690, 2.0986, 2.1040, 2.1931, 2.0631,
        2.0917, 2.0014, 2.0774, 1.8498, 1.8968, 2.1439, 1.9327, 2.0869, 2.1567,
        1.9760, 2.1401, 1.9547, 2.2018, 1.9345, 1.9655, 1.9996, 1.9677, 1.8891,
        2.1570, 2.0854, 1.7238, 2.0024, 2.1032, 2.0581, 1.9916, 2.1147, 1.9997,
        1.9987, 2.1948, 2.0187, 2.0479, 2.0436, 2.1438, 2.0421, 2.0650, 2.0503,
        2.0932, 2.1495, 2.1429, 2.2573, 2.2058, 2.0425, 1.9905, 2.0903, 2.0920,
        2.0021, 1.9449, 1.8799, 1.9051, 2.0264, 2.0167, 2.0677, 2.1733, 2.1022,
        2.0488, 1.9244, 2.0255, 1.9785, 2.0561, 2.1360, 2.0916, 1.9735, 2.0471,
        2.0240, 2.0103, 2.0339, 2.0940, 2.1347, 1.8951, 1.9661, 2.0123, 1.6906,
        3.3178, 1.8554, 2.0427, 2.0867, 2.0731, 2.2051, 2.0521, 1.8706, 2.1517,
        2.0741, 1.9758, 2.0706, 2.1696, 2.1816, 1.6692, 1.9433, 2.1614, 1.9843,
        2.0820, 2.0234, 2.0356, 1.9761, 2.0929, 2.0717, 1.9660, 1.7830, 2.1155,
        1.8459, 2.1288, 2.1077, 1.9489, 1.8428, 2.0365, 2.0475, 1.9203, 2.0346,
        1.1667, 1.9564, 2.0408, 2.1060, 1.9239, 2.1521, 2.0335, 1.3680, 1.7952,
        2.1439, 2.1176, 1.9656, 2.1946, 2.0850, 1.9810, 2.1021, 2.0342, 2.0878,
        2.1313, 1.8232, 2.0304, 2.0742, 2.0921, 2.0573, 2.1955, 2.1340, 1.9932,
        2.0807, 2.0696, 2.0617, 2.0044, 2.0714, 2.1352, 2.1044, 1.9892, 1.9525,
        2.2479, 1.9878, 2.0778, 2.0956, 2.1284, 2.0560, 2.0330, 2.0734, 2.1860,
        1.5747, 2.0099, 2.1652, 2.0593, 1.9409, 2.1205, 2.0521, 2.1560, 2.0191,
        1.9861, 2.0404, 2.0371, 2.1400, 2.0281, 2.0016, 2.1504, 1.8761, 1.9561,
        2.0932, 2.0882, 1.9444, 1.9052, 2.0580, 1.9558, 2.0516, 1.9564, 2.0826,
        2.0919, 2.1240, 1.9847, 2.0485, 2.0422, 2.0356, 1.8621, 2.1438, 2.1475,
        1.9393, 2.0578, 2.0111, 2.2414, 2.0166, 2.0661, 2.1818, 2.1259, 2.1101,
        1.9721, 1.9622, 1.9813, 2.1706, 1.9655, 1.9623, 1.9839, 2.0574, 2.1466,
        2.1507, 2.1680, 1.4906, 2.1190, 1.9092, 2.1146, 2.0637, 2.1615, 1.9260,
        1.9775, 1.5948, 2.1446, 1.5686, 1.9936, 2.1131, 1.9389, 2.0891, 2.0374,
        2.1002, 2.0653, 1.9287, 2.0661, 1.9063, 1.8354, 2.0778, 2.1387, 2.0642,
        2.0024, 1.9499, 2.0517, 2.1462, 1.8669, 2.0599, 2.1175, 2.1816, 2.1317,
        2.0351, 2.0384, 2.2207, 2.0393, 1.9515, 2.0600, 2.0513, 2.0241, 1.4406,
        2.0965, 2.0465, 2.1819, 2.0059, 1.9621, 2.0900, 1.9422, 1.9949, 2.1607,
        2.0713, 2.0750, 1.9913, 1.9107, 2.0848, 2.0837, 2.2114, 2.1490, 2.0948,
        2.0114, 2.1234, 1.8489, 1.9168, 1.8525, 2.0741, 1.9893, 2.0585, 1.9955,
        2.1011, 1.9959, 1.7628, 2.1660, 2.0795, 2.1166, 2.0747, 1.9695, 2.0061,
        2.1132, 2.0464, 2.0033, 1.9074, 2.1169, 2.0778, 2.1615, 2.0775, 2.0127,
        2.1388, 2.0702, 1.9780, 1.9862, 2.0076, 1.9101, 2.1197, 2.0821, 2.1660,
        1.6451, 2.1807, 2.1510, 2.0006, 2.1235, 1.9724, 2.0940, 1.9664, 1.9653,
        2.0226, 2.1127, 1.6622, 2.0052, 2.0679, 2.0634, 2.0308, 1.9230, 2.3308,
        2.0401, 2.0522, 2.1555, 1.6648, 2.1525, 1.9948, 2.0326, 2.1897, 1.9837,
        2.1599, 2.0922, 2.1043, 2.1130, 2.1266, 2.0320, 1.7905, 2.1134, 1.9856,
        2.1800, 2.0651, 2.2059, 1.6809, 2.0747, 1.8652, 1.9064, 1.9484, 2.2097,
        2.1325, 1.9957, 2.1914, 2.0445, 2.0817, 2.0053, 1.9492, 1.9617, 1.7545,
        2.0156, 2.0356, 2.1489, 2.0271, 1.9697, 2.0002, 2.1147, 2.1199, 2.0303,
        1.8091, 1.9793, 2.0675, 1.9589, 1.9105, 2.1151, 2.0203, 2.0771, 1.9647,
        2.0655, 2.0748, 2.0341, 1.9724, 2.0934, 1.9961, 0.3405, 1.9650, 2.0412,
        2.1093, 2.0053, 2.0580, 2.1316, 2.1214, 1.8425, 2.1187, 1.7724, 2.0784,
        2.0464, 1.7946, 2.1334, 2.0780, 2.1812, 2.1858, 2.1440, 2.1193, 2.0504,
        1.9470, 1.9928, 1.9448, 2.1769, 2.0388, 2.1214, 2.0781, 1.8957, 1.9711,
        2.0179, 1.9501, 2.0254, 2.0303, 2.0969, 2.0911, 1.3677, 1.9383, 1.8941,
        1.9511, 1.9579, 1.9971, 2.0300, 2.0202, 1.3250, 1.6754, 2.1596, 2.0482,
        2.0071, 2.1047, 1.7998, 1.9864, 1.8247, 2.0342, 1.9502, 2.0478, 2.0405,
        1.8985, 2.0594, 2.0769, 1.8037, 2.0254, 2.1384, 2.1213, 2.1802, 2.1189,
        2.0923, 1.9126, 2.1282, 2.1499, 1.9930, 2.0616, 1.7846, 1.9661, 2.2175,
        2.1521, 2.0787, 2.1360, 1.9558, 1.9896, 2.0940, 1.9441, 2.0977, 1.9642,
        2.1160, 1.8274, 1.9144, 2.1407, 2.0104, 2.0967, 2.0736, 1.9755, 2.1098,
        1.9763, 2.0681, 2.0670, 2.1317, 2.0077, 2.0983, 2.1026, 2.0234, 2.0871,
        2.0295, 1.9842, 1.9524, 1.9992, 2.2744, 2.0831, 2.0668, 2.0192, 1.9225,
        1.9729, 1.9364, 2.1503, 2.0694, 2.0884, 2.0392, 1.9470, 2.0753, 1.9972,
        1.9704, 2.0931, 2.1665, 2.0933, 1.9537, 2.0864, 2.0178, 1.9763, 2.0968,
        1.9622, 1.9361, 2.1092, 1.9623, 2.1026, 1.9378, 2.1092, 2.0311, 1.8948,
        1.9659, 1.9941, 2.0935, 2.2296, 2.2099, 1.9437, 2.0376, 2.0139, 1.9951,
        1.9778, 2.0384, 2.0941, 2.0700, 2.0591, 2.1484, 2.0313, 2.1585, 1.9633,
        2.1104, 2.0213, 2.0351, 1.5722, 2.0605, 2.1667, 2.1725, 2.0506, 2.0942,
        2.1590, 1.8917, 2.0401, 2.0299, 2.0578, 2.0229, 2.0145, 2.1082, 2.1442,
        1.9256, 2.1155, 1.5651, 2.1935, 2.0792, 2.1289, 2.0517, 2.0612, 1.9459,
        2.2552, 1.9714, 2.0947, 2.0609, 2.0716, 1.4566, 1.9967, 2.0037, 1.8967,
        2.1645, 1.9572, 2.0952, 2.1335, 2.1983, 1.9248, 2.0946, 2.0167, 1.9967,
        2.0781, 2.0708, 2.2572, 2.0508, 2.1440, 2.1336, 2.1300, 1.9258, 2.0525,
        2.0640, 2.1309, 1.9936, 2.0384, 2.1007, 1.3213, 2.1270, 2.0090, 2.1278,
        2.0224, 2.0256, 2.1023, 1.8309, 1.9433, 1.9495, 2.0881, 2.1420, 2.1381,
        2.0138, 1.8643, 2.0748, 1.9934, 2.0834, 1.9823, 2.1178, 2.2130, 2.0325,
        1.9908, 2.1174, 2.2464, 2.0913, 2.0609, 2.0776, 2.1028, 2.0192, 2.0302,
        1.9206, 2.1787, 2.0304, 1.9062, 3.4342, 1.9552, 1.9041, 2.0857, 2.0320,
        1.9365, 1.9956, 1.9609, 1.9605, 1.6706, 2.1107, 1.9736, 2.0025, 2.0189,
        1.9959, 2.0646, 2.1581, 1.8589, 2.0009, 2.2918, 2.0199, 1.9572, 1.8224,
        1.8725, 2.0706, 2.1529, 2.1153, 2.0897, 1.9255, 2.1299, 2.1575, 2.0383,
        2.0991, 2.0648, 2.1076, 2.0169, 1.8750, 2.1080, 2.1821, 1.9574, 2.1477,
        2.0282, 1.6939, 2.0273], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 2.8504e-02, -6.6268e-02,  2.0469e-02, -2.4063e-01, -7.6139e-01,
         2.2901e-01, -1.9181e-02, -1.6229e-01,  1.4640e-01, -7.2078e-02,
        -6.4448e-02,  1.3004e-01, -2.8852e-02, -2.2445e-01, -1.5333e-01,
        -2.0017e-02, -1.4645e-02,  1.3456e-01,  1.1367e-01,  8.5705e-04,
        -1.9154e-01, -1.1060e-01,  7.7911e-02,  1.0782e-03,  1.8011e-01,
        -6.9190e-02,  8.1234e-02,  4.6244e-03,  2.0738e-01,  2.1546e-01,
        -1.2781e-01,  2.1756e-01,  1.9662e-01,  6.6177e-02, -9.6555e-02,
         3.6276e-02,  3.0880e-01, -4.2114e-02, -1.7260e-01, -3.2207e-02,
         8.0750e-02,  1.4804e-01,  1.5997e-01, -1.2746e-02, -2.0021e-01,
         2.4657e-01, -1.5625e-01,  4.1060e-01,  1.1039e-02,  1.7342e-03,
        -2.8080e-01,  7.2084e-02, -9.5679e-03,  2.8248e-01, -1.3109e-01,
        -4.9942e-02, -7.1812e-03,  2.3124e-01,  3.6284e-01,  2.3893e-01,
         4.1646e-02,  1.7018e-01,  1.5995e-01, -1.8938e-02, -1.4604e-01,
         3.8497e-01, -2.0005e-01,  1.1361e-01, -8.6606e-02,  8.1025e-02,
         1.0023e-01,  1.3133e-02,  1.4986e-01, -1.4484e-01, -1.8983e-01,
         1.2057e-01,  6.8089e-02,  1.7732e-02, -1.4275e-02,  4.1753e-01,
        -1.7650e-01, -1.2653e-01, -1.9440e-01,  3.2383e-02,  6.3769e-03,
        -6.3109e-02,  3.4128e-01,  1.1140e-01,  5.1841e-02, -7.8993e-02,
         3.1750e-01, -2.7008e-01,  2.6093e-01, -4.3071e-02, -1.6392e-01,
        -5.4121e-02, -2.5731e-01,  1.8952e-01,  8.5067e-02, -6.1931e-02,
         1.1788e-01,  3.1476e-01,  9.4151e-02, -1.6559e-01,  1.5590e-01,
         1.8307e-01,  3.4897e-01,  3.2071e-01, -9.9986e-02,  1.2352e-01,
        -3.6178e-02,  1.7537e-01, -8.1322e-02, -3.0396e-02,  5.5304e-02,
        -2.4548e-02,  8.7976e-02, -1.4368e-01,  2.1969e-02,  1.4105e-01,
        -2.4492e-02, -2.6660e-01,  1.4491e-01,  1.0095e-01,  1.1811e-02,
         7.9764e-02,  1.2176e-01, -5.9894e-02,  6.5163e-02,  2.0217e-02,
         7.2439e-02,  9.9869e-02,  2.5884e-01, -1.3904e-01, -7.1748e-02,
        -1.6325e-01, -3.4289e-01, -4.1664e-02, -2.6442e-01, -2.1141e-01,
        -3.2832e-01, -1.9760e-01,  1.9742e-01,  6.4940e-02,  1.1064e-01,
         9.8830e-02, -5.5894e-02, -7.5366e-02, -2.5354e-02,  1.8192e-01,
        -1.5770e-01, -9.7434e-02, -2.0390e-02,  1.2097e-01,  1.3304e-01,
        -2.2001e-01, -1.9000e-01,  1.0213e-01,  2.8652e-01,  7.8545e-02,
        -4.1900e-01, -1.5198e-01,  8.8589e-01,  6.4870e-02,  1.9507e-01,
         1.1244e-01,  1.3662e-01, -5.5619e-03, -1.8421e-01,  3.6384e-01,
         1.0898e-01,  1.7704e-02, -1.4382e-01,  3.1085e-02,  4.4398e-02,
         2.6900e-01,  2.5362e-01, -2.0190e-01, -2.3754e-01, -2.3838e-01,
         6.8463e-02, -5.0972e-02, -7.0668e-02, -1.4725e-01, -1.0647e-01,
        -2.4313e-01, -2.0898e-02, -6.2361e-03, -9.2734e-02,  1.2089e-01,
         1.3466e-01, -1.5528e-01,  9.1784e-02,  3.1565e-01, -8.8425e-02,
         1.1853e-01,  2.9177e-01,  4.9970e-02,  2.9435e-01, -2.6363e-02,
        -3.5723e-03,  1.1744e-02, -3.8822e-01, -6.3808e-02, -2.5130e-02,
        -2.9781e-01,  2.4423e-01,  1.2359e-01, -2.0310e-01,  1.1666e-01,
        -1.0383e-01, -2.4320e-01,  1.0476e-01, -6.4406e-02, -1.1334e-01,
        -1.6384e-01,  4.5622e-02,  4.6126e-02, -2.3101e-01, -4.2995e-02,
        -3.2435e-02,  1.8009e-01,  1.0649e-01,  1.6906e-01,  2.8517e-01,
         1.5568e-01,  1.0221e-01,  1.8800e-01,  1.2888e-01,  1.1685e-01,
        -1.8799e-02, -8.4322e-02, -2.2413e-01, -3.1196e-01,  2.1720e-01,
        -2.6198e-01, -2.5196e-02,  9.2941e-02,  6.2847e-02,  2.4234e-01,
        -3.4218e-01, -1.8627e-01,  1.3247e-01, -3.4954e-01, -3.0751e-01,
        -1.5220e-01,  1.3842e-01, -7.7362e-02, -8.6115e-02,  8.0118e-02,
        -1.3368e-01, -1.9051e-01,  1.4877e-01,  1.5263e-01,  6.0897e-02,
        -2.5285e-01,  2.9866e-01,  1.6658e-01,  1.3632e-01, -3.1249e-01,
         5.9045e-02,  1.4344e-01, -8.2676e-02,  3.4193e-01, -1.9562e-01,
        -8.8454e-03,  8.0644e-02, -1.5615e-02,  1.6132e-01,  9.9109e-03,
        -2.1831e-01,  2.0527e-04,  2.0767e-01, -1.9229e-01, -6.2333e-02,
         2.8975e-01,  2.2820e-01,  5.4081e-02, -2.8020e-02, -7.5214e-02,
         1.3673e-01,  2.2390e-01,  8.9420e-02,  2.9266e-01,  8.7919e-03,
        -5.7573e-02,  2.4258e-01,  3.9646e-02,  3.2063e-01,  2.2784e-01,
         9.1253e-02,  1.3482e-01,  5.0732e-02,  8.2332e-02, -2.5443e-01,
        -5.3945e-02,  3.5411e-02,  1.5352e-01, -3.5287e-02,  6.9929e-02,
         5.6327e-02,  2.1955e-01, -1.0062e-01, -2.5025e-02,  1.6035e-01,
        -1.7752e-01,  3.2281e-03,  3.3873e-01, -2.4688e-02, -3.2176e-01,
         1.9693e-01, -7.4028e-02, -4.5915e-02, -7.9669e-02, -1.4538e-01,
         8.4735e-02, -9.0724e-02, -1.1218e-02, -5.2998e-02, -2.8057e-01,
         2.5006e-01, -7.6606e-02,  8.9481e-02,  2.1017e-01, -1.4844e-01,
        -1.1927e-01, -1.7030e-01, -2.6984e-02, -1.2455e-01, -1.7030e-02,
        -2.3333e-03, -3.4794e-02,  1.1147e-01,  6.8249e-02,  1.1888e-01,
        -6.2072e-03, -5.5677e-02,  2.5506e-02,  1.4834e-01,  2.8043e-01,
         7.5211e-02,  3.2190e-01,  1.0136e-01,  1.3366e-01, -1.1125e-01,
        -2.2113e-02, -1.9412e-01,  5.0625e-02,  1.1823e-01, -1.0454e-01,
        -9.8780e-03,  7.1319e-02, -1.9997e-02, -1.6023e-02, -3.0358e-01,
         4.9813e-02, -3.7291e-02,  4.1195e-02,  4.0840e-02,  2.5038e-01,
        -1.3593e-01, -3.9288e-02,  1.4432e-01,  1.5827e-01,  3.7552e-01,
         5.9903e-02,  3.5905e-01,  2.4094e-01,  8.8207e-02, -4.8586e-02,
         8.4939e-03, -1.3572e-02,  1.3245e-01, -2.6827e-01,  2.1792e-01,
         1.3221e-02, -1.5648e-01,  1.1359e-02, -1.0186e-01,  4.2942e-03,
         1.7744e-01,  2.8789e-02,  4.6753e-02, -9.7045e-02,  1.3429e-01,
        -9.7212e-02, -7.1507e-02, -3.4633e-03,  1.3883e-01, -2.0727e-01,
         5.2512e-03,  9.8836e-02, -1.7465e-01, -4.8214e-02,  2.0891e-01,
        -1.7837e-01,  1.4867e-01, -6.8707e-02,  2.1542e-01,  2.3465e-02,
        -1.6120e-01, -7.1607e-03,  6.9563e-02,  2.0662e-01, -1.3214e-02,
        -3.6621e-03,  3.9815e-02,  1.1840e-01, -9.3086e-04,  1.6077e-01,
         2.5887e-01,  3.3082e-02,  9.1206e-02, -8.1345e-02,  1.5747e-01,
         2.2100e-01,  1.4447e-01,  1.3298e-01,  5.6416e-02,  7.1703e-02,
         2.7277e-02, -3.4643e-02,  1.1375e-02, -1.2775e-01,  1.4280e-01,
        -1.1996e-03,  1.4385e-01,  6.2964e-02,  7.8224e-04,  3.1198e-01,
        -8.2213e-02,  3.0901e-01, -1.8916e-01,  6.4224e-02, -6.0739e-02,
         3.6575e-01,  2.7061e-01, -4.9588e-01,  3.7487e-02,  1.9776e-01,
         9.6408e-02, -2.3510e-02, -2.2246e-01,  1.2251e-01,  6.9865e-02,
        -1.0218e-01, -1.2041e-01,  3.6202e-02,  1.3946e-01,  1.2657e-01,
         4.5796e-01,  1.1498e-01,  1.9502e-01,  7.9131e-02, -2.3154e-01,
        -1.8005e-01, -9.3380e-02, -7.7939e-02,  2.3785e-03, -1.6296e-02,
        -2.1455e-01,  1.5535e-01,  6.0736e-02, -2.5630e-01, -5.4613e-02,
        -1.0549e-01,  2.1634e-01,  1.6198e-01,  1.9333e-02, -1.3153e-01,
        -7.9205e-02,  1.5459e-01,  1.2753e-01,  3.5562e-01, -2.0996e+00,
         1.8678e-01,  1.3896e-01,  1.3344e-01,  1.9373e-01, -1.3733e-01,
        -4.8426e-02,  1.0204e-01, -5.3012e-02,  6.2561e-02,  2.0782e-01,
        -2.7372e-02,  1.2175e-01,  9.3349e-02, -1.4915e-02,  1.4731e-01,
        -4.1648e-02,  1.9402e-01, -4.0652e-02,  1.3574e-01,  2.3930e-01,
         2.1554e-01,  1.9693e-01,  3.4045e-01,  5.3095e-03, -1.1799e-01,
         2.0984e-01,  1.4215e-01,  1.6345e-01, -9.9329e-02,  2.8398e-01,
         1.3910e-01, -1.9960e-02, -4.4554e-02, -4.9254e-02,  1.2946e-01,
         1.6621e-02, -1.4321e-01, -3.1374e-01,  1.4049e-01, -1.4755e-01,
         2.1822e-01, -1.5111e-01,  9.3312e-02,  7.1476e-01, -4.9110e-01,
         1.7405e-01,  9.4529e-02, -1.6380e-01,  2.5346e-01, -1.6801e-01,
         1.5994e-01, -1.4535e-01, -1.8851e-02,  4.0526e-01,  9.0625e-02,
         1.1590e-03,  7.2827e-02, -1.7893e-01,  6.2739e-03, -2.0410e-02,
         3.5180e-01, -1.2868e-01, -8.4570e-02,  8.2254e-02,  9.5615e-02,
        -4.3183e-02, -1.3872e-01,  2.5896e-02, -2.4962e-01,  1.2058e-01,
         4.3327e-02,  2.0339e-01,  2.3865e-02,  9.1863e-02,  5.4388e-02,
         1.1565e-01, -9.1775e-02,  1.7982e-01, -6.4217e-04, -6.8558e-02,
         2.7526e-01,  1.1384e-01,  3.2608e-01, -2.5336e-01, -4.9204e-01,
        -1.1743e-01, -6.1600e-03,  1.7388e-02, -1.4776e-01,  2.1700e-02,
         5.5266e-02, -9.0456e-02, -3.9565e-01,  5.1087e-02, -9.5472e-03,
         1.0449e-01,  8.0698e-02, -1.3287e-01, -1.2388e-02, -1.5503e-01,
         1.4343e-01, -1.7563e-01,  1.2543e-01, -1.0902e-01,  3.5237e-01,
         1.3982e-02, -1.6298e-01,  7.5430e-02,  1.4752e-01, -2.1335e-01,
         2.0591e-02,  8.5383e-02, -1.4727e-01,  1.1335e-01,  1.0891e-01,
         8.6156e-02,  2.1769e-02, -9.0225e-02, -6.0408e-02,  1.7196e-01,
         1.7566e-01,  5.9273e-02, -2.7534e-01,  1.5645e-01,  2.8370e-02,
         9.7209e-02,  1.5087e-01, -1.5383e-01, -1.1292e-01,  1.7840e-01,
         1.4085e-01,  3.9992e-01,  3.1400e-02, -3.0622e-01,  4.0452e-02,
         1.0277e-01,  2.9261e-01,  7.9035e-02, -1.6428e-01,  1.3205e-01,
        -5.9142e-02,  6.1661e-02, -2.2273e-01, -1.1950e-01,  1.1244e-02,
        -7.5128e-04,  6.2115e-02, -7.5307e-02, -1.9999e-01, -5.0906e-02,
         1.5986e-01, -1.5384e-01,  1.8486e-02,  5.3391e-02, -2.2856e-02,
         1.6538e-02,  1.6673e-01,  1.6526e-01,  3.7644e-01,  2.2116e-02,
         1.6101e-02,  8.4632e-02,  1.6122e-01, -1.2911e-01, -4.2537e-03,
         4.3188e-01, -1.6758e-01,  3.1329e-01, -3.9120e-02,  2.3113e-01,
        -4.7350e-02, -1.7430e-03, -4.6825e-03,  9.6474e-02, -1.3785e-02,
         3.6050e-01, -2.3145e-01,  9.9413e-02, -7.9733e-02,  2.7168e-01,
        -7.9808e-02, -9.3619e-02,  2.1798e-02, -7.2180e-02, -6.3289e-02,
         1.9584e-01, -4.4046e-02,  4.8561e-01,  1.6550e-01,  2.8046e-01,
        -2.8508e-01, -1.3410e-01, -3.0835e-01, -8.8017e-02, -8.2227e-02,
        -2.3335e-01, -3.8007e-01, -1.8516e-01, -8.0878e-02,  6.9986e-02,
         3.1223e-03, -1.6395e-01,  6.1631e-02,  1.0742e-01,  1.4890e-01,
         3.1640e-01,  3.0610e-01, -1.3100e-01, -2.2691e-01,  2.3236e-01,
         2.3160e-02, -2.4449e-02,  2.5452e-01,  9.4654e-02, -2.5230e-01,
         1.0485e-01, -3.9629e-02,  2.7753e-01, -8.5534e-02, -2.8560e-01,
        -1.0625e-01, -1.9122e-01, -7.7833e-02,  1.7479e-02,  1.1107e-01,
         1.9104e-01, -5.3806e-02,  1.4329e-01,  7.4348e-02, -3.7891e-02,
        -1.6470e-01,  2.7303e-01,  1.9698e-01, -3.4510e-01, -3.8307e-02,
         1.8732e-01, -7.2547e-02, -1.5918e-01, -1.2051e-01,  1.5882e-01,
        -2.4738e-01, -1.3463e-02,  2.5781e-01, -1.8179e-01,  3.0621e-01,
         3.7397e-02, -6.3673e-02, -6.0211e-02,  3.0543e-01,  1.8674e+00,
         2.9141e-01, -2.1947e-01, -4.4114e-02, -1.2505e-01,  3.7997e-01,
        -1.4247e-02, -3.6082e-01,  1.8770e-01,  9.6182e-03,  1.2768e-01,
        -1.4299e-01,  2.9161e-01, -3.4586e-01, -4.6834e-02,  1.1860e-01,
         5.2585e-03,  1.3938e-01, -3.3451e-01,  9.1791e-02,  2.3278e-01,
         4.8837e-03, -3.0325e-01,  3.3830e-01,  4.9341e-02,  1.1152e-02,
         9.8717e-02,  3.3592e-02,  4.5998e-01,  1.2436e-02,  1.7771e-01,
         8.9274e-02, -1.4505e-01, -4.3361e-03, -6.2916e-02, -2.6059e-01,
         3.9149e-01, -7.0545e-02,  7.1020e-02,  1.5872e-01,  1.6568e-01,
         1.2057e-01, -2.5532e-01,  1.2026e-01], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 0.0032,  0.0204, -0.0020,  ..., -0.0134,  0.0182,  0.0079],
        [ 0.0039,  0.0227,  0.0147,  ...,  0.0069, -0.0088,  0.0002],
        [ 0.0234,  0.0145,  0.0353,  ..., -0.0183, -0.0043, -0.0039],
        ...,
        [-0.0105,  0.0073,  0.0207,  ...,  0.0153, -0.0183, -0.0241],
        [ 0.0283,  0.0126, -0.0240,  ...,  0.0002, -0.0133,  0.0178],
        [-0.0102,  0.0041,  0.0092,  ...,  0.0073,  0.0087, -0.0314]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.0820,  0.2947, -0.0189,  ...,  0.0406, -0.0679, -0.0443],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0014, -0.0327, -0.0172,  ...,  0.0096, -0.0359,  0.0276],
        [-0.0273, -0.0061, -0.0044,  ..., -0.0093, -0.0338, -0.0004],
        [ 0.0194,  0.0110,  0.0128,  ...,  0.0142,  0.0174, -0.0026],
        ...,
        [-0.0471, -0.0134, -0.0056,  ...,  0.0004,  0.0095,  0.0284],
        [ 0.0096, -0.0149, -0.0111,  ..., -0.0030, -0.0090, -0.0091],
        [-0.0043, -0.0149, -0.0157,  ...,  0.0204,  0.0081,  0.0181]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-5.8228e-02,  9.1492e-02,  1.6858e-01,  2.8473e-02,  4.3750e-01,
        -7.2021e-02, -2.1652e-02, -9.1125e-02, -6.7215e-03,  4.0222e-02,
         4.4312e-02, -9.1797e-02, -6.2561e-02, -9.0454e-02,  1.1432e-01,
        -1.3196e-01,  5.2002e-02, -4.8706e-02, -9.7198e-03, -3.0615e-01,
         6.1760e-03,  2.2324e-02,  2.8467e-01, -2.8275e-02, -7.9956e-02,
        -5.9326e-02,  2.8210e-03, -1.2201e-01,  6.4514e-02, -5.7251e-02,
        -1.1969e-01,  1.6772e-01, -1.1429e-02, -2.2766e-01, -1.7120e-02,
         9.6497e-02, -9.4482e-02, -6.5796e-02,  8.2825e-02,  1.0254e-01,
         1.3342e-01,  1.1084e-01,  3.7720e-02,  1.1597e-01,  1.7407e-01,
         1.3196e-01, -5.4245e-03,  2.9572e-02,  1.1803e-02,  8.5220e-03,
         8.0383e-02, -2.5684e-01,  1.3562e-01, -4.3610e-02,  1.2646e-01,
         1.2489e-02, -5.5237e-02, -1.2744e-01,  8.9478e-02, -1.6980e-01,
         1.3574e-01, -4.9866e-02, -1.4001e-01, -2.0660e-02,  6.9214e-02,
        -6.4697e-02,  3.5547e-01, -3.0365e-02, -5.8868e-02,  1.6205e-02,
         1.5617e-02, -9.3933e-02,  1.4429e-01,  1.6769e-02, -2.1301e-02,
         1.3391e-01,  1.0559e-01, -1.5356e-01,  4.4189e-02, -2.2449e-03,
         9.0759e-02,  1.7395e-01, -4.8218e-03, -2.0233e-02, -4.1809e-02,
        -5.4810e-02,  1.4270e-01,  5.6488e-02, -9.1492e-02, -4.0222e-02,
         2.7740e-02, -4.9652e-02,  1.6858e-01,  1.0675e-01, -4.3945e-02,
         2.2293e-02,  1.4795e-01,  2.4377e-01, -1.6150e-01,  5.3650e-02,
         5.6976e-02, -1.5576e-01,  3.2959e-02,  1.8567e-01,  1.5454e-01,
         1.0437e-01, -2.0410e-01, -7.5798e-03,  1.5210e-01, -2.3022e-01,
         1.0480e-01,  5.8289e-02, -2.3041e-02,  6.0944e-02, -3.7323e-02,
        -2.0605e-01,  1.6260e-01,  7.9468e-02, -1.2671e-01,  8.2397e-02,
        -2.4463e-01,  1.0339e-01,  7.5562e-02,  1.0004e-01,  2.4573e-01,
        -2.6108e-02, -1.3513e-01,  1.2445e-01, -1.6321e-01,  2.8882e-01,
         5.3619e-02,  7.2571e-02, -1.2964e-01, -5.5237e-02,  2.8782e-03,
         2.3651e-03,  2.6764e-02,  3.5286e-03, -6.9580e-02, -1.9470e-02,
         7.2021e-02,  1.6602e-02, -7.2365e-03,  1.0645e-01, -1.3831e-01,
        -5.7892e-02,  1.5576e-01, -1.9287e-01,  6.3782e-03, -1.9714e-01,
        -1.0419e-01, -1.8665e-01,  1.1884e-01, -4.6173e-02, -8.6121e-02,
        -1.4722e-01, -3.5675e-02,  3.3374e-01, -7.7209e-02, -1.1346e-01,
        -9.3018e-02, -7.9468e-02,  2.8535e+00,  3.7811e-02, -9.5398e-02,
         8.0261e-02,  2.8591e-03, -2.5757e-02,  2.4185e-02,  1.0553e-01,
         6.7505e-02, -1.7297e-01,  1.6626e-01, -1.7505e-01, -2.6581e-02,
         1.6931e-01, -6.7566e-02,  2.5543e-02, -7.9163e-02, -9.2468e-02,
        -2.1130e-01, -1.6833e-01,  3.4149e-02,  1.6858e-01, -1.4880e-01,
        -9.3933e-02,  2.8000e-02,  6.6643e-03,  9.3323e-02, -4.2480e-02,
        -8.1787e-02,  2.1350e-01, -1.1328e-01, -1.4856e-01,  3.6011e-01,
         1.2314e-02, -1.8286e-01,  9.1797e-02, -1.3260e-02, -1.6101e-01,
         1.8054e-01, -1.7990e-02, -8.9539e-02, -4.3274e-02,  1.7993e-01,
        -5.6519e-02,  4.4495e-02, -6.6223e-02,  1.9678e-01,  2.0523e-02,
         1.8774e-01, -2.0374e-01,  5.2551e-02, -2.1033e-01, -2.0740e-01,
         1.9202e-01,  8.7952e-02,  1.6663e-02,  6.9397e-02,  2.4512e-01,
        -2.5681e-02,  8.7402e-02, -1.1890e-01, -1.2939e-01,  3.6621e-02,
        -6.2134e-02,  2.7115e-02,  8.1238e-02, -1.6418e-01, -1.2463e-01,
         1.0876e-01,  1.0577e-01,  1.5613e-01,  1.6516e-01, -8.8501e-03,
        -1.0059e-01,  9.1492e-02, -1.0480e-01,  3.3905e-02, -9.6054e-03,
         5.0262e-02, -7.0496e-02, -2.2437e-01,  1.1566e-01, -4.0649e-02,
         1.2708e-01,  5.4413e-02, -3.6011e-02,  1.9592e-02,  1.8945e-01,
         7.6965e-02,  2.3938e-01,  7.8735e-02,  2.8491e-01, -1.3708e-01,
        -5.0507e-02, -5.0171e-02, -3.5309e-02, -1.9373e-01,  9.1309e-02,
         4.2084e-02,  1.0658e-02, -4.9255e-02, -2.0422e-01,  1.0266e-01,
         9.4971e-02,  6.9641e-02,  6.3477e-02, -6.7322e-02, -5.6580e-02,
         3.9673e-02,  1.8860e-02,  1.3806e-01,  5.4626e-02, -2.3145e-01,
        -2.0032e-01,  1.3086e-01,  7.6233e-02, -2.4536e-02, -2.3453e-02,
        -1.0394e-01,  1.5649e-01, -7.0740e-02, -9.3811e-02, -1.2732e-01,
        -1.7175e-01, -1.4087e-01, -7.5623e-02, -6.1462e-02, -3.4058e-01,
        -1.1115e-01,  1.1787e-02, -3.9246e-02,  2.5269e-02,  8.7524e-02,
        -9.3842e-03,  1.1627e-01, -1.1115e-01,  4.2755e-02,  1.8188e-01,
        -5.9540e-02,  1.7908e-01,  4.2725e-02,  6.0760e-02, -5.7922e-02,
        -2.4011e-01, -1.2184e-02, -1.0999e-01, -5.3345e-02,  7.1533e-02,
         5.6244e-02, -4.5898e-02,  2.4487e-01,  7.8964e-03,  1.2585e-01,
        -9.2285e-02,  1.8652e-01, -5.9387e-02,  2.8076e-02, -1.2856e-02,
         1.7444e-01,  3.6373e-03,  3.7750e-02,  6.0272e-02,  8.0017e-02,
        -5.8075e-02,  1.0547e-01, -4.0588e-02, -1.3220e-01,  1.0645e-01,
         1.3599e-01,  5.1056e-02, -9.2529e-02,  1.2915e-01, -1.7993e-01,
        -6.0577e-02, -4.3488e-02, -2.8824e-02,  1.1823e-01, -1.9882e-02,
        -7.2449e-02, -1.6785e-02,  6.5247e-02, -4.7073e-03,  8.0444e-02,
        -6.8909e-02, -7.3486e-02,  4.6204e-02,  3.7598e-02, -5.6580e-02,
        -9.2102e-02, -1.3403e-01,  8.9905e-02, -4.8950e-02, -5.6854e-02,
         1.6617e-02,  5.0232e-02,  5.3741e-02,  6.7017e-02, -1.7114e-01,
         1.2390e-01, -6.1584e-02, -4.9225e-02,  7.4341e-02, -7.8430e-02,
         1.1859e-01, -1.8127e-02,  9.7942e-04,  1.1499e-01, -8.2947e-02,
         2.5043e-03, -7.2510e-02,  4.4373e-02,  2.0361e-01,  9.3567e-02,
         1.2164e-01,  9.6130e-02,  1.3733e-01,  8.3618e-02, -1.2573e-01,
        -5.1346e-03,  7.1350e-02, -5.2673e-02,  1.3770e-01, -7.1289e-02,
         2.0294e-02, -8.5754e-02,  3.6713e-02, -4.5197e-02, -4.0161e-01,
         1.0187e-01, -1.6760e-01,  5.4840e-02,  5.2338e-02, -1.0120e-01,
        -6.1707e-02,  8.3984e-02, -1.0864e-01,  3.5492e-02,  1.5149e-01,
         4.5410e-02,  2.1240e-02, -1.2988e-01, -1.4722e-01, -6.7406e-03,
        -1.7258e-02, -6.3477e-02,  6.5430e-02, -9.7290e-02, -1.2115e-01,
        -1.1426e-01,  1.0071e-01,  5.2490e-02,  4.4785e-03,  6.3538e-02,
        -1.0834e-01, -1.5417e-01,  1.1505e-01,  6.0822e-02,  2.7664e-02,
         5.8823e-03, -1.2610e-01,  6.7505e-02,  1.6823e-03, -6.2256e-02,
         1.6064e-01, -8.3923e-03,  8.4595e-02,  1.4200e-03,  7.7148e-02,
         8.1055e-02,  1.6068e-02, -5.9998e-02, -4.0588e-03,  2.3987e-01,
        -1.4062e-01, -3.8544e-02, -1.5723e-01,  1.7532e-02,  1.0724e-01,
        -1.0303e-01, -2.9022e-02, -3.3417e-02, -6.9763e-02, -4.3457e-02,
         1.1212e-01,  1.0406e-01,  1.4978e-01,  1.7517e-01, -1.1664e-01,
        -5.3375e-02, -1.3794e-02, -8.0139e-02, -8.2153e-02,  1.7322e-01,
        -1.9226e-02,  5.3589e-02,  7.4890e-02,  6.2805e-02,  2.8290e-02,
         8.4473e-02,  7.5073e-03, -4.3518e-02,  2.6270e-01, -3.5980e-02,
         2.6636e-01,  1.1414e-01,  1.8091e-01,  1.1169e-02, -4.5105e-02,
        -3.8788e-02,  8.1116e-02,  1.2131e-02, -1.3477e-01, -7.1875e+00,
        -6.6948e-04,  6.2073e-02, -1.0217e-01,  5.1819e-02, -1.5869e-01,
        -8.2336e-02, -8.9233e-02,  1.2091e-01,  8.8440e-02, -5.6244e-02,
        -1.1438e-01, -4.9561e-02, -1.1359e-01, -1.4734e-01, -1.4368e-01,
         6.4636e-02, -6.9702e-02,  3.9093e-02,  2.3727e-02,  2.1576e-02,
        -3.1647e-02, -3.7384e-02, -1.6968e-01, -1.0895e-01,  7.3425e-02,
         1.0376e-01, -2.7954e-02,  7.8430e-02,  1.5466e-01,  8.2520e-02,
         1.8262e-01, -1.7676e-01, -9.2346e-02, -3.9124e-02, -9.7168e-02,
        -9.1797e-02, -1.9019e-01,  3.2471e-02, -1.5845e-01,  8.7559e-05,
         1.1029e-01, -5.4283e-03, -1.0956e-01,  1.0370e-01, -6.5430e-02,
        -9.5642e-02,  2.7481e-02, -1.2177e-01, -7.0923e-02, -7.9834e-02,
        -2.0538e-02,  7.6904e-02,  7.3059e-02, -2.0508e-01,  1.0223e-01,
         2.0459e-01,  1.5503e-01,  1.3893e-02, -7.2449e-02, -1.5030e-02,
        -1.5747e-01, -1.8713e-01,  3.1036e-02,  8.1543e-02, -1.2535e-02,
         2.3605e-02, -6.7406e-03, -1.7578e-01,  1.0181e-01,  1.6003e-01,
        -2.9556e-02, -5.6877e-03,  5.2856e-02,  1.7859e-01, -2.2717e-01,
        -4.3396e-02,  4.4128e-02,  9.1980e-02, -6.7810e-02, -3.9795e-02,
         2.7649e-02,  5.9021e-02,  1.1407e-01, -5.8350e-02,  2.4109e-01,
         1.3379e-01, -1.2671e-01, -8.6060e-02, -8.9188e-03,  1.5100e-01,
        -1.6919e-01, -7.2144e-02,  3.1372e-02,  1.4722e-01,  2.0889e-02,
         7.8796e-02, -1.1115e-01, -1.5640e-02, -1.8872e-01,  6.1798e-02,
         9.5215e-03, -8.1787e-02, -3.3741e-03,  6.9275e-02, -5.0995e-02,
        -4.7882e-02,  1.1646e-01,  1.3098e-01,  4.6875e-02,  1.1987e-01,
        -7.9163e-02, -1.6675e-01, -9.0027e-02, -7.5150e-03, -8.6426e-02,
        -4.0009e-02, -4.3274e-02, -8.6792e-02,  1.4563e-01, -1.9653e-02,
        -3.6201e-03,  6.0425e-02,  1.8652e-01,  1.4319e-01, -2.4048e-02,
        -1.0248e-01, -1.7126e-01,  9.4238e-02,  1.2769e-01,  6.8481e-02,
        -7.2250e-03, -2.3804e-02,  4.5746e-02, -4.7974e-02,  4.1107e-02,
        -2.5925e-02, -1.0913e-01,  2.2995e-02, -1.8433e-01, -4.4678e-02,
        -4.7913e-02,  1.2396e-01,  2.5732e-01, -3.2654e-02,  2.1130e-01,
        -4.9438e-02,  1.1487e-01, -2.1240e-01, -5.9692e-02,  3.6224e-02,
        -9.7900e-02,  1.1742e-02, -1.3220e-01,  1.2268e-01,  1.2964e-01,
         1.8579e-01, -3.6278e-03,  1.5778e-02,  2.1326e-01,  3.6426e-01,
        -5.0415e-02,  3.1128e-02, -2.0715e-01,  9.7656e-02,  5.9631e-02,
         1.8799e-01,  1.7859e-01, -1.3708e-01,  8.4167e-02,  5.9967e-02,
         2.7075e-01,  1.2421e-02,  8.4778e-02, -2.0642e-01, -1.2195e-01,
        -1.5100e-01, -1.1253e-02,  1.1139e-01, -2.0068e-01,  2.1915e-03,
        -7.4890e-02,  5.7434e-02,  1.8335e-01,  1.2036e-01,  1.4185e-01,
         8.3801e-02,  7.1716e-02, -4.1931e-02,  1.2561e-01, -9.9060e-02,
         6.2141e-03, -1.6968e-02,  7.0923e-02,  9.8206e-02, -7.6538e-02,
         1.5515e-01, -6.4880e-02, -1.1786e-01, -1.8347e-01, -5.0468e-03,
        -3.6377e-02, -8.3862e-02, -8.6731e-02,  4.1168e-02,  1.9547e-02,
         1.6602e-01, -1.2854e-01, -9.2590e-02, -2.1729e-01,  2.9507e-03,
         4.5074e-02, -2.8076e-02,  1.9189e-01,  3.4698e-02, -9.9182e-02,
        -6.1096e-02,  9.2102e-02, -7.0129e-02,  1.2903e-01, -8.4473e-02,
        -3.5648e-03, -1.9250e-01,  1.5979e-01,  9.2407e-02, -2.2266e-01,
         2.2839e-01, -2.3155e-03,  7.7759e-02, -2.7481e-02, -1.5442e-01,
        -1.3928e-01,  2.9648e-02,  1.9943e-02, -1.4435e-02,  1.0114e-01,
        -6.4941e-02, -1.0944e-01, -6.1157e-02,  8.1253e-03,  8.9661e-02,
         2.9190e-02,  6.3293e-02, -2.1326e-01, -2.2949e-01, -1.9250e-01,
        -2.0825e-01,  9.8694e-02, -6.9153e-02,  2.5464e-01, -2.1606e-01,
         1.5637e-01, -6.4819e-02, -4.6539e-02,  3.7262e-02, -1.1328e-01,
         6.9275e-02, -3.6346e-02,  1.9379e-02,  1.6556e-03,  8.8074e-02,
         7.0923e-02,  8.2642e-02,  6.3086e-04, -1.6983e-02, -1.0858e-01,
         3.4644e-01,  1.2134e-01,  3.4973e-02,  4.4128e-02, -1.4160e-01,
        -1.7761e-01,  3.7384e-02,  1.3477e-01, -9.5398e-02, -7.1899e-02,
        -8.4900e-02, -3.9825e-02,  1.1810e-02, -4.6326e-02,  1.3867e-01,
         9.7107e-02, -1.0150e-01, -3.7441e-03,  1.1786e-01,  2.5803e-02,
        -1.5845e-01, -2.1252e-01,  8.8135e-02, -1.6309e-01,  3.8879e-02,
         2.0004e-02, -8.9111e-03, -2.5162e-02], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([1.8728, 1.7547, 1.8036, 1.8577, 2.1824, 1.6302, 1.7899, 1.8454, 1.8129,
        1.7273, 1.7220, 1.6906, 1.7052, 1.8640, 1.8814, 1.7893, 1.8372, 1.7668,
        1.8278, 1.8139, 1.6810, 1.7612, 1.9072, 1.9237, 1.7676, 1.7158, 1.7243,
        1.8889, 1.9533, 1.7293, 1.8389, 1.7399, 1.7465, 1.8322, 1.7277, 1.7053,
        1.8235, 1.7012, 1.8006, 1.9238, 1.7329, 1.9680, 1.7680, 1.6244, 1.6845,
        1.7426, 2.0288, 1.8557, 1.8263, 1.7483, 1.8057, 1.7374, 1.6137, 1.7142,
        1.8097, 1.7203, 1.6717, 2.2543, 1.7730, 1.9483, 1.7105, 1.6814, 2.1237,
        1.8089, 1.8255, 2.6967, 1.8107, 1.6820, 1.7113, 1.7730, 1.9706, 1.9000,
        1.8153, 1.8284, 1.6183, 1.8163, 1.7406, 1.7113, 2.0869, 1.7986, 1.9555,
        1.8223, 1.8276, 1.7193, 1.8223, 1.8769, 1.7694, 1.7360, 1.7613, 1.6688,
        1.7022, 1.7667, 1.7065, 1.6603, 1.8120, 1.7049, 1.9174, 1.6893, 1.7328,
        1.7920, 1.7548, 1.7317, 1.6880, 1.7914, 1.9288, 1.8672, 1.7392, 1.7381,
        1.7374, 1.6784, 1.6162, 1.7353, 1.8124, 1.8877, 1.6735, 1.8267, 1.7956,
        1.7927, 1.7485, 1.7837, 1.8291, 1.6297, 1.7247, 1.8656, 1.9152, 1.6948,
        1.8756, 1.7589, 1.7512, 1.7557, 1.6466, 1.7673, 1.6632, 1.8049, 1.8177,
        1.7658, 1.7416, 1.9632, 1.7444, 1.6705, 1.7454, 1.7714, 1.6047, 1.8853,
        1.8237, 1.7208, 1.8655, 1.7895, 1.6101, 1.8483, 1.8135, 1.9494, 1.7945,
        1.7738, 1.8106, 1.7011, 1.8042, 1.7543, 1.6437, 1.7617, 1.7073, 1.9108,
        2.2943, 1.7387, 1.8584, 1.7676, 1.8051, 1.6746, 1.8105, 1.8633, 1.7904,
        1.7631, 1.6917, 1.6902, 1.8191, 1.8009, 2.6139, 2.0224, 1.6584, 1.7414,
        1.8323, 1.8182, 1.6671, 1.7693, 1.8126, 1.9716, 1.7960, 1.8354, 1.7195,
        1.7754, 1.6959, 1.6688, 1.7538, 1.8228, 1.9487, 1.7090, 1.7782, 1.7023,
        2.8828, 1.8100, 1.6670, 1.7395, 1.7515, 1.8846, 1.7747, 1.7251, 1.9862,
        1.7202, 1.6708, 1.7447, 1.5925, 1.8382, 1.7598, 1.6398, 1.8064, 1.8713,
        1.7965, 1.8117, 1.7469, 1.7719, 1.6719, 1.7656, 1.7959, 1.7350, 1.9117,
        1.7505, 1.8743, 1.8043, 1.7795, 1.5903, 1.7346, 1.8705, 1.7257, 1.7646,
        1.7982, 1.7138, 1.7988, 1.6006, 1.7668, 1.8458, 1.8039, 1.7732, 1.7253,
        1.9010, 1.6667, 1.6973, 1.7982, 1.9759, 1.7445, 1.8074, 1.8802, 1.7144,
        1.8232, 1.8887, 1.8002, 1.8160, 1.7317, 1.9969, 1.5780, 1.8576, 1.7781,
        1.8187, 1.8604, 1.8072, 1.7960, 1.7139, 1.7889, 1.7381, 2.0270, 1.7207,
        1.7016, 1.8712, 1.9156, 1.7318, 1.8476, 1.9266, 1.8543, 1.7598, 1.7594,
        1.6991, 1.7971, 1.7460, 1.7098, 1.6603, 1.7289, 1.6441, 1.5901, 1.7972,
        1.8199, 1.8494, 2.0410, 1.7342, 1.9012, 2.0701, 1.7688, 1.8882, 1.8565,
        1.6923, 1.7557, 1.8527, 1.7336, 1.8296, 1.6045, 1.8099, 1.6580, 1.8173,
        1.9574, 2.4231, 1.6354, 2.2620, 1.8750, 1.7571, 1.8287, 1.7256, 1.9842,
        1.8685, 1.7660, 1.8027, 1.7770, 1.8104, 1.8142, 1.7283, 1.6328, 1.6241,
        1.8117, 1.7310, 1.6545, 1.6059, 1.8148, 1.8364, 1.8917, 1.6462, 1.8115,
        1.8170, 1.7312, 1.7646, 1.6505, 1.7751, 1.7892, 1.8509, 1.9477, 2.3132,
        1.8190, 1.9001, 1.8442, 1.6871, 1.8139, 1.8468, 2.0367, 1.7056, 1.7112,
        1.7670, 1.8652, 1.7420, 1.8898, 1.7194, 1.7252, 1.7193, 1.7331, 1.7921,
        1.8597, 1.6955, 1.8549, 1.8233, 1.9745, 1.8662, 1.7944, 1.7510, 1.7609,
        1.8620, 1.7772, 1.7749, 1.8237, 1.6827, 1.7449, 1.8918, 2.0390, 1.6744,
        1.6268, 1.9001, 1.7299, 1.8229, 1.7977, 1.6669, 1.6024, 1.8188, 1.7334,
        1.8002, 1.7300, 2.1726, 1.6700, 1.7431, 1.8026, 1.7612, 1.7199, 1.6923,
        1.6644, 1.7084, 1.9433, 1.7660, 1.6032, 1.7658, 1.7543, 1.7204, 1.8941,
        1.9148, 1.6203, 1.9567, 1.7905, 1.6558, 1.8582, 1.8381, 2.0396, 1.9045,
        1.7705, 1.7511, 1.9576, 1.7374, 1.7310, 1.8140, 1.7889, 1.6220, 1.7523,
        1.7130, 1.7670, 1.9267, 1.6834, 1.7794, 1.6861, 1.8663, 1.8656, 1.8087,
        1.7948, 1.7197, 1.5690, 2.2319, 1.8988, 1.7588, 1.8252, 1.7361, 1.7862,
        1.7354, 1.7093, 1.6775, 1.8468, 1.7330, 1.8936, 1.7130, 1.9545, 1.7298,
        1.6729, 1.7887, 1.6690, 1.8468, 1.7840, 1.6812, 1.7597, 1.7837, 1.6500,
        1.6694, 1.9539, 2.0421, 1.8921, 1.6931, 1.9127, 1.6547, 1.7876, 1.8291,
        1.7312, 1.7571, 1.8522, 1.6939, 1.7974, 1.7912, 0.8882, 1.6792, 1.9011,
        1.9224, 1.9471, 1.8223, 1.7650, 1.6055, 1.8410, 1.8062, 1.8044, 1.6980,
        1.6566, 1.8625, 1.6706, 1.8128, 1.6267, 1.6477, 1.7553, 1.9766, 1.7803,
        1.8333, 1.8784, 1.8143, 1.6159, 1.7287, 1.7773, 1.8109, 1.7313, 1.7073,
        1.7380, 1.8799, 1.9860, 1.7834, 1.7660, 1.7020, 1.7776, 1.7250, 1.7347,
        1.8215, 1.8359, 1.6774, 1.6558, 1.9027, 1.8354, 1.8742, 1.9091, 1.5760,
        1.7863, 1.7307, 1.7591, 1.8141, 1.7780, 1.6756, 1.8921, 1.7581, 1.7023,
        2.0466, 1.8074, 1.7839, 1.8754, 1.7286, 1.7876, 1.7002, 2.0919, 1.8398,
        1.8331, 1.8582, 1.6425, 1.7268, 1.7142, 1.6585, 1.9305, 1.7850, 1.8508,
        1.8482, 1.7189, 1.7981, 1.7123, 1.9352, 1.6617, 1.8423, 1.6198, 1.8271,
        1.5744, 1.5694, 1.9952, 1.7246, 1.7511, 1.7516, 1.8460, 1.7108, 1.6408,
        1.7795, 1.7720, 1.7731, 1.8246, 1.7610, 1.7569, 1.7468, 1.6235, 1.6202,
        1.8685, 1.9336, 1.8680, 1.6949, 1.6995, 1.7635, 1.9551, 1.7691, 1.8124,
        1.6820, 1.7588, 1.7007, 1.8804, 1.7494, 1.7261, 1.6887, 1.7230, 1.6999,
        1.8258, 1.8233, 1.7310, 1.7570, 1.8468, 1.8310, 1.8870, 1.8488, 1.6868,
        1.6953, 1.8011, 1.7912, 1.8304, 1.7616, 1.7405, 1.7687, 1.6651, 1.8245,
        1.9685, 1.7588, 1.9942, 1.7595, 1.7126, 1.8003, 1.7433, 1.8008, 1.6225,
        1.7806, 1.9816, 1.6298, 1.8152, 1.7408, 1.6533, 1.8026, 1.6771, 1.8380,
        1.6243, 1.6354, 1.7953, 1.6672, 1.8827, 1.7984, 1.7944, 1.8477, 1.7569,
        1.7022, 1.7916, 1.7132, 1.8886, 1.8595, 1.8478, 1.6918, 1.7125, 1.8510,
        1.7128, 1.6492, 2.3644, 1.7261, 1.7210, 1.7260, 1.6130, 1.7513, 1.8390,
        1.5612, 1.7037, 1.7211, 2.0017, 1.8191, 1.6772, 1.9493, 1.7455, 1.7580,
        1.8318, 1.8643, 1.8332, 1.6268, 1.7308, 1.7625, 1.8703, 1.9427, 1.6668,
        1.7555, 1.9277, 1.8045, 1.8333, 1.8089, 1.8974, 1.8570, 1.6222, 1.6393,
        1.8872, 1.7288, 1.7060, 1.8189, 1.8870, 1.5363, 1.7967, 1.8584, 1.8575,
        1.8540, 1.9067, 1.7273, 1.8267, 1.6695, 1.7934, 2.0291, 1.8780, 1.8184,
        1.7510, 2.0461, 1.7423, 1.9446, 1.7160, 1.8476, 1.7205, 1.5758, 1.7424,
        1.8978, 1.6879, 1.7293, 1.7713, 2.0305, 1.7728, 1.7149, 1.7348, 1.8070,
        1.7017, 1.7348, 1.7224, 1.6461, 3.0367, 2.0496, 1.9265, 1.7293, 1.8574,
        1.7300, 1.9038, 1.9109, 1.7025, 1.8784, 1.7179, 1.7195, 1.8131, 1.7751,
        1.5399, 1.6682, 1.6308, 1.8071, 1.6887, 1.8143, 1.6503, 1.7686, 2.0083,
        1.8872, 1.8092, 1.8261, 1.8082, 1.7445, 1.7919, 1.7134, 1.7019, 1.8723,
        1.7027, 1.6786, 1.9378, 1.6835, 1.7235, 1.7465, 1.7253, 1.5730, 1.7733,
        1.7328, 1.8270, 1.6885], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 5.2365e-01, -7.1796e-01, -5.0786e-01, -4.9917e-01, -7.3772e-01,
         6.9723e-01, -3.5357e-01,  1.9300e-01,  8.1534e-01, -4.2375e-01,
        -5.5178e-01,  1.4156e-01,  4.5392e-01, -3.9982e-01,  9.1892e-02,
         2.9702e-01, -1.6993e-01,  5.2459e-01,  3.2687e-01,  8.4204e-01,
        -3.4065e-01, -8.2934e-02, -9.8496e-02, -1.5335e-01,  1.2601e-01,
        -2.3323e-01,  4.7130e-02,  1.0745e-01,  5.4473e-01,  1.7098e-01,
        -2.7386e-02, -1.1410e-01,  6.8870e-01,  3.2801e-01,  4.2133e-02,
        -7.9934e-01,  7.5729e-01,  3.4921e-01, -6.0004e-01,  1.1502e-01,
        -1.5464e-01,  3.9600e-02, -1.5119e-01, -6.2914e-01, -4.5401e-01,
        -3.7856e-01, -9.9999e-01,  6.7442e-01, -3.6019e-01, -7.1740e-02,
        -1.2775e+00,  9.6797e-01,  1.9182e-01,  7.7241e-01, -3.6558e-01,
        -3.3722e-01,  6.6506e-01, -4.6364e-01,  5.5047e-01,  7.0849e-01,
        -4.1890e-01,  1.7221e-01, -5.1826e-01, -2.4845e-01, -3.9541e-01,
        -3.9335e-01, -1.0862e+00,  1.6473e-01,  1.7781e-01, -3.8569e-02,
        -3.6542e-01,  5.1494e-01, -8.3685e-01, -4.0427e-02, -4.5675e-01,
        -1.7859e-01, -3.8725e-01,  8.1883e-01,  7.5297e-01,  9.6626e-01,
         6.9364e-01, -5.0789e-01, -2.9852e-01,  1.2151e-01,  5.9669e-01,
        -3.1539e-01,  2.4300e-01, -9.6667e-02,  2.1359e-01,  6.6510e-02,
         1.2985e-01, -6.6999e-01, -1.6000e-01, -7.6950e-01,  3.1830e-01,
         2.8169e-01, -1.0485e+00, -1.2472e-01,  4.7124e-01, -4.2533e-02,
         4.8433e-01,  6.8543e-01,  3.5366e-01, -7.9901e-01,  1.6506e-01,
        -4.2159e-01,  9.2392e-01,  9.5828e-01, -5.4434e-01,  1.0712e+00,
        -2.4472e-01,  7.0082e-01,  1.6221e-01,  1.4343e-01,  1.1916e-01,
         6.4597e-01, -1.1901e+00, -3.2111e-01,  3.1017e-01,  2.6418e-02,
         7.9504e-01, -3.1324e-01,  1.2178e-01,  1.0480e-01, -4.9646e-01,
         2.0638e-01,  8.8050e-01, -6.3201e-01,  6.5160e-01, -9.2663e-01,
        -2.6332e-01, -2.4718e-01,  7.2113e-01, -7.5739e-02, -7.9578e-01,
         2.6718e-02, -8.4464e-01,  9.4241e-01, -3.8431e-01, -2.1190e-01,
        -8.9685e-01, -8.7849e-01,  3.6954e-01,  1.9354e-01,  5.9779e-01,
         5.4826e-01, -1.2341e-01,  5.3911e-01, -1.6569e-01,  8.4252e-01,
         2.7934e-01,  1.2656e-02, -5.4159e-01,  6.6726e-01,  4.8667e-01,
         6.7404e-01,  1.9638e-01, -9.7165e-01,  5.7017e-01,  4.4719e-01,
        -4.4060e-01,  4.2154e-02,  5.2086e-01,  5.6147e-01,  3.0391e-01,
         7.2406e-02,  7.2969e-01,  1.9448e-01, -5.1772e-01,  7.0601e-01,
        -6.6572e-02,  3.0350e-01, -6.1288e-01,  1.8081e-01, -2.8082e-01,
         4.0170e-01, -7.3233e-01,  7.8688e-01,  4.8578e-01,  5.3802e-02,
         7.2736e-01, -1.2462e-01, -4.1762e-01, -5.2198e-01,  7.9471e-01,
        -2.1844e-01, -3.9000e-01, -7.0649e-02,  1.5447e-01,  3.3449e-01,
        -1.9374e-01, -1.0065e+00,  5.1472e-01,  4.9429e-01, -1.0720e+00,
         7.3440e-02,  7.5565e-01, -3.9948e-01, -2.7195e-01,  1.0148e+00,
        -6.9053e-01, -3.2311e-01, -9.6137e-02, -1.9534e-01, -3.0459e-01,
         1.9061e-01,  1.8854e-01,  1.7647e-01, -8.0750e-01,  6.3126e-03,
        -3.9351e-01,  8.9944e-03, -4.0237e-01,  7.4425e-01,  4.2765e-01,
         7.7177e-02, -3.1802e-01, -4.0009e-01, -9.7929e-01, -7.0733e-01,
        -4.4645e-02,  1.8130e-01,  7.0742e-01,  6.4393e-01,  7.3340e-01,
         5.9089e-01,  2.3528e-01, -1.2541e-01,  6.3674e-01,  5.6344e-01,
        -5.0353e-01,  9.3068e-02, -1.0097e+00, -8.5259e-01,  5.3163e-01,
        -2.0954e-01, -6.7231e-01,  5.8950e-01, -1.1848e-01,  5.2578e-01,
        -1.2783e+00,  1.8730e-02,  1.1769e+00,  6.8309e-01, -4.2683e-01,
        -3.9154e-01, -3.4926e-01, -5.6174e-01, -3.3042e-01,  8.3069e-02,
        -8.3279e-01, -1.1924e+00,  2.7345e-01, -1.9008e-01,  6.7653e-01,
         5.9039e-01,  8.5830e-01,  9.9681e-01,  7.1562e-01, -3.2135e-01,
        -1.6521e-01,  3.2021e-01,  4.2866e-01,  7.8429e-01, -7.3655e-01,
        -5.8396e-01, -1.3525e-01,  4.8509e-01,  5.6491e-02, -1.0362e-01,
        -6.2418e-01,  5.4077e-01, -1.6874e-01, -6.9453e-01,  7.5331e-01,
         7.8803e-01,  4.4891e-01, -1.9796e-01,  2.5425e-01,  3.6405e-01,
         3.2291e-01, -1.2160e-01,  6.6354e-01,  8.8945e-01,  8.0281e-01,
         2.7114e-01,  8.3829e-01,  3.3341e-01,  5.7132e-01,  1.4304e+00,
        -2.1592e-01,  9.8776e-02,  1.9750e-01, -9.7931e-01, -2.5027e-01,
        -7.6657e-02,  2.8210e-02,  5.7256e-01, -3.3332e-01, -3.7285e-01,
        -5.8753e-02, -3.2304e-03, -7.2074e-01, -3.4369e-01,  3.6055e-01,
         6.7007e-01, -6.9041e-01, -8.7381e-01,  5.3381e-01,  5.5394e-02,
        -1.1908e-01,  2.9258e-01, -2.1546e-01, -8.9148e-02, -1.7491e-01,
         9.0487e-01, -5.3882e-01,  8.9418e-01, -1.2240e-01, -1.8015e-01,
         2.9930e-01, -3.0655e-02,  4.2033e-01,  3.7214e-01, -4.8523e-01,
        -3.4483e-03, -5.0961e-01,  7.6495e-02,  5.2234e-01,  3.0821e-01,
        -1.6111e-02, -1.8429e-01,  2.2425e-01, -1.6561e-01,  5.7110e-01,
        -5.3549e-01,  1.0812e-01, -2.6734e-01, -1.0017e-03,  2.8036e-01,
         4.8367e-02,  6.0758e-03, -1.0951e-01,  1.6691e-01,  1.4140e-01,
         8.8960e-02, -1.9603e-01, -2.5611e-01,  1.3564e-01,  4.0519e-01,
         3.0434e-01,  2.0968e-01,  3.4171e-01,  1.7606e-01, -6.2441e-01,
         9.6200e-02, -6.4161e-01,  1.8472e-02, -4.7658e-01,  8.7947e-01,
        -4.6602e-01,  4.7910e-02,  5.2541e-01, -5.8759e-02, -1.3030e-01,
         3.8045e-01,  2.3253e-01,  4.5987e-01, -3.4145e-01,  2.9264e-01,
        -2.1473e-01, -4.5932e-01,  3.1802e-01, -9.6810e-01, -2.1568e-01,
        -7.9277e-01,  2.5702e-01, -6.7151e-01, -4.9960e-01,  2.7052e-01,
         6.7502e-01, -3.3173e-01,  9.9372e-01, -4.2096e-01,  2.1906e-01,
         3.3975e-02,  4.1775e-01, -2.8452e-01,  8.6553e-01,  3.9230e-01,
         1.1056e-01,  5.2030e-01, -2.8196e-01, -2.2177e-01,  7.0446e-01,
        -8.6578e-02,  1.3139e-01, -8.7306e-02,  1.1427e-01,  8.5759e-02,
        -1.4070e-01, -2.9741e-01,  8.9071e-01,  4.8202e-01, -5.9489e-02,
         8.9789e-01,  5.3881e-01,  6.0686e-01, -2.2308e-01,  8.6642e-01,
         4.9077e-01, -5.5091e-02, -4.3449e-01, -6.7774e-01,  3.7506e-02,
         2.6588e-01,  1.5247e-01, -2.2813e-01,  9.3671e-02,  3.6551e-01,
         5.5127e-01,  1.6033e-01, -2.5638e-01, -4.1804e-02, -3.0825e-02,
         2.9872e-01,  6.0427e-03, -1.8824e-01, -1.0125e-02,  8.9432e-01,
        -5.2097e-01,  8.5012e-01, -2.2281e-01, -4.3597e-01, -8.0840e-01,
        -9.1855e-01,  3.0273e-01,  1.0803e-01,  1.1152e+00, -1.5719e-01,
         1.1975e-02,  3.6843e-01, -2.9271e-01,  4.6732e-01,  1.7604e-01,
        -3.3923e-01, -2.9831e-01, -4.8610e-01, -7.5712e-02,  4.3407e-01,
         1.1111e+00,  5.1410e-01,  6.9860e-01,  4.0695e-01, -1.0682e+00,
         1.8455e-01, -4.9250e-01, -3.1826e-01, -2.9163e-01, -2.3546e-01,
        -7.8442e-01,  9.2442e-01,  1.6520e-01, -1.2188e+00,  5.7878e-01,
        -8.5155e-01,  6.7063e-02,  2.6550e-02, -4.3127e-01, -2.7951e-01,
         1.8323e-02,  1.9658e-02,  4.7443e-01,  6.1131e-01, -2.3093e+00,
         3.5181e-01,  2.5726e-01,  4.7805e-01, -1.7803e-01, -1.2482e-01,
         4.9293e-01,  5.0750e-01, -3.7749e-01, -2.6515e-01,  2.8590e-01,
         5.1095e-01,  1.3173e-01, -8.0175e-03,  1.1047e-01,  8.3509e-01,
        -4.4624e-01,  4.7691e-01,  1.7419e-01, -4.7241e-01,  3.1154e-01,
         2.9991e-01,  4.3542e-01,  1.2321e+00,  2.3803e-01, -4.3119e-01,
         3.7148e-01,  4.9344e-01, -3.0547e-01, -5.3341e-01, -3.1853e-02,
        -2.7490e-01,  3.2135e-01,  2.6234e-01, -1.9682e-02,  7.0717e-01,
        -4.2983e-01,  4.6524e-01, -5.3194e-01,  9.8069e-01,  5.8870e-01,
         2.1646e-01, -3.4338e-01,  4.2862e-02,  2.7347e-01, -2.2594e-01,
         1.0674e+00,  4.2078e-01,  4.8330e-01,  6.1738e-01,  3.2063e-01,
         1.7535e-01,  9.7011e-02, -1.8358e-01,  9.3179e-01, -2.2381e-02,
        -9.4270e-01,  1.1792e-01, -2.6643e-01,  2.1232e-01,  4.9830e-01,
         7.6591e-01,  2.9870e-02, -4.3988e-01,  4.0557e-01,  2.6872e-01,
        -4.1879e-02,  7.5626e-02,  6.6198e-01, -8.3230e-01, -3.0211e-01,
        -2.0827e-01, -7.0032e-01, -1.4148e-01, -1.6967e-01,  7.0183e-01,
         8.5013e-02, -4.1698e-01, -6.0489e-02,  3.0557e-01,  3.9021e-02,
         6.1819e-01, -6.1305e-01,  3.1326e-01, -3.0995e-01, -1.5488e+00,
        -3.0525e-01,  8.8306e-02,  9.2133e-01, -6.2372e-01, -1.9223e-01,
         3.8362e-01,  5.9477e-01, -6.0004e-01,  1.2034e-01, -2.1234e-01,
         5.9456e-02,  6.9879e-01, -3.6176e-01,  6.5457e-01, -4.7436e-01,
         1.7996e-01,  3.0792e-01,  7.9678e-02, -6.7011e-01,  5.5580e-01,
         1.0828e-01, -1.7653e-01,  4.6241e-02,  2.5056e-01, -8.9252e-01,
         3.1166e-01,  7.0509e-01,  6.8284e-01, -6.6525e-03,  6.1851e-01,
        -5.9312e-02,  8.9731e-02,  2.2366e-01, -5.5600e-01,  1.0849e+00,
         2.4813e-01,  1.1594e-01, -1.2167e+00,  1.5649e-01, -8.0943e-02,
         1.0084e+00,  5.9600e-01, -7.4801e-01, -6.5032e-01,  2.3075e-01,
         3.7942e-01,  4.2537e-01,  1.2587e-01, -4.7321e-01,  3.1443e-01,
         5.0104e-01,  9.8989e-01,  8.0250e-01,  3.8222e-01,  7.4728e-01,
         5.6268e-01, -4.8270e-02, -1.1665e+00,  3.3048e-02, -7.2375e-01,
         2.0525e-01, -2.7832e-01,  8.6653e-02, -2.8562e-02,  3.1306e-01,
         7.6600e-01, -8.0847e-01,  5.4784e-01, -2.8227e-01, -6.6010e-01,
        -9.8385e-01,  2.1724e-01,  2.2076e-02,  7.8854e-01, -9.0521e-01,
         3.7701e-01,  1.6502e-01,  4.0696e-01, -7.0733e-01,  3.6993e-01,
         5.1732e-01, -8.8974e-01,  2.7491e-01, -6.1666e-01,  5.1971e-01,
        -7.9852e-01, -2.4639e-01, -7.7815e-02,  7.8357e-01,  8.9507e-01,
        -6.5967e-01, -5.7339e-01, -3.1713e-01,  4.5455e-01,  1.1265e-01,
         2.2680e-01, -1.7445e-01, -3.9281e-01, -2.5807e-01, -3.0213e-01,
         3.4656e-01, -7.4623e-01, -2.8558e-01,  1.7893e-01,  1.1342e-01,
        -8.0159e-01, -2.1829e-01, -9.9254e-01, -1.5305e-01, -1.0703e-03,
        -1.1041e+00, -4.0161e-01, -3.3409e-02,  1.1676e-02, -2.7472e-01,
         1.2675e-01, -4.5706e-01,  4.1187e-01, -7.0135e-01,  7.1590e-01,
         5.4015e-01,  1.0804e+00,  1.8346e-01,  6.5775e-01,  4.7076e-01,
        -1.9371e-01, -7.5276e-02,  4.3526e-01,  3.9449e-01,  4.0887e-01,
        -5.3782e-02, -4.4384e-01,  1.9879e-02, -9.1176e-02,  2.1702e-01,
         1.1800e-01,  5.0274e-01, -7.4890e-01, -4.1458e-01,  4.5222e-01,
        -3.2062e-01, -7.9956e-02,  6.9394e-01, -1.0899e+00,  3.4254e-01,
        -1.7928e-01,  6.2240e-01,  6.2699e-02, -1.3474e-01, -5.6386e-01,
         3.9947e-01,  1.0371e+00,  3.3629e-01, -4.3833e-01,  1.2022e-01,
        -5.9730e-01, -4.6715e-01,  1.3945e+00,  7.4473e-01,  8.4637e-01,
         6.8066e-01, -1.5404e-01,  3.6263e-01, -7.1079e-02, -2.0128e-01,
         1.5455e-02, -5.0949e-01,  1.2212e-01, -5.4024e-01,  9.5051e-01,
        -6.7772e-01, -5.8960e-01, -3.9299e-01,  3.8189e-01, -9.1321e-02,
        -8.0553e-01,  5.3645e-01, -1.0406e+00,  2.0083e-01,  1.0773e+00,
        -1.1015e+00, -3.1549e-01, -3.2300e-01,  2.5178e-01,  6.0958e-01,
         4.2411e-01,  1.3246e+00, -6.4755e-02,  4.4368e-02,  5.3248e-01,
         4.2212e-01, -2.6460e-01,  8.3685e-01, -5.3068e-01, -1.3308e-01,
        -1.4837e-01,  3.9950e-02,  5.2396e-02,  4.1660e-01, -4.7404e-01,
         9.1058e-01,  4.1668e-01, -2.8680e-02,  3.0489e-01, -5.8090e-02,
         3.7067e-01,  4.8497e-02,  4.4988e-01], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[-0.0061,  0.0216,  0.0378,  ...,  0.0087, -0.0147, -0.0091],
        [-0.0168, -0.0002,  0.0211,  ..., -0.0099,  0.0221, -0.0031],
        [-0.0080,  0.0057,  0.0215,  ..., -0.0246, -0.0132, -0.0026],
        ...,
        [-0.0203, -0.0034, -0.0117,  ...,  0.0065, -0.0158, -0.0084],
        [ 0.0024,  0.0246, -0.0107,  ...,  0.0023, -0.0117, -0.0079],
        [-0.0057, -0.0211, -0.0082,  ..., -0.0046,  0.0054, -0.0058]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.5361, -0.4248, -0.5415,  ..., -0.5884, -0.7373, -0.2561],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0039, -0.0219, -0.0056,  ...,  0.0122, -0.0174,  0.0043],
        [ 0.0055,  0.0005,  0.0034,  ...,  0.0172, -0.0199,  0.0144],
        [-0.0199, -0.0112, -0.0061,  ...,  0.0151, -0.0054,  0.0031],
        ...,
        [-0.0059,  0.0005,  0.0246,  ..., -0.0153,  0.0092,  0.0027],
        [ 0.0113,  0.0209,  0.0061,  ...,  0.0087,  0.0100, -0.0050],
        [-0.0006,  0.0108,  0.0045,  ...,  0.0284, -0.0053,  0.0162]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-2.9590e-01, -4.0253e-02, -1.9165e-01, -4.5410e-01, -2.3712e-02,
         4.4067e-02,  7.4768e-02,  4.4703e-04, -1.4130e-02,  4.3945e-02,
        -6.3904e-02, -1.5918e-01, -2.5528e-02, -1.0309e-01, -8.4351e-02,
         3.9581e-02, -1.4819e-01,  3.3569e-02,  7.2998e-02, -1.4124e-01,
        -2.0667e-01, -1.0364e-01,  3.0542e-01, -2.5732e-01, -4.1077e-02,
        -8.7158e-02,  9.7717e-02, -1.2891e-01,  1.2901e-02, -4.3030e-02,
        -2.5903e-01,  7.9117e-03, -1.3269e-01, -1.2366e-01, -3.4546e-02,
         1.7041e-01, -2.7969e-02, -5.4169e-02,  7.0862e-02, -2.1399e-01,
         8.4991e-03, -7.7248e-03,  3.8574e-02,  1.6992e-01, -9.3384e-02,
         2.0813e-01,  3.1281e-02,  1.2964e-01,  2.5574e-02, -4.7272e-02,
         1.1328e-01,  3.9856e-02,  1.1298e-01, -4.8553e-02,  3.2776e-02,
         3.4351e-03, -3.6011e-02,  1.0506e-02, -3.4790e-02,  2.0508e-01,
        -5.3040e-02,  4.5258e-02,  1.4977e-02,  3.6804e-02,  2.9343e-02,
         1.5283e-01,  3.9978e-02,  3.1952e-02,  1.9760e-02,  1.4709e-01,
        -1.2500e-01, -7.8857e-02,  1.5894e-01, -7.4463e-02, -6.4331e-02,
         2.6636e-01,  2.1851e-01, -2.4927e-01, -5.5908e-02, -1.9379e-02,
        -1.1237e-01,  3.1555e-02,  6.7291e-03,  1.0150e-01, -1.0187e-01,
         2.2903e-02,  2.2144e-01,  8.0200e-02,  1.8555e-01, -7.0839e-03,
         7.8552e-02, -8.8196e-02,  8.1360e-02,  6.9946e-02, -1.7773e-01,
        -2.0695e-03, -3.5156e-01,  2.8809e-01, -2.0264e-02, -8.1238e-02,
        -2.9739e-02,  1.1401e-01, -2.1606e-02,  1.4795e-01,  8.8072e-04,
        -8.4381e-03,  2.5864e-02,  4.0771e-02, -1.0229e-01, -1.2225e-01,
        -6.2561e-02, -1.0016e-01, -2.7368e-01, -8.6060e-02, -3.1433e-02,
         3.0670e-02,  6.1798e-02,  1.9455e-02, -1.5533e-02,  2.2266e-01,
        -1.4453e-01,  1.1023e-01,  5.7007e-02, -5.2917e-02,  2.1851e-02,
        -2.9083e-02, -2.3743e-02,  1.5674e-01, -1.9775e-01, -2.3056e-02,
        -9.3460e-03,  1.0193e-01, -1.8726e-01, -6.4392e-02, -6.2805e-02,
         7.5562e-02, -1.6760e-01, -1.1688e-01,  2.8015e-02, -1.0919e-01,
        -1.1879e-02, -5.0049e-02,  1.8845e-02, -7.2205e-02,  2.4365e-01,
         2.6855e-02, -1.5388e-02,  6.9763e-02,  5.0232e-02,  1.3477e-01,
        -9.5459e-02, -7.9224e-02,  2.4048e-01, -8.8501e-02, -1.0388e-01,
        -7.4463e-02,  8.4229e-02, -5.0476e-02,  2.0111e-02, -1.8762e-01,
         7.7209e-02,  1.3318e-01,  1.2168e+00, -1.1208e-02,  6.1646e-02,
        -5.8319e-02, -2.2003e-02,  1.2573e-01, -7.0679e-02, -1.4001e-01,
        -1.2064e-03,  2.1530e-02, -2.8473e-02,  2.4551e-02,  3.5461e-02,
         5.5206e-02,  5.6122e-02, -1.0205e-01,  3.4088e-02, -1.1230e-01,
        -2.6147e-01, -4.8431e-02,  1.2146e-01, -6.3721e-02, -4.0527e-02,
         2.0642e-01,  9.1553e-02, -2.6779e-02,  1.3525e-01, -7.9712e-02,
         1.5480e-02,  9.6680e-02, -6.1005e-02, -5.5237e-02,  1.9275e-01,
        -1.2561e-01, -7.4707e-02,  2.5269e-01,  8.6792e-02, -1.7395e-02,
        -5.9387e-02,  6.9580e-02, -7.6782e-02,  2.5879e-02,  3.7842e-02,
        -1.1726e-02,  6.5063e-02,  1.3573e-02,  7.0129e-02,  9.0714e-03,
         4.5776e-02, -6.6040e-02,  6.6650e-02, -4.7852e-02, -1.3025e-01,
         1.4587e-01,  2.0972e-01,  7.6599e-02, -1.8701e-01,  8.8928e-02,
         1.6003e-01, -7.3814e-03, -1.5283e-01, -2.7252e-02,  4.1168e-02,
         3.6621e-02, -6.9092e-02,  3.0716e-02, -6.1432e-02, -3.8330e-02,
        -1.2793e-01, -1.3354e-01,  5.9082e-02, -4.7180e-02, -8.1360e-02,
        -7.0572e-03, -3.7537e-02, -1.4343e-01,  3.2959e-02, -2.9236e-02,
         1.8384e-01,  1.3403e-01, -2.1606e-01,  5.8136e-02, -1.0608e-01,
         2.0752e-01,  1.9458e-01, -2.6337e-02,  2.3779e-01,  2.1301e-02,
        -3.8422e-02,  2.4426e-01, -6.0394e-02, -6.2561e-02, -8.0750e-02,
         4.0680e-02, -7.0810e-04, -1.9019e-01, -1.3342e-01,  4.2023e-02,
         3.3245e-03,  1.8262e-01, -3.8971e-02, -7.0007e-02,  7.7896e-03,
        -1.2781e-01,  2.5903e-01,  7.0557e-02,  1.2024e-01,  1.7273e-01,
         8.8562e-02,  2.0825e-01, -9.3231e-03, -9.8389e-02, -6.7017e-02,
        -8.6426e-02, -1.3452e-01,  2.9614e-01,  1.4542e-02, -1.7590e-01,
        -2.1387e-01,  1.6223e-01, -1.8713e-01, -8.6243e-02, -7.2510e-02,
        -4.6120e-03, -2.8290e-02,  3.9307e-02,  3.2471e-01, -6.1157e-02,
         3.3646e-03, -4.1382e-02, -1.2463e-01,  1.7383e-01, -5.4131e-03,
        -1.9263e-01, -1.0979e-02, -2.4951e-01,  1.4343e-01,  1.7053e-01,
         6.4697e-02,  1.6575e-03, -1.6586e-02,  1.5839e-02,  3.9246e-02,
        -1.6687e-01,  1.7468e-01,  1.8457e-01, -1.1737e-01,  2.4216e-02,
         1.3708e-01,  1.4595e-02,  2.1631e-01, -2.5742e-02,  1.6064e-01,
        -3.2129e-01,  5.4016e-02, -1.0266e-01,  4.7264e-03, -1.2238e-01,
         3.3765e-01, -9.9640e-03, -1.0114e-01,  5.9326e-02, -8.7036e-02,
        -1.0773e-01, -3.5187e-02, -3.4332e-02,  4.6295e-02,  5.4688e-02,
         8.1177e-02,  4.6448e-02,  5.7190e-02, -1.2164e-01, -2.7481e-02,
         1.1151e-01, -2.0630e-01,  1.9028e-02,  1.1517e-01, -3.5431e-02,
        -4.4769e-02, -1.0028e-01,  2.7954e-01,  1.3000e-01,  2.0691e-01,
        -3.9124e-02, -1.0669e-01, -4.2480e-02,  1.7346e-01, -3.8528e-03,
        -1.4026e-01, -1.1115e-01,  5.2338e-03,  7.7087e-02, -3.5645e-02,
         2.6489e-02,  1.3062e-02,  5.5756e-02,  1.2769e-01,  4.7455e-02,
        -8.1726e-02,  3.9520e-02, -2.2339e-02,  1.7590e-01, -4.4365e-03,
        -3.5919e-02,  8.0322e-02,  2.7393e-01, -7.9895e-02, -1.2390e-01,
        -6.1310e-02, -1.6711e-01, -1.7410e-02,  2.0178e-01, -8.9417e-02,
         8.0017e-02, -1.4490e-01,  1.3135e-01, -1.1688e-01, -7.2693e-02,
         4.4037e-02, -2.5314e-02, -1.9446e-01,  9.2590e-02,  9.1858e-03,
        -2.0493e-02,  1.7712e-01,  5.5511e-02, -1.1908e-01,  9.3872e-02,
         2.5749e-03, -7.6721e-02,  3.7098e-03, -9.8648e-03,  9.6680e-02,
        -6.0059e-02, -1.2744e-01,  2.9354e-03, -4.1656e-02,  3.8544e-02,
        -8.4106e-02,  1.1084e-01, -8.6182e-02,  7.4219e-02,  2.6749e-02,
         5.2795e-02, -2.4509e-03, -1.1700e-01, -3.4393e-02,  2.2781e-02,
        -2.9037e-02, -1.1212e-01,  3.5522e-02,  7.7332e-02,  5.1392e-02,
        -3.5065e-02,  5.5115e-02,  6.6452e-03,  8.0078e-02, -1.5393e-01,
        -2.7930e-01,  9.9640e-03, -4.9438e-02, -3.7628e-02, -1.9458e-01,
         5.6793e-02, -1.0315e-01,  4.5074e-02,  1.7365e-02,  2.3651e-02,
         2.1408e-02,  8.1787e-03, -1.7480e-01,  6.2927e-02,  8.3801e-02,
         5.3558e-02,  7.6180e-03, -1.1334e-01, -7.8247e-02,  2.8667e-03,
        -6.3416e-02,  6.2988e-02,  2.1643e-01,  6.2561e-03, -2.1362e-01,
        -5.3986e-02,  1.7725e-01,  1.1670e-01, -1.8994e-01,  1.9653e-01,
         1.9019e-01, -9.4652e-04,  3.6835e-02, -4.3762e-02,  3.2684e-02,
        -1.6992e-01, -1.6174e-01, -1.2720e-01,  4.3610e-02, -1.9141e-01,
         1.6809e-01, -3.8696e-02, -1.2769e-01, -1.7896e-01, -7.1228e-02,
        -1.0086e-02,  1.3794e-01, -1.3940e-01, -2.0279e-02, -8.5999e-02,
         1.5839e-02,  1.9910e-01, -1.0004e-01,  7.7438e-03, -2.9102e+00,
         2.0081e-01,  3.2074e-02, -1.2115e-01,  4.0863e-02, -3.6865e-02,
         2.2018e-02,  6.1249e-02, -9.7168e-02,  5.5695e-02,  1.5930e-01,
        -1.0162e-01,  7.9712e-02,  3.5217e-02,  5.8472e-02,  8.0444e-02,
         6.3599e-02, -1.4801e-02,  1.7357e-03,  3.2153e-01, -5.4321e-02,
         6.0883e-02, -2.1484e-01,  1.0399e-02, -7.3181e-02,  7.9468e-02,
        -6.1401e-02, -3.4302e-01,  4.6875e-02,  7.9468e-02,  8.6914e-02,
         3.0899e-02, -1.9702e-01,  2.4084e-01,  1.0040e-01,  1.2244e-01,
        -1.7847e-01, -1.3257e-01,  9.2712e-02, -6.2286e-02, -8.8318e-02,
         1.8164e-01, -4.6082e-02, -8.9233e-02,  2.8412e-02, -9.6893e-03,
        -1.6504e-01,  6.8970e-02,  3.8116e-02,  2.6367e-01, -2.3645e-01,
         3.8208e-02, -2.3556e-03, -5.4932e-03, -3.3539e-02,  7.7307e-05,
        -1.7676e-01, -1.7505e-01,  2.1347e-02, -1.4832e-02, -1.3879e-01,
        -7.8796e-02, -7.0862e-02, -6.1096e-02, -1.3818e-01,  6.1859e-02,
         3.8086e-02, -1.9897e-01,  1.4941e-01,  1.1169e-01,  1.1578e-01,
        -6.3110e-02,  1.6968e-01,  8.6594e-03, -4.9194e-02, -2.4329e-01,
         1.7261e-01,  4.1504e-02,  1.1957e-01, -1.5649e-01, -1.4307e-01,
        -5.1544e-02,  6.3660e-02,  1.9409e-02,  1.9608e-02,  1.0785e-01,
        -1.7151e-01, -5.6610e-02, -1.0071e-03, -2.2839e-01, -1.1896e-01,
         1.8127e-01, -1.7071e-03, -7.4158e-02, -1.1340e-01,  1.3538e-01,
        -1.1639e-01, -1.1559e-02,  6.2500e-02, -4.6631e-02, -6.3171e-02,
         4.8599e-03, -1.6675e-01, -3.7994e-02,  3.4637e-02,  1.6577e-01,
         9.7595e-02, -2.4826e-02, -3.0766e-03,  1.1060e-01,  1.0114e-01,
         1.2231e-01,  1.2878e-02, -2.4268e-01,  6.8045e-04,  4.6021e-02,
         1.0437e-02, -1.8762e-01, -1.3147e-01,  1.5393e-01,  2.9388e-02,
         2.2205e-01,  2.8610e-02,  3.0365e-02, -3.2257e-02,  8.6731e-02,
        -7.7820e-02, -6.4392e-02, -1.0144e-01, -4.7302e-02, -7.9529e-02,
        -6.4468e-03,  2.5317e-01,  1.1414e-01,  1.2793e-01,  9.6893e-03,
        -1.3931e-02,  4.8950e-02,  6.9275e-02, -4.1779e-02,  1.1096e-01,
         6.3057e-03,  1.3818e-01, -1.8625e-03, -1.6809e-01,  2.0935e-01,
         1.3208e-01, -1.3184e-01, -7.0984e-02,  1.1829e-01,  1.5942e-01,
        -1.6711e-01,  5.1697e-02, -2.3438e-02, -8.4106e-02,  7.5867e-02,
         6.1127e-02,  1.7639e-01,  7.8630e-04, -1.8591e-01, -5.0842e-02,
        -2.4402e-01, -4.5258e-02, -8.4000e-03,  5.9204e-02, -1.2611e-02,
        -2.6184e-02,  5.6488e-02,  1.5857e-01,  3.0322e-01, -1.6931e-01,
         6.1432e-02,  2.2424e-01, -3.1952e-02, -1.4270e-01, -1.4087e-01,
        -1.9455e-02,  9.7778e-02, -2.0703e-01, -2.0337e-01, -1.9699e-02,
        -2.3120e-01, -3.6133e-02,  6.3538e-02,  3.3569e-02,  3.3203e-02,
         4.7455e-02, -7.8613e-02,  2.0178e-01, -2.6016e-02,  1.5186e-01,
        -7.0740e-02,  2.3071e-02,  6.3171e-02,  7.5302e-03, -5.4550e-03,
         1.4429e-03, -1.9873e-01,  8.9844e-02,  3.3966e-02,  1.4978e-01,
        -1.0706e-01, -9.5642e-02, -1.9055e-01, -7.9651e-02, -2.3010e-01,
        -4.8218e-02,  1.1768e-01, -2.6993e-02,  2.4216e-02,  1.2805e-01,
         1.0254e-01, -6.7017e-02,  2.0532e-01, -1.9141e-01, -2.3706e-01,
        -1.5503e-01, -6.0272e-02,  7.2083e-02,  3.3169e-03, -2.0618e-01,
         1.2245e-02, -7.1289e-02,  8.6792e-02,  5.0232e-02, -3.9307e-02,
        -2.7512e-02,  8.1848e-02,  3.9368e-02,  9.4788e-02, -1.5373e-02,
         1.3367e-01, -4.4785e-03,  9.5398e-02, -2.1851e-02,  1.7017e-01,
         6.8481e-02, -3.4454e-02, -8.7036e-02, -1.4661e-01, -8.5938e-02,
        -2.7563e-01,  5.7190e-02, -1.0846e-01, -1.9312e-01, -1.0974e-01,
        -1.2012e-01,  3.4973e-02, -6.6650e-02,  1.8726e-01,  1.7651e-01,
        -4.8431e-02,  7.8247e-02,  3.9001e-02, -9.8755e-02,  9.0149e-02,
         2.0471e-01, -2.8613e-01,  1.0962e-01, -6.2256e-02,  2.3242e-01,
         4.7760e-02, -4.9957e-02, -1.1151e-01,  9.3384e-02,  6.3354e-02,
         1.0455e-01,  1.4001e-01,  1.1639e-01,  3.6774e-02, -1.3770e-01,
        -3.3325e-01, -1.2366e-01, -1.2402e-01,  2.1672e-04, -5.4688e-02,
        -3.9948e-02,  1.3342e-01, -1.2469e-01, -6.0196e-03,  7.2571e-02,
         8.7769e-02, -7.4463e-02, -2.3181e-01, -1.7639e-01, -1.3623e-01,
         8.8013e-02, -1.0242e-01,  5.3772e-02,  3.7933e-02, -1.6785e-01,
         4.8492e-02, -8.1421e-02, -1.2634e-01], device='cuda:0',
       requires_grad=True)]
2024-05-15 09:45:44.714 | INFO     | __main__:train:38 - [Parameter containing:
tensor([[ 0.0140,  0.0086, -0.0101,  ..., -0.0117, -0.0243,  0.0044],
        [-0.0255,  0.0537,  0.0109,  ...,  0.0122, -0.0141, -0.0050],
        [ 0.0052,  0.0088, -0.0233,  ..., -0.0006, -0.0374, -0.0270],
        ...,
        [-0.0082,  0.0195,  0.0208,  ...,  0.0177,  0.0125,  0.0179],
        [ 0.0060,  0.0263,  0.0043,  ...,  0.0071,  0.0258,  0.0259],
        [ 0.0220,  0.0209,  0.0165,  ..., -0.0156,  0.0295, -0.0047]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0138, -0.0109, -0.0047,  ...,  0.0097, -0.0180,  0.0049],
        [-0.0048,  0.0361, -0.0159,  ..., -0.0111,  0.0125,  0.0179],
        [-0.0059,  0.0111, -0.0034,  ..., -0.0119,  0.0029,  0.0209],
        ...,
        [-0.0116,  0.0152, -0.0191,  ...,  0.0415,  0.0013,  0.0277],
        [ 0.0257, -0.0091, -0.0276,  ...,  0.0088,  0.0040,  0.0081],
        [-0.0384,  0.0135, -0.0108,  ..., -0.0070, -0.0051,  0.0077]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0228, -0.0118, -0.0041,  ..., -0.0367,  0.0029,  0.0461],
        [-0.0385,  0.0101,  0.0005,  ..., -0.0171, -0.0031, -0.0187],
        [ 0.0123, -0.0247, -0.0075,  ...,  0.0023,  0.0374, -0.0162],
        ...,
        [ 0.0168, -0.0095, -0.0204,  ...,  0.0013,  0.0051,  0.0040],
        [ 0.0025, -0.0468, -0.0601,  ...,  0.0210, -0.0471,  0.0403],
        [-0.0046,  0.0167, -0.0252,  ...,  0.0092, -0.0288, -0.0052]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0270, -0.0419, -0.0426,  ...,  0.0132, -0.0078,  0.0168],
        [ 0.0090,  0.0196, -0.0313,  ..., -0.0302,  0.0011, -0.0198],
        [ 0.0317, -0.0116, -0.0089,  ...,  0.0257,  0.0005, -0.0264],
        ...,
        [-0.0150, -0.0305,  0.0109,  ...,  0.0053,  0.0420,  0.0390],
        [-0.0012, -0.0393,  0.0149,  ..., -0.0209, -0.0091,  0.0196],
        [-0.0146,  0.0405,  0.0099,  ...,  0.0173, -0.0179, -0.0395]],
       device='cuda:0', requires_grad=True)]
2024-05-15 09:45:55.011 | INFO     | __main__:train:123 - Epoch: [0][0/63]	 loss 9.63819	 cls_loss: 5.4335 cluster_loss: 5.2400 sup_con_loss: 2.5879 contrastive_loss: 5.2688 
2024-05-15 09:46:12.373 | INFO     | __main__:train:123 - Epoch: [0][20/63]	 loss 7.56500	 cls_loss: 2.8793 cluster_loss: 4.4488 sup_con_loss: 1.6864 contrastive_loss: 4.7313 
2024-05-15 09:46:30.082 | INFO     | __main__:train:123 - Epoch: [0][40/63]	 loss 6.19395	 cls_loss: 1.4441 cluster_loss: 3.2038 sup_con_loss: 1.5938 contrastive_loss: 4.6896 
2024-05-15 09:46:45.983 | INFO     | __main__:train:123 - Epoch: [0][60/63]	 loss 5.87958	 cls_loss: 1.3354 cluster_loss: 2.9267 sup_con_loss: 1.3504 contrastive_loss: 4.6726 
2024-05-15 09:46:47.628 | INFO     | __main__:train:126 - Train Epoch: 0 Avg Loss: 7.0485 
2024-05-15 09:46:47.629 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:47:23.613 | INFO     | __main__:train:135 - Train Accuracies: All 0.3934 | Old 0.7094 | New 0.0802
2024-05-15 09:47:30.945 | INFO     | __main__:train:123 - Epoch: [1][0/63]	 loss 5.80356	 cls_loss: 1.2591 cluster_loss: 2.8502 sup_con_loss: 1.3331 contrastive_loss: 4.6826 
2024-05-15 09:47:49.631 | INFO     | __main__:train:123 - Epoch: [1][20/63]	 loss 5.69756	 cls_loss: 1.0598 cluster_loss: 2.8362 sup_con_loss: 1.2751 contrastive_loss: 4.6720 
2024-05-15 09:48:07.159 | INFO     | __main__:train:123 - Epoch: [1][40/63]	 loss 5.70524	 cls_loss: 1.1289 cluster_loss: 2.8805 sup_con_loss: 1.1539 contrastive_loss: 4.6676 
2024-05-15 09:48:23.050 | INFO     | __main__:train:123 - Epoch: [1][60/63]	 loss 5.49542	 cls_loss: 0.9772 cluster_loss: 2.6496 sup_con_loss: 1.1359 contrastive_loss: 4.6671 
2024-05-15 09:48:24.824 | INFO     | __main__:train:126 - Train Epoch: 1 Avg Loss: 5.6575 
2024-05-15 09:48:24.825 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:48:58.359 | INFO     | __main__:train:135 - Train Accuracies: All 0.4482 | Old 0.7796 | New 0.1198
2024-05-15 09:49:06.879 | INFO     | __main__:train:123 - Epoch: [2][0/63]	 loss 5.38975	 cls_loss: 0.9339 cluster_loss: 2.5549 sup_con_loss: 1.0597 contrastive_loss: 4.6636 
2024-05-15 09:49:25.779 | INFO     | __main__:train:123 - Epoch: [2][20/63]	 loss 5.32382	 cls_loss: 0.8954 cluster_loss: 2.4884 sup_con_loss: 1.0053 contrastive_loss: 4.6787 
2024-05-15 09:49:43.391 | INFO     | __main__:train:123 - Epoch: [2][40/63]	 loss 5.40343	 cls_loss: 0.9234 cluster_loss: 2.5195 sup_con_loss: 1.1945 contrastive_loss: 4.6531 
2024-05-15 09:49:59.271 | INFO     | __main__:train:123 - Epoch: [2][60/63]	 loss 5.39580	 cls_loss: 0.8716 cluster_loss: 2.4613 sup_con_loss: 1.3048 contrastive_loss: 4.6681 
2024-05-15 09:50:00.953 | INFO     | __main__:train:126 - Train Epoch: 2 Avg Loss: 5.4335 
2024-05-15 09:50:00.954 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:50:34.483 | INFO     | __main__:train:135 - Train Accuracies: All 0.5052 | Old 0.7896 | New 0.2233
2024-05-15 09:50:41.347 | INFO     | __main__:train:123 - Epoch: [3][0/63]	 loss 5.46258	 cls_loss: 0.9124 cluster_loss: 2.4535 sup_con_loss: 1.4725 contrastive_loss: 4.6664 
2024-05-15 09:51:00.367 | INFO     | __main__:train:123 - Epoch: [3][20/63]	 loss 5.29494	 cls_loss: 0.8549 cluster_loss: 2.4051 sup_con_loss: 1.1590 contrastive_loss: 4.6565 
2024-05-15 09:51:18.734 | INFO     | __main__:train:123 - Epoch: [3][40/63]	 loss 5.25683	 cls_loss: 0.8307 cluster_loss: 2.3275 sup_con_loss: 1.2146 contrastive_loss: 4.6586 
2024-05-15 09:51:35.003 | INFO     | __main__:train:123 - Epoch: [3][60/63]	 loss 5.21914	 cls_loss: 0.8204 cluster_loss: 2.3412 sup_con_loss: 1.0811 contrastive_loss: 4.6643 
2024-05-15 09:51:36.665 | INFO     | __main__:train:126 - Train Epoch: 3 Avg Loss: 5.2866 
2024-05-15 09:51:36.665 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:52:09.692 | INFO     | __main__:train:135 - Train Accuracies: All 0.5540 | Old 0.8131 | New 0.2974
2024-05-15 09:52:16.837 | INFO     | __main__:train:123 - Epoch: [4][0/63]	 loss 5.17596	 cls_loss: 0.8156 cluster_loss: 2.2706 sup_con_loss: 1.0970 contrastive_loss: 4.6626 
2024-05-15 09:52:35.872 | INFO     | __main__:train:123 - Epoch: [4][20/63]	 loss 5.16618	 cls_loss: 0.8452 cluster_loss: 2.2993 sup_con_loss: 0.9830 contrastive_loss: 4.6643 
2024-05-15 09:52:53.984 | INFO     | __main__:train:123 - Epoch: [4][40/63]	 loss 5.10253	 cls_loss: 0.8223 cluster_loss: 2.2199 sup_con_loss: 0.9683 contrastive_loss: 4.6659 
2024-05-15 09:53:09.777 | INFO     | __main__:train:123 - Epoch: [4][60/63]	 loss 5.10426	 cls_loss: 0.8003 cluster_loss: 2.1265 sup_con_loss: 1.1948 contrastive_loss: 4.6520 
2024-05-15 09:53:11.485 | INFO     | __main__:train:126 - Train Epoch: 4 Avg Loss: 5.1675 
2024-05-15 09:53:11.486 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:53:44.074 | INFO     | __main__:train:135 - Train Accuracies: All 0.6007 | Old 0.8268 | New 0.3766
2024-05-15 09:53:51.509 | INFO     | __main__:train:123 - Epoch: [5][0/63]	 loss 5.21371	 cls_loss: 0.9144 cluster_loss: 2.2105 sup_con_loss: 1.2179 contrastive_loss: 4.6624 
2024-05-15 09:54:10.663 | INFO     | __main__:train:123 - Epoch: [5][20/63]	 loss 5.00414	 cls_loss: 0.7200 cluster_loss: 2.0144 sup_con_loss: 1.1793 contrastive_loss: 4.6616 
2024-05-15 09:54:29.055 | INFO     | __main__:train:123 - Epoch: [5][40/63]	 loss 5.09475	 cls_loss: 0.8154 cluster_loss: 2.0182 sup_con_loss: 1.3633 contrastive_loss: 4.6468 
2024-05-15 09:54:44.657 | INFO     | __main__:train:123 - Epoch: [5][60/63]	 loss 4.91626	 cls_loss: 0.7303 cluster_loss: 1.9940 sup_con_loss: 0.9637 contrastive_loss: 4.6573 
2024-05-15 09:54:46.337 | INFO     | __main__:train:126 - Train Epoch: 5 Avg Loss: 5.0610 
2024-05-15 09:54:46.337 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:55:20.071 | INFO     | __main__:train:135 - Train Accuracies: All 0.6260 | Old 0.8206 | New 0.4333
2024-05-15 09:55:28.015 | INFO     | __main__:train:123 - Epoch: [6][0/63]	 loss 5.02843	 cls_loss: 0.8097 cluster_loss: 2.0713 sup_con_loss: 1.0502 contrastive_loss: 4.6632 
2024-05-15 09:55:46.672 | INFO     | __main__:train:123 - Epoch: [6][20/63]	 loss 4.97832	 cls_loss: 0.8173 cluster_loss: 2.0184 sup_con_loss: 1.0171 contrastive_loss: 4.6528 
2024-05-15 09:56:03.769 | INFO     | __main__:train:123 - Epoch: [6][40/63]	 loss 5.07275	 cls_loss: 0.8030 cluster_loss: 2.0996 sup_con_loss: 1.1518 contrastive_loss: 4.6520 
2024-05-15 09:56:20.105 | INFO     | __main__:train:123 - Epoch: [6][60/63]	 loss 4.92108	 cls_loss: 0.7455 cluster_loss: 2.0467 sup_con_loss: 0.8462 contrastive_loss: 4.6671 
2024-05-15 09:56:21.813 | INFO     | __main__:train:126 - Train Epoch: 6 Avg Loss: 5.0030 
2024-05-15 09:56:21.813 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:56:55.924 | INFO     | __main__:train:135 - Train Accuracies: All 0.6477 | Old 0.8233 | New 0.4736
2024-05-15 09:57:02.960 | INFO     | __main__:train:123 - Epoch: [7][0/63]	 loss 5.13161	 cls_loss: 0.8461 cluster_loss: 2.0648 sup_con_loss: 1.3229 contrastive_loss: 4.6621 
2024-05-15 09:57:22.961 | INFO     | __main__:train:123 - Epoch: [7][20/63]	 loss 4.95717	 cls_loss: 0.8080 cluster_loss: 1.9099 sup_con_loss: 1.1706 contrastive_loss: 4.6511 
2024-05-15 09:57:41.470 | INFO     | __main__:train:123 - Epoch: [7][40/63]	 loss 4.87398	 cls_loss: 0.7364 cluster_loss: 1.8862 sup_con_loss: 1.0668 contrastive_loss: 4.6413 
2024-05-15 09:57:57.811 | INFO     | __main__:train:123 - Epoch: [7][60/63]	 loss 5.14037	 cls_loss: 0.7581 cluster_loss: 2.0383 sup_con_loss: 1.4900 contrastive_loss: 4.6595 
2024-05-15 09:57:59.594 | INFO     | __main__:train:126 - Train Epoch: 7 Avg Loss: 4.9541 
2024-05-15 09:57:59.594 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 09:58:34.212 | INFO     | __main__:train:135 - Train Accuracies: All 0.6622 | Old 0.8291 | New 0.4969
2024-05-15 09:58:42.425 | INFO     | __main__:train:123 - Epoch: [8][0/63]	 loss 4.92635	 cls_loss: 0.7265 cluster_loss: 1.8915 sup_con_loss: 1.1951 contrastive_loss: 4.6527 
2024-05-15 09:59:01.156 | INFO     | __main__:train:123 - Epoch: [8][20/63]	 loss 4.83156	 cls_loss: 0.7486 cluster_loss: 1.9043 sup_con_loss: 0.8715 contrastive_loss: 4.6565 
2024-05-15 09:59:18.165 | INFO     | __main__:train:123 - Epoch: [8][40/63]	 loss 4.93435	 cls_loss: 0.7787 cluster_loss: 1.8968 sup_con_loss: 1.1587 contrastive_loss: 4.6513 
2024-05-15 09:59:33.689 | INFO     | __main__:train:123 - Epoch: [8][60/63]	 loss 4.99764	 cls_loss: 0.7158 cluster_loss: 1.9821 sup_con_loss: 1.2393 contrastive_loss: 4.6539 
2024-05-15 09:59:35.391 | INFO     | __main__:train:126 - Train Epoch: 8 Avg Loss: 4.9058 
2024-05-15 09:59:35.392 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:00:07.436 | INFO     | __main__:train:135 - Train Accuracies: All 0.6825 | Old 0.8341 | New 0.5323
2024-05-15 10:00:15.154 | INFO     | __main__:train:123 - Epoch: [9][0/63]	 loss 4.72164	 cls_loss: 0.6615 cluster_loss: 1.8015 sup_con_loss: 0.8529 contrastive_loss: 4.6471 
2024-05-15 10:00:33.486 | INFO     | __main__:train:123 - Epoch: [9][20/63]	 loss 4.85907	 cls_loss: 0.7517 cluster_loss: 1.9351 sup_con_loss: 0.8850 contrastive_loss: 4.6590 
2024-05-15 10:00:51.249 | INFO     | __main__:train:123 - Epoch: [9][40/63]	 loss 4.93041	 cls_loss: 0.7842 cluster_loss: 1.9039 sup_con_loss: 1.1261 contrastive_loss: 4.6527 
2024-05-15 10:01:07.085 | INFO     | __main__:train:123 - Epoch: [9][60/63]	 loss 4.80726	 cls_loss: 0.7235 cluster_loss: 1.8065 sup_con_loss: 1.0112 contrastive_loss: 4.6552 
2024-05-15 10:01:08.795 | INFO     | __main__:train:126 - Train Epoch: 9 Avg Loss: 4.8264 
2024-05-15 10:01:08.796 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:01:41.815 | INFO     | __main__:train:135 - Train Accuracies: All 0.7005 | Old 0.8408 | New 0.5615
2024-05-15 10:01:49.982 | INFO     | __main__:train:123 - Epoch: [10][0/63]	 loss 4.76235	 cls_loss: 0.7506 cluster_loss: 1.7666 sup_con_loss: 0.9590 contrastive_loss: 4.6395 
2024-05-15 10:02:07.518 | INFO     | __main__:train:123 - Epoch: [10][20/63]	 loss 4.84281	 cls_loss: 0.7778 cluster_loss: 1.8266 sup_con_loss: 1.0312 contrastive_loss: 4.6499 
2024-05-15 10:02:25.283 | INFO     | __main__:train:123 - Epoch: [10][40/63]	 loss 4.69012	 cls_loss: 0.6555 cluster_loss: 1.7351 sup_con_loss: 0.8873 contrastive_loss: 4.6497 
2024-05-15 10:02:40.765 | INFO     | __main__:train:123 - Epoch: [10][60/63]	 loss 4.73490	 cls_loss: 0.7406 cluster_loss: 1.7377 sup_con_loss: 0.9294 contrastive_loss: 4.6475 
2024-05-15 10:02:42.549 | INFO     | __main__:train:126 - Train Epoch: 10 Avg Loss: 4.7937 
2024-05-15 10:02:42.550 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:03:15.015 | INFO     | __main__:train:135 - Train Accuracies: All 0.7081 | Old 0.8373 | New 0.5801
2024-05-15 10:03:22.210 | INFO     | __main__:train:123 - Epoch: [11][0/63]	 loss 4.71027	 cls_loss: 0.6391 cluster_loss: 1.7096 sup_con_loss: 1.0093 contrastive_loss: 4.6493 
2024-05-15 10:03:39.850 | INFO     | __main__:train:123 - Epoch: [11][20/63]	 loss 4.86274	 cls_loss: 0.6869 cluster_loss: 1.9579 sup_con_loss: 0.8937 contrastive_loss: 4.6722 
2024-05-15 10:03:57.577 | INFO     | __main__:train:123 - Epoch: [11][40/63]	 loss 4.83170	 cls_loss: 0.6731 cluster_loss: 1.8113 sup_con_loss: 1.1257 contrastive_loss: 4.6536 
2024-05-15 10:04:13.212 | INFO     | __main__:train:123 - Epoch: [11][60/63]	 loss 4.85167	 cls_loss: 0.7041 cluster_loss: 1.8280 sup_con_loss: 1.1313 contrastive_loss: 4.6478 
2024-05-15 10:04:14.910 | INFO     | __main__:train:126 - Train Epoch: 11 Avg Loss: 4.7516 
2024-05-15 10:04:14.911 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:04:47.304 | INFO     | __main__:train:135 - Train Accuracies: All 0.7253 | Old 0.8406 | New 0.6110
2024-05-15 10:04:54.438 | INFO     | __main__:train:123 - Epoch: [12][0/63]	 loss 4.67089	 cls_loss: 0.6378 cluster_loss: 1.6936 sup_con_loss: 0.9430 contrastive_loss: 4.6412 
2024-05-15 10:05:12.374 | INFO     | __main__:train:123 - Epoch: [12][20/63]	 loss 4.80213	 cls_loss: 0.6787 cluster_loss: 1.8059 sup_con_loss: 1.0522 contrastive_loss: 4.6500 
2024-05-15 10:05:30.264 | INFO     | __main__:train:123 - Epoch: [12][40/63]	 loss 4.67635	 cls_loss: 0.6464 cluster_loss: 1.6612 sup_con_loss: 1.0171 contrastive_loss: 4.6375 
2024-05-15 10:05:46.320 | INFO     | __main__:train:123 - Epoch: [12][60/63]	 loss 4.71793	 cls_loss: 0.6321 cluster_loss: 1.7319 sup_con_loss: 0.9850 contrastive_loss: 4.6557 
2024-05-15 10:05:48.053 | INFO     | __main__:train:126 - Train Epoch: 12 Avg Loss: 4.7192 
2024-05-15 10:05:48.055 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:06:21.020 | INFO     | __main__:train:135 - Train Accuracies: All 0.7337 | Old 0.8406 | New 0.6279
2024-05-15 10:06:28.451 | INFO     | __main__:train:123 - Epoch: [13][0/63]	 loss 4.64793	 cls_loss: 0.6823 cluster_loss: 1.6799 sup_con_loss: 0.8403 contrastive_loss: 4.6509 
2024-05-15 10:06:47.602 | INFO     | __main__:train:123 - Epoch: [13][20/63]	 loss 4.67300	 cls_loss: 0.7042 cluster_loss: 1.6676 sup_con_loss: 0.9148 contrastive_loss: 4.6499 
2024-05-15 10:07:04.671 | INFO     | __main__:train:123 - Epoch: [13][40/63]	 loss 4.64534	 cls_loss: 0.7030 cluster_loss: 1.6600 sup_con_loss: 0.8547 contrastive_loss: 4.6479 
2024-05-15 10:07:20.668 | INFO     | __main__:train:123 - Epoch: [13][60/63]	 loss 4.57849	 cls_loss: 0.6356 cluster_loss: 1.6551 sup_con_loss: 0.7467 contrastive_loss: 4.6445 
2024-05-15 10:07:22.379 | INFO     | __main__:train:126 - Train Epoch: 13 Avg Loss: 4.6568 
2024-05-15 10:07:22.380 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:07:54.949 | INFO     | __main__:train:135 - Train Accuracies: All 0.7351 | Old 0.8428 | New 0.6284
2024-05-15 10:08:02.635 | INFO     | __main__:train:123 - Epoch: [14][0/63]	 loss 4.53562	 cls_loss: 0.6699 cluster_loss: 1.6744 sup_con_loss: 0.5623 contrastive_loss: 4.6400 
2024-05-15 10:08:20.404 | INFO     | __main__:train:123 - Epoch: [14][20/63]	 loss 4.57869	 cls_loss: 0.6548 cluster_loss: 1.5556 sup_con_loss: 0.9036 contrastive_loss: 4.6495 
2024-05-15 10:08:38.100 | INFO     | __main__:train:123 - Epoch: [14][40/63]	 loss 4.70108	 cls_loss: 0.6558 cluster_loss: 1.6967 sup_con_loss: 0.9858 contrastive_loss: 4.6518 
2024-05-15 10:08:54.723 | INFO     | __main__:train:123 - Epoch: [14][60/63]	 loss 4.69381	 cls_loss: 0.7283 cluster_loss: 1.6633 sup_con_loss: 0.9595 contrastive_loss: 4.6492 
2024-05-15 10:08:56.458 | INFO     | __main__:train:126 - Train Epoch: 14 Avg Loss: 4.6392 
2024-05-15 10:08:56.458 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:09:31.831 | INFO     | __main__:train:135 - Train Accuracies: All 0.7412 | Old 0.8373 | New 0.6460
2024-05-15 10:09:40.298 | INFO     | __main__:train:123 - Epoch: [15][0/63]	 loss 4.56122	 cls_loss: 0.6505 cluster_loss: 1.5475 sup_con_loss: 0.8807 contrastive_loss: 4.6452 
2024-05-15 10:09:58.620 | INFO     | __main__:train:123 - Epoch: [15][20/63]	 loss 4.66267	 cls_loss: 0.6292 cluster_loss: 1.6509 sup_con_loss: 0.9996 contrastive_loss: 4.6453 
2024-05-15 10:10:17.224 | INFO     | __main__:train:123 - Epoch: [15][40/63]	 loss 4.46007	 cls_loss: 0.6176 cluster_loss: 1.4784 sup_con_loss: 0.7708 contrastive_loss: 4.6356 
2024-05-15 10:10:33.896 | INFO     | __main__:train:123 - Epoch: [15][60/63]	 loss 4.53130	 cls_loss: 0.6450 cluster_loss: 1.5463 sup_con_loss: 0.8105 contrastive_loss: 4.6412 
2024-05-15 10:10:35.692 | INFO     | __main__:train:126 - Train Epoch: 15 Avg Loss: 4.6064 
2024-05-15 10:10:35.692 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:11:09.344 | INFO     | __main__:train:135 - Train Accuracies: All 0.7504 | Old 0.8441 | New 0.6576
2024-05-15 10:11:16.687 | INFO     | __main__:train:123 - Epoch: [16][0/63]	 loss 4.70024	 cls_loss: 0.5931 cluster_loss: 1.6766 sup_con_loss: 1.0867 contrastive_loss: 4.6500 
2024-05-15 10:11:35.304 | INFO     | __main__:train:123 - Epoch: [16][20/63]	 loss 4.54443	 cls_loss: 0.6096 cluster_loss: 1.5036 sup_con_loss: 0.9539 contrastive_loss: 4.6459 
2024-05-15 10:11:53.862 | INFO     | __main__:train:123 - Epoch: [16][40/63]	 loss 4.57717	 cls_loss: 0.6245 cluster_loss: 1.6194 sup_con_loss: 0.8098 contrastive_loss: 4.6501 
2024-05-15 10:12:09.811 | INFO     | __main__:train:123 - Epoch: [16][60/63]	 loss 4.52768	 cls_loss: 0.5964 cluster_loss: 1.5586 sup_con_loss: 0.8179 contrastive_loss: 4.6456 
2024-05-15 10:12:11.585 | INFO     | __main__:train:126 - Train Epoch: 16 Avg Loss: 4.5889 
2024-05-15 10:12:11.586 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:12:46.358 | INFO     | __main__:train:135 - Train Accuracies: All 0.7535 | Old 0.8401 | New 0.6677
2024-05-15 10:12:53.231 | INFO     | __main__:train:123 - Epoch: [17][0/63]	 loss 4.61022	 cls_loss: 0.6405 cluster_loss: 1.5635 sup_con_loss: 1.0192 contrastive_loss: 4.6354 
2024-05-15 10:13:12.386 | INFO     | __main__:train:123 - Epoch: [17][20/63]	 loss 4.60566	 cls_loss: 0.6958 cluster_loss: 1.6150 sup_con_loss: 0.8365 contrastive_loss: 4.6455 
2024-05-15 10:13:30.858 | INFO     | __main__:train:123 - Epoch: [17][40/63]	 loss 4.49663	 cls_loss: 0.6204 cluster_loss: 1.4908 sup_con_loss: 0.8485 contrastive_loss: 4.6361 
2024-05-15 10:13:46.904 | INFO     | __main__:train:123 - Epoch: [17][60/63]	 loss 4.52857	 cls_loss: 0.6539 cluster_loss: 1.5011 sup_con_loss: 0.8697 contrastive_loss: 4.6456 
2024-05-15 10:13:48.645 | INFO     | __main__:train:126 - Train Epoch: 17 Avg Loss: 4.5500 
2024-05-15 10:13:48.646 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:14:22.775 | INFO     | __main__:train:135 - Train Accuracies: All 0.7555 | Old 0.8401 | New 0.6717
2024-05-15 10:14:30.251 | INFO     | __main__:train:123 - Epoch: [18][0/63]	 loss 4.58352	 cls_loss: 0.6561 cluster_loss: 1.6271 sup_con_loss: 0.7802 contrastive_loss: 4.6510 
2024-05-15 10:14:48.696 | INFO     | __main__:train:123 - Epoch: [18][20/63]	 loss 4.63923	 cls_loss: 0.6285 cluster_loss: 1.5283 sup_con_loss: 1.1669 contrastive_loss: 4.6422 
2024-05-15 10:15:06.937 | INFO     | __main__:train:123 - Epoch: [18][40/63]	 loss 4.48557	 cls_loss: 0.5601 cluster_loss: 1.5561 sup_con_loss: 0.7555 contrastive_loss: 4.6364 
2024-05-15 10:15:23.069 | INFO     | __main__:train:123 - Epoch: [18][60/63]	 loss 4.48528	 cls_loss: 0.5319 cluster_loss: 1.5410 sup_con_loss: 0.7939 contrastive_loss: 4.6455 
2024-05-15 10:15:24.865 | INFO     | __main__:train:126 - Train Epoch: 18 Avg Loss: 4.5196 
2024-05-15 10:15:24.866 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:15:58.478 | INFO     | __main__:train:135 - Train Accuracies: All 0.7621 | Old 0.8453 | New 0.6796
2024-05-15 10:16:07.024 | INFO     | __main__:train:123 - Epoch: [19][0/63]	 loss 4.50528	 cls_loss: 0.5881 cluster_loss: 1.5290 sup_con_loss: 0.8259 contrastive_loss: 4.6407 
2024-05-15 10:16:25.461 | INFO     | __main__:train:123 - Epoch: [19][20/63]	 loss 4.67204	 cls_loss: 0.6505 cluster_loss: 1.5404 sup_con_loss: 1.2076 contrastive_loss: 4.6468 
2024-05-15 10:16:43.928 | INFO     | __main__:train:123 - Epoch: [19][40/63]	 loss 4.63265	 cls_loss: 0.6822 cluster_loss: 1.5768 sup_con_loss: 1.0138 contrastive_loss: 4.6372 
2024-05-15 10:16:59.918 | INFO     | __main__:train:123 - Epoch: [19][60/63]	 loss 4.46935	 cls_loss: 0.6354 cluster_loss: 1.4601 sup_con_loss: 0.7978 contrastive_loss: 4.6441 
2024-05-15 10:17:01.622 | INFO     | __main__:train:126 - Train Epoch: 19 Avg Loss: 4.5128 
2024-05-15 10:17:01.623 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:17:36.536 | INFO     | __main__:train:135 - Train Accuracies: All 0.7657 | Old 0.8503 | New 0.6819
2024-05-15 10:17:44.129 | INFO     | __main__:train:123 - Epoch: [20][0/63]	 loss 4.43474	 cls_loss: 0.5389 cluster_loss: 1.4326 sup_con_loss: 0.8587 contrastive_loss: 4.6376 
2024-05-15 10:18:02.943 | INFO     | __main__:train:123 - Epoch: [20][20/63]	 loss 4.42159	 cls_loss: 0.5469 cluster_loss: 1.4117 sup_con_loss: 0.8598 contrastive_loss: 4.6333 
2024-05-15 10:18:20.801 | INFO     | __main__:train:123 - Epoch: [20][40/63]	 loss 4.40086	 cls_loss: 0.5763 cluster_loss: 1.4731 sup_con_loss: 0.6304 contrastive_loss: 4.6478 
2024-05-15 10:18:36.595 | INFO     | __main__:train:123 - Epoch: [20][60/63]	 loss 4.60114	 cls_loss: 0.6099 cluster_loss: 1.5604 sup_con_loss: 1.0129 contrastive_loss: 4.6444 
2024-05-15 10:18:38.406 | INFO     | __main__:train:126 - Train Epoch: 20 Avg Loss: 4.4712 
2024-05-15 10:18:38.407 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:19:11.588 | INFO     | __main__:train:135 - Train Accuracies: All 0.7728 | Old 0.8543 | New 0.6920
2024-05-15 10:19:18.858 | INFO     | __main__:train:123 - Epoch: [21][0/63]	 loss 4.25100	 cls_loss: 0.4820 cluster_loss: 1.3912 sup_con_loss: 0.4583 contrastive_loss: 4.6425 
2024-05-15 10:19:37.398 | INFO     | __main__:train:123 - Epoch: [21][20/63]	 loss 4.50819	 cls_loss: 0.5410 cluster_loss: 1.4966 sup_con_loss: 0.9379 contrastive_loss: 4.6427 
2024-05-15 10:19:55.781 | INFO     | __main__:train:123 - Epoch: [21][40/63]	 loss 4.60109	 cls_loss: 0.5361 cluster_loss: 1.5372 sup_con_loss: 1.1290 contrastive_loss: 4.6448 
2024-05-15 10:20:12.159 | INFO     | __main__:train:123 - Epoch: [21][60/63]	 loss 4.40520	 cls_loss: 0.5879 cluster_loss: 1.3391 sup_con_loss: 0.9151 contrastive_loss: 4.6288 
2024-05-15 10:20:13.857 | INFO     | __main__:train:126 - Train Epoch: 21 Avg Loss: 4.4629 
2024-05-15 10:20:13.857 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:20:47.793 | INFO     | __main__:train:135 - Train Accuracies: All 0.7744 | Old 0.8466 | New 0.7029
2024-05-15 10:20:55.866 | INFO     | __main__:train:123 - Epoch: [22][0/63]	 loss 4.41737	 cls_loss: 0.5495 cluster_loss: 1.4154 sup_con_loss: 0.8379 contrastive_loss: 4.6336 
2024-05-15 10:21:14.305 | INFO     | __main__:train:123 - Epoch: [22][20/63]	 loss 4.53288	 cls_loss: 0.6091 cluster_loss: 1.4278 sup_con_loss: 1.0737 contrastive_loss: 4.6398 
2024-05-15 10:21:32.549 | INFO     | __main__:train:123 - Epoch: [22][40/63]	 loss 4.47437	 cls_loss: 0.5028 cluster_loss: 1.4324 sup_con_loss: 1.0058 contrastive_loss: 4.6389 
2024-05-15 10:21:48.892 | INFO     | __main__:train:123 - Epoch: [22][60/63]	 loss 4.41496	 cls_loss: 0.5676 cluster_loss: 1.4901 sup_con_loss: 0.6614 contrastive_loss: 4.6403 
2024-05-15 10:21:50.618 | INFO     | __main__:train:126 - Train Epoch: 22 Avg Loss: 4.4385 
2024-05-15 10:21:50.618 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:22:25.767 | INFO     | __main__:train:135 - Train Accuracies: All 0.7799 | Old 0.8553 | New 0.7051
2024-05-15 10:22:33.396 | INFO     | __main__:train:123 - Epoch: [23][0/63]	 loss 4.40257	 cls_loss: 0.5094 cluster_loss: 1.3842 sup_con_loss: 0.8891 contrastive_loss: 4.6359 
2024-05-15 10:22:52.238 | INFO     | __main__:train:123 - Epoch: [23][20/63]	 loss 4.38420	 cls_loss: 0.5335 cluster_loss: 1.3946 sup_con_loss: 0.8039 contrastive_loss: 4.6301 
2024-05-15 10:23:10.217 | INFO     | __main__:train:123 - Epoch: [23][40/63]	 loss 4.42975	 cls_loss: 0.5303 cluster_loss: 1.4427 sup_con_loss: 0.8323 contrastive_loss: 4.6386 
2024-05-15 10:23:26.703 | INFO     | __main__:train:123 - Epoch: [23][60/63]	 loss 4.40643	 cls_loss: 0.5406 cluster_loss: 1.4050 sup_con_loss: 0.8181 contrastive_loss: 4.6425 
2024-05-15 10:23:28.474 | INFO     | __main__:train:126 - Train Epoch: 23 Avg Loss: 4.4031 
2024-05-15 10:23:28.475 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:24:01.748 | INFO     | __main__:train:135 - Train Accuracies: All 0.7860 | Old 0.8531 | New 0.7195
2024-05-15 10:24:09.746 | INFO     | __main__:train:123 - Epoch: [24][0/63]	 loss 4.47955	 cls_loss: 0.5534 cluster_loss: 1.5607 sup_con_loss: 0.7129 contrastive_loss: 4.6490 
2024-05-15 10:24:28.322 | INFO     | __main__:train:123 - Epoch: [24][20/63]	 loss 4.42896	 cls_loss: 0.5433 cluster_loss: 1.4053 sup_con_loss: 0.8901 contrastive_loss: 4.6367 
2024-05-15 10:24:46.409 | INFO     | __main__:train:123 - Epoch: [24][40/63]	 loss 4.52990	 cls_loss: 0.6452 cluster_loss: 1.4756 sup_con_loss: 0.9174 contrastive_loss: 4.6520 
2024-05-15 10:25:02.527 | INFO     | __main__:train:123 - Epoch: [24][60/63]	 loss 4.36095	 cls_loss: 0.5701 cluster_loss: 1.4135 sup_con_loss: 0.6727 contrastive_loss: 4.6264 
2024-05-15 10:25:04.298 | INFO     | __main__:train:126 - Train Epoch: 24 Avg Loss: 4.3995 
2024-05-15 10:25:04.299 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:25:38.458 | INFO     | __main__:train:135 - Train Accuracies: All 0.7820 | Old 0.8553 | New 0.7093
2024-05-15 10:25:45.915 | INFO     | __main__:train:123 - Epoch: [25][0/63]	 loss 4.47282	 cls_loss: 0.5582 cluster_loss: 1.4404 sup_con_loss: 0.9501 contrastive_loss: 4.6287 
2024-05-15 10:26:03.977 | INFO     | __main__:train:123 - Epoch: [25][20/63]	 loss 4.23367	 cls_loss: 0.5075 cluster_loss: 1.2819 sup_con_loss: 0.6149 contrastive_loss: 4.6271 
2024-05-15 10:26:22.493 | INFO     | __main__:train:123 - Epoch: [25][40/63]	 loss 4.41255	 cls_loss: 0.5317 cluster_loss: 1.3873 sup_con_loss: 0.8965 contrastive_loss: 4.6322 
2024-05-15 10:26:38.522 | INFO     | __main__:train:123 - Epoch: [25][60/63]	 loss 4.38786	 cls_loss: 0.5505 cluster_loss: 1.3919 sup_con_loss: 0.7995 contrastive_loss: 4.6318 
2024-05-15 10:26:40.264 | INFO     | __main__:train:126 - Train Epoch: 25 Avg Loss: 4.3903 
2024-05-15 10:26:40.265 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:27:13.146 | INFO     | __main__:train:135 - Train Accuracies: All 0.7811 | Old 0.8523 | New 0.7106
2024-05-15 10:27:20.539 | INFO     | __main__:train:123 - Epoch: [26][0/63]	 loss 4.38623	 cls_loss: 0.5284 cluster_loss: 1.2882 sup_con_loss: 1.0256 contrastive_loss: 4.6231 
2024-05-15 10:27:39.318 | INFO     | __main__:train:123 - Epoch: [26][20/63]	 loss 4.36796	 cls_loss: 0.5378 cluster_loss: 1.3969 sup_con_loss: 0.7319 contrastive_loss: 4.6393 
2024-05-15 10:27:57.639 | INFO     | __main__:train:123 - Epoch: [26][40/63]	 loss 4.35648	 cls_loss: 0.4704 cluster_loss: 1.3141 sup_con_loss: 0.9505 contrastive_loss: 4.6231 
2024-05-15 10:28:13.343 | INFO     | __main__:train:123 - Epoch: [26][60/63]	 loss 4.28770	 cls_loss: 0.4926 cluster_loss: 1.3208 sup_con_loss: 0.7049 contrastive_loss: 4.6309 
2024-05-15 10:28:15.065 | INFO     | __main__:train:126 - Train Epoch: 26 Avg Loss: 4.3549 
2024-05-15 10:28:15.066 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:28:48.957 | INFO     | __main__:train:135 - Train Accuracies: All 0.7842 | Old 0.8558 | New 0.7133
2024-05-15 10:28:57.117 | INFO     | __main__:train:123 - Epoch: [27][0/63]	 loss 4.57795	 cls_loss: 0.6236 cluster_loss: 1.4639 sup_con_loss: 1.1207 contrastive_loss: 4.6399 
2024-05-15 10:29:15.759 | INFO     | __main__:train:123 - Epoch: [27][20/63]	 loss 4.26368	 cls_loss: 0.5571 cluster_loss: 1.3283 sup_con_loss: 0.5606 contrastive_loss: 4.6293 
2024-05-15 10:29:33.993 | INFO     | __main__:train:123 - Epoch: [27][40/63]	 loss 4.34464	 cls_loss: 0.5146 cluster_loss: 1.3288 sup_con_loss: 0.8301 contrastive_loss: 4.6312 
2024-05-15 10:29:50.065 | INFO     | __main__:train:123 - Epoch: [27][60/63]	 loss 4.24955	 cls_loss: 0.4526 cluster_loss: 1.3357 sup_con_loss: 0.5890 contrastive_loss: 4.6411 
2024-05-15 10:29:51.861 | INFO     | __main__:train:126 - Train Epoch: 27 Avg Loss: 4.3502 
2024-05-15 10:29:51.862 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:30:25.959 | INFO     | __main__:train:135 - Train Accuracies: All 0.7885 | Old 0.8578 | New 0.7197
2024-05-15 10:30:33.411 | INFO     | __main__:train:123 - Epoch: [28][0/63]	 loss 4.23042	 cls_loss: 0.5227 cluster_loss: 1.2532 sup_con_loss: 0.6283 contrastive_loss: 4.6355 
2024-05-15 10:30:52.044 | INFO     | __main__:train:123 - Epoch: [28][20/63]	 loss 4.42783	 cls_loss: 0.4619 cluster_loss: 1.4421 sup_con_loss: 0.9063 contrastive_loss: 4.6332 
2024-05-15 10:31:10.328 | INFO     | __main__:train:123 - Epoch: [28][40/63]	 loss 4.26757	 cls_loss: 0.4622 cluster_loss: 1.2885 sup_con_loss: 0.7333 contrastive_loss: 4.6332 
2024-05-15 10:31:26.478 | INFO     | __main__:train:123 - Epoch: [28][60/63]	 loss 4.27901	 cls_loss: 0.5154 cluster_loss: 1.3087 sup_con_loss: 0.6829 contrastive_loss: 4.6292 
2024-05-15 10:31:28.237 | INFO     | __main__:train:126 - Train Epoch: 28 Avg Loss: 4.3419 
2024-05-15 10:31:28.237 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:32:01.471 | INFO     | __main__:train:135 - Train Accuracies: All 0.7897 | Old 0.8561 | New 0.7239
2024-05-15 10:32:09.137 | INFO     | __main__:train:123 - Epoch: [29][0/63]	 loss 4.36314	 cls_loss: 0.5634 cluster_loss: 1.3955 sup_con_loss: 0.7077 contrastive_loss: 4.6326 
2024-05-15 10:32:27.467 | INFO     | __main__:train:123 - Epoch: [29][20/63]	 loss 4.24792	 cls_loss: 0.4993 cluster_loss: 1.2688 sup_con_loss: 0.6665 contrastive_loss: 4.6387 
2024-05-15 10:32:45.970 | INFO     | __main__:train:123 - Epoch: [29][40/63]	 loss 4.51804	 cls_loss: 0.5327 cluster_loss: 1.4788 sup_con_loss: 1.0119 contrastive_loss: 4.6403 
2024-05-15 10:33:01.941 | INFO     | __main__:train:123 - Epoch: [29][60/63]	 loss 4.43216	 cls_loss: 0.5287 cluster_loss: 1.3942 sup_con_loss: 0.9385 contrastive_loss: 4.6345 
2024-05-15 10:33:03.766 | INFO     | __main__:train:126 - Train Epoch: 29 Avg Loss: 4.3167 
2024-05-15 10:33:03.767 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:33:37.942 | INFO     | __main__:train:135 - Train Accuracies: All 0.7937 | Old 0.8478 | New 0.7400
2024-05-15 10:33:44.752 | INFO     | __main__:train:123 - Epoch: [30][0/63]	 loss 4.44045	 cls_loss: 0.5631 cluster_loss: 1.3499 sup_con_loss: 1.0201 contrastive_loss: 4.6291 
2024-05-15 10:34:03.633 | INFO     | __main__:train:123 - Epoch: [30][20/63]	 loss 4.27440	 cls_loss: 0.4458 cluster_loss: 1.3172 sup_con_loss: 0.7158 contrastive_loss: 4.6334 
2024-05-15 10:34:21.879 | INFO     | __main__:train:123 - Epoch: [30][40/63]	 loss 4.21436	 cls_loss: 0.5083 cluster_loss: 1.2684 sup_con_loss: 0.5861 contrastive_loss: 4.6259 
2024-05-15 10:34:38.053 | INFO     | __main__:train:123 - Epoch: [30][60/63]	 loss 4.23351	 cls_loss: 0.4971 cluster_loss: 1.2733 sup_con_loss: 0.6283 contrastive_loss: 4.6338 
2024-05-15 10:34:39.740 | INFO     | __main__:train:126 - Train Epoch: 30 Avg Loss: 4.3201 
2024-05-15 10:34:39.740 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:35:13.656 | INFO     | __main__:train:135 - Train Accuracies: All 0.7904 | Old 0.8471 | New 0.7343
2024-05-15 10:35:20.579 | INFO     | __main__:train:123 - Epoch: [31][0/63]	 loss 4.29807	 cls_loss: 0.4575 cluster_loss: 1.3045 sup_con_loss: 0.8085 contrastive_loss: 4.6262 
2024-05-15 10:35:40.212 | INFO     | __main__:train:123 - Epoch: [31][20/63]	 loss 4.23825	 cls_loss: 0.4882 cluster_loss: 1.3004 sup_con_loss: 0.6151 contrastive_loss: 4.6259 
2024-05-15 10:35:58.179 | INFO     | __main__:train:123 - Epoch: [31][40/63]	 loss 4.27887	 cls_loss: 0.4842 cluster_loss: 1.3303 sup_con_loss: 0.6792 contrastive_loss: 4.6261 
2024-05-15 10:36:14.134 | INFO     | __main__:train:123 - Epoch: [31][60/63]	 loss 4.38644	 cls_loss: 0.4913 cluster_loss: 1.4327 sup_con_loss: 0.7688 contrastive_loss: 4.6372 
2024-05-15 10:36:15.846 | INFO     | __main__:train:126 - Train Epoch: 31 Avg Loss: 4.2860 
2024-05-15 10:36:15.846 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:36:49.379 | INFO     | __main__:train:135 - Train Accuracies: All 0.7922 | Old 0.8508 | New 0.7341
2024-05-15 10:36:57.656 | INFO     | __main__:train:123 - Epoch: [32][0/63]	 loss 4.24347	 cls_loss: 0.4765 cluster_loss: 1.3511 sup_con_loss: 0.5311 contrastive_loss: 4.6347 
2024-05-15 10:37:16.354 | INFO     | __main__:train:123 - Epoch: [32][20/63]	 loss 4.14982	 cls_loss: 0.4152 cluster_loss: 1.2320 sup_con_loss: 0.5585 contrastive_loss: 4.6280 
2024-05-15 10:37:34.995 | INFO     | __main__:train:123 - Epoch: [32][40/63]	 loss 4.24660	 cls_loss: 0.5102 cluster_loss: 1.2480 sup_con_loss: 0.7375 contrastive_loss: 4.6134 
2024-05-15 10:37:51.151 | INFO     | __main__:train:123 - Epoch: [32][60/63]	 loss 4.18004	 cls_loss: 0.4681 cluster_loss: 1.2203 sup_con_loss: 0.6345 contrastive_loss: 4.6169 
2024-05-15 10:37:52.857 | INFO     | __main__:train:126 - Train Epoch: 32 Avg Loss: 4.2869 
2024-05-15 10:37:52.858 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:38:27.187 | INFO     | __main__:train:135 - Train Accuracies: All 0.7937 | Old 0.8488 | New 0.7390
2024-05-15 10:38:34.633 | INFO     | __main__:train:123 - Epoch: [33][0/63]	 loss 4.28695	 cls_loss: 0.5227 cluster_loss: 1.2893 sup_con_loss: 0.7321 contrastive_loss: 4.6304 
2024-05-15 10:38:53.237 | INFO     | __main__:train:123 - Epoch: [33][20/63]	 loss 4.31149	 cls_loss: 0.4627 cluster_loss: 1.3215 sup_con_loss: 0.7958 contrastive_loss: 4.6339 
2024-05-15 10:39:11.217 | INFO     | __main__:train:123 - Epoch: [33][40/63]	 loss 4.17566	 cls_loss: 0.4334 cluster_loss: 1.2269 sup_con_loss: 0.6257 contrastive_loss: 4.6269 
2024-05-15 10:39:27.426 | INFO     | __main__:train:123 - Epoch: [33][60/63]	 loss 4.25304	 cls_loss: 0.4452 cluster_loss: 1.2469 sup_con_loss: 0.8066 contrastive_loss: 4.6222 
2024-05-15 10:39:29.193 | INFO     | __main__:train:126 - Train Epoch: 33 Avg Loss: 4.2843 
2024-05-15 10:39:29.194 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:40:02.314 | INFO     | __main__:train:135 - Train Accuracies: All 0.7918 | Old 0.8506 | New 0.7336
2024-05-15 10:40:10.094 | INFO     | __main__:train:123 - Epoch: [34][0/63]	 loss 4.36218	 cls_loss: 0.4922 cluster_loss: 1.3687 sup_con_loss: 0.8295 contrastive_loss: 4.6307 
2024-05-15 10:40:28.149 | INFO     | __main__:train:123 - Epoch: [34][20/63]	 loss 4.26229	 cls_loss: 0.4403 cluster_loss: 1.2673 sup_con_loss: 0.7765 contrastive_loss: 4.6349 
2024-05-15 10:40:46.954 | INFO     | __main__:train:123 - Epoch: [34][40/63]	 loss 4.31806	 cls_loss: 0.4530 cluster_loss: 1.3337 sup_con_loss: 0.8187 contrastive_loss: 4.6247 
2024-05-15 10:41:02.892 | INFO     | __main__:train:123 - Epoch: [34][60/63]	 loss 4.32488	 cls_loss: 0.4825 cluster_loss: 1.3444 sup_con_loss: 0.7672 contrastive_loss: 4.6364 
2024-05-15 10:41:04.736 | INFO     | __main__:train:126 - Train Epoch: 34 Avg Loss: 4.2896 
2024-05-15 10:41:04.736 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:41:38.125 | INFO     | __main__:train:135 - Train Accuracies: All 0.7969 | Old 0.8528 | New 0.7415
2024-05-15 10:41:46.103 | INFO     | __main__:train:123 - Epoch: [35][0/63]	 loss 4.42001	 cls_loss: 0.4962 cluster_loss: 1.3925 sup_con_loss: 0.9564 contrastive_loss: 4.6254 
2024-05-15 10:42:04.850 | INFO     | __main__:train:123 - Epoch: [35][20/63]	 loss 4.38975	 cls_loss: 0.4776 cluster_loss: 1.4012 sup_con_loss: 0.8623 contrastive_loss: 4.6307 
2024-05-15 10:42:23.051 | INFO     | __main__:train:123 - Epoch: [35][40/63]	 loss 4.36305	 cls_loss: 0.4750 cluster_loss: 1.3206 sup_con_loss: 0.9451 contrastive_loss: 4.6271 
2024-05-15 10:42:39.070 | INFO     | __main__:train:123 - Epoch: [35][60/63]	 loss 4.30992	 cls_loss: 0.4608 cluster_loss: 1.3188 sup_con_loss: 0.8093 contrastive_loss: 4.6279 
2024-05-15 10:42:40.778 | INFO     | __main__:train:126 - Train Epoch: 35 Avg Loss: 4.2862 
2024-05-15 10:42:40.779 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:43:14.189 | INFO     | __main__:train:135 - Train Accuracies: All 0.7963 | Old 0.8523 | New 0.7408
2024-05-15 10:43:21.331 | INFO     | __main__:train:123 - Epoch: [36][0/63]	 loss 4.28551	 cls_loss: 0.4326 cluster_loss: 1.3401 sup_con_loss: 0.7223 contrastive_loss: 4.6311 
2024-05-15 10:43:39.725 | INFO     | __main__:train:123 - Epoch: [36][20/63]	 loss 4.21544	 cls_loss: 0.4519 cluster_loss: 1.2476 sup_con_loss: 0.6804 contrastive_loss: 4.6280 
2024-05-15 10:43:58.287 | INFO     | __main__:train:123 - Epoch: [36][40/63]	 loss 4.33670	 cls_loss: 0.4541 cluster_loss: 1.3647 sup_con_loss: 0.7966 contrastive_loss: 4.6336 
2024-05-15 10:44:14.333 | INFO     | __main__:train:123 - Epoch: [36][60/63]	 loss 4.35290	 cls_loss: 0.4777 cluster_loss: 1.3315 sup_con_loss: 0.9006 contrastive_loss: 4.6231 
2024-05-15 10:44:16.161 | INFO     | __main__:train:126 - Train Epoch: 36 Avg Loss: 4.2676 
2024-05-15 10:44:16.161 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:44:50.652 | INFO     | __main__:train:135 - Train Accuracies: All 0.8016 | Old 0.8516 | New 0.7522
2024-05-15 10:44:59.295 | INFO     | __main__:train:123 - Epoch: [37][0/63]	 loss 4.21743	 cls_loss: 0.4546 cluster_loss: 1.2972 sup_con_loss: 0.5968 contrastive_loss: 4.6251 
2024-05-15 10:45:17.053 | INFO     | __main__:train:123 - Epoch: [37][20/63]	 loss 4.21083	 cls_loss: 0.4699 cluster_loss: 1.2760 sup_con_loss: 0.5986 contrastive_loss: 4.6268 
2024-05-15 10:45:35.548 | INFO     | __main__:train:123 - Epoch: [37][40/63]	 loss 4.34087	 cls_loss: 0.4593 cluster_loss: 1.3215 sup_con_loss: 0.9116 contrastive_loss: 4.6186 
2024-05-15 10:45:51.581 | INFO     | __main__:train:123 - Epoch: [37][60/63]	 loss 4.29727	 cls_loss: 0.4635 cluster_loss: 1.2918 sup_con_loss: 0.8324 contrastive_loss: 4.6216 
2024-05-15 10:45:53.306 | INFO     | __main__:train:126 - Train Epoch: 37 Avg Loss: 4.2642 
2024-05-15 10:45:53.306 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:46:26.991 | INFO     | __main__:train:135 - Train Accuracies: All 0.8023 | Old 0.8556 | New 0.7494
2024-05-15 10:46:33.994 | INFO     | __main__:train:123 - Epoch: [38][0/63]	 loss 4.10069	 cls_loss: 0.4519 cluster_loss: 1.1520 sup_con_loss: 0.5462 contrastive_loss: 4.6193 
2024-05-15 10:46:52.916 | INFO     | __main__:train:123 - Epoch: [38][20/63]	 loss 4.27360	 cls_loss: 0.4196 cluster_loss: 1.3139 sup_con_loss: 0.7666 contrastive_loss: 4.6221 
2024-05-15 10:47:11.394 | INFO     | __main__:train:123 - Epoch: [38][40/63]	 loss 4.31398	 cls_loss: 0.4879 cluster_loss: 1.3192 sup_con_loss: 0.7950 contrastive_loss: 4.6269 
2024-05-15 10:47:27.534 | INFO     | __main__:train:123 - Epoch: [38][60/63]	 loss 4.28159	 cls_loss: 0.4487 cluster_loss: 1.2660 sup_con_loss: 0.8654 contrastive_loss: 4.6135 
2024-05-15 10:47:29.296 | INFO     | __main__:train:126 - Train Epoch: 38 Avg Loss: 4.2565 
2024-05-15 10:47:29.297 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:48:03.386 | INFO     | __main__:train:135 - Train Accuracies: All 0.8030 | Old 0.8536 | New 0.7529
2024-05-15 10:48:09.811 | INFO     | __main__:train:123 - Epoch: [39][0/63]	 loss 4.16460	 cls_loss: 0.4037 cluster_loss: 1.2759 sup_con_loss: 0.5325 contrastive_loss: 4.6270 
2024-05-15 10:48:28.516 | INFO     | __main__:train:123 - Epoch: [39][20/63]	 loss 4.24182	 cls_loss: 0.3956 cluster_loss: 1.3037 sup_con_loss: 0.6988 contrastive_loss: 4.6329 
2024-05-15 10:48:47.086 | INFO     | __main__:train:123 - Epoch: [39][40/63]	 loss 4.26094	 cls_loss: 0.4409 cluster_loss: 1.2387 sup_con_loss: 0.8543 contrastive_loss: 4.6192 
2024-05-15 10:49:02.972 | INFO     | __main__:train:123 - Epoch: [39][60/63]	 loss 4.15630	 cls_loss: 0.3945 cluster_loss: 1.2438 sup_con_loss: 0.5896 contrastive_loss: 4.6206 
2024-05-15 10:49:04.742 | INFO     | __main__:train:126 - Train Epoch: 39 Avg Loss: 4.2434 
2024-05-15 10:49:04.743 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:49:39.225 | INFO     | __main__:train:135 - Train Accuracies: All 0.8025 | Old 0.8511 | New 0.7544
2024-05-15 10:49:46.067 | INFO     | __main__:train:123 - Epoch: [40][0/63]	 loss 4.12439	 cls_loss: 0.4121 cluster_loss: 1.1614 sup_con_loss: 0.6424 contrastive_loss: 4.6161 
2024-05-15 10:50:04.812 | INFO     | __main__:train:123 - Epoch: [40][20/63]	 loss 4.23912	 cls_loss: 0.5019 cluster_loss: 1.2422 sup_con_loss: 0.7132 contrastive_loss: 4.6253 
2024-05-15 10:50:23.443 | INFO     | __main__:train:123 - Epoch: [40][40/63]	 loss 4.24567	 cls_loss: 0.4854 cluster_loss: 1.2616 sup_con_loss: 0.7180 contrastive_loss: 4.6222 
2024-05-15 10:50:39.821 | INFO     | __main__:train:123 - Epoch: [40][60/63]	 loss 4.17774	 cls_loss: 0.4932 cluster_loss: 1.2319 sup_con_loss: 0.5709 contrastive_loss: 4.6224 
2024-05-15 10:50:41.584 | INFO     | __main__:train:126 - Train Epoch: 40 Avg Loss: 4.2397 
2024-05-15 10:50:41.585 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:51:15.499 | INFO     | __main__:train:135 - Train Accuracies: All 0.7982 | Old 0.8456 | New 0.7512
2024-05-15 10:51:23.501 | INFO     | __main__:train:123 - Epoch: [41][0/63]	 loss 4.23257	 cls_loss: 0.4761 cluster_loss: 1.2439 sup_con_loss: 0.7252 contrastive_loss: 4.6209 
2024-05-15 10:51:41.644 | INFO     | __main__:train:123 - Epoch: [41][20/63]	 loss 4.20350	 cls_loss: 0.4310 cluster_loss: 1.2873 sup_con_loss: 0.6096 contrastive_loss: 4.6194 
2024-05-15 10:51:59.463 | INFO     | __main__:train:123 - Epoch: [41][40/63]	 loss 4.32265	 cls_loss: 0.4548 cluster_loss: 1.3356 sup_con_loss: 0.8235 contrastive_loss: 4.6264 
2024-05-15 10:52:15.342 | INFO     | __main__:train:123 - Epoch: [41][60/63]	 loss 4.07228	 cls_loss: 0.4106 cluster_loss: 1.1682 sup_con_loss: 0.4644 contrastive_loss: 4.6257 
2024-05-15 10:52:17.127 | INFO     | __main__:train:126 - Train Epoch: 41 Avg Loss: 4.2252 
2024-05-15 10:52:17.128 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:52:50.748 | INFO     | __main__:train:135 - Train Accuracies: All 0.8008 | Old 0.8503 | New 0.7517
2024-05-15 10:52:59.626 | INFO     | __main__:train:123 - Epoch: [42][0/63]	 loss 4.32097	 cls_loss: 0.4724 cluster_loss: 1.2953 sup_con_loss: 0.8783 contrastive_loss: 4.6251 
2024-05-15 10:53:18.178 | INFO     | __main__:train:123 - Epoch: [42][20/63]	 loss 4.20683	 cls_loss: 0.3889 cluster_loss: 1.2261 sup_con_loss: 0.7782 contrastive_loss: 4.6175 
2024-05-15 10:53:36.562 | INFO     | __main__:train:123 - Epoch: [42][40/63]	 loss 4.12857	 cls_loss: 0.4252 cluster_loss: 1.1995 sup_con_loss: 0.5873 contrastive_loss: 4.6070 
2024-05-15 10:53:52.618 | INFO     | __main__:train:123 - Epoch: [42][60/63]	 loss 4.20929	 cls_loss: 0.4122 cluster_loss: 1.2030 sup_con_loss: 0.8063 contrastive_loss: 4.6167 
2024-05-15 10:53:54.333 | INFO     | __main__:train:126 - Train Epoch: 42 Avg Loss: 4.2252 
2024-05-15 10:53:54.333 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:54:29.183 | INFO     | __main__:train:135 - Train Accuracies: All 0.8009 | Old 0.8491 | New 0.7532
2024-05-15 10:54:36.171 | INFO     | __main__:train:123 - Epoch: [43][0/63]	 loss 4.31526	 cls_loss: 0.4431 cluster_loss: 1.2830 sup_con_loss: 0.9265 contrastive_loss: 4.6184 
2024-05-15 10:54:55.197 | INFO     | __main__:train:123 - Epoch: [43][20/63]	 loss 4.24435	 cls_loss: 0.4459 cluster_loss: 1.2955 sup_con_loss: 0.6903 contrastive_loss: 4.6225 
2024-05-15 10:55:13.278 | INFO     | __main__:train:123 - Epoch: [43][40/63]	 loss 4.18078	 cls_loss: 0.4311 cluster_loss: 1.2991 sup_con_loss: 0.5194 contrastive_loss: 4.6210 
2024-05-15 10:55:29.168 | INFO     | __main__:train:123 - Epoch: [43][60/63]	 loss 4.25426	 cls_loss: 0.4796 cluster_loss: 1.2714 sup_con_loss: 0.7307 contrastive_loss: 4.6219 
2024-05-15 10:55:30.937 | INFO     | __main__:train:126 - Train Epoch: 43 Avg Loss: 4.2244 
2024-05-15 10:55:30.937 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:56:04.394 | INFO     | __main__:train:135 - Train Accuracies: All 0.8005 | Old 0.8498 | New 0.7517
2024-05-15 10:56:12.077 | INFO     | __main__:train:123 - Epoch: [44][0/63]	 loss 4.14248	 cls_loss: 0.4220 cluster_loss: 1.2248 sup_con_loss: 0.5552 contrastive_loss: 4.6221 
2024-05-15 10:56:30.577 | INFO     | __main__:train:123 - Epoch: [44][20/63]	 loss 4.26048	 cls_loss: 0.4601 cluster_loss: 1.3004 sup_con_loss: 0.7198 contrastive_loss: 4.6189 
2024-05-15 10:56:49.397 | INFO     | __main__:train:123 - Epoch: [44][40/63]	 loss 4.24670	 cls_loss: 0.4230 cluster_loss: 1.2452 sup_con_loss: 0.8240 contrastive_loss: 4.6166 
2024-05-15 10:57:05.542 | INFO     | __main__:train:123 - Epoch: [44][60/63]	 loss 4.20034	 cls_loss: 0.3873 cluster_loss: 1.2183 sup_con_loss: 0.7804 contrastive_loss: 4.6150 
2024-05-15 10:57:07.280 | INFO     | __main__:train:126 - Train Epoch: 44 Avg Loss: 4.2191 
2024-05-15 10:57:07.280 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:57:41.270 | INFO     | __main__:train:135 - Train Accuracies: All 0.8062 | Old 0.8513 | New 0.7616
2024-05-15 10:57:49.023 | INFO     | __main__:train:123 - Epoch: [45][0/63]	 loss 4.23133	 cls_loss: 0.3690 cluster_loss: 1.2889 sup_con_loss: 0.7563 contrastive_loss: 4.6149 
2024-05-15 10:58:07.636 | INFO     | __main__:train:123 - Epoch: [45][20/63]	 loss 4.13981	 cls_loss: 0.4271 cluster_loss: 1.1847 sup_con_loss: 0.6322 contrastive_loss: 4.6138 
2024-05-15 10:58:25.951 | INFO     | __main__:train:123 - Epoch: [45][40/63]	 loss 4.25502	 cls_loss: 0.3828 cluster_loss: 1.3115 sup_con_loss: 0.7642 contrastive_loss: 4.6170 
2024-05-15 10:58:41.936 | INFO     | __main__:train:123 - Epoch: [45][60/63]	 loss 4.12435	 cls_loss: 0.3693 cluster_loss: 1.1869 sup_con_loss: 0.6401 contrastive_loss: 4.6147 
2024-05-15 10:58:43.681 | INFO     | __main__:train:126 - Train Epoch: 45 Avg Loss: 4.2177 
2024-05-15 10:58:43.681 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 10:59:18.247 | INFO     | __main__:train:135 - Train Accuracies: All 0.8043 | Old 0.8483 | New 0.7606
2024-05-15 10:59:26.280 | INFO     | __main__:train:123 - Epoch: [46][0/63]	 loss 4.15663	 cls_loss: 0.4248 cluster_loss: 1.2017 sup_con_loss: 0.6486 contrastive_loss: 4.6151 
2024-05-15 10:59:44.590 | INFO     | __main__:train:123 - Epoch: [46][20/63]	 loss 4.20227	 cls_loss: 0.3985 cluster_loss: 1.2513 sup_con_loss: 0.7114 contrastive_loss: 4.6160 
2024-05-15 11:00:02.882 | INFO     | __main__:train:123 - Epoch: [46][40/63]	 loss 4.13172	 cls_loss: 0.4031 cluster_loss: 1.2027 sup_con_loss: 0.5769 contrastive_loss: 4.6261 
2024-05-15 11:00:18.900 | INFO     | __main__:train:123 - Epoch: [46][60/63]	 loss 4.19429	 cls_loss: 0.4259 cluster_loss: 1.2312 sup_con_loss: 0.6882 contrastive_loss: 4.6216 
2024-05-15 11:00:20.598 | INFO     | __main__:train:126 - Train Epoch: 46 Avg Loss: 4.2077 
2024-05-15 11:00:20.599 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:00:54.212 | INFO     | __main__:train:135 - Train Accuracies: All 0.8039 | Old 0.8483 | New 0.7598
2024-05-15 11:01:01.685 | INFO     | __main__:train:123 - Epoch: [47][0/63]	 loss 4.22841	 cls_loss: 0.4419 cluster_loss: 1.2972 sup_con_loss: 0.6557 contrastive_loss: 4.6171 
2024-05-15 11:01:20.650 | INFO     | __main__:train:123 - Epoch: [47][20/63]	 loss 4.16973	 cls_loss: 0.4633 cluster_loss: 1.2672 sup_con_loss: 0.4931 contrastive_loss: 4.6328 
2024-05-15 11:01:38.773 | INFO     | __main__:train:123 - Epoch: [47][40/63]	 loss 4.23937	 cls_loss: 0.4137 cluster_loss: 1.2441 sup_con_loss: 0.8306 contrastive_loss: 4.6079 
2024-05-15 11:01:54.805 | INFO     | __main__:train:123 - Epoch: [47][60/63]	 loss 4.12947	 cls_loss: 0.3910 cluster_loss: 1.2265 sup_con_loss: 0.5531 contrastive_loss: 4.6181 
2024-05-15 11:01:56.575 | INFO     | __main__:train:126 - Train Epoch: 47 Avg Loss: 4.2001 
2024-05-15 11:01:56.575 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:02:31.168 | INFO     | __main__:train:135 - Train Accuracies: All 0.8035 | Old 0.8521 | New 0.7554
2024-05-15 11:02:38.118 | INFO     | __main__:train:123 - Epoch: [48][0/63]	 loss 4.23853	 cls_loss: 0.4094 cluster_loss: 1.3390 sup_con_loss: 0.6331 contrastive_loss: 4.6204 
2024-05-15 11:02:56.828 | INFO     | __main__:train:123 - Epoch: [48][20/63]	 loss 4.17695	 cls_loss: 0.3829 cluster_loss: 1.1872 sup_con_loss: 0.7782 contrastive_loss: 4.6137 
2024-05-15 11:03:15.620 | INFO     | __main__:train:123 - Epoch: [48][40/63]	 loss 4.20173	 cls_loss: 0.4527 cluster_loss: 1.2090 sup_con_loss: 0.7115 contrastive_loss: 4.6283 
2024-05-15 11:03:31.344 | INFO     | __main__:train:123 - Epoch: [48][60/63]	 loss 4.18511	 cls_loss: 0.4068 cluster_loss: 1.1967 sup_con_loss: 0.7532 contrastive_loss: 4.6174 
2024-05-15 11:03:33.036 | INFO     | __main__:train:126 - Train Epoch: 48 Avg Loss: 4.2007 
2024-05-15 11:03:33.036 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:04:06.369 | INFO     | __main__:train:135 - Train Accuracies: All 0.8034 | Old 0.8496 | New 0.7576
2024-05-15 11:04:13.990 | INFO     | __main__:train:123 - Epoch: [49][0/63]	 loss 4.23529	 cls_loss: 0.4132 cluster_loss: 1.2926 sup_con_loss: 0.7013 contrastive_loss: 4.6231 
2024-05-15 11:04:32.478 | INFO     | __main__:train:123 - Epoch: [49][20/63]	 loss 4.23993	 cls_loss: 0.3722 cluster_loss: 1.3080 sup_con_loss: 0.7520 contrastive_loss: 4.6096 
2024-05-15 11:04:51.167 | INFO     | __main__:train:123 - Epoch: [49][40/63]	 loss 4.16162	 cls_loss: 0.4225 cluster_loss: 1.1701 sup_con_loss: 0.7188 contrastive_loss: 4.6178 
2024-05-15 11:05:07.218 | INFO     | __main__:train:123 - Epoch: [49][60/63]	 loss 4.22612	 cls_loss: 0.3854 cluster_loss: 1.2958 sup_con_loss: 0.7145 contrastive_loss: 4.6137 
2024-05-15 11:05:08.993 | INFO     | __main__:train:126 - Train Epoch: 49 Avg Loss: 4.2111 
2024-05-15 11:05:08.994 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:05:43.024 | INFO     | __main__:train:135 - Train Accuracies: All 0.8038 | Old 0.8491 | New 0.7589
2024-05-15 11:05:50.541 | INFO     | __main__:train:123 - Epoch: [50][0/63]	 loss 4.17461	 cls_loss: 0.3919 cluster_loss: 1.2198 sup_con_loss: 0.6944 contrastive_loss: 4.6177 
2024-05-15 11:06:09.447 | INFO     | __main__:train:123 - Epoch: [50][20/63]	 loss 4.11814	 cls_loss: 0.4183 cluster_loss: 1.2145 sup_con_loss: 0.5265 contrastive_loss: 4.6124 
2024-05-15 11:06:28.056 | INFO     | __main__:train:123 - Epoch: [50][40/63]	 loss 4.14812	 cls_loss: 0.3676 cluster_loss: 1.2284 sup_con_loss: 0.6314 contrastive_loss: 4.6154 
2024-05-15 11:06:44.308 | INFO     | __main__:train:123 - Epoch: [50][60/63]	 loss 4.10257	 cls_loss: 0.4085 cluster_loss: 1.2161 sup_con_loss: 0.4909 contrastive_loss: 4.6112 
2024-05-15 11:06:46.049 | INFO     | __main__:train:126 - Train Epoch: 50 Avg Loss: 4.1811 
2024-05-15 11:06:46.050 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:07:20.897 | INFO     | __main__:train:135 - Train Accuracies: All 0.7995 | Old 0.8451 | New 0.7544
2024-05-15 11:07:29.295 | INFO     | __main__:train:123 - Epoch: [51][0/63]	 loss 4.47125	 cls_loss: 0.4286 cluster_loss: 1.4518 sup_con_loss: 1.0577 contrastive_loss: 4.6268 
2024-05-15 11:07:48.200 | INFO     | __main__:train:123 - Epoch: [51][20/63]	 loss 4.26093	 cls_loss: 0.4166 cluster_loss: 1.2658 sup_con_loss: 0.8191 contrastive_loss: 4.6241 
2024-05-15 11:08:06.334 | INFO     | __main__:train:123 - Epoch: [51][40/63]	 loss 4.32514	 cls_loss: 0.3790 cluster_loss: 1.4123 sup_con_loss: 0.7605 contrastive_loss: 4.6282 
2024-05-15 11:08:22.274 | INFO     | __main__:train:123 - Epoch: [51][60/63]	 loss 4.17360	 cls_loss: 0.3581 cluster_loss: 1.2693 sup_con_loss: 0.6314 contrastive_loss: 4.6188 
2024-05-15 11:08:24.029 | INFO     | __main__:train:126 - Train Epoch: 51 Avg Loss: 4.1983 
2024-05-15 11:08:24.029 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:08:58.662 | INFO     | __main__:train:135 - Train Accuracies: All 0.8010 | Old 0.8458 | New 0.7566
2024-05-15 11:09:06.897 | INFO     | __main__:train:123 - Epoch: [52][0/63]	 loss 4.08279	 cls_loss: 0.3474 cluster_loss: 1.1707 sup_con_loss: 0.5901 contrastive_loss: 4.6057 
2024-05-15 11:09:25.150 | INFO     | __main__:train:123 - Epoch: [52][20/63]	 loss 4.16211	 cls_loss: 0.3862 cluster_loss: 1.2066 sup_con_loss: 0.7019 contrastive_loss: 4.6108 
2024-05-15 11:09:43.265 | INFO     | __main__:train:123 - Epoch: [52][40/63]	 loss 4.22601	 cls_loss: 0.3866 cluster_loss: 1.2792 sup_con_loss: 0.7364 contrastive_loss: 4.6177 
2024-05-15 11:09:58.763 | INFO     | __main__:train:123 - Epoch: [52][60/63]	 loss 4.17551	 cls_loss: 0.3794 cluster_loss: 1.2472 sup_con_loss: 0.6538 contrastive_loss: 4.6203 
2024-05-15 11:10:00.454 | INFO     | __main__:train:126 - Train Epoch: 52 Avg Loss: 4.1840 
2024-05-15 11:10:00.455 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:10:35.529 | INFO     | __main__:train:135 - Train Accuracies: All 0.8035 | Old 0.8483 | New 0.7591
2024-05-15 11:10:43.564 | INFO     | __main__:train:123 - Epoch: [53][0/63]	 loss 4.19008	 cls_loss: 0.4117 cluster_loss: 1.2724 sup_con_loss: 0.6331 contrastive_loss: 4.6113 
2024-05-15 11:11:02.161 | INFO     | __main__:train:123 - Epoch: [53][20/63]	 loss 4.16406	 cls_loss: 0.3966 cluster_loss: 1.2474 sup_con_loss: 0.6108 contrastive_loss: 4.6164 
2024-05-15 11:11:20.703 | INFO     | __main__:train:123 - Epoch: [53][40/63]	 loss 4.19892	 cls_loss: 0.3927 cluster_loss: 1.2679 sup_con_loss: 0.6782 contrastive_loss: 4.6153 
2024-05-15 11:11:36.429 | INFO     | __main__:train:123 - Epoch: [53][60/63]	 loss 4.15136	 cls_loss: 0.3860 cluster_loss: 1.2147 sup_con_loss: 0.6450 contrastive_loss: 4.6169 
2024-05-15 11:11:38.173 | INFO     | __main__:train:126 - Train Epoch: 53 Avg Loss: 4.1710 
2024-05-15 11:11:38.173 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:12:12.240 | INFO     | __main__:train:135 - Train Accuracies: All 0.8026 | Old 0.8443 | New 0.7613
2024-05-15 11:12:20.245 | INFO     | __main__:train:123 - Epoch: [54][0/63]	 loss 4.08930	 cls_loss: 0.4235 cluster_loss: 1.1275 sup_con_loss: 0.6036 contrastive_loss: 4.6107 
2024-05-15 11:12:38.503 | INFO     | __main__:train:123 - Epoch: [54][20/63]	 loss 4.17957	 cls_loss: 0.4355 cluster_loss: 1.1995 sup_con_loss: 0.7079 contrastive_loss: 4.6150 
2024-05-15 11:12:57.003 | INFO     | __main__:train:123 - Epoch: [54][40/63]	 loss 4.27382	 cls_loss: 0.4090 cluster_loss: 1.3098 sup_con_loss: 0.7885 contrastive_loss: 4.6205 
2024-05-15 11:13:13.404 | INFO     | __main__:train:123 - Epoch: [54][60/63]	 loss 4.20040	 cls_loss: 0.3605 cluster_loss: 1.3396 sup_con_loss: 0.5774 contrastive_loss: 4.6176 
2024-05-15 11:13:15.151 | INFO     | __main__:train:126 - Train Epoch: 54 Avg Loss: 4.1743 
2024-05-15 11:13:15.152 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:13:49.265 | INFO     | __main__:train:135 - Train Accuracies: All 0.8043 | Old 0.8461 | New 0.7628
2024-05-15 11:13:57.727 | INFO     | __main__:train:123 - Epoch: [55][0/63]	 loss 4.06272	 cls_loss: 0.4052 cluster_loss: 1.1911 sup_con_loss: 0.4416 contrastive_loss: 4.6032 
2024-05-15 11:14:15.888 | INFO     | __main__:train:123 - Epoch: [55][20/63]	 loss 4.28669	 cls_loss: 0.4263 cluster_loss: 1.2985 sup_con_loss: 0.8409 contrastive_loss: 4.6141 
2024-05-15 11:14:33.893 | INFO     | __main__:train:123 - Epoch: [55][40/63]	 loss 4.18991	 cls_loss: 0.4005 cluster_loss: 1.2829 sup_con_loss: 0.6039 contrastive_loss: 4.6223 
2024-05-15 11:14:50.670 | INFO     | __main__:train:123 - Epoch: [55][60/63]	 loss 4.17372	 cls_loss: 0.4275 cluster_loss: 1.2455 sup_con_loss: 0.6185 contrastive_loss: 4.6124 
2024-05-15 11:14:52.393 | INFO     | __main__:train:126 - Train Epoch: 55 Avg Loss: 4.1660 
2024-05-15 11:14:52.394 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:15:27.046 | INFO     | __main__:train:135 - Train Accuracies: All 0.8060 | Old 0.8491 | New 0.7633
2024-05-15 11:15:34.306 | INFO     | __main__:train:123 - Epoch: [56][0/63]	 loss 4.23937	 cls_loss: 0.4184 cluster_loss: 1.2520 sup_con_loss: 0.7997 contrastive_loss: 4.6142 
2024-05-15 11:15:52.994 | INFO     | __main__:train:123 - Epoch: [56][20/63]	 loss 4.18431	 cls_loss: 0.3785 cluster_loss: 1.2350 sup_con_loss: 0.7138 contrastive_loss: 4.6143 
2024-05-15 11:16:11.591 | INFO     | __main__:train:123 - Epoch: [56][40/63]	 loss 4.22646	 cls_loss: 0.3738 cluster_loss: 1.2685 sup_con_loss: 0.7671 contrastive_loss: 4.6194 
2024-05-15 11:16:27.661 | INFO     | __main__:train:123 - Epoch: [56][60/63]	 loss 4.10767	 cls_loss: 0.4207 cluster_loss: 1.2012 sup_con_loss: 0.5142 contrastive_loss: 4.6149 
2024-05-15 11:16:29.370 | INFO     | __main__:train:126 - Train Epoch: 56 Avg Loss: 4.1841 
2024-05-15 11:16:29.371 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:17:02.809 | INFO     | __main__:train:135 - Train Accuracies: All 0.8074 | Old 0.8516 | New 0.7636
2024-05-15 11:17:11.602 | INFO     | __main__:train:123 - Epoch: [57][0/63]	 loss 4.18397	 cls_loss: 0.4117 cluster_loss: 1.3051 sup_con_loss: 0.5560 contrastive_loss: 4.6107 
2024-05-15 11:17:30.257 | INFO     | __main__:train:123 - Epoch: [57][20/63]	 loss 4.12192	 cls_loss: 0.3459 cluster_loss: 1.2661 sup_con_loss: 0.5180 contrastive_loss: 4.6102 
2024-05-15 11:17:48.234 | INFO     | __main__:train:123 - Epoch: [57][40/63]	 loss 4.09779	 cls_loss: 0.3862 cluster_loss: 1.1753 sup_con_loss: 0.5752 contrastive_loss: 4.6113 
2024-05-15 11:18:04.319 | INFO     | __main__:train:123 - Epoch: [57][60/63]	 loss 4.07811	 cls_loss: 0.3978 cluster_loss: 1.1354 sup_con_loss: 0.5842 contrastive_loss: 4.6099 
2024-05-15 11:18:06.038 | INFO     | __main__:train:126 - Train Epoch: 57 Avg Loss: 4.1689 
2024-05-15 11:18:06.039 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:18:40.434 | INFO     | __main__:train:135 - Train Accuracies: All 0.8041 | Old 0.8473 | New 0.7613
2024-05-15 11:18:48.145 | INFO     | __main__:train:123 - Epoch: [58][0/63]	 loss 4.09389	 cls_loss: 0.3504 cluster_loss: 1.1985 sup_con_loss: 0.5691 contrastive_loss: 4.6047 
2024-05-15 11:19:06.217 | INFO     | __main__:train:123 - Epoch: [58][20/63]	 loss 4.18682	 cls_loss: 0.3610 cluster_loss: 1.2921 sup_con_loss: 0.6626 contrastive_loss: 4.5980 
2024-05-15 11:19:25.195 | INFO     | __main__:train:123 - Epoch: [58][40/63]	 loss 4.10217	 cls_loss: 0.4005 cluster_loss: 1.1809 sup_con_loss: 0.5639 contrastive_loss: 4.6109 
2024-05-15 11:19:41.173 | INFO     | __main__:train:123 - Epoch: [58][60/63]	 loss 4.20618	 cls_loss: 0.3907 cluster_loss: 1.2531 sup_con_loss: 0.7373 contrastive_loss: 4.6105 
2024-05-15 11:19:42.860 | INFO     | __main__:train:126 - Train Epoch: 58 Avg Loss: 4.1685 
2024-05-15 11:19:42.861 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:20:16.209 | INFO     | __main__:train:135 - Train Accuracies: All 0.8071 | Old 0.8503 | New 0.7643
2024-05-15 11:20:24.399 | INFO     | __main__:train:123 - Epoch: [59][0/63]	 loss 4.19597	 cls_loss: 0.3998 cluster_loss: 1.2420 sup_con_loss: 0.7178 contrastive_loss: 4.6115 
2024-05-15 11:20:42.499 | INFO     | __main__:train:123 - Epoch: [59][20/63]	 loss 4.20054	 cls_loss: 0.3917 cluster_loss: 1.2380 sup_con_loss: 0.7400 contrastive_loss: 4.6150 
2024-05-15 11:20:59.943 | INFO     | __main__:train:123 - Epoch: [59][40/63]	 loss 4.30737	 cls_loss: 0.3622 cluster_loss: 1.3751 sup_con_loss: 0.8104 contrastive_loss: 4.6202 
2024-05-15 11:21:15.965 | INFO     | __main__:train:123 - Epoch: [59][60/63]	 loss 4.08221	 cls_loss: 0.3643 cluster_loss: 1.1511 sup_con_loss: 0.6010 contrastive_loss: 4.6094 
2024-05-15 11:21:17.719 | INFO     | __main__:train:126 - Train Epoch: 59 Avg Loss: 4.1779 
2024-05-15 11:21:17.720 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:21:51.271 | INFO     | __main__:train:135 - Train Accuracies: All 0.8062 | Old 0.8523 | New 0.7606
2024-05-15 11:21:58.550 | INFO     | __main__:train:123 - Epoch: [60][0/63]	 loss 4.13519	 cls_loss: 0.3803 cluster_loss: 1.1915 sup_con_loss: 0.6663 contrastive_loss: 4.6068 
2024-05-15 11:22:17.404 | INFO     | __main__:train:123 - Epoch: [60][20/63]	 loss 4.07559	 cls_loss: 0.4063 cluster_loss: 1.1346 sup_con_loss: 0.5729 contrastive_loss: 4.6083 
2024-05-15 11:22:35.329 | INFO     | __main__:train:123 - Epoch: [60][40/63]	 loss 4.11225	 cls_loss: 0.4013 cluster_loss: 1.1902 sup_con_loss: 0.5905 contrastive_loss: 4.6023 
2024-05-15 11:22:51.243 | INFO     | __main__:train:123 - Epoch: [60][60/63]	 loss 4.15599	 cls_loss: 0.4032 cluster_loss: 1.2627 sup_con_loss: 0.5629 contrastive_loss: 4.6109 
2024-05-15 11:22:52.970 | INFO     | __main__:train:126 - Train Epoch: 60 Avg Loss: 4.1613 
2024-05-15 11:22:52.972 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:23:26.988 | INFO     | __main__:train:135 - Train Accuracies: All 0.8059 | Old 0.8498 | New 0.7623
2024-05-15 11:23:35.108 | INFO     | __main__:train:123 - Epoch: [61][0/63]	 loss 4.13804	 cls_loss: 0.3315 cluster_loss: 1.1822 sup_con_loss: 0.7361 contrastive_loss: 4.6091 
2024-05-15 11:23:53.877 | INFO     | __main__:train:123 - Epoch: [61][20/63]	 loss 4.16790	 cls_loss: 0.3439 cluster_loss: 1.2686 sup_con_loss: 0.6527 contrastive_loss: 4.6069 
2024-05-15 11:24:11.599 | INFO     | __main__:train:123 - Epoch: [61][40/63]	 loss 4.19028	 cls_loss: 0.3423 cluster_loss: 1.2750 sup_con_loss: 0.6928 contrastive_loss: 4.6142 
2024-05-15 11:24:27.482 | INFO     | __main__:train:123 - Epoch: [61][60/63]	 loss 4.21889	 cls_loss: 0.3868 cluster_loss: 1.2223 sup_con_loss: 0.8433 contrastive_loss: 4.6059 
2024-05-15 11:24:29.170 | INFO     | __main__:train:126 - Train Epoch: 61 Avg Loss: 4.1693 
2024-05-15 11:24:29.171 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:25:02.229 | INFO     | __main__:train:135 - Train Accuracies: All 0.8056 | Old 0.8481 | New 0.7636
2024-05-15 11:25:09.196 | INFO     | __main__:train:123 - Epoch: [62][0/63]	 loss 4.15427	 cls_loss: 0.3810 cluster_loss: 1.2497 sup_con_loss: 0.5987 contrastive_loss: 4.6140 
2024-05-15 11:25:27.597 | INFO     | __main__:train:123 - Epoch: [62][20/63]	 loss 4.14242	 cls_loss: 0.3632 cluster_loss: 1.2176 sup_con_loss: 0.6503 contrastive_loss: 4.6096 
2024-05-15 11:25:46.370 | INFO     | __main__:train:123 - Epoch: [62][40/63]	 loss 4.03079	 cls_loss: 0.4160 cluster_loss: 1.1584 sup_con_loss: 0.3811 contrastive_loss: 4.6136 
2024-05-15 11:26:02.350 | INFO     | __main__:train:123 - Epoch: [62][60/63]	 loss 4.16337	 cls_loss: 0.3666 cluster_loss: 1.2225 sup_con_loss: 0.7075 contrastive_loss: 4.6043 
2024-05-15 11:26:04.095 | INFO     | __main__:train:126 - Train Epoch: 62 Avg Loss: 4.1513 
2024-05-15 11:26:04.096 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:26:37.763 | INFO     | __main__:train:135 - Train Accuracies: All 0.8087 | Old 0.8481 | New 0.7697
2024-05-15 11:26:45.117 | INFO     | __main__:train:123 - Epoch: [63][0/63]	 loss 4.08769	 cls_loss: 0.3458 cluster_loss: 1.2549 sup_con_loss: 0.4467 contrastive_loss: 4.6071 
2024-05-15 11:27:03.599 | INFO     | __main__:train:123 - Epoch: [63][20/63]	 loss 4.25970	 cls_loss: 0.3576 cluster_loss: 1.2933 sup_con_loss: 0.8500 contrastive_loss: 4.6098 
2024-05-15 11:27:22.003 | INFO     | __main__:train:123 - Epoch: [63][40/63]	 loss 4.04410	 cls_loss: 0.3617 cluster_loss: 1.1489 sup_con_loss: 0.4880 contrastive_loss: 4.6152 
2024-05-15 11:27:37.865 | INFO     | __main__:train:123 - Epoch: [63][60/63]	 loss 4.17462	 cls_loss: 0.3762 cluster_loss: 1.2535 sup_con_loss: 0.6474 contrastive_loss: 4.6178 
2024-05-15 11:27:39.601 | INFO     | __main__:train:126 - Train Epoch: 63 Avg Loss: 4.1579 
2024-05-15 11:27:39.602 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:28:13.249 | INFO     | __main__:train:135 - Train Accuracies: All 0.8090 | Old 0.8468 | New 0.7715
2024-05-15 11:28:20.605 | INFO     | __main__:train:123 - Epoch: [64][0/63]	 loss 4.18628	 cls_loss: 0.3306 cluster_loss: 1.2202 sup_con_loss: 0.8016 contrastive_loss: 4.6106 
2024-05-15 11:28:39.256 | INFO     | __main__:train:123 - Epoch: [64][20/63]	 loss 4.01576	 cls_loss: 0.3458 cluster_loss: 1.1841 sup_con_loss: 0.3624 contrastive_loss: 4.6127 
2024-05-15 11:28:57.257 | INFO     | __main__:train:123 - Epoch: [64][40/63]	 loss 4.15882	 cls_loss: 0.3337 cluster_loss: 1.2106 sup_con_loss: 0.7354 contrastive_loss: 4.6120 
2024-05-15 11:29:13.172 | INFO     | __main__:train:123 - Epoch: [64][60/63]	 loss 4.08950	 cls_loss: 0.3781 cluster_loss: 1.2225 sup_con_loss: 0.4759 contrastive_loss: 4.6092 
2024-05-15 11:29:14.998 | INFO     | __main__:train:126 - Train Epoch: 64 Avg Loss: 4.1497 
2024-05-15 11:29:14.999 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:29:48.487 | INFO     | __main__:train:135 - Train Accuracies: All 0.8077 | Old 0.8503 | New 0.7655
2024-05-15 11:29:55.060 | INFO     | __main__:train:123 - Epoch: [65][0/63]	 loss 4.14964	 cls_loss: 0.3734 cluster_loss: 1.2145 sup_con_loss: 0.6811 contrastive_loss: 4.6018 
2024-05-15 11:30:13.376 | INFO     | __main__:train:123 - Epoch: [65][20/63]	 loss 4.07611	 cls_loss: 0.3731 cluster_loss: 1.2214 sup_con_loss: 0.4513 contrastive_loss: 4.6056 
2024-05-15 11:30:31.867 | INFO     | __main__:train:123 - Epoch: [65][40/63]	 loss 4.19684	 cls_loss: 0.3555 cluster_loss: 1.2880 sup_con_loss: 0.6935 contrastive_loss: 4.6038 
2024-05-15 11:30:48.712 | INFO     | __main__:train:123 - Epoch: [65][60/63]	 loss 4.11537	 cls_loss: 0.3768 cluster_loss: 1.1854 sup_con_loss: 0.6233 contrastive_loss: 4.6074 
2024-05-15 11:30:50.407 | INFO     | __main__:train:126 - Train Epoch: 65 Avg Loss: 4.1420 
2024-05-15 11:30:50.408 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:31:24.715 | INFO     | __main__:train:135 - Train Accuracies: All 0.8117 | Old 0.8516 | New 0.7722
2024-05-15 11:31:32.760 | INFO     | __main__:train:123 - Epoch: [66][0/63]	 loss 4.09299	 cls_loss: 0.3601 cluster_loss: 1.1832 sup_con_loss: 0.5772 contrastive_loss: 4.6090 
2024-05-15 11:31:50.996 | INFO     | __main__:train:123 - Epoch: [66][20/63]	 loss 4.18082	 cls_loss: 0.3449 cluster_loss: 1.2554 sup_con_loss: 0.6909 contrastive_loss: 4.6189 
2024-05-15 11:32:09.121 | INFO     | __main__:train:123 - Epoch: [66][40/63]	 loss 4.08322	 cls_loss: 0.3283 cluster_loss: 1.2124 sup_con_loss: 0.5433 contrastive_loss: 4.6001 
2024-05-15 11:32:25.155 | INFO     | __main__:train:123 - Epoch: [66][60/63]	 loss 4.02308	 cls_loss: 0.3652 cluster_loss: 1.0864 sup_con_loss: 0.5734 contrastive_loss: 4.5975 
2024-05-15 11:32:26.934 | INFO     | __main__:train:126 - Train Epoch: 66 Avg Loss: 4.1399 
2024-05-15 11:32:26.934 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:33:00.588 | INFO     | __main__:train:135 - Train Accuracies: All 0.8098 | Old 0.8503 | New 0.7697
2024-05-15 11:33:09.047 | INFO     | __main__:train:123 - Epoch: [67][0/63]	 loss 4.16960	 cls_loss: 0.4070 cluster_loss: 1.2337 sup_con_loss: 0.6682 contrastive_loss: 4.6021 
2024-05-15 11:33:27.665 | INFO     | __main__:train:123 - Epoch: [67][20/63]	 loss 4.21376	 cls_loss: 0.3323 cluster_loss: 1.2656 sup_con_loss: 0.8054 contrastive_loss: 4.6046 
2024-05-15 11:33:46.757 | INFO     | __main__:train:123 - Epoch: [67][40/63]	 loss 4.21413	 cls_loss: 0.3905 cluster_loss: 1.2595 sup_con_loss: 0.7560 contrastive_loss: 4.6064 
2024-05-15 11:34:02.883 | INFO     | __main__:train:123 - Epoch: [67][60/63]	 loss 4.07709	 cls_loss: 0.3673 cluster_loss: 1.1261 sup_con_loss: 0.6392 contrastive_loss: 4.6044 
2024-05-15 11:34:04.621 | INFO     | __main__:train:126 - Train Epoch: 67 Avg Loss: 4.1627 
2024-05-15 11:34:04.622 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:34:38.767 | INFO     | __main__:train:135 - Train Accuracies: All 0.8070 | Old 0.8508 | New 0.7636
2024-05-15 11:34:46.588 | INFO     | __main__:train:123 - Epoch: [68][0/63]	 loss 4.08757	 cls_loss: 0.3331 cluster_loss: 1.2510 sup_con_loss: 0.4540 contrastive_loss: 4.6137 
2024-05-15 11:35:04.566 | INFO     | __main__:train:123 - Epoch: [68][20/63]	 loss 3.96193	 cls_loss: 0.3455 cluster_loss: 1.1327 sup_con_loss: 0.3256 contrastive_loss: 4.6012 
2024-05-15 11:35:22.689 | INFO     | __main__:train:123 - Epoch: [68][40/63]	 loss 4.20878	 cls_loss: 0.3918 cluster_loss: 1.2318 sup_con_loss: 0.7743 contrastive_loss: 4.6154 
2024-05-15 11:35:38.748 | INFO     | __main__:train:123 - Epoch: [68][60/63]	 loss 4.12829	 cls_loss: 0.3748 cluster_loss: 1.2041 sup_con_loss: 0.6365 contrastive_loss: 4.6026 
2024-05-15 11:35:40.476 | INFO     | __main__:train:126 - Train Epoch: 68 Avg Loss: 4.1528 
2024-05-15 11:35:40.477 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:36:14.314 | INFO     | __main__:train:135 - Train Accuracies: All 0.8086 | Old 0.8491 | New 0.7685
2024-05-15 11:36:21.865 | INFO     | __main__:train:123 - Epoch: [69][0/63]	 loss 4.12553	 cls_loss: 0.3472 cluster_loss: 1.1911 sup_con_loss: 0.6739 contrastive_loss: 4.6060 
2024-05-15 11:36:40.321 | INFO     | __main__:train:123 - Epoch: [69][20/63]	 loss 4.21859	 cls_loss: 0.3521 cluster_loss: 1.3184 sup_con_loss: 0.6945 contrastive_loss: 4.6081 
2024-05-15 11:36:58.594 | INFO     | __main__:train:123 - Epoch: [69][40/63]	 loss 4.19904	 cls_loss: 0.3686 cluster_loss: 1.2624 sup_con_loss: 0.7157 contrastive_loss: 4.6138 
2024-05-15 11:37:14.661 | INFO     | __main__:train:123 - Epoch: [69][60/63]	 loss 4.00920	 cls_loss: 0.3587 cluster_loss: 1.1578 sup_con_loss: 0.4018 contrastive_loss: 4.6007 
2024-05-15 11:37:16.412 | INFO     | __main__:train:126 - Train Epoch: 69 Avg Loss: 4.1432 
2024-05-15 11:37:16.413 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:37:50.423 | INFO     | __main__:train:135 - Train Accuracies: All 0.8070 | Old 0.8483 | New 0.7660
2024-05-15 11:37:57.579 | INFO     | __main__:train:123 - Epoch: [70][0/63]	 loss 4.13085	 cls_loss: 0.3490 cluster_loss: 1.2547 sup_con_loss: 0.5757 contrastive_loss: 4.6025 
2024-05-15 11:38:16.301 | INFO     | __main__:train:123 - Epoch: [70][20/63]	 loss 4.04247	 cls_loss: 0.3698 cluster_loss: 1.1520 sup_con_loss: 0.4850 contrastive_loss: 4.6068 
2024-05-15 11:38:34.547 | INFO     | __main__:train:123 - Epoch: [70][40/63]	 loss 4.17936	 cls_loss: 0.3527 cluster_loss: 1.2261 sup_con_loss: 0.7494 contrastive_loss: 4.6102 
2024-05-15 11:38:50.795 | INFO     | __main__:train:123 - Epoch: [70][60/63]	 loss 4.05948	 cls_loss: 0.2973 cluster_loss: 1.1516 sup_con_loss: 0.6126 contrastive_loss: 4.6038 
2024-05-15 11:38:52.596 | INFO     | __main__:train:126 - Train Epoch: 70 Avg Loss: 4.1481 
2024-05-15 11:38:52.597 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:39:27.255 | INFO     | __main__:train:135 - Train Accuracies: All 0.8100 | Old 0.8526 | New 0.7678
2024-05-15 11:39:35.585 | INFO     | __main__:train:123 - Epoch: [71][0/63]	 loss 4.08642	 cls_loss: 0.3677 cluster_loss: 1.1648 sup_con_loss: 0.5951 contrastive_loss: 4.6036 
2024-05-15 11:39:54.231 | INFO     | __main__:train:123 - Epoch: [71][20/63]	 loss 4.14720	 cls_loss: 0.3471 cluster_loss: 1.2182 sup_con_loss: 0.6893 contrastive_loss: 4.6040 
2024-05-15 11:40:12.263 | INFO     | __main__:train:123 - Epoch: [71][40/63]	 loss 4.21414	 cls_loss: 0.3074 cluster_loss: 1.3013 sup_con_loss: 0.7575 contrastive_loss: 4.6086 
2024-05-15 11:40:28.119 | INFO     | __main__:train:123 - Epoch: [71][60/63]	 loss 4.24171	 cls_loss: 0.3179 cluster_loss: 1.3656 sup_con_loss: 0.7104 contrastive_loss: 4.6064 
2024-05-15 11:40:29.930 | INFO     | __main__:train:126 - Train Epoch: 71 Avg Loss: 4.1515 
2024-05-15 11:40:29.930 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:41:03.270 | INFO     | __main__:train:135 - Train Accuracies: All 0.8082 | Old 0.8476 | New 0.7692
2024-05-15 11:41:10.810 | INFO     | __main__:train:123 - Epoch: [72][0/63]	 loss 4.16152	 cls_loss: 0.3806 cluster_loss: 1.2578 sup_con_loss: 0.6133 contrastive_loss: 4.6094 
2024-05-15 11:41:29.226 | INFO     | __main__:train:123 - Epoch: [72][20/63]	 loss 4.03182	 cls_loss: 0.3397 cluster_loss: 1.1619 sup_con_loss: 0.4739 contrastive_loss: 4.6028 
2024-05-15 11:41:47.845 | INFO     | __main__:train:123 - Epoch: [72][40/63]	 loss 4.07338	 cls_loss: 0.3145 cluster_loss: 1.2124 sup_con_loss: 0.5034 contrastive_loss: 4.6140 
2024-05-15 11:42:04.341 | INFO     | __main__:train:123 - Epoch: [72][60/63]	 loss 3.99259	 cls_loss: 0.3286 cluster_loss: 1.1425 sup_con_loss: 0.4066 contrastive_loss: 4.6041 
2024-05-15 11:42:06.127 | INFO     | __main__:train:126 - Train Epoch: 72 Avg Loss: 4.1312 
2024-05-15 11:42:06.128 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:42:40.014 | INFO     | __main__:train:135 - Train Accuracies: All 0.8110 | Old 0.8513 | New 0.7710
2024-05-15 11:42:47.512 | INFO     | __main__:train:123 - Epoch: [73][0/63]	 loss 4.04951	 cls_loss: 0.3376 cluster_loss: 1.1448 sup_con_loss: 0.5628 contrastive_loss: 4.6004 
2024-05-15 11:43:06.811 | INFO     | __main__:train:123 - Epoch: [73][20/63]	 loss 4.05888	 cls_loss: 0.3346 cluster_loss: 1.1772 sup_con_loss: 0.5321 contrastive_loss: 4.6005 
2024-05-15 11:43:25.231 | INFO     | __main__:train:123 - Epoch: [73][40/63]	 loss 4.21439	 cls_loss: 0.3455 cluster_loss: 1.2551 sup_con_loss: 0.8225 contrastive_loss: 4.5996 
2024-05-15 11:43:41.299 | INFO     | __main__:train:123 - Epoch: [73][60/63]	 loss 4.04007	 cls_loss: 0.3785 cluster_loss: 1.1446 sup_con_loss: 0.4751 contrastive_loss: 4.6113 
2024-05-15 11:43:43.083 | INFO     | __main__:train:126 - Train Epoch: 73 Avg Loss: 4.1398 
2024-05-15 11:43:43.085 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:44:17.219 | INFO     | __main__:train:135 - Train Accuracies: All 0.8060 | Old 0.8451 | New 0.7673
2024-05-15 11:44:25.146 | INFO     | __main__:train:123 - Epoch: [74][0/63]	 loss 4.22555	 cls_loss: 0.3282 cluster_loss: 1.3070 sup_con_loss: 0.7630 contrastive_loss: 4.6063 
2024-05-15 11:44:43.353 | INFO     | __main__:train:123 - Epoch: [74][20/63]	 loss 4.11402	 cls_loss: 0.3779 cluster_loss: 1.2002 sup_con_loss: 0.5930 contrastive_loss: 4.6063 
2024-05-15 11:45:01.841 | INFO     | __main__:train:123 - Epoch: [74][40/63]	 loss 4.04143	 cls_loss: 0.3614 cluster_loss: 1.1695 sup_con_loss: 0.4581 contrastive_loss: 4.6068 
2024-05-15 11:45:17.726 | INFO     | __main__:train:123 - Epoch: [74][60/63]	 loss 4.19675	 cls_loss: 0.3227 cluster_loss: 1.2688 sup_con_loss: 0.7410 contrastive_loss: 4.6150 
2024-05-15 11:45:19.432 | INFO     | __main__:train:126 - Train Epoch: 74 Avg Loss: 4.1337 
2024-05-15 11:45:19.432 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:45:53.122 | INFO     | __main__:train:135 - Train Accuracies: All 0.8080 | Old 0.8461 | New 0.7702
2024-05-15 11:46:00.034 | INFO     | __main__:train:123 - Epoch: [75][0/63]	 loss 4.07156	 cls_loss: 0.3351 cluster_loss: 1.1601 sup_con_loss: 0.6069 contrastive_loss: 4.5966 
2024-05-15 11:46:19.870 | INFO     | __main__:train:123 - Epoch: [75][20/63]	 loss 4.28867	 cls_loss: 0.3557 cluster_loss: 1.3192 sup_con_loss: 0.8868 contrastive_loss: 4.6097 
2024-05-15 11:46:38.659 | INFO     | __main__:train:123 - Epoch: [75][40/63]	 loss 4.15946	 cls_loss: 0.3628 cluster_loss: 1.2712 sup_con_loss: 0.6016 contrastive_loss: 4.6086 
2024-05-15 11:46:54.573 | INFO     | __main__:train:123 - Epoch: [75][60/63]	 loss 4.07057	 cls_loss: 0.3324 cluster_loss: 1.1831 sup_con_loss: 0.5438 contrastive_loss: 4.6075 
2024-05-15 11:46:56.325 | INFO     | __main__:train:126 - Train Epoch: 75 Avg Loss: 4.1235 
2024-05-15 11:46:56.325 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:47:31.551 | INFO     | __main__:train:135 - Train Accuracies: All 0.8110 | Old 0.8496 | New 0.7727
2024-05-15 11:47:38.599 | INFO     | __main__:train:123 - Epoch: [76][0/63]	 loss 4.07861	 cls_loss: 0.3175 cluster_loss: 1.1918 sup_con_loss: 0.5793 contrastive_loss: 4.6001 
2024-05-15 11:47:57.293 | INFO     | __main__:train:123 - Epoch: [76][20/63]	 loss 4.05244	 cls_loss: 0.3379 cluster_loss: 1.1485 sup_con_loss: 0.5731 contrastive_loss: 4.5955 
2024-05-15 11:48:15.643 | INFO     | __main__:train:123 - Epoch: [76][40/63]	 loss 4.22620	 cls_loss: 0.3561 cluster_loss: 1.3065 sup_con_loss: 0.7258 contrastive_loss: 4.6128 
2024-05-15 11:48:31.893 | INFO     | __main__:train:123 - Epoch: [76][60/63]	 loss 4.17816	 cls_loss: 0.3373 cluster_loss: 1.2734 sup_con_loss: 0.6711 contrastive_loss: 4.6116 
2024-05-15 11:48:33.623 | INFO     | __main__:train:126 - Train Epoch: 76 Avg Loss: 4.1239 
2024-05-15 11:48:33.624 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:49:06.571 | INFO     | __main__:train:135 - Train Accuracies: All 0.8108 | Old 0.8476 | New 0.7744
2024-05-15 11:49:14.125 | INFO     | __main__:train:123 - Epoch: [77][0/63]	 loss 4.06293	 cls_loss: 0.3389 cluster_loss: 1.1481 sup_con_loss: 0.5812 contrastive_loss: 4.6071 
2024-05-15 11:49:32.620 | INFO     | __main__:train:123 - Epoch: [77][20/63]	 loss 4.19796	 cls_loss: 0.3218 cluster_loss: 1.2681 sup_con_loss: 0.7859 contrastive_loss: 4.5939 
2024-05-15 11:49:51.513 | INFO     | __main__:train:123 - Epoch: [77][40/63]	 loss 4.22348	 cls_loss: 0.3324 cluster_loss: 1.2985 sup_con_loss: 0.7541 contrastive_loss: 4.6141 
2024-05-15 11:50:07.639 | INFO     | __main__:train:123 - Epoch: [77][60/63]	 loss 4.14326	 cls_loss: 0.3378 cluster_loss: 1.2129 sup_con_loss: 0.7071 contrastive_loss: 4.5988 
2024-05-15 11:50:09.416 | INFO     | __main__:train:126 - Train Epoch: 77 Avg Loss: 4.1494 
2024-05-15 11:50:09.416 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:50:44.005 | INFO     | __main__:train:135 - Train Accuracies: All 0.8105 | Old 0.8501 | New 0.7712
2024-05-15 11:50:51.940 | INFO     | __main__:train:123 - Epoch: [78][0/63]	 loss 4.08032	 cls_loss: 0.3022 cluster_loss: 1.2162 sup_con_loss: 0.5463 contrastive_loss: 4.6043 
2024-05-15 11:51:10.528 | INFO     | __main__:train:123 - Epoch: [78][20/63]	 loss 4.06951	 cls_loss: 0.3434 cluster_loss: 1.1947 sup_con_loss: 0.5202 contrastive_loss: 4.6011 
2024-05-15 11:51:28.915 | INFO     | __main__:train:123 - Epoch: [78][40/63]	 loss 4.07234	 cls_loss: 0.3254 cluster_loss: 1.1415 sup_con_loss: 0.6561 contrastive_loss: 4.5952 
2024-05-15 11:51:44.565 | INFO     | __main__:train:123 - Epoch: [78][60/63]	 loss 4.16400	 cls_loss: 0.3071 cluster_loss: 1.2394 sup_con_loss: 0.7490 contrastive_loss: 4.5980 
2024-05-15 11:51:46.280 | INFO     | __main__:train:126 - Train Epoch: 78 Avg Loss: 4.1278 
2024-05-15 11:51:46.281 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:52:20.672 | INFO     | __main__:train:135 - Train Accuracies: All 0.8103 | Old 0.8476 | New 0.7735
2024-05-15 11:52:28.011 | INFO     | __main__:train:123 - Epoch: [79][0/63]	 loss 4.18895	 cls_loss: 0.3404 cluster_loss: 1.2434 sup_con_loss: 0.7823 contrastive_loss: 4.5966 
2024-05-15 11:52:46.662 | INFO     | __main__:train:123 - Epoch: [79][20/63]	 loss 4.13923	 cls_loss: 0.3602 cluster_loss: 1.2004 sup_con_loss: 0.6736 contrastive_loss: 4.6110 
2024-05-15 11:53:05.007 | INFO     | __main__:train:123 - Epoch: [79][40/63]	 loss 4.07147	 cls_loss: 0.3425 cluster_loss: 1.1690 sup_con_loss: 0.5806 contrastive_loss: 4.5978 
2024-05-15 11:53:21.347 | INFO     | __main__:train:123 - Epoch: [79][60/63]	 loss 4.11676	 cls_loss: 0.3765 cluster_loss: 1.1932 sup_con_loss: 0.6105 contrastive_loss: 4.6088 
2024-05-15 11:53:23.079 | INFO     | __main__:train:126 - Train Epoch: 79 Avg Loss: 4.1272 
2024-05-15 11:53:23.080 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:53:57.326 | INFO     | __main__:train:135 - Train Accuracies: All 0.8116 | Old 0.8491 | New 0.7744
2024-05-15 11:54:05.662 | INFO     | __main__:train:123 - Epoch: [80][0/63]	 loss 4.06937	 cls_loss: 0.3300 cluster_loss: 1.1760 sup_con_loss: 0.5589 contrastive_loss: 4.6059 
2024-05-15 11:54:24.569 | INFO     | __main__:train:123 - Epoch: [80][20/63]	 loss 4.08758	 cls_loss: 0.3368 cluster_loss: 1.1610 sup_con_loss: 0.6530 contrastive_loss: 4.5946 
2024-05-15 11:54:42.696 | INFO     | __main__:train:123 - Epoch: [80][40/63]	 loss 4.10197	 cls_loss: 0.3415 cluster_loss: 1.2019 sup_con_loss: 0.6006 contrastive_loss: 4.6016 
2024-05-15 11:54:58.526 | INFO     | __main__:train:123 - Epoch: [80][60/63]	 loss 4.17004	 cls_loss: 0.3277 cluster_loss: 1.2728 sup_con_loss: 0.6642 contrastive_loss: 4.6085 
2024-05-15 11:55:00.268 | INFO     | __main__:train:126 - Train Epoch: 80 Avg Loss: 4.1341 
2024-05-15 11:55:00.268 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:55:35.961 | INFO     | __main__:train:135 - Train Accuracies: All 0.8105 | Old 0.8516 | New 0.7697
2024-05-15 11:55:43.974 | INFO     | __main__:train:123 - Epoch: [81][0/63]	 loss 4.13506	 cls_loss: 0.3209 cluster_loss: 1.2714 sup_con_loss: 0.5730 contrastive_loss: 4.6089 
2024-05-15 11:56:02.469 | INFO     | __main__:train:123 - Epoch: [81][20/63]	 loss 4.20496	 cls_loss: 0.3051 cluster_loss: 1.2622 sup_con_loss: 0.8147 contrastive_loss: 4.6040 
2024-05-15 11:56:20.454 | INFO     | __main__:train:123 - Epoch: [81][40/63]	 loss 4.09490	 cls_loss: 0.3249 cluster_loss: 1.2456 sup_con_loss: 0.4983 contrastive_loss: 4.6110 
2024-05-15 11:56:36.930 | INFO     | __main__:train:123 - Epoch: [81][60/63]	 loss 4.04875	 cls_loss: 0.3343 cluster_loss: 1.1613 sup_con_loss: 0.5291 contrastive_loss: 4.6026 
2024-05-15 11:56:38.661 | INFO     | __main__:train:126 - Train Epoch: 81 Avg Loss: 4.1206 
2024-05-15 11:56:38.662 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:57:12.931 | INFO     | __main__:train:135 - Train Accuracies: All 0.8090 | Old 0.8451 | New 0.7732
2024-05-15 11:57:21.554 | INFO     | __main__:train:123 - Epoch: [82][0/63]	 loss 4.16357	 cls_loss: 0.3542 cluster_loss: 1.2533 sup_con_loss: 0.6395 contrastive_loss: 4.6171 
2024-05-15 11:57:39.869 | INFO     | __main__:train:123 - Epoch: [82][20/63]	 loss 4.15947	 cls_loss: 0.3242 cluster_loss: 1.2963 sup_con_loss: 0.5958 contrastive_loss: 4.6075 
2024-05-15 11:57:58.187 | INFO     | __main__:train:123 - Epoch: [82][40/63]	 loss 4.26982	 cls_loss: 0.3164 cluster_loss: 1.2973 sup_con_loss: 0.9255 contrastive_loss: 4.6030 
2024-05-15 11:58:13.853 | INFO     | __main__:train:123 - Epoch: [82][60/63]	 loss 3.99761	 cls_loss: 0.3409 cluster_loss: 1.1589 sup_con_loss: 0.3784 contrastive_loss: 4.6040 
2024-05-15 11:58:15.661 | INFO     | __main__:train:126 - Train Epoch: 82 Avg Loss: 4.1111 
2024-05-15 11:58:15.662 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 11:58:49.219 | INFO     | __main__:train:135 - Train Accuracies: All 0.8085 | Old 0.8456 | New 0.7717
2024-05-15 11:58:56.793 | INFO     | __main__:train:123 - Epoch: [83][0/63]	 loss 4.03573	 cls_loss: 0.3365 cluster_loss: 1.1563 sup_con_loss: 0.5017 contrastive_loss: 4.6012 
2024-05-15 11:59:15.086 | INFO     | __main__:train:123 - Epoch: [83][20/63]	 loss 4.18110	 cls_loss: 0.3227 cluster_loss: 1.2257 sup_con_loss: 0.8073 contrastive_loss: 4.5984 
2024-05-15 11:59:34.011 | INFO     | __main__:train:123 - Epoch: [83][40/63]	 loss 4.09613	 cls_loss: 0.3470 cluster_loss: 1.1836 sup_con_loss: 0.6121 contrastive_loss: 4.6017 
2024-05-15 11:59:50.385 | INFO     | __main__:train:123 - Epoch: [83][60/63]	 loss 4.05445	 cls_loss: 0.3297 cluster_loss: 1.1771 sup_con_loss: 0.5263 contrastive_loss: 4.5996 
2024-05-15 11:59:52.143 | INFO     | __main__:train:126 - Train Epoch: 83 Avg Loss: 4.1365 
2024-05-15 11:59:52.144 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:00:26.731 | INFO     | __main__:train:135 - Train Accuracies: All 0.8120 | Old 0.8503 | New 0.7740
2024-05-15 12:00:34.127 | INFO     | __main__:train:123 - Epoch: [84][0/63]	 loss 4.10597	 cls_loss: 0.3253 cluster_loss: 1.1911 sup_con_loss: 0.6434 contrastive_loss: 4.6041 
2024-05-15 12:00:53.236 | INFO     | __main__:train:123 - Epoch: [84][20/63]	 loss 4.12334	 cls_loss: 0.3158 cluster_loss: 1.2099 sup_con_loss: 0.6762 contrastive_loss: 4.5996 
2024-05-15 12:01:11.251 | INFO     | __main__:train:123 - Epoch: [84][40/63]	 loss 4.06137	 cls_loss: 0.3475 cluster_loss: 1.1594 sup_con_loss: 0.5513 contrastive_loss: 4.6049 
2024-05-15 12:01:27.585 | INFO     | __main__:train:123 - Epoch: [84][60/63]	 loss 4.04687	 cls_loss: 0.3085 cluster_loss: 1.1535 sup_con_loss: 0.5658 contrastive_loss: 4.6016 
2024-05-15 12:01:29.298 | INFO     | __main__:train:126 - Train Epoch: 84 Avg Loss: 4.1162 
2024-05-15 12:01:29.298 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:02:02.581 | INFO     | __main__:train:135 - Train Accuracies: All 0.8118 | Old 0.8486 | New 0.7754
2024-05-15 12:02:11.254 | INFO     | __main__:train:123 - Epoch: [85][0/63]	 loss 4.15110	 cls_loss: 0.3148 cluster_loss: 1.2366 sup_con_loss: 0.7040 contrastive_loss: 4.6011 
2024-05-15 12:02:29.746 | INFO     | __main__:train:123 - Epoch: [85][20/63]	 loss 4.05382	 cls_loss: 0.3451 cluster_loss: 1.2033 sup_con_loss: 0.4584 contrastive_loss: 4.6006 
2024-05-15 12:02:47.817 | INFO     | __main__:train:123 - Epoch: [85][40/63]	 loss 4.19674	 cls_loss: 0.3303 cluster_loss: 1.2470 sup_con_loss: 0.7953 contrastive_loss: 4.6034 
2024-05-15 12:03:03.926 | INFO     | __main__:train:123 - Epoch: [85][60/63]	 loss 4.11632	 cls_loss: 0.3306 cluster_loss: 1.2224 sup_con_loss: 0.6098 contrastive_loss: 4.6041 
2024-05-15 12:03:05.803 | INFO     | __main__:train:126 - Train Epoch: 85 Avg Loss: 4.1352 
2024-05-15 12:03:05.805 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:03:40.010 | INFO     | __main__:train:135 - Train Accuracies: All 0.8069 | Old 0.8436 | New 0.7705
2024-05-15 12:03:47.607 | INFO     | __main__:train:123 - Epoch: [86][0/63]	 loss 4.24637	 cls_loss: 0.3122 cluster_loss: 1.3240 sup_con_loss: 0.8114 contrastive_loss: 4.6039 
2024-05-15 12:04:05.967 | INFO     | __main__:train:123 - Epoch: [86][20/63]	 loss 4.19431	 cls_loss: 0.3247 cluster_loss: 1.2708 sup_con_loss: 0.7458 contrastive_loss: 4.6055 
2024-05-15 12:04:24.177 | INFO     | __main__:train:123 - Epoch: [86][40/63]	 loss 4.07171	 cls_loss: 0.3393 cluster_loss: 1.2028 sup_con_loss: 0.5045 contrastive_loss: 4.6071 
2024-05-15 12:04:40.299 | INFO     | __main__:train:123 - Epoch: [86][60/63]	 loss 4.06176	 cls_loss: 0.3201 cluster_loss: 1.2020 sup_con_loss: 0.5035 contrastive_loss: 4.6034 
2024-05-15 12:04:42.061 | INFO     | __main__:train:126 - Train Epoch: 86 Avg Loss: 4.1214 
2024-05-15 12:04:42.061 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:05:15.170 | INFO     | __main__:train:135 - Train Accuracies: All 0.8123 | Old 0.8483 | New 0.7767
2024-05-15 12:05:21.689 | INFO     | __main__:train:123 - Epoch: [87][0/63]	 loss 4.10810	 cls_loss: 0.3562 cluster_loss: 1.1917 sup_con_loss: 0.6187 contrastive_loss: 4.6035 
2024-05-15 12:05:40.893 | INFO     | __main__:train:123 - Epoch: [87][20/63]	 loss 4.11368	 cls_loss: 0.3127 cluster_loss: 1.1948 sup_con_loss: 0.6838 contrastive_loss: 4.5973 
2024-05-15 12:05:59.273 | INFO     | __main__:train:123 - Epoch: [87][40/63]	 loss 4.08905	 cls_loss: 0.3084 cluster_loss: 1.1881 sup_con_loss: 0.6104 contrastive_loss: 4.6080 
2024-05-15 12:06:15.440 | INFO     | __main__:train:123 - Epoch: [87][60/63]	 loss 4.31040	 cls_loss: 0.3103 cluster_loss: 1.3614 sup_con_loss: 0.9176 contrastive_loss: 4.6088 
2024-05-15 12:06:17.232 | INFO     | __main__:train:126 - Train Epoch: 87 Avg Loss: 4.1111 
2024-05-15 12:06:17.232 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:06:51.222 | INFO     | __main__:train:135 - Train Accuracies: All 0.8125 | Old 0.8471 | New 0.7782
2024-05-15 12:06:58.523 | INFO     | __main__:train:123 - Epoch: [88][0/63]	 loss 4.12866	 cls_loss: 0.2961 cluster_loss: 1.1927 sup_con_loss: 0.7502 contrastive_loss: 4.5957 
2024-05-15 12:07:17.029 | INFO     | __main__:train:123 - Epoch: [88][20/63]	 loss 3.96115	 cls_loss: 0.3046 cluster_loss: 1.1807 sup_con_loss: 0.2615 contrastive_loss: 4.6085 
2024-05-15 12:07:35.285 | INFO     | __main__:train:123 - Epoch: [88][40/63]	 loss 4.03884	 cls_loss: 0.3060 cluster_loss: 1.2118 sup_con_loss: 0.4424 contrastive_loss: 4.5988 
2024-05-15 12:07:51.332 | INFO     | __main__:train:123 - Epoch: [88][60/63]	 loss 4.08427	 cls_loss: 0.3415 cluster_loss: 1.1321 sup_con_loss: 0.6889 contrastive_loss: 4.5966 
2024-05-15 12:07:53.198 | INFO     | __main__:train:126 - Train Epoch: 88 Avg Loss: 4.1108 
2024-05-15 12:07:53.199 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:08:27.591 | INFO     | __main__:train:135 - Train Accuracies: All 0.8115 | Old 0.8456 | New 0.7777
2024-05-15 12:08:34.523 | INFO     | __main__:train:123 - Epoch: [89][0/63]	 loss 4.06641	 cls_loss: 0.3174 cluster_loss: 1.2123 sup_con_loss: 0.5089 contrastive_loss: 4.5989 
2024-05-15 12:08:52.945 | INFO     | __main__:train:123 - Epoch: [89][20/63]	 loss 4.15944	 cls_loss: 0.3409 cluster_loss: 1.2179 sup_con_loss: 0.7367 contrastive_loss: 4.6010 
2024-05-15 12:09:11.641 | INFO     | __main__:train:123 - Epoch: [89][40/63]	 loss 3.96613	 cls_loss: 0.3174 cluster_loss: 1.0952 sup_con_loss: 0.4535 contrastive_loss: 4.5915 
2024-05-15 12:09:28.136 | INFO     | __main__:train:123 - Epoch: [89][60/63]	 loss 4.21331	 cls_loss: 0.3150 cluster_loss: 1.2828 sup_con_loss: 0.8083 contrastive_loss: 4.5943 
2024-05-15 12:09:29.920 | INFO     | __main__:train:126 - Train Epoch: 89 Avg Loss: 4.0966 
2024-05-15 12:09:29.921 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:10:03.092 | INFO     | __main__:train:135 - Train Accuracies: All 0.8106 | Old 0.8453 | New 0.7762
2024-05-15 12:10:11.964 | INFO     | __main__:train:123 - Epoch: [90][0/63]	 loss 4.15578	 cls_loss: 0.2881 cluster_loss: 1.2642 sup_con_loss: 0.7116 contrastive_loss: 4.5910 
2024-05-15 12:10:30.400 | INFO     | __main__:train:123 - Epoch: [90][20/63]	 loss 4.11421	 cls_loss: 0.3313 cluster_loss: 1.2084 sup_con_loss: 0.6255 contrastive_loss: 4.6060 
2024-05-15 12:10:48.673 | INFO     | __main__:train:123 - Epoch: [90][40/63]	 loss 4.04180	 cls_loss: 0.2977 cluster_loss: 1.2162 sup_con_loss: 0.4344 contrastive_loss: 4.6078 
2024-05-15 12:11:04.252 | INFO     | __main__:train:123 - Epoch: [90][60/63]	 loss 4.17675	 cls_loss: 0.3422 cluster_loss: 1.2414 sup_con_loss: 0.7412 contrastive_loss: 4.6010 
2024-05-15 12:11:06.056 | INFO     | __main__:train:126 - Train Epoch: 90 Avg Loss: 4.1118 
2024-05-15 12:11:06.056 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:11:40.033 | INFO     | __main__:train:135 - Train Accuracies: All 0.8081 | Old 0.8473 | New 0.7692
2024-05-15 12:11:48.331 | INFO     | __main__:train:123 - Epoch: [91][0/63]	 loss 4.11349	 cls_loss: 0.3361 cluster_loss: 1.2153 sup_con_loss: 0.6179 contrastive_loss: 4.5994 
2024-05-15 12:12:07.094 | INFO     | __main__:train:123 - Epoch: [91][20/63]	 loss 4.11651	 cls_loss: 0.3516 cluster_loss: 1.1765 sup_con_loss: 0.6796 contrastive_loss: 4.6013 
2024-05-15 12:12:25.403 | INFO     | __main__:train:123 - Epoch: [91][40/63]	 loss 4.09167	 cls_loss: 0.3195 cluster_loss: 1.1409 sup_con_loss: 0.7222 contrastive_loss: 4.5931 
2024-05-15 12:12:41.368 | INFO     | __main__:train:123 - Epoch: [91][60/63]	 loss 4.06644	 cls_loss: 0.3755 cluster_loss: 1.1871 sup_con_loss: 0.4830 contrastive_loss: 4.6067 
2024-05-15 12:12:43.058 | INFO     | __main__:train:126 - Train Epoch: 91 Avg Loss: 4.1241 
2024-05-15 12:12:43.058 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:13:16.801 | INFO     | __main__:train:135 - Train Accuracies: All 0.8082 | Old 0.8433 | New 0.7735
2024-05-15 12:13:24.399 | INFO     | __main__:train:123 - Epoch: [92][0/63]	 loss 4.08706	 cls_loss: 0.3533 cluster_loss: 1.2256 sup_con_loss: 0.4907 contrastive_loss: 4.6078 
2024-05-15 12:13:43.101 | INFO     | __main__:train:123 - Epoch: [92][20/63]	 loss 4.19410	 cls_loss: 0.3000 cluster_loss: 1.2901 sup_con_loss: 0.7367 contrastive_loss: 4.6042 
2024-05-15 12:14:01.183 | INFO     | __main__:train:123 - Epoch: [92][40/63]	 loss 4.23613	 cls_loss: 0.3101 cluster_loss: 1.2930 sup_con_loss: 0.8286 contrastive_loss: 4.6110 
2024-05-15 12:14:17.264 | INFO     | __main__:train:123 - Epoch: [92][60/63]	 loss 4.09459	 cls_loss: 0.3001 cluster_loss: 1.1888 sup_con_loss: 0.6491 contrastive_loss: 4.5994 
2024-05-15 12:14:18.977 | INFO     | __main__:train:126 - Train Epoch: 92 Avg Loss: 4.1083 
2024-05-15 12:14:18.978 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:14:52.927 | INFO     | __main__:train:135 - Train Accuracies: All 0.8100 | Old 0.8466 | New 0.7737
2024-05-15 12:15:00.589 | INFO     | __main__:train:123 - Epoch: [93][0/63]	 loss 4.08600	 cls_loss: 0.3001 cluster_loss: 1.2251 sup_con_loss: 0.5495 contrastive_loss: 4.6036 
2024-05-15 12:15:19.129 | INFO     | __main__:train:123 - Epoch: [93][20/63]	 loss 4.13248	 cls_loss: 0.3184 cluster_loss: 1.2173 sup_con_loss: 0.6989 contrastive_loss: 4.5926 
2024-05-15 12:15:37.420 | INFO     | __main__:train:123 - Epoch: [93][40/63]	 loss 4.16055	 cls_loss: 0.2863 cluster_loss: 1.2854 sup_con_loss: 0.6763 contrastive_loss: 4.5971 
2024-05-15 12:15:53.250 | INFO     | __main__:train:123 - Epoch: [93][60/63]	 loss 4.11627	 cls_loss: 0.3075 cluster_loss: 1.1930 sup_con_loss: 0.7044 contrastive_loss: 4.5948 
2024-05-15 12:15:54.956 | INFO     | __main__:train:126 - Train Epoch: 93 Avg Loss: 4.0912 
2024-05-15 12:15:54.956 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:16:29.069 | INFO     | __main__:train:135 - Train Accuracies: All 0.8107 | Old 0.8488 | New 0.7730
2024-05-15 12:16:37.680 | INFO     | __main__:train:123 - Epoch: [94][0/63]	 loss 4.06319	 cls_loss: 0.2942 cluster_loss: 1.1822 sup_con_loss: 0.5961 contrastive_loss: 4.5895 
2024-05-15 12:16:55.875 | INFO     | __main__:train:123 - Epoch: [94][20/63]	 loss 4.02187	 cls_loss: 0.3174 cluster_loss: 1.1470 sup_con_loss: 0.4944 contrastive_loss: 4.6034 
2024-05-15 12:17:14.172 | INFO     | __main__:train:123 - Epoch: [94][40/63]	 loss 4.13677	 cls_loss: 0.2907 cluster_loss: 1.2741 sup_con_loss: 0.6101 contrastive_loss: 4.6051 
2024-05-15 12:17:29.863 | INFO     | __main__:train:123 - Epoch: [94][60/63]	 loss 4.07440	 cls_loss: 0.2824 cluster_loss: 1.1574 sup_con_loss: 0.6773 contrastive_loss: 4.5942 
2024-05-15 12:17:31.576 | INFO     | __main__:train:126 - Train Epoch: 94 Avg Loss: 4.1023 
2024-05-15 12:17:31.576 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:18:04.984 | INFO     | __main__:train:135 - Train Accuracies: All 0.8092 | Old 0.8463 | New 0.7725
2024-05-15 12:18:12.777 | INFO     | __main__:train:123 - Epoch: [95][0/63]	 loss 4.10527	 cls_loss: 0.3100 cluster_loss: 1.2047 sup_con_loss: 0.6520 contrastive_loss: 4.5931 
2024-05-15 12:18:30.973 | INFO     | __main__:train:123 - Epoch: [95][20/63]	 loss 4.06043	 cls_loss: 0.3094 cluster_loss: 1.2177 sup_con_loss: 0.4862 contrastive_loss: 4.6007 
2024-05-15 12:18:49.609 | INFO     | __main__:train:123 - Epoch: [95][40/63]	 loss 4.04207	 cls_loss: 0.3181 cluster_loss: 1.1480 sup_con_loss: 0.5585 contrastive_loss: 4.5985 
2024-05-15 12:19:05.639 | INFO     | __main__:train:123 - Epoch: [95][60/63]	 loss 4.09254	 cls_loss: 0.3282 cluster_loss: 1.2323 sup_con_loss: 0.5377 contrastive_loss: 4.5976 
2024-05-15 12:19:07.413 | INFO     | __main__:train:126 - Train Epoch: 95 Avg Loss: 4.1020 
2024-05-15 12:19:07.414 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:19:40.598 | INFO     | __main__:train:135 - Train Accuracies: All 0.8125 | Old 0.8506 | New 0.7747
2024-05-15 12:19:47.326 | INFO     | __main__:train:123 - Epoch: [96][0/63]	 loss 4.06785	 cls_loss: 0.3088 cluster_loss: 1.1994 sup_con_loss: 0.5588 contrastive_loss: 4.5917 
2024-05-15 12:20:06.537 | INFO     | __main__:train:123 - Epoch: [96][20/63]	 loss 4.11867	 cls_loss: 0.2919 cluster_loss: 1.2159 sup_con_loss: 0.6849 contrastive_loss: 4.5945 
2024-05-15 12:20:24.766 | INFO     | __main__:train:123 - Epoch: [96][40/63]	 loss 4.01920	 cls_loss: 0.2904 cluster_loss: 1.1277 sup_con_loss: 0.5611 contrastive_loss: 4.5972 
2024-05-15 12:20:40.492 | INFO     | __main__:train:123 - Epoch: [96][60/63]	 loss 4.16115	 cls_loss: 0.3470 cluster_loss: 1.2271 sup_con_loss: 0.7235 contrastive_loss: 4.5982 
2024-05-15 12:20:42.290 | INFO     | __main__:train:126 - Train Epoch: 96 Avg Loss: 4.1208 
2024-05-15 12:20:42.291 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:21:15.516 | INFO     | __main__:train:135 - Train Accuracies: All 0.8122 | Old 0.8481 | New 0.7767
2024-05-15 12:21:22.986 | INFO     | __main__:train:123 - Epoch: [97][0/63]	 loss 4.12756	 cls_loss: 0.3266 cluster_loss: 1.2146 sup_con_loss: 0.6826 contrastive_loss: 4.5921 
2024-05-15 12:21:41.277 | INFO     | __main__:train:123 - Epoch: [97][20/63]	 loss 4.07665	 cls_loss: 0.3029 cluster_loss: 1.1509 sup_con_loss: 0.6839 contrastive_loss: 4.5895 
2024-05-15 12:21:59.638 | INFO     | __main__:train:123 - Epoch: [97][40/63]	 loss 4.24098	 cls_loss: 0.2982 cluster_loss: 1.3339 sup_con_loss: 0.7972 contrastive_loss: 4.6009 
2024-05-15 12:22:16.013 | INFO     | __main__:train:123 - Epoch: [97][60/63]	 loss 4.07180	 cls_loss: 0.3106 cluster_loss: 1.2016 sup_con_loss: 0.5548 contrastive_loss: 4.5967 
2024-05-15 12:22:17.721 | INFO     | __main__:train:126 - Train Epoch: 97 Avg Loss: 4.1012 
2024-05-15 12:22:17.721 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:22:51.453 | INFO     | __main__:train:135 - Train Accuracies: All 0.8095 | Old 0.8458 | New 0.7735
2024-05-15 12:22:59.520 | INFO     | __main__:train:123 - Epoch: [98][0/63]	 loss 4.14563	 cls_loss: 0.3108 cluster_loss: 1.2400 sup_con_loss: 0.6899 contrastive_loss: 4.5990 
2024-05-15 12:23:17.759 | INFO     | __main__:train:123 - Epoch: [98][20/63]	 loss 4.01124	 cls_loss: 0.3192 cluster_loss: 1.1335 sup_con_loss: 0.5120 contrastive_loss: 4.5901 
2024-05-15 12:23:36.170 | INFO     | __main__:train:123 - Epoch: [98][40/63]	 loss 4.17217	 cls_loss: 0.2704 cluster_loss: 1.2381 sup_con_loss: 0.8172 contrastive_loss: 4.5950 
2024-05-15 12:23:52.207 | INFO     | __main__:train:123 - Epoch: [98][60/63]	 loss 4.04636	 cls_loss: 0.3125 cluster_loss: 1.1594 sup_con_loss: 0.5596 contrastive_loss: 4.5962 
2024-05-15 12:23:53.914 | INFO     | __main__:train:126 - Train Epoch: 98 Avg Loss: 4.0975 
2024-05-15 12:23:53.914 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:24:28.542 | INFO     | __main__:train:135 - Train Accuracies: All 0.8098 | Old 0.8448 | New 0.7752
2024-05-15 12:24:35.400 | INFO     | __main__:train:123 - Epoch: [99][0/63]	 loss 4.07768	 cls_loss: 0.2900 cluster_loss: 1.2096 sup_con_loss: 0.5770 contrastive_loss: 4.5970 
2024-05-15 12:24:53.850 | INFO     | __main__:train:123 - Epoch: [99][20/63]	 loss 4.09559	 cls_loss: 0.3070 cluster_loss: 1.2540 sup_con_loss: 0.5113 contrastive_loss: 4.6062 
2024-05-15 12:25:12.321 | INFO     | __main__:train:123 - Epoch: [99][40/63]	 loss 3.96920	 cls_loss: 0.3214 cluster_loss: 1.0897 sup_con_loss: 0.4729 contrastive_loss: 4.5891 
2024-05-15 12:25:28.655 | INFO     | __main__:train:123 - Epoch: [99][60/63]	 loss 4.07941	 cls_loss: 0.3244 cluster_loss: 1.1898 sup_con_loss: 0.5907 contrastive_loss: 4.5934 
2024-05-15 12:25:30.350 | INFO     | __main__:train:126 - Train Epoch: 99 Avg Loss: 4.0879 
2024-05-15 12:25:30.351 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:26:03.548 | INFO     | __main__:train:135 - Train Accuracies: All 0.8130 | Old 0.8461 | New 0.7801
2024-05-15 12:26:11.573 | INFO     | __main__:train:123 - Epoch: [100][0/63]	 loss 4.21899	 cls_loss: 0.3226 cluster_loss: 1.2866 sup_con_loss: 0.8051 contrastive_loss: 4.5969 
2024-05-15 12:26:30.215 | INFO     | __main__:train:123 - Epoch: [100][20/63]	 loss 3.93359	 cls_loss: 0.3257 cluster_loss: 1.0436 sup_con_loss: 0.4438 contrastive_loss: 4.5937 
2024-05-15 12:26:48.430 | INFO     | __main__:train:123 - Epoch: [100][40/63]	 loss 3.99355	 cls_loss: 0.2897 cluster_loss: 1.1241 sup_con_loss: 0.4894 contrastive_loss: 4.6002 
2024-05-15 12:27:04.672 | INFO     | __main__:train:123 - Epoch: [100][60/63]	 loss 4.08882	 cls_loss: 0.2716 cluster_loss: 1.2005 sup_con_loss: 0.6381 contrastive_loss: 4.6001 
2024-05-15 12:27:06.397 | INFO     | __main__:train:126 - Train Epoch: 100 Avg Loss: 4.0920 
2024-05-15 12:27:06.398 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:27:41.544 | INFO     | __main__:train:135 - Train Accuracies: All 0.8106 | Old 0.8466 | New 0.7749
2024-05-15 12:27:50.397 | INFO     | __main__:train:123 - Epoch: [101][0/63]	 loss 3.95240	 cls_loss: 0.2816 cluster_loss: 1.1076 sup_con_loss: 0.4288 contrastive_loss: 4.5905 
2024-05-15 12:28:08.714 | INFO     | __main__:train:123 - Epoch: [101][20/63]	 loss 4.15517	 cls_loss: 0.2872 cluster_loss: 1.2467 sup_con_loss: 0.7426 contrastive_loss: 4.5913 
2024-05-15 12:28:26.743 | INFO     | __main__:train:123 - Epoch: [101][40/63]	 loss 4.13879	 cls_loss: 0.2799 cluster_loss: 1.2475 sup_con_loss: 0.6800 contrastive_loss: 4.6031 
2024-05-15 12:28:43.099 | INFO     | __main__:train:123 - Epoch: [101][60/63]	 loss 4.23428	 cls_loss: 0.2924 cluster_loss: 1.2751 sup_con_loss: 0.8986 contrastive_loss: 4.5979 
2024-05-15 12:28:44.808 | INFO     | __main__:train:126 - Train Epoch: 101 Avg Loss: 4.0923 
2024-05-15 12:28:44.809 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:29:18.301 | INFO     | __main__:train:135 - Train Accuracies: All 0.8101 | Old 0.8436 | New 0.7769
2024-05-15 12:29:25.709 | INFO     | __main__:train:123 - Epoch: [102][0/63]	 loss 4.14056	 cls_loss: 0.2929 cluster_loss: 1.2473 sup_con_loss: 0.6932 contrastive_loss: 4.5918 
2024-05-15 12:29:45.612 | INFO     | __main__:train:123 - Epoch: [102][20/63]	 loss 4.12293	 cls_loss: 0.3170 cluster_loss: 1.2180 sup_con_loss: 0.6596 contrastive_loss: 4.5991 
2024-05-15 12:30:03.559 | INFO     | __main__:train:123 - Epoch: [102][40/63]	 loss 4.12919	 cls_loss: 0.3181 cluster_loss: 1.2169 sup_con_loss: 0.6828 contrastive_loss: 4.5968 
2024-05-15 12:30:19.703 | INFO     | __main__:train:123 - Epoch: [102][60/63]	 loss 4.09514	 cls_loss: 0.2935 cluster_loss: 1.2603 sup_con_loss: 0.5188 contrastive_loss: 4.6025 
2024-05-15 12:30:21.409 | INFO     | __main__:train:126 - Train Epoch: 102 Avg Loss: 4.1138 
2024-05-15 12:30:21.410 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:30:56.353 | INFO     | __main__:train:135 - Train Accuracies: All 0.8101 | Old 0.8468 | New 0.7737
2024-05-15 12:31:03.919 | INFO     | __main__:train:123 - Epoch: [103][0/63]	 loss 3.99050	 cls_loss: 0.3141 cluster_loss: 1.1769 sup_con_loss: 0.3637 contrastive_loss: 4.5973 
2024-05-15 12:31:22.182 | INFO     | __main__:train:123 - Epoch: [103][20/63]	 loss 3.94542	 cls_loss: 0.3321 cluster_loss: 1.0884 sup_con_loss: 0.3809 contrastive_loss: 4.5975 
2024-05-15 12:31:40.249 | INFO     | __main__:train:123 - Epoch: [103][40/63]	 loss 4.03502	 cls_loss: 0.2833 cluster_loss: 1.1872 sup_con_loss: 0.4991 contrastive_loss: 4.5992 
2024-05-15 12:31:56.178 | INFO     | __main__:train:123 - Epoch: [103][60/63]	 loss 4.04674	 cls_loss: 0.3128 cluster_loss: 1.1537 sup_con_loss: 0.5737 contrastive_loss: 4.5946 
2024-05-15 12:31:57.889 | INFO     | __main__:train:126 - Train Epoch: 103 Avg Loss: 4.1028 
2024-05-15 12:31:57.889 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:32:32.273 | INFO     | __main__:train:135 - Train Accuracies: All 0.8117 | Old 0.8473 | New 0.7764
2024-05-15 12:32:39.811 | INFO     | __main__:train:123 - Epoch: [104][0/63]	 loss 4.07225	 cls_loss: 0.3220 cluster_loss: 1.2160 sup_con_loss: 0.5189 contrastive_loss: 4.5962 
2024-05-15 12:32:58.397 | INFO     | __main__:train:123 - Epoch: [104][20/63]	 loss 4.06056	 cls_loss: 0.2850 cluster_loss: 1.2183 sup_con_loss: 0.5338 contrastive_loss: 4.5879 
2024-05-15 12:33:16.539 | INFO     | __main__:train:123 - Epoch: [104][40/63]	 loss 4.16889	 cls_loss: 0.3075 cluster_loss: 1.2441 sup_con_loss: 0.7713 contrastive_loss: 4.5886 
2024-05-15 12:33:32.313 | INFO     | __main__:train:123 - Epoch: [104][60/63]	 loss 4.15034	 cls_loss: 0.3107 cluster_loss: 1.2386 sup_con_loss: 0.6989 contrastive_loss: 4.6030 
2024-05-15 12:33:34.003 | INFO     | __main__:train:126 - Train Epoch: 104 Avg Loss: 4.0908 
2024-05-15 12:33:34.004 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:34:07.977 | INFO     | __main__:train:135 - Train Accuracies: All 0.8116 | Old 0.8468 | New 0.7767
2024-05-15 12:34:14.836 | INFO     | __main__:train:123 - Epoch: [105][0/63]	 loss 4.06718	 cls_loss: 0.3088 cluster_loss: 1.1623 sup_con_loss: 0.6219 contrastive_loss: 4.5937 
2024-05-15 12:34:33.640 | INFO     | __main__:train:123 - Epoch: [105][20/63]	 loss 4.07093	 cls_loss: 0.3075 cluster_loss: 1.1855 sup_con_loss: 0.5728 contrastive_loss: 4.6035 
2024-05-15 12:34:51.778 | INFO     | __main__:train:123 - Epoch: [105][40/63]	 loss 4.16829	 cls_loss: 0.3034 cluster_loss: 1.2757 sup_con_loss: 0.6976 contrastive_loss: 4.5981 
2024-05-15 12:35:08.344 | INFO     | __main__:train:123 - Epoch: [105][60/63]	 loss 4.01786	 cls_loss: 0.3091 cluster_loss: 1.1277 sup_con_loss: 0.5491 contrastive_loss: 4.5916 
2024-05-15 12:35:10.166 | INFO     | __main__:train:126 - Train Epoch: 105 Avg Loss: 4.1056 
2024-05-15 12:35:10.166 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:35:43.678 | INFO     | __main__:train:135 - Train Accuracies: All 0.8122 | Old 0.8453 | New 0.7794
2024-05-15 12:35:50.473 | INFO     | __main__:train:123 - Epoch: [106][0/63]	 loss 4.09178	 cls_loss: 0.2894 cluster_loss: 1.1869 sup_con_loss: 0.6612 contrastive_loss: 4.5963 
2024-05-15 12:36:09.642 | INFO     | __main__:train:123 - Epoch: [106][20/63]	 loss 4.09499	 cls_loss: 0.3093 cluster_loss: 1.2375 sup_con_loss: 0.5563 contrastive_loss: 4.5964 
2024-05-15 12:36:28.261 | INFO     | __main__:train:123 - Epoch: [106][40/63]	 loss 4.04350	 cls_loss: 0.2936 cluster_loss: 1.1495 sup_con_loss: 0.5995 contrastive_loss: 4.5904 
2024-05-15 12:36:44.383 | INFO     | __main__:train:123 - Epoch: [106][60/63]	 loss 4.15053	 cls_loss: 0.2961 cluster_loss: 1.2699 sup_con_loss: 0.6567 contrastive_loss: 4.6025 
2024-05-15 12:36:46.093 | INFO     | __main__:train:126 - Train Epoch: 106 Avg Loss: 4.0787 
2024-05-15 12:36:46.094 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:37:19.773 | INFO     | __main__:train:135 - Train Accuracies: All 0.8090 | Old 0.8416 | New 0.7767
2024-05-15 12:37:28.623 | INFO     | __main__:train:123 - Epoch: [107][0/63]	 loss 4.12922	 cls_loss: 0.2960 cluster_loss: 1.2484 sup_con_loss: 0.6444 contrastive_loss: 4.5979 
2024-05-15 12:37:46.790 | INFO     | __main__:train:123 - Epoch: [107][20/63]	 loss 4.12196	 cls_loss: 0.2985 cluster_loss: 1.2100 sup_con_loss: 0.6900 contrastive_loss: 4.5992 
2024-05-15 12:38:04.765 | INFO     | __main__:train:123 - Epoch: [107][40/63]	 loss 4.13329	 cls_loss: 0.3069 cluster_loss: 1.2554 sup_con_loss: 0.6314 contrastive_loss: 4.5983 
2024-05-15 12:38:20.577 | INFO     | __main__:train:123 - Epoch: [107][60/63]	 loss 4.05774	 cls_loss: 0.2911 cluster_loss: 1.1495 sup_con_loss: 0.6341 contrastive_loss: 4.5950 
2024-05-15 12:38:22.290 | INFO     | __main__:train:126 - Train Epoch: 107 Avg Loss: 4.1006 
2024-05-15 12:38:22.290 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:38:56.071 | INFO     | __main__:train:135 - Train Accuracies: All 0.8112 | Old 0.8436 | New 0.7792
2024-05-15 12:39:02.931 | INFO     | __main__:train:123 - Epoch: [108][0/63]	 loss 4.07476	 cls_loss: 0.3230 cluster_loss: 1.1988 sup_con_loss: 0.5428 contrastive_loss: 4.6038 
2024-05-15 12:39:21.236 | INFO     | __main__:train:123 - Epoch: [108][20/63]	 loss 4.16248	 cls_loss: 0.3251 cluster_loss: 1.2718 sup_con_loss: 0.6687 contrastive_loss: 4.5969 
2024-05-15 12:39:40.234 | INFO     | __main__:train:123 - Epoch: [108][40/63]	 loss 4.09908	 cls_loss: 0.3073 cluster_loss: 1.2275 sup_con_loss: 0.5881 contrastive_loss: 4.5967 
2024-05-15 12:39:56.403 | INFO     | __main__:train:123 - Epoch: [108][60/63]	 loss 4.02343	 cls_loss: 0.2943 cluster_loss: 1.1400 sup_con_loss: 0.5407 contrastive_loss: 4.6004 
2024-05-15 12:39:58.189 | INFO     | __main__:train:126 - Train Epoch: 108 Avg Loss: 4.0993 
2024-05-15 12:39:58.190 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:40:32.059 | INFO     | __main__:train:135 - Train Accuracies: All 0.8113 | Old 0.8441 | New 0.7789
2024-05-15 12:40:40.010 | INFO     | __main__:train:123 - Epoch: [109][0/63]	 loss 4.12406	 cls_loss: 0.2920 cluster_loss: 1.2049 sup_con_loss: 0.7248 contrastive_loss: 4.5923 
2024-05-15 12:40:57.850 | INFO     | __main__:train:123 - Epoch: [109][20/63]	 loss 4.18960	 cls_loss: 0.3199 cluster_loss: 1.2596 sup_con_loss: 0.7762 contrastive_loss: 4.5958 
2024-05-15 12:41:16.187 | INFO     | __main__:train:123 - Epoch: [109][40/63]	 loss 4.11252	 cls_loss: 0.3069 cluster_loss: 1.2117 sup_con_loss: 0.6519 contrastive_loss: 4.5990 
2024-05-15 12:41:31.959 | INFO     | __main__:train:123 - Epoch: [109][60/63]	 loss 4.07691	 cls_loss: 0.3463 cluster_loss: 1.1967 sup_con_loss: 0.5255 contrastive_loss: 4.6061 
2024-05-15 12:41:33.674 | INFO     | __main__:train:126 - Train Epoch: 109 Avg Loss: 4.1249 
2024-05-15 12:41:33.674 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:42:07.015 | INFO     | __main__:train:135 - Train Accuracies: All 0.8084 | Old 0.8408 | New 0.7762
2024-05-15 12:42:14.717 | INFO     | __main__:train:123 - Epoch: [110][0/63]	 loss 4.04781	 cls_loss: 0.3302 cluster_loss: 1.1875 sup_con_loss: 0.4952 contrastive_loss: 4.5954 
2024-05-15 12:42:32.829 | INFO     | __main__:train:123 - Epoch: [110][20/63]	 loss 4.16728	 cls_loss: 0.2944 cluster_loss: 1.2580 sup_con_loss: 0.7435 contrastive_loss: 4.5943 
2024-05-15 12:42:51.403 | INFO     | __main__:train:123 - Epoch: [110][40/63]	 loss 4.08330	 cls_loss: 0.3002 cluster_loss: 1.2022 sup_con_loss: 0.6061 contrastive_loss: 4.5918 
2024-05-15 12:43:07.573 | INFO     | __main__:train:123 - Epoch: [110][60/63]	 loss 4.10835	 cls_loss: 0.2878 cluster_loss: 1.2354 sup_con_loss: 0.5985 contrastive_loss: 4.6079 
2024-05-15 12:43:09.252 | INFO     | __main__:train:126 - Train Epoch: 110 Avg Loss: 4.1031 
2024-05-15 12:43:09.253 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:43:43.521 | INFO     | __main__:train:135 - Train Accuracies: All 0.8117 | Old 0.8451 | New 0.7787
2024-05-15 12:43:50.479 | INFO     | __main__:train:123 - Epoch: [111][0/63]	 loss 4.15682	 cls_loss: 0.2973 cluster_loss: 1.2619 sup_con_loss: 0.7023 contrastive_loss: 4.5949 
2024-05-15 12:44:09.547 | INFO     | __main__:train:123 - Epoch: [111][20/63]	 loss 4.02540	 cls_loss: 0.2885 cluster_loss: 1.1715 sup_con_loss: 0.5072 contrastive_loss: 4.5929 
2024-05-15 12:44:27.981 | INFO     | __main__:train:123 - Epoch: [111][40/63]	 loss 4.08616	 cls_loss: 0.3128 cluster_loss: 1.2084 sup_con_loss: 0.5878 contrastive_loss: 4.5931 
2024-05-15 12:44:44.023 | INFO     | __main__:train:123 - Epoch: [111][60/63]	 loss 4.08201	 cls_loss: 0.2877 cluster_loss: 1.1651 sup_con_loss: 0.6892 contrastive_loss: 4.5889 
2024-05-15 12:44:45.744 | INFO     | __main__:train:126 - Train Epoch: 111 Avg Loss: 4.0840 
2024-05-15 12:44:45.745 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:45:18.976 | INFO     | __main__:train:135 - Train Accuracies: All 0.8103 | Old 0.8436 | New 0.7774
2024-05-15 12:45:27.367 | INFO     | __main__:train:123 - Epoch: [112][0/63]	 loss 4.03602	 cls_loss: 0.3043 cluster_loss: 1.1237 sup_con_loss: 0.6047 contrastive_loss: 4.5961 
2024-05-15 12:45:45.837 | INFO     | __main__:train:123 - Epoch: [112][20/63]	 loss 3.98763	 cls_loss: 0.3274 cluster_loss: 1.1139 sup_con_loss: 0.4737 contrastive_loss: 4.5896 
2024-05-15 12:46:03.744 | INFO     | __main__:train:123 - Epoch: [112][40/63]	 loss 4.02397	 cls_loss: 0.2912 cluster_loss: 1.1601 sup_con_loss: 0.5061 contrastive_loss: 4.6013 
2024-05-15 12:46:19.736 | INFO     | __main__:train:123 - Epoch: [112][60/63]	 loss 4.11164	 cls_loss: 0.3062 cluster_loss: 1.2069 sup_con_loss: 0.6778 contrastive_loss: 4.5889 
2024-05-15 12:46:21.468 | INFO     | __main__:train:126 - Train Epoch: 112 Avg Loss: 4.0909 
2024-05-15 12:46:21.469 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:46:55.280 | INFO     | __main__:train:135 - Train Accuracies: All 0.8105 | Old 0.8418 | New 0.7794
2024-05-15 12:47:03.872 | INFO     | __main__:train:123 - Epoch: [113][0/63]	 loss 4.04415	 cls_loss: 0.2977 cluster_loss: 1.1667 sup_con_loss: 0.5499 contrastive_loss: 4.5986 
2024-05-15 12:47:22.451 | INFO     | __main__:train:123 - Epoch: [113][20/63]	 loss 4.12815	 cls_loss: 0.2811 cluster_loss: 1.2312 sup_con_loss: 0.7010 contrastive_loss: 4.5909 
2024-05-15 12:47:40.666 | INFO     | __main__:train:123 - Epoch: [113][40/63]	 loss 4.02760	 cls_loss: 0.3060 cluster_loss: 1.1530 sup_con_loss: 0.5130 contrastive_loss: 4.6023 
2024-05-15 12:47:56.484 | INFO     | __main__:train:123 - Epoch: [113][60/63]	 loss 4.06204	 cls_loss: 0.2824 cluster_loss: 1.1996 sup_con_loss: 0.5676 contrastive_loss: 4.5920 
2024-05-15 12:47:58.228 | INFO     | __main__:train:126 - Train Epoch: 113 Avg Loss: 4.0969 
2024-05-15 12:47:58.229 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:48:32.663 | INFO     | __main__:train:135 - Train Accuracies: All 0.8115 | Old 0.8423 | New 0.7809
2024-05-15 12:48:39.733 | INFO     | __main__:train:123 - Epoch: [114][0/63]	 loss 4.07286	 cls_loss: 0.2910 cluster_loss: 1.1963 sup_con_loss: 0.6003 contrastive_loss: 4.5897 
2024-05-15 12:48:58.771 | INFO     | __main__:train:123 - Epoch: [114][20/63]	 loss 4.21838	 cls_loss: 0.2788 cluster_loss: 1.2938 sup_con_loss: 0.8277 contrastive_loss: 4.6002 
2024-05-15 12:49:17.617 | INFO     | __main__:train:123 - Epoch: [114][40/63]	 loss 4.22200	 cls_loss: 0.2995 cluster_loss: 1.2772 sup_con_loss: 0.8406 contrastive_loss: 4.6043 
2024-05-15 12:49:33.522 | INFO     | __main__:train:123 - Epoch: [114][60/63]	 loss 4.00093	 cls_loss: 0.2824 cluster_loss: 1.1196 sup_con_loss: 0.5490 contrastive_loss: 4.5880 
2024-05-15 12:49:35.249 | INFO     | __main__:train:126 - Train Epoch: 114 Avg Loss: 4.0917 
2024-05-15 12:49:35.250 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:50:09.484 | INFO     | __main__:train:135 - Train Accuracies: All 0.8103 | Old 0.8426 | New 0.7784
2024-05-15 12:50:16.199 | INFO     | __main__:train:123 - Epoch: [115][0/63]	 loss 4.13865	 cls_loss: 0.2892 cluster_loss: 1.2699 sup_con_loss: 0.6474 contrastive_loss: 4.5929 
2024-05-15 12:50:35.480 | INFO     | __main__:train:123 - Epoch: [115][20/63]	 loss 4.12476	 cls_loss: 0.3006 cluster_loss: 1.1998 sup_con_loss: 0.7166 contrastive_loss: 4.5983 
2024-05-15 12:50:53.271 | INFO     | __main__:train:123 - Epoch: [115][40/63]	 loss 4.03601	 cls_loss: 0.2948 cluster_loss: 1.1859 sup_con_loss: 0.4899 contrastive_loss: 4.6008 
2024-05-15 12:51:09.539 | INFO     | __main__:train:123 - Epoch: [115][60/63]	 loss 4.13326	 cls_loss: 0.2855 cluster_loss: 1.2770 sup_con_loss: 0.6179 contrastive_loss: 4.5954 
2024-05-15 12:51:11.260 | INFO     | __main__:train:126 - Train Epoch: 115 Avg Loss: 4.0988 
2024-05-15 12:51:11.261 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:51:45.374 | INFO     | __main__:train:135 - Train Accuracies: All 0.8120 | Old 0.8451 | New 0.7792
2024-05-15 12:51:53.216 | INFO     | __main__:train:123 - Epoch: [116][0/63]	 loss 3.95972	 cls_loss: 0.2850 cluster_loss: 1.1258 sup_con_loss: 0.4057 contrastive_loss: 4.5942 
2024-05-15 12:52:11.642 | INFO     | __main__:train:123 - Epoch: [116][20/63]	 loss 4.09782	 cls_loss: 0.3004 cluster_loss: 1.2315 sup_con_loss: 0.5857 contrastive_loss: 4.5957 
2024-05-15 12:52:29.854 | INFO     | __main__:train:123 - Epoch: [116][40/63]	 loss 4.24858	 cls_loss: 0.2947 cluster_loss: 1.3113 sup_con_loss: 0.8716 contrastive_loss: 4.5970 
2024-05-15 12:52:45.535 | INFO     | __main__:train:123 - Epoch: [116][60/63]	 loss 3.95833	 cls_loss: 0.2862 cluster_loss: 1.1127 sup_con_loss: 0.4104 contrastive_loss: 4.6020 
2024-05-15 12:52:47.251 | INFO     | __main__:train:126 - Train Epoch: 116 Avg Loss: 4.1011 
2024-05-15 12:52:47.251 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:53:21.303 | INFO     | __main__:train:135 - Train Accuracies: All 0.8126 | Old 0.8466 | New 0.7789
2024-05-15 12:53:28.504 | INFO     | __main__:train:123 - Epoch: [117][0/63]	 loss 4.07098	 cls_loss: 0.3153 cluster_loss: 1.2015 sup_con_loss: 0.5432 contrastive_loss: 4.5992 
2024-05-15 12:53:47.165 | INFO     | __main__:train:123 - Epoch: [117][20/63]	 loss 4.01624	 cls_loss: 0.3414 cluster_loss: 1.1306 sup_con_loss: 0.5128 contrastive_loss: 4.5882 
2024-05-15 12:54:05.484 | INFO     | __main__:train:123 - Epoch: [117][40/63]	 loss 4.08029	 cls_loss: 0.2724 cluster_loss: 1.1971 sup_con_loss: 0.6415 contrastive_loss: 4.5882 
2024-05-15 12:54:21.312 | INFO     | __main__:train:123 - Epoch: [117][60/63]	 loss 4.02652	 cls_loss: 0.3109 cluster_loss: 1.1644 sup_con_loss: 0.5097 contrastive_loss: 4.5884 
2024-05-15 12:54:23.048 | INFO     | __main__:train:126 - Train Epoch: 117 Avg Loss: 4.0793 
2024-05-15 12:54:23.050 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:54:57.503 | INFO     | __main__:train:135 - Train Accuracies: All 0.8112 | Old 0.8456 | New 0.7772
2024-05-15 12:55:05.944 | INFO     | __main__:train:123 - Epoch: [118][0/63]	 loss 4.01271	 cls_loss: 0.3078 cluster_loss: 1.1645 sup_con_loss: 0.4567 contrastive_loss: 4.5972 
2024-05-15 12:55:24.011 | INFO     | __main__:train:123 - Epoch: [118][20/63]	 loss 4.01944	 cls_loss: 0.2807 cluster_loss: 1.1952 sup_con_loss: 0.4460 contrastive_loss: 4.5972 
2024-05-15 12:55:42.154 | INFO     | __main__:train:123 - Epoch: [118][40/63]	 loss 4.01866	 cls_loss: 0.2781 cluster_loss: 1.1134 sup_con_loss: 0.6097 contrastive_loss: 4.5911 
2024-05-15 12:55:58.254 | INFO     | __main__:train:123 - Epoch: [118][60/63]	 loss 4.10870	 cls_loss: 0.2890 cluster_loss: 1.1973 sup_con_loss: 0.6955 contrastive_loss: 4.5937 
2024-05-15 12:55:59.967 | INFO     | __main__:train:126 - Train Epoch: 118 Avg Loss: 4.0875 
2024-05-15 12:55:59.967 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:56:35.158 | INFO     | __main__:train:135 - Train Accuracies: All 0.8107 | Old 0.8451 | New 0.7767
2024-05-15 12:56:41.070 | INFO     | __main__:train:123 - Epoch: [119][0/63]	 loss 4.17745	 cls_loss: 0.3080 cluster_loss: 1.2652 sup_con_loss: 0.7293 contrastive_loss: 4.6032 
2024-05-15 12:57:00.197 | INFO     | __main__:train:123 - Epoch: [119][20/63]	 loss 4.03915	 cls_loss: 0.2960 cluster_loss: 1.1305 sup_con_loss: 0.6095 contrastive_loss: 4.5961 
2024-05-15 12:57:18.572 | INFO     | __main__:train:123 - Epoch: [119][40/63]	 loss 4.23479	 cls_loss: 0.3058 cluster_loss: 1.2427 sup_con_loss: 0.9572 contrastive_loss: 4.5923 
2024-05-15 12:57:34.893 | INFO     | __main__:train:123 - Epoch: [119][60/63]	 loss 4.20938	 cls_loss: 0.2897 cluster_loss: 1.2606 sup_con_loss: 0.8528 contrastive_loss: 4.6002 
2024-05-15 12:57:36.652 | INFO     | __main__:train:126 - Train Epoch: 119 Avg Loss: 4.0804 
2024-05-15 12:57:36.652 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:58:09.844 | INFO     | __main__:train:135 - Train Accuracies: All 0.8133 | Old 0.8493 | New 0.7777
2024-05-15 12:58:17.214 | INFO     | __main__:train:123 - Epoch: [120][0/63]	 loss 4.11915	 cls_loss: 0.2913 cluster_loss: 1.2186 sup_con_loss: 0.6917 contrastive_loss: 4.5892 
2024-05-15 12:58:36.077 | INFO     | __main__:train:123 - Epoch: [120][20/63]	 loss 4.18787	 cls_loss: 0.2843 cluster_loss: 1.2833 sup_con_loss: 0.7706 contrastive_loss: 4.5916 
2024-05-15 12:58:54.413 | INFO     | __main__:train:123 - Epoch: [120][40/63]	 loss 4.10432	 cls_loss: 0.2990 cluster_loss: 1.1673 sup_con_loss: 0.7424 contrastive_loss: 4.5863 
2024-05-15 12:59:10.455 | INFO     | __main__:train:123 - Epoch: [120][60/63]	 loss 4.08212	 cls_loss: 0.2688 cluster_loss: 1.1708 sup_con_loss: 0.7022 contrastive_loss: 4.5865 
2024-05-15 12:59:12.197 | INFO     | __main__:train:126 - Train Epoch: 120 Avg Loss: 4.0973 
2024-05-15 12:59:12.197 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 12:59:46.163 | INFO     | __main__:train:135 - Train Accuracies: All 0.8091 | Old 0.8433 | New 0.7752
2024-05-15 12:59:53.828 | INFO     | __main__:train:123 - Epoch: [121][0/63]	 loss 4.15199	 cls_loss: 0.2708 cluster_loss: 1.2715 sup_con_loss: 0.6988 contrastive_loss: 4.5941 
2024-05-15 13:00:12.523 | INFO     | __main__:train:123 - Epoch: [121][20/63]	 loss 4.07925	 cls_loss: 0.2926 cluster_loss: 1.2315 sup_con_loss: 0.5273 contrastive_loss: 4.6028 
2024-05-15 13:00:30.951 | INFO     | __main__:train:123 - Epoch: [121][40/63]	 loss 4.30944	 cls_loss: 0.3267 cluster_loss: 1.3371 sup_con_loss: 0.9621 contrastive_loss: 4.5988 
2024-05-15 13:00:46.858 | INFO     | __main__:train:123 - Epoch: [121][60/63]	 loss 4.04478	 cls_loss: 0.3007 cluster_loss: 1.1444 sup_con_loss: 0.5851 contrastive_loss: 4.6014 
2024-05-15 13:00:48.643 | INFO     | __main__:train:126 - Train Epoch: 121 Avg Loss: 4.0963 
2024-05-15 13:00:48.644 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:01:22.319 | INFO     | __main__:train:135 - Train Accuracies: All 0.8116 | Old 0.8451 | New 0.7784
2024-05-15 13:01:31.008 | INFO     | __main__:train:123 - Epoch: [122][0/63]	 loss 4.05517	 cls_loss: 0.2906 cluster_loss: 1.1847 sup_con_loss: 0.5651 contrastive_loss: 4.5932 
2024-05-15 13:01:49.010 | INFO     | __main__:train:123 - Epoch: [122][20/63]	 loss 4.12627	 cls_loss: 0.2979 cluster_loss: 1.2223 sup_con_loss: 0.6915 contrastive_loss: 4.5931 
2024-05-15 13:02:07.197 | INFO     | __main__:train:123 - Epoch: [122][40/63]	 loss 4.06942	 cls_loss: 0.2849 cluster_loss: 1.1774 sup_con_loss: 0.6277 contrastive_loss: 4.5918 
2024-05-15 13:02:23.336 | INFO     | __main__:train:123 - Epoch: [122][60/63]	 loss 4.05650	 cls_loss: 0.2927 cluster_loss: 1.2005 sup_con_loss: 0.5327 contrastive_loss: 4.5958 
2024-05-15 13:02:25.037 | INFO     | __main__:train:126 - Train Epoch: 122 Avg Loss: 4.0828 
2024-05-15 13:02:25.037 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:02:58.941 | INFO     | __main__:train:135 - Train Accuracies: All 0.8116 | Old 0.8453 | New 0.7782
2024-05-15 13:03:06.414 | INFO     | __main__:train:123 - Epoch: [123][0/63]	 loss 4.01673	 cls_loss: 0.3121 cluster_loss: 1.1366 sup_con_loss: 0.5231 contrastive_loss: 4.5933 
2024-05-15 13:03:25.988 | INFO     | __main__:train:123 - Epoch: [123][20/63]	 loss 4.18253	 cls_loss: 0.2937 cluster_loss: 1.2669 sup_con_loss: 0.7722 contrastive_loss: 4.5938 
2024-05-15 13:03:44.568 | INFO     | __main__:train:123 - Epoch: [123][40/63]	 loss 4.08696	 cls_loss: 0.2781 cluster_loss: 1.2055 sup_con_loss: 0.6365 contrastive_loss: 4.5896 
2024-05-15 13:04:00.662 | INFO     | __main__:train:123 - Epoch: [123][60/63]	 loss 4.07899	 cls_loss: 0.2794 cluster_loss: 1.1924 sup_con_loss: 0.6348 contrastive_loss: 4.5907 
2024-05-15 13:04:02.421 | INFO     | __main__:train:126 - Train Epoch: 123 Avg Loss: 4.0739 
2024-05-15 13:04:02.422 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:04:37.237 | INFO     | __main__:train:135 - Train Accuracies: All 0.8148 | Old 0.8473 | New 0.7826
2024-05-15 13:04:44.359 | INFO     | __main__:train:123 - Epoch: [124][0/63]	 loss 4.10168	 cls_loss: 0.2812 cluster_loss: 1.2114 sup_con_loss: 0.6590 contrastive_loss: 4.5926 
2024-05-15 13:05:03.119 | INFO     | __main__:train:123 - Epoch: [124][20/63]	 loss 3.93876	 cls_loss: 0.2934 cluster_loss: 1.0742 sup_con_loss: 0.4546 contrastive_loss: 4.5827 
2024-05-15 13:05:21.642 | INFO     | __main__:train:123 - Epoch: [124][40/63]	 loss 3.99061	 cls_loss: 0.2673 cluster_loss: 1.1477 sup_con_loss: 0.4799 contrastive_loss: 4.5894 
2024-05-15 13:05:37.687 | INFO     | __main__:train:123 - Epoch: [124][60/63]	 loss 4.07970	 cls_loss: 0.2927 cluster_loss: 1.2026 sup_con_loss: 0.5961 contrastive_loss: 4.5953 
2024-05-15 13:05:39.473 | INFO     | __main__:train:126 - Train Epoch: 124 Avg Loss: 4.0775 
2024-05-15 13:05:39.473 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:06:12.850 | INFO     | __main__:train:135 - Train Accuracies: All 0.8136 | Old 0.8451 | New 0.7824
2024-05-15 13:06:21.663 | INFO     | __main__:train:123 - Epoch: [125][0/63]	 loss 3.98987	 cls_loss: 0.2872 cluster_loss: 1.1481 sup_con_loss: 0.4574 contrastive_loss: 4.5892 
2024-05-15 13:06:40.276 | INFO     | __main__:train:123 - Epoch: [125][20/63]	 loss 4.03435	 cls_loss: 0.3014 cluster_loss: 1.1374 sup_con_loss: 0.5810 contrastive_loss: 4.5942 
2024-05-15 13:06:58.315 | INFO     | __main__:train:123 - Epoch: [125][40/63]	 loss 4.12808	 cls_loss: 0.2849 cluster_loss: 1.2268 sup_con_loss: 0.7051 contrastive_loss: 4.5910 
2024-05-15 13:07:13.974 | INFO     | __main__:train:123 - Epoch: [125][60/63]	 loss 4.20269	 cls_loss: 0.2834 cluster_loss: 1.2825 sup_con_loss: 0.8156 contrastive_loss: 4.5914 
2024-05-15 13:07:15.693 | INFO     | __main__:train:126 - Train Epoch: 125 Avg Loss: 4.0854 
2024-05-15 13:07:15.694 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:07:49.444 | INFO     | __main__:train:135 - Train Accuracies: All 0.8101 | Old 0.8428 | New 0.7777
2024-05-15 13:07:57.518 | INFO     | __main__:train:123 - Epoch: [126][0/63]	 loss 4.10393	 cls_loss: 0.3079 cluster_loss: 1.2274 sup_con_loss: 0.6047 contrastive_loss: 4.5950 
2024-05-15 13:08:15.880 | INFO     | __main__:train:123 - Epoch: [126][20/63]	 loss 4.08301	 cls_loss: 0.2962 cluster_loss: 1.1795 sup_con_loss: 0.6480 contrastive_loss: 4.5936 
2024-05-15 13:08:34.564 | INFO     | __main__:train:123 - Epoch: [126][40/63]	 loss 4.10495	 cls_loss: 0.3128 cluster_loss: 1.2279 sup_con_loss: 0.5838 contrastive_loss: 4.6046 
2024-05-15 13:08:50.593 | INFO     | __main__:train:123 - Epoch: [126][60/63]	 loss 3.96785	 cls_loss: 0.2731 cluster_loss: 1.0952 sup_con_loss: 0.5076 contrastive_loss: 4.5889 
2024-05-15 13:08:52.332 | INFO     | __main__:train:126 - Train Epoch: 126 Avg Loss: 4.0801 
2024-05-15 13:08:52.333 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:09:26.122 | INFO     | __main__:train:135 - Train Accuracies: All 0.8137 | Old 0.8486 | New 0.7792
2024-05-15 13:09:34.446 | INFO     | __main__:train:123 - Epoch: [127][0/63]	 loss 4.03407	 cls_loss: 0.2949 cluster_loss: 1.1690 sup_con_loss: 0.5318 contrastive_loss: 4.5921 
2024-05-15 13:09:52.732 | INFO     | __main__:train:123 - Epoch: [127][20/63]	 loss 4.09352	 cls_loss: 0.2689 cluster_loss: 1.1686 sup_con_loss: 0.7315 contrastive_loss: 4.5904 
2024-05-15 13:10:10.380 | INFO     | __main__:train:123 - Epoch: [127][40/63]	 loss 4.00940	 cls_loss: 0.3127 cluster_loss: 1.1205 sup_con_loss: 0.5431 contrastive_loss: 4.5869 
2024-05-15 13:10:26.363 | INFO     | __main__:train:123 - Epoch: [127][60/63]	 loss 4.09270	 cls_loss: 0.2761 cluster_loss: 1.1973 sup_con_loss: 0.6684 contrastive_loss: 4.5906 
2024-05-15 13:10:28.085 | INFO     | __main__:train:126 - Train Epoch: 127 Avg Loss: 4.0859 
2024-05-15 13:10:28.085 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:11:01.871 | INFO     | __main__:train:135 - Train Accuracies: All 0.8118 | Old 0.8463 | New 0.7777
2024-05-15 13:11:09.805 | INFO     | __main__:train:123 - Epoch: [128][0/63]	 loss 3.99131	 cls_loss: 0.2755 cluster_loss: 1.1224 sup_con_loss: 0.5186 contrastive_loss: 4.5905 
2024-05-15 13:11:28.236 | INFO     | __main__:train:123 - Epoch: [128][20/63]	 loss 4.09523	 cls_loss: 0.2970 cluster_loss: 1.2371 sup_con_loss: 0.5846 contrastive_loss: 4.5885 
2024-05-15 13:11:46.351 | INFO     | __main__:train:123 - Epoch: [128][40/63]	 loss 4.13305	 cls_loss: 0.2980 cluster_loss: 1.1932 sup_con_loss: 0.7542 contrastive_loss: 4.5988 
2024-05-15 13:12:02.345 | INFO     | __main__:train:123 - Epoch: [128][60/63]	 loss 4.02133	 cls_loss: 0.2793 cluster_loss: 1.1770 sup_con_loss: 0.5011 contrastive_loss: 4.5895 
2024-05-15 13:12:04.039 | INFO     | __main__:train:126 - Train Epoch: 128 Avg Loss: 4.0842 
2024-05-15 13:12:04.040 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:12:37.657 | INFO     | __main__:train:135 - Train Accuracies: All 0.8123 | Old 0.8456 | New 0.7794
2024-05-15 13:12:45.446 | INFO     | __main__:train:123 - Epoch: [129][0/63]	 loss 4.18948	 cls_loss: 0.3245 cluster_loss: 1.2635 sup_con_loss: 0.7758 contrastive_loss: 4.5894 
2024-05-15 13:13:03.928 | INFO     | __main__:train:123 - Epoch: [129][20/63]	 loss 4.16743	 cls_loss: 0.3056 cluster_loss: 1.2705 sup_con_loss: 0.6903 contrastive_loss: 4.6047 
2024-05-15 13:13:21.825 | INFO     | __main__:train:123 - Epoch: [129][40/63]	 loss 4.06400	 cls_loss: 0.2915 cluster_loss: 1.1548 sup_con_loss: 0.6470 contrastive_loss: 4.5922 
2024-05-15 13:13:37.599 | INFO     | __main__:train:123 - Epoch: [129][60/63]	 loss 4.03745	 cls_loss: 0.2860 cluster_loss: 1.1821 sup_con_loss: 0.5219 contrastive_loss: 4.5944 
2024-05-15 13:13:39.314 | INFO     | __main__:train:126 - Train Epoch: 129 Avg Loss: 4.0839 
2024-05-15 13:13:39.315 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:14:12.693 | INFO     | __main__:train:135 - Train Accuracies: All 0.8113 | Old 0.8436 | New 0.7794
2024-05-15 13:14:19.606 | INFO     | __main__:train:123 - Epoch: [130][0/63]	 loss 3.97803	 cls_loss: 0.3017 cluster_loss: 1.1166 sup_con_loss: 0.4731 contrastive_loss: 4.5862 
2024-05-15 13:14:38.604 | INFO     | __main__:train:123 - Epoch: [130][20/63]	 loss 4.09257	 cls_loss: 0.2788 cluster_loss: 1.2200 sup_con_loss: 0.6247 contrastive_loss: 4.5898 
2024-05-15 13:14:56.937 | INFO     | __main__:train:123 - Epoch: [130][40/63]	 loss 4.04605	 cls_loss: 0.2685 cluster_loss: 1.1920 sup_con_loss: 0.5533 contrastive_loss: 4.5902 
2024-05-15 13:15:13.437 | INFO     | __main__:train:123 - Epoch: [130][60/63]	 loss 4.23939	 cls_loss: 0.2947 cluster_loss: 1.2996 sup_con_loss: 0.8722 contrastive_loss: 4.5941 
2024-05-15 13:15:15.169 | INFO     | __main__:train:126 - Train Epoch: 130 Avg Loss: 4.0806 
2024-05-15 13:15:15.170 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:15:49.294 | INFO     | __main__:train:135 - Train Accuracies: All 0.8125 | Old 0.8436 | New 0.7816
2024-05-15 13:15:57.015 | INFO     | __main__:train:123 - Epoch: [131][0/63]	 loss 4.09157	 cls_loss: 0.3004 cluster_loss: 1.2137 sup_con_loss: 0.6045 contrastive_loss: 4.5938 
2024-05-15 13:16:15.406 | INFO     | __main__:train:123 - Epoch: [131][20/63]	 loss 4.00780	 cls_loss: 0.2833 cluster_loss: 1.1377 sup_con_loss: 0.5360 contrastive_loss: 4.5869 
2024-05-15 13:16:33.781 | INFO     | __main__:train:123 - Epoch: [131][40/63]	 loss 4.19819	 cls_loss: 0.3329 cluster_loss: 1.2565 sup_con_loss: 0.7995 contrastive_loss: 4.5925 
2024-05-15 13:16:49.985 | INFO     | __main__:train:123 - Epoch: [131][60/63]	 loss 4.02030	 cls_loss: 0.3007 cluster_loss: 1.1602 sup_con_loss: 0.5003 contrastive_loss: 4.5936 
2024-05-15 13:16:51.737 | INFO     | __main__:train:126 - Train Epoch: 131 Avg Loss: 4.0663 
2024-05-15 13:16:51.737 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:17:26.754 | INFO     | __main__:train:135 - Train Accuracies: All 0.8069 | Old 0.8366 | New 0.7774
2024-05-15 13:17:35.141 | INFO     | __main__:train:123 - Epoch: [132][0/63]	 loss 4.06446	 cls_loss: 0.2936 cluster_loss: 1.1733 sup_con_loss: 0.6083 contrastive_loss: 4.5941 
2024-05-15 13:17:53.634 | INFO     | __main__:train:123 - Epoch: [132][20/63]	 loss 4.22149	 cls_loss: 0.2756 cluster_loss: 1.2924 sup_con_loss: 0.8622 contrastive_loss: 4.5895 
2024-05-15 13:18:11.501 | INFO     | __main__:train:123 - Epoch: [132][40/63]	 loss 4.08886	 cls_loss: 0.3104 cluster_loss: 1.2057 sup_con_loss: 0.6135 contrastive_loss: 4.5873 
2024-05-15 13:18:27.329 | INFO     | __main__:train:123 - Epoch: [132][60/63]	 loss 4.09103	 cls_loss: 0.2901 cluster_loss: 1.2218 sup_con_loss: 0.5882 contrastive_loss: 4.5992 
2024-05-15 13:18:29.012 | INFO     | __main__:train:126 - Train Epoch: 132 Avg Loss: 4.0803 
2024-05-15 13:18:29.013 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:19:03.664 | INFO     | __main__:train:135 - Train Accuracies: All 0.8108 | Old 0.8431 | New 0.7789
2024-05-15 13:19:10.431 | INFO     | __main__:train:123 - Epoch: [133][0/63]	 loss 4.05877	 cls_loss: 0.2998 cluster_loss: 1.1986 sup_con_loss: 0.5371 contrastive_loss: 4.5950 
2024-05-15 13:19:27.735 | INFO     | __main__:train:123 - Epoch: [133][20/63]	 loss 4.10539	 cls_loss: 0.2908 cluster_loss: 1.1957 sup_con_loss: 0.6834 contrastive_loss: 4.5957 
2024-05-15 13:19:44.447 | INFO     | __main__:train:123 - Epoch: [133][40/63]	 loss 4.14927	 cls_loss: 0.2946 cluster_loss: 1.2275 sup_con_loss: 0.7515 contrastive_loss: 4.5927 
2024-05-15 13:19:59.708 | INFO     | __main__:train:123 - Epoch: [133][60/63]	 loss 3.97086	 cls_loss: 0.2790 cluster_loss: 1.1560 sup_con_loss: 0.3884 contrastive_loss: 4.5936 
2024-05-15 13:20:01.350 | INFO     | __main__:train:126 - Train Epoch: 133 Avg Loss: 4.0684 
2024-05-15 13:20:01.351 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:20:33.475 | INFO     | __main__:train:135 - Train Accuracies: All 0.8108 | Old 0.8411 | New 0.7809
2024-05-15 13:20:40.798 | INFO     | __main__:train:123 - Epoch: [134][0/63]	 loss 4.17036	 cls_loss: 0.2684 cluster_loss: 1.2812 sup_con_loss: 0.7394 contrastive_loss: 4.5921 
2024-05-15 13:20:58.202 | INFO     | __main__:train:123 - Epoch: [134][20/63]	 loss 4.05346	 cls_loss: 0.3076 cluster_loss: 1.1390 sup_con_loss: 0.6323 contrastive_loss: 4.5910 
2024-05-15 13:21:15.082 | INFO     | __main__:train:123 - Epoch: [134][40/63]	 loss 3.88400	 cls_loss: 0.2692 cluster_loss: 1.0649 sup_con_loss: 0.3258 contrastive_loss: 4.5900 
2024-05-15 13:21:29.937 | INFO     | __main__:train:123 - Epoch: [134][60/63]	 loss 4.07817	 cls_loss: 0.3046 cluster_loss: 1.1799 sup_con_loss: 0.6210 contrastive_loss: 4.5957 
2024-05-15 13:21:31.556 | INFO     | __main__:train:126 - Train Epoch: 134 Avg Loss: 4.0908 
2024-05-15 13:21:31.556 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:22:03.951 | INFO     | __main__:train:135 - Train Accuracies: All 0.8107 | Old 0.8413 | New 0.7804
2024-05-15 13:22:11.305 | INFO     | __main__:train:123 - Epoch: [135][0/63]	 loss 4.09725	 cls_loss: 0.2735 cluster_loss: 1.1547 sup_con_loss: 0.7604 contrastive_loss: 4.5921 
2024-05-15 13:22:27.923 | INFO     | __main__:train:123 - Epoch: [135][20/63]	 loss 4.03262	 cls_loss: 0.2607 cluster_loss: 1.1875 sup_con_loss: 0.5315 contrastive_loss: 4.5900 
2024-05-15 13:22:44.544 | INFO     | __main__:train:123 - Epoch: [135][40/63]	 loss 4.11643	 cls_loss: 0.2702 cluster_loss: 1.2639 sup_con_loss: 0.6040 contrastive_loss: 4.5984 
2024-05-15 13:22:59.648 | INFO     | __main__:train:123 - Epoch: [135][60/63]	 loss 4.05277	 cls_loss: 0.2586 cluster_loss: 1.2359 sup_con_loss: 0.5023 contrastive_loss: 4.5895 
2024-05-15 13:23:01.291 | INFO     | __main__:train:126 - Train Epoch: 135 Avg Loss: 4.0702 
2024-05-15 13:23:01.292 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:23:33.693 | INFO     | __main__:train:135 - Train Accuracies: All 0.8136 | Old 0.8446 | New 0.7829
2024-05-15 13:23:41.822 | INFO     | __main__:train:123 - Epoch: [136][0/63]	 loss 4.03660	 cls_loss: 0.2768 cluster_loss: 1.2092 sup_con_loss: 0.4899 contrastive_loss: 4.5882 
2024-05-15 13:23:58.606 | INFO     | __main__:train:123 - Epoch: [136][20/63]	 loss 4.04343	 cls_loss: 0.2930 cluster_loss: 1.1881 sup_con_loss: 0.5204 contrastive_loss: 4.5946 
2024-05-15 13:24:15.128 | INFO     | __main__:train:123 - Epoch: [136][40/63]	 loss 4.21173	 cls_loss: 0.2690 cluster_loss: 1.2960 sup_con_loss: 0.8335 contrastive_loss: 4.5899 
2024-05-15 13:24:30.185 | INFO     | __main__:train:123 - Epoch: [136][60/63]	 loss 4.16424	 cls_loss: 0.2719 cluster_loss: 1.2209 sup_con_loss: 0.8310 contrastive_loss: 4.5917 
2024-05-15 13:24:31.818 | INFO     | __main__:train:126 - Train Epoch: 136 Avg Loss: 4.0756 
2024-05-15 13:24:31.818 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:25:03.784 | INFO     | __main__:train:135 - Train Accuracies: All 0.8127 | Old 0.8456 | New 0.7801
2024-05-15 13:25:10.476 | INFO     | __main__:train:123 - Epoch: [137][0/63]	 loss 4.03392	 cls_loss: 0.2802 cluster_loss: 1.1334 sup_con_loss: 0.6190 contrastive_loss: 4.5885 
2024-05-15 13:25:27.521 | INFO     | __main__:train:123 - Epoch: [137][20/63]	 loss 4.08587	 cls_loss: 0.2909 cluster_loss: 1.1878 sup_con_loss: 0.6471 contrastive_loss: 4.5931 
2024-05-15 13:25:45.133 | INFO     | __main__:train:123 - Epoch: [137][40/63]	 loss 4.03034	 cls_loss: 0.2959 cluster_loss: 1.1476 sup_con_loss: 0.5577 contrastive_loss: 4.5933 
2024-05-15 13:26:00.842 | INFO     | __main__:train:123 - Epoch: [137][60/63]	 loss 4.14799	 cls_loss: 0.3021 cluster_loss: 1.2391 sup_con_loss: 0.7276 contrastive_loss: 4.5879 
2024-05-15 13:26:02.485 | INFO     | __main__:train:126 - Train Epoch: 137 Avg Loss: 4.0787 
2024-05-15 13:26:02.485 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:26:34.632 | INFO     | __main__:train:135 - Train Accuracies: All 0.8120 | Old 0.8476 | New 0.7767
2024-05-15 13:26:41.631 | INFO     | __main__:train:123 - Epoch: [138][0/63]	 loss 4.03677	 cls_loss: 0.2782 cluster_loss: 1.1571 sup_con_loss: 0.5822 contrastive_loss: 4.5900 
2024-05-15 13:27:00.282 | INFO     | __main__:train:123 - Epoch: [138][20/63]	 loss 4.07015	 cls_loss: 0.2680 cluster_loss: 1.2233 sup_con_loss: 0.5592 contrastive_loss: 4.5931 
2024-05-15 13:27:17.312 | INFO     | __main__:train:123 - Epoch: [138][40/63]	 loss 4.10546	 cls_loss: 0.2993 cluster_loss: 1.1572 sup_con_loss: 0.7658 contrastive_loss: 4.5853 
2024-05-15 13:27:32.538 | INFO     | __main__:train:123 - Epoch: [138][60/63]	 loss 4.03132	 cls_loss: 0.2662 cluster_loss: 1.1530 sup_con_loss: 0.5668 contrastive_loss: 4.6005 
2024-05-15 13:27:34.152 | INFO     | __main__:train:126 - Train Epoch: 138 Avg Loss: 4.0692 
2024-05-15 13:27:34.152 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:28:06.486 | INFO     | __main__:train:135 - Train Accuracies: All 0.8113 | Old 0.8451 | New 0.7779
2024-05-15 13:28:14.140 | INFO     | __main__:train:123 - Epoch: [139][0/63]	 loss 4.07427	 cls_loss: 0.2785 cluster_loss: 1.2180 sup_con_loss: 0.5592 contrastive_loss: 4.5990 
2024-05-15 13:28:31.366 | INFO     | __main__:train:123 - Epoch: [139][20/63]	 loss 3.99934	 cls_loss: 0.2555 cluster_loss: 1.1564 sup_con_loss: 0.4907 contrastive_loss: 4.5946 
2024-05-15 13:28:48.176 | INFO     | __main__:train:123 - Epoch: [139][40/63]	 loss 4.04872	 cls_loss: 0.2713 cluster_loss: 1.2243 sup_con_loss: 0.4943 contrastive_loss: 4.5922 
2024-05-15 13:29:03.631 | INFO     | __main__:train:123 - Epoch: [139][60/63]	 loss 3.99926	 cls_loss: 0.2803 cluster_loss: 1.1223 sup_con_loss: 0.5348 contrastive_loss: 4.5915 
2024-05-15 13:29:05.292 | INFO     | __main__:train:126 - Train Epoch: 139 Avg Loss: 4.0688 
2024-05-15 13:29:05.293 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:29:38.189 | INFO     | __main__:train:135 - Train Accuracies: All 0.8130 | Old 0.8483 | New 0.7779
2024-05-15 13:29:45.775 | INFO     | __main__:train:123 - Epoch: [140][0/63]	 loss 4.04814	 cls_loss: 0.2661 cluster_loss: 1.1535 sup_con_loss: 0.6317 contrastive_loss: 4.5910 
2024-05-15 13:30:03.184 | INFO     | __main__:train:123 - Epoch: [140][20/63]	 loss 4.01714	 cls_loss: 0.2710 cluster_loss: 1.1535 sup_con_loss: 0.5453 contrastive_loss: 4.5872 
2024-05-15 13:30:19.792 | INFO     | __main__:train:123 - Epoch: [140][40/63]	 loss 4.11819	 cls_loss: 0.2631 cluster_loss: 1.2274 sup_con_loss: 0.6915 contrastive_loss: 4.5943 
2024-05-15 13:30:35.386 | INFO     | __main__:train:123 - Epoch: [140][60/63]	 loss 4.08931	 cls_loss: 0.2951 cluster_loss: 1.2331 sup_con_loss: 0.5682 contrastive_loss: 4.5933 
2024-05-15 13:30:37.029 | INFO     | __main__:train:126 - Train Epoch: 140 Avg Loss: 4.0758 
2024-05-15 13:30:37.029 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:31:08.559 | INFO     | __main__:train:135 - Train Accuracies: All 0.8130 | Old 0.8463 | New 0.7799
2024-05-15 13:31:16.529 | INFO     | __main__:train:123 - Epoch: [141][0/63]	 loss 4.00462	 cls_loss: 0.2862 cluster_loss: 1.1543 sup_con_loss: 0.4855 contrastive_loss: 4.5911 
2024-05-15 13:31:33.946 | INFO     | __main__:train:123 - Epoch: [141][20/63]	 loss 3.99922	 cls_loss: 0.3089 cluster_loss: 1.1125 sup_con_loss: 0.5240 contrastive_loss: 4.5916 
2024-05-15 13:31:51.340 | INFO     | __main__:train:123 - Epoch: [141][40/63]	 loss 4.18045	 cls_loss: 0.3190 cluster_loss: 1.2820 sup_con_loss: 0.7166 contrastive_loss: 4.5918 
2024-05-15 13:32:06.691 | INFO     | __main__:train:123 - Epoch: [141][60/63]	 loss 4.04398	 cls_loss: 0.3101 cluster_loss: 1.1553 sup_con_loss: 0.5663 contrastive_loss: 4.5943 
2024-05-15 13:32:08.314 | INFO     | __main__:train:126 - Train Epoch: 141 Avg Loss: 4.0711 
2024-05-15 13:32:08.314 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:32:40.788 | INFO     | __main__:train:135 - Train Accuracies: All 0.8132 | Old 0.8453 | New 0.7814
2024-05-15 13:32:48.638 | INFO     | __main__:train:123 - Epoch: [142][0/63]	 loss 4.02558	 cls_loss: 0.2985 cluster_loss: 1.1416 sup_con_loss: 0.5603 contrastive_loss: 4.5892 
2024-05-15 13:33:06.483 | INFO     | __main__:train:123 - Epoch: [142][20/63]	 loss 3.95588	 cls_loss: 0.2800 cluster_loss: 1.1506 sup_con_loss: 0.3544 contrastive_loss: 4.5937 
2024-05-15 13:33:23.503 | INFO     | __main__:train:123 - Epoch: [142][40/63]	 loss 4.10188	 cls_loss: 0.2762 cluster_loss: 1.2011 sup_con_loss: 0.6827 contrastive_loss: 4.5931 
2024-05-15 13:33:38.784 | INFO     | __main__:train:123 - Epoch: [142][60/63]	 loss 3.99591	 cls_loss: 0.2799 cluster_loss: 1.1363 sup_con_loss: 0.5005 contrastive_loss: 4.5911 
2024-05-15 13:33:40.444 | INFO     | __main__:train:126 - Train Epoch: 142 Avg Loss: 4.0742 
2024-05-15 13:33:40.445 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:34:11.916 | INFO     | __main__:train:135 - Train Accuracies: All 0.8128 | Old 0.8453 | New 0.7806
2024-05-15 13:34:20.028 | INFO     | __main__:train:123 - Epoch: [143][0/63]	 loss 4.04894	 cls_loss: 0.2627 cluster_loss: 1.1528 sup_con_loss: 0.6545 contrastive_loss: 4.5824 
2024-05-15 13:34:38.172 | INFO     | __main__:train:123 - Epoch: [143][20/63]	 loss 4.07377	 cls_loss: 0.2864 cluster_loss: 1.1538 sup_con_loss: 0.6948 contrastive_loss: 4.5852 
2024-05-15 13:34:55.645 | INFO     | __main__:train:123 - Epoch: [143][40/63]	 loss 4.08552	 cls_loss: 0.2989 cluster_loss: 1.2586 sup_con_loss: 0.4891 contrastive_loss: 4.6025 
2024-05-15 13:35:10.740 | INFO     | __main__:train:123 - Epoch: [143][60/63]	 loss 4.20269	 cls_loss: 0.2921 cluster_loss: 1.2430 sup_con_loss: 0.8795 contrastive_loss: 4.5918 
2024-05-15 13:35:12.378 | INFO     | __main__:train:126 - Train Epoch: 143 Avg Loss: 4.0624 
2024-05-15 13:35:12.379 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:35:45.012 | INFO     | __main__:train:135 - Train Accuracies: All 0.8113 | Old 0.8441 | New 0.7789
2024-05-15 13:35:52.838 | INFO     | __main__:train:123 - Epoch: [144][0/63]	 loss 4.03305	 cls_loss: 0.2728 cluster_loss: 1.1477 sup_con_loss: 0.6052 contrastive_loss: 4.5842 
2024-05-15 13:36:10.359 | INFO     | __main__:train:123 - Epoch: [144][20/63]	 loss 3.95066	 cls_loss: 0.2869 cluster_loss: 1.1021 sup_con_loss: 0.4225 contrastive_loss: 4.5939 
2024-05-15 13:36:27.832 | INFO     | __main__:train:123 - Epoch: [144][40/63]	 loss 4.05712	 cls_loss: 0.2900 cluster_loss: 1.1487 sup_con_loss: 0.6413 contrastive_loss: 4.5915 
2024-05-15 13:36:42.985 | INFO     | __main__:train:123 - Epoch: [144][60/63]	 loss 4.09506	 cls_loss: 0.2821 cluster_loss: 1.1813 sup_con_loss: 0.7026 contrastive_loss: 4.5886 
2024-05-15 13:36:44.678 | INFO     | __main__:train:126 - Train Epoch: 144 Avg Loss: 4.0797 
2024-05-15 13:36:44.679 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:37:16.373 | INFO     | __main__:train:135 - Train Accuracies: All 0.8130 | Old 0.8453 | New 0.7809
2024-05-15 13:37:23.356 | INFO     | __main__:train:123 - Epoch: [145][0/63]	 loss 4.05865	 cls_loss: 0.2910 cluster_loss: 1.1808 sup_con_loss: 0.5880 contrastive_loss: 4.5900 
2024-05-15 13:37:41.129 | INFO     | __main__:train:123 - Epoch: [145][20/63]	 loss 3.95320	 cls_loss: 0.2874 cluster_loss: 1.1116 sup_con_loss: 0.4086 contrastive_loss: 4.5954 
2024-05-15 13:37:58.449 | INFO     | __main__:train:123 - Epoch: [145][40/63]	 loss 4.18213	 cls_loss: 0.2844 cluster_loss: 1.2975 sup_con_loss: 0.7238 contrastive_loss: 4.5937 
2024-05-15 13:38:14.062 | INFO     | __main__:train:123 - Epoch: [145][60/63]	 loss 4.14380	 cls_loss: 0.2603 cluster_loss: 1.2454 sup_con_loss: 0.7358 contrastive_loss: 4.5933 
2024-05-15 13:38:15.859 | INFO     | __main__:train:126 - Train Epoch: 145 Avg Loss: 4.0917 
2024-05-15 13:38:15.860 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:38:48.103 | INFO     | __main__:train:135 - Train Accuracies: All 0.8111 | Old 0.8441 | New 0.7784
2024-05-15 13:38:55.684 | INFO     | __main__:train:123 - Epoch: [146][0/63]	 loss 4.05889	 cls_loss: 0.2656 cluster_loss: 1.1543 sup_con_loss: 0.6603 contrastive_loss: 4.5916 
2024-05-15 13:39:12.922 | INFO     | __main__:train:123 - Epoch: [146][20/63]	 loss 4.07507	 cls_loss: 0.2770 cluster_loss: 1.1776 sup_con_loss: 0.6583 contrastive_loss: 4.5881 
2024-05-15 13:39:30.267 | INFO     | __main__:train:123 - Epoch: [146][40/63]	 loss 4.08394	 cls_loss: 0.2687 cluster_loss: 1.1662 sup_con_loss: 0.7082 contrastive_loss: 4.5908 
2024-05-15 13:39:45.547 | INFO     | __main__:train:123 - Epoch: [146][60/63]	 loss 4.04680	 cls_loss: 0.2804 cluster_loss: 1.1633 sup_con_loss: 0.5896 contrastive_loss: 4.5941 
2024-05-15 13:39:47.270 | INFO     | __main__:train:126 - Train Epoch: 146 Avg Loss: 4.0527 
2024-05-15 13:39:47.270 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:40:19.181 | INFO     | __main__:train:135 - Train Accuracies: All 0.8121 | Old 0.8438 | New 0.7806
2024-05-15 13:40:26.601 | INFO     | __main__:train:123 - Epoch: [147][0/63]	 loss 4.01184	 cls_loss: 0.2753 cluster_loss: 1.1733 sup_con_loss: 0.4876 contrastive_loss: 4.5879 
2024-05-15 13:40:44.615 | INFO     | __main__:train:123 - Epoch: [147][20/63]	 loss 4.13532	 cls_loss: 0.2774 cluster_loss: 1.2252 sup_con_loss: 0.7258 contrastive_loss: 4.5967 
2024-05-15 13:41:02.185 | INFO     | __main__:train:123 - Epoch: [147][40/63]	 loss 4.12316	 cls_loss: 0.2826 cluster_loss: 1.2166 sup_con_loss: 0.7020 contrastive_loss: 4.5966 
2024-05-15 13:41:17.493 | INFO     | __main__:train:123 - Epoch: [147][60/63]	 loss 3.94558	 cls_loss: 0.2793 cluster_loss: 1.1381 sup_con_loss: 0.3481 contrastive_loss: 4.5941 
2024-05-15 13:41:19.248 | INFO     | __main__:train:126 - Train Epoch: 147 Avg Loss: 4.0688 
2024-05-15 13:41:19.248 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:41:52.503 | INFO     | __main__:train:135 - Train Accuracies: All 0.8132 | Old 0.8483 | New 0.7784
2024-05-15 13:42:00.369 | INFO     | __main__:train:123 - Epoch: [148][0/63]	 loss 4.06977	 cls_loss: 0.2877 cluster_loss: 1.1599 sup_con_loss: 0.6606 contrastive_loss: 4.5906 
2024-05-15 13:42:17.897 | INFO     | __main__:train:123 - Epoch: [148][20/63]	 loss 4.00747	 cls_loss: 0.2940 cluster_loss: 1.1530 sup_con_loss: 0.4989 contrastive_loss: 4.5854 
2024-05-15 13:42:35.782 | INFO     | __main__:train:123 - Epoch: [148][40/63]	 loss 3.96574	 cls_loss: 0.2802 cluster_loss: 1.1289 sup_con_loss: 0.4385 contrastive_loss: 4.5853 
2024-05-15 13:42:51.315 | INFO     | __main__:train:123 - Epoch: [148][60/63]	 loss 4.01388	 cls_loss: 0.2904 cluster_loss: 1.1086 sup_con_loss: 0.6056 contrastive_loss: 4.5842 
2024-05-15 13:42:52.997 | INFO     | __main__:train:126 - Train Epoch: 148 Avg Loss: 4.0688 
2024-05-15 13:42:52.998 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:43:25.129 | INFO     | __main__:train:135 - Train Accuracies: All 0.8128 | Old 0.8448 | New 0.7811
2024-05-15 13:43:31.589 | INFO     | __main__:train:123 - Epoch: [149][0/63]	 loss 3.97990	 cls_loss: 0.2562 cluster_loss: 1.1794 sup_con_loss: 0.3922 contrastive_loss: 4.5944 
2024-05-15 13:43:49.594 | INFO     | __main__:train:123 - Epoch: [149][20/63]	 loss 4.05812	 cls_loss: 0.2785 cluster_loss: 1.1542 sup_con_loss: 0.6549 contrastive_loss: 4.5865 
2024-05-15 13:44:07.398 | INFO     | __main__:train:123 - Epoch: [149][40/63]	 loss 4.07202	 cls_loss: 0.2989 cluster_loss: 1.2108 sup_con_loss: 0.5445 contrastive_loss: 4.5997 
2024-05-15 13:44:22.601 | INFO     | __main__:train:123 - Epoch: [149][60/63]	 loss 4.03651	 cls_loss: 0.2823 cluster_loss: 1.1628 sup_con_loss: 0.5695 contrastive_loss: 4.5886 
2024-05-15 13:44:24.260 | INFO     | __main__:train:126 - Train Epoch: 149 Avg Loss: 4.0712 
2024-05-15 13:44:24.261 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:44:56.133 | INFO     | __main__:train:135 - Train Accuracies: All 0.8131 | Old 0.8463 | New 0.7801
2024-05-15 13:45:02.565 | INFO     | __main__:train:123 - Epoch: [150][0/63]	 loss 4.07448	 cls_loss: 0.2896 cluster_loss: 1.2186 sup_con_loss: 0.5694 contrastive_loss: 4.5873 
2024-05-15 13:45:20.553 | INFO     | __main__:train:123 - Epoch: [150][20/63]	 loss 4.14894	 cls_loss: 0.2974 cluster_loss: 1.2563 sup_con_loss: 0.6946 contrastive_loss: 4.5925 
2024-05-15 13:45:38.157 | INFO     | __main__:train:123 - Epoch: [150][40/63]	 loss 3.91657	 cls_loss: 0.2952 cluster_loss: 1.1139 sup_con_loss: 0.3012 contrastive_loss: 4.5905 
2024-05-15 13:45:53.656 | INFO     | __main__:train:123 - Epoch: [150][60/63]	 loss 4.19300	 cls_loss: 0.2912 cluster_loss: 1.2681 sup_con_loss: 0.8139 contrastive_loss: 4.5876 
2024-05-15 13:45:55.328 | INFO     | __main__:train:126 - Train Epoch: 150 Avg Loss: 4.0833 
2024-05-15 13:45:55.329 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:46:27.989 | INFO     | __main__:train:135 - Train Accuracies: All 0.8122 | Old 0.8423 | New 0.7824
2024-05-15 13:46:35.254 | INFO     | __main__:train:123 - Epoch: [151][0/63]	 loss 4.12609	 cls_loss: 0.2961 cluster_loss: 1.1998 sup_con_loss: 0.7368 contrastive_loss: 4.5919 
2024-05-15 13:46:53.490 | INFO     | __main__:train:123 - Epoch: [151][20/63]	 loss 4.16026	 cls_loss: 0.2852 cluster_loss: 1.2744 sup_con_loss: 0.7157 contrastive_loss: 4.5871 
2024-05-15 13:47:11.635 | INFO     | __main__:train:123 - Epoch: [151][40/63]	 loss 4.05993	 cls_loss: 0.2611 cluster_loss: 1.1919 sup_con_loss: 0.5896 contrastive_loss: 4.5961 
2024-05-15 13:47:26.822 | INFO     | __main__:train:123 - Epoch: [151][60/63]	 loss 4.15097	 cls_loss: 0.2918 cluster_loss: 1.2424 sup_con_loss: 0.7232 contrastive_loss: 4.5972 
2024-05-15 13:47:28.527 | INFO     | __main__:train:126 - Train Epoch: 151 Avg Loss: 4.0689 
2024-05-15 13:47:28.527 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:48:00.637 | INFO     | __main__:train:135 - Train Accuracies: All 0.8112 | Old 0.8398 | New 0.7829
2024-05-15 13:48:08.000 | INFO     | __main__:train:123 - Epoch: [152][0/63]	 loss 3.97945	 cls_loss: 0.2786 cluster_loss: 1.0908 sup_con_loss: 0.5442 contrastive_loss: 4.5884 
2024-05-15 13:48:25.674 | INFO     | __main__:train:123 - Epoch: [152][20/63]	 loss 4.09586	 cls_loss: 0.2786 cluster_loss: 1.2259 sup_con_loss: 0.6252 contrastive_loss: 4.5888 
2024-05-15 13:48:43.459 | INFO     | __main__:train:123 - Epoch: [152][40/63]	 loss 3.97683	 cls_loss: 0.2883 cluster_loss: 1.1201 sup_con_loss: 0.4662 contrastive_loss: 4.5919 
2024-05-15 13:48:58.720 | INFO     | __main__:train:123 - Epoch: [152][60/63]	 loss 4.03081	 cls_loss: 0.2784 cluster_loss: 1.2039 sup_con_loss: 0.4736 contrastive_loss: 4.5924 
2024-05-15 13:49:00.386 | INFO     | __main__:train:126 - Train Epoch: 152 Avg Loss: 4.0778 
2024-05-15 13:49:00.387 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:49:32.867 | INFO     | __main__:train:135 - Train Accuracies: All 0.8108 | Old 0.8408 | New 0.7811
2024-05-15 13:49:40.739 | INFO     | __main__:train:123 - Epoch: [153][0/63]	 loss 4.00208	 cls_loss: 0.2753 cluster_loss: 1.1773 sup_con_loss: 0.4503 contrastive_loss: 4.5890 
2024-05-15 13:49:58.456 | INFO     | __main__:train:123 - Epoch: [153][20/63]	 loss 4.18525	 cls_loss: 0.2566 cluster_loss: 1.2441 sup_con_loss: 0.8649 contrastive_loss: 4.5909 
2024-05-15 13:50:15.573 | INFO     | __main__:train:123 - Epoch: [153][40/63]	 loss 4.08510	 cls_loss: 0.2910 cluster_loss: 1.2116 sup_con_loss: 0.6027 contrastive_loss: 4.5920 
2024-05-15 13:50:30.745 | INFO     | __main__:train:123 - Epoch: [153][60/63]	 loss 4.11550	 cls_loss: 0.2632 cluster_loss: 1.2064 sup_con_loss: 0.7306 contrastive_loss: 4.5900 
2024-05-15 13:50:32.462 | INFO     | __main__:train:126 - Train Epoch: 153 Avg Loss: 4.0794 
2024-05-15 13:50:32.462 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:51:04.673 | INFO     | __main__:train:135 - Train Accuracies: All 0.8127 | Old 0.8463 | New 0.7794
2024-05-15 13:51:11.940 | INFO     | __main__:train:123 - Epoch: [154][0/63]	 loss 4.05821	 cls_loss: 0.2795 cluster_loss: 1.1944 sup_con_loss: 0.5777 contrastive_loss: 4.5874 
2024-05-15 13:51:29.693 | INFO     | __main__:train:123 - Epoch: [154][20/63]	 loss 4.14394	 cls_loss: 0.3093 cluster_loss: 1.2414 sup_con_loss: 0.6869 contrastive_loss: 4.5975 
2024-05-15 13:51:47.529 | INFO     | __main__:train:123 - Epoch: [154][40/63]	 loss 4.09481	 cls_loss: 0.2871 cluster_loss: 1.1678 sup_con_loss: 0.7216 contrastive_loss: 4.5887 
2024-05-15 13:52:03.055 | INFO     | __main__:train:123 - Epoch: [154][60/63]	 loss 4.15984	 cls_loss: 0.3090 cluster_loss: 1.2952 sup_con_loss: 0.6375 contrastive_loss: 4.5949 
2024-05-15 13:52:04.730 | INFO     | __main__:train:126 - Train Epoch: 154 Avg Loss: 4.0583 
2024-05-15 13:52:04.730 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:52:37.159 | INFO     | __main__:train:135 - Train Accuracies: All 0.8108 | Old 0.8411 | New 0.7809
2024-05-15 13:52:44.911 | INFO     | __main__:train:123 - Epoch: [155][0/63]	 loss 4.18910	 cls_loss: 0.2921 cluster_loss: 1.2604 sup_con_loss: 0.8080 contrastive_loss: 4.5920 
2024-05-15 13:53:02.765 | INFO     | __main__:train:123 - Epoch: [155][20/63]	 loss 4.05691	 cls_loss: 0.2777 cluster_loss: 1.1711 sup_con_loss: 0.6104 contrastive_loss: 4.5921 
2024-05-15 13:53:20.085 | INFO     | __main__:train:123 - Epoch: [155][40/63]	 loss 4.02927	 cls_loss: 0.2942 cluster_loss: 1.1677 sup_con_loss: 0.5264 contrastive_loss: 4.5893 
2024-05-15 13:53:35.326 | INFO     | __main__:train:123 - Epoch: [155][60/63]	 loss 4.01791	 cls_loss: 0.2672 cluster_loss: 1.1671 sup_con_loss: 0.5103 contrastive_loss: 4.5956 
2024-05-15 13:53:37.017 | INFO     | __main__:train:126 - Train Epoch: 155 Avg Loss: 4.0745 
2024-05-15 13:53:37.018 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:54:09.431 | INFO     | __main__:train:135 - Train Accuracies: All 0.8128 | Old 0.8433 | New 0.7826
2024-05-15 13:54:16.325 | INFO     | __main__:train:123 - Epoch: [156][0/63]	 loss 4.07938	 cls_loss: 0.2888 cluster_loss: 1.2145 sup_con_loss: 0.5843 contrastive_loss: 4.5913 
2024-05-15 13:54:33.748 | INFO     | __main__:train:123 - Epoch: [156][20/63]	 loss 4.08742	 cls_loss: 0.2947 cluster_loss: 1.1736 sup_con_loss: 0.6870 contrastive_loss: 4.5861 
2024-05-15 13:54:51.801 | INFO     | __main__:train:123 - Epoch: [156][40/63]	 loss 3.95960	 cls_loss: 0.2843 cluster_loss: 1.1075 sup_con_loss: 0.4393 contrastive_loss: 4.5946 
2024-05-15 13:55:07.217 | INFO     | __main__:train:123 - Epoch: [156][60/63]	 loss 4.23024	 cls_loss: 0.2809 cluster_loss: 1.2964 sup_con_loss: 0.8664 contrastive_loss: 4.5939 
2024-05-15 13:55:08.862 | INFO     | __main__:train:126 - Train Epoch: 156 Avg Loss: 4.0895 
2024-05-15 13:55:08.862 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:55:41.144 | INFO     | __main__:train:135 - Train Accuracies: All 0.8140 | Old 0.8468 | New 0.7814
2024-05-15 13:55:49.229 | INFO     | __main__:train:123 - Epoch: [157][0/63]	 loss 4.10510	 cls_loss: 0.2890 cluster_loss: 1.2277 sup_con_loss: 0.6276 contrastive_loss: 4.5943 
2024-05-15 13:56:07.018 | INFO     | __main__:train:123 - Epoch: [157][20/63]	 loss 4.11396	 cls_loss: 0.2727 cluster_loss: 1.2320 sup_con_loss: 0.6669 contrastive_loss: 4.5913 
2024-05-15 13:56:23.989 | INFO     | __main__:train:123 - Epoch: [157][40/63]	 loss 4.18698	 cls_loss: 0.2865 cluster_loss: 1.2812 sup_con_loss: 0.7663 contrastive_loss: 4.5935 
2024-05-15 13:56:39.372 | INFO     | __main__:train:123 - Epoch: [157][60/63]	 loss 4.09319	 cls_loss: 0.2963 cluster_loss: 1.2497 sup_con_loss: 0.5346 contrastive_loss: 4.6002 
2024-05-15 13:56:41.053 | INFO     | __main__:train:126 - Train Epoch: 157 Avg Loss: 4.0865 
2024-05-15 13:56:41.053 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:57:12.985 | INFO     | __main__:train:135 - Train Accuracies: All 0.8132 | Old 0.8448 | New 0.7819
2024-05-15 13:57:19.644 | INFO     | __main__:train:123 - Epoch: [158][0/63]	 loss 4.09455	 cls_loss: 0.2707 cluster_loss: 1.1914 sup_con_loss: 0.7013 contrastive_loss: 4.5846 
2024-05-15 13:57:38.145 | INFO     | __main__:train:123 - Epoch: [158][20/63]	 loss 4.16523	 cls_loss: 0.2782 cluster_loss: 1.2433 sup_con_loss: 0.7855 contrastive_loss: 4.5920 
2024-05-15 13:57:55.712 | INFO     | __main__:train:123 - Epoch: [158][40/63]	 loss 4.20689	 cls_loss: 0.2909 cluster_loss: 1.2791 sup_con_loss: 0.8199 contrastive_loss: 4.5949 
2024-05-15 13:58:11.137 | INFO     | __main__:train:123 - Epoch: [158][60/63]	 loss 4.15667	 cls_loss: 0.2757 cluster_loss: 1.2880 sup_con_loss: 0.6828 contrastive_loss: 4.5908 
2024-05-15 13:58:12.813 | INFO     | __main__:train:126 - Train Epoch: 158 Avg Loss: 4.0766 
2024-05-15 13:58:12.813 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 13:58:45.608 | INFO     | __main__:train:135 - Train Accuracies: All 0.8120 | Old 0.8423 | New 0.7819
2024-05-15 13:58:53.179 | INFO     | __main__:train:123 - Epoch: [159][0/63]	 loss 4.02683	 cls_loss: 0.2762 cluster_loss: 1.1499 sup_con_loss: 0.5780 contrastive_loss: 4.5853 
2024-05-15 13:59:10.810 | INFO     | __main__:train:123 - Epoch: [159][20/63]	 loss 4.07354	 cls_loss: 0.3109 cluster_loss: 1.1636 sup_con_loss: 0.6446 contrastive_loss: 4.5889 
2024-05-15 13:59:28.276 | INFO     | __main__:train:123 - Epoch: [159][40/63]	 loss 4.14508	 cls_loss: 0.2694 cluster_loss: 1.2478 sup_con_loss: 0.7309 contrastive_loss: 4.5906 
2024-05-15 13:59:43.544 | INFO     | __main__:train:123 - Epoch: [159][60/63]	 loss 4.04307	 cls_loss: 0.2916 cluster_loss: 1.1551 sup_con_loss: 0.5944 contrastive_loss: 4.5879 
2024-05-15 13:59:45.233 | INFO     | __main__:train:126 - Train Epoch: 159 Avg Loss: 4.0711 
2024-05-15 13:59:45.233 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:00:17.310 | INFO     | __main__:train:135 - Train Accuracies: All 0.8136 | Old 0.8463 | New 0.7811
2024-05-15 14:00:24.779 | INFO     | __main__:train:123 - Epoch: [160][0/63]	 loss 4.07295	 cls_loss: 0.2697 cluster_loss: 1.1772 sup_con_loss: 0.6446 contrastive_loss: 4.5965 
2024-05-15 14:00:42.987 | INFO     | __main__:train:123 - Epoch: [160][20/63]	 loss 4.16540	 cls_loss: 0.2653 cluster_loss: 1.2591 sup_con_loss: 0.7724 contrastive_loss: 4.5904 
2024-05-15 14:01:00.355 | INFO     | __main__:train:123 - Epoch: [160][40/63]	 loss 4.00069	 cls_loss: 0.2769 cluster_loss: 1.1230 sup_con_loss: 0.5457 contrastive_loss: 4.5889 
2024-05-15 14:01:15.697 | INFO     | __main__:train:123 - Epoch: [160][60/63]	 loss 4.01951	 cls_loss: 0.2710 cluster_loss: 1.1997 sup_con_loss: 0.4528 contrastive_loss: 4.5945 
2024-05-15 14:01:17.442 | INFO     | __main__:train:126 - Train Epoch: 160 Avg Loss: 4.0694 
2024-05-15 14:01:17.442 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:01:50.614 | INFO     | __main__:train:135 - Train Accuracies: All 0.8135 | Old 0.8448 | New 0.7824
2024-05-15 14:01:58.216 | INFO     | __main__:train:123 - Epoch: [161][0/63]	 loss 4.09716	 cls_loss: 0.2586 cluster_loss: 1.1921 sup_con_loss: 0.7178 contrastive_loss: 4.5854 
2024-05-15 14:02:15.993 | INFO     | __main__:train:123 - Epoch: [161][20/63]	 loss 3.99733	 cls_loss: 0.2710 cluster_loss: 1.1666 sup_con_loss: 0.4580 contrastive_loss: 4.5906 
2024-05-15 14:02:33.594 | INFO     | __main__:train:123 - Epoch: [161][40/63]	 loss 4.00938	 cls_loss: 0.2957 cluster_loss: 1.1029 sup_con_loss: 0.5976 contrastive_loss: 4.5843 
2024-05-15 14:02:49.019 | INFO     | __main__:train:123 - Epoch: [161][60/63]	 loss 4.01483	 cls_loss: 0.2868 cluster_loss: 1.1811 sup_con_loss: 0.4545 contrastive_loss: 4.5964 
2024-05-15 14:02:50.673 | INFO     | __main__:train:126 - Train Epoch: 161 Avg Loss: 4.0731 
2024-05-15 14:02:50.674 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:03:23.036 | INFO     | __main__:train:135 - Train Accuracies: All 0.8148 | Old 0.8456 | New 0.7844
2024-05-15 14:03:29.656 | INFO     | __main__:train:123 - Epoch: [162][0/63]	 loss 4.07040	 cls_loss: 0.2781 cluster_loss: 1.2003 sup_con_loss: 0.5841 contrastive_loss: 4.5976 
2024-05-15 14:03:47.625 | INFO     | __main__:train:123 - Epoch: [162][20/63]	 loss 4.03390	 cls_loss: 0.2868 cluster_loss: 1.1454 sup_con_loss: 0.5873 contrastive_loss: 4.5900 
2024-05-15 14:04:05.073 | INFO     | __main__:train:123 - Epoch: [162][40/63]	 loss 4.03175	 cls_loss: 0.2563 cluster_loss: 1.1464 sup_con_loss: 0.6092 contrastive_loss: 4.5903 
2024-05-15 14:04:20.490 | INFO     | __main__:train:123 - Epoch: [162][60/63]	 loss 4.02474	 cls_loss: 0.2721 cluster_loss: 1.1809 sup_con_loss: 0.5067 contrastive_loss: 4.5916 
2024-05-15 14:04:22.170 | INFO     | __main__:train:126 - Train Epoch: 162 Avg Loss: 4.0638 
2024-05-15 14:04:22.170 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:04:54.148 | INFO     | __main__:train:135 - Train Accuracies: All 0.8141 | Old 0.8471 | New 0.7814
2024-05-15 14:05:02.001 | INFO     | __main__:train:123 - Epoch: [163][0/63]	 loss 4.14767	 cls_loss: 0.2751 cluster_loss: 1.2763 sup_con_loss: 0.6783 contrastive_loss: 4.5914 
2024-05-15 14:05:19.602 | INFO     | __main__:train:123 - Epoch: [163][20/63]	 loss 4.22307	 cls_loss: 0.2944 cluster_loss: 1.3044 sup_con_loss: 0.8322 contrastive_loss: 4.5859 
2024-05-15 14:05:37.339 | INFO     | __main__:train:123 - Epoch: [163][40/63]	 loss 4.00944	 cls_loss: 0.2787 cluster_loss: 1.1493 sup_con_loss: 0.5260 contrastive_loss: 4.5858 
2024-05-15 14:05:52.881 | INFO     | __main__:train:123 - Epoch: [163][60/63]	 loss 4.03253	 cls_loss: 0.2703 cluster_loss: 1.1229 sup_con_loss: 0.6502 contrastive_loss: 4.5853 
2024-05-15 14:05:54.548 | INFO     | __main__:train:126 - Train Epoch: 163 Avg Loss: 4.0914 
2024-05-15 14:05:54.549 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:06:26.922 | INFO     | __main__:train:135 - Train Accuracies: All 0.8140 | Old 0.8448 | New 0.7834
2024-05-15 14:06:34.487 | INFO     | __main__:train:123 - Epoch: [164][0/63]	 loss 4.11553	 cls_loss: 0.2783 cluster_loss: 1.2069 sup_con_loss: 0.7236 contrastive_loss: 4.5852 
2024-05-15 14:06:52.204 | INFO     | __main__:train:123 - Epoch: [164][20/63]	 loss 3.99903	 cls_loss: 0.2791 cluster_loss: 1.1211 sup_con_loss: 0.5361 contrastive_loss: 4.5923 
2024-05-15 14:07:09.748 | INFO     | __main__:train:123 - Epoch: [164][40/63]	 loss 4.18889	 cls_loss: 0.2739 cluster_loss: 1.2894 sup_con_loss: 0.7753 contrastive_loss: 4.5901 
2024-05-15 14:07:25.019 | INFO     | __main__:train:123 - Epoch: [164][60/63]	 loss 3.97752	 cls_loss: 0.2831 cluster_loss: 1.1310 sup_con_loss: 0.4557 contrastive_loss: 4.5904 
2024-05-15 14:07:26.715 | INFO     | __main__:train:126 - Train Epoch: 164 Avg Loss: 4.0554 
2024-05-15 14:07:26.715 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:07:58.470 | INFO     | __main__:train:135 - Train Accuracies: All 0.8156 | Old 0.8471 | New 0.7844
2024-05-15 14:08:05.430 | INFO     | __main__:train:123 - Epoch: [165][0/63]	 loss 3.99789	 cls_loss: 0.2699 cluster_loss: 1.1334 sup_con_loss: 0.5271 contrastive_loss: 4.5880 
2024-05-15 14:08:23.897 | INFO     | __main__:train:123 - Epoch: [165][20/63]	 loss 4.15488	 cls_loss: 0.2672 cluster_loss: 1.2576 sup_con_loss: 0.7315 contrastive_loss: 4.5968 
2024-05-15 14:08:41.259 | INFO     | __main__:train:123 - Epoch: [165][40/63]	 loss 3.97452	 cls_loss: 0.2711 cluster_loss: 1.0994 sup_con_loss: 0.5229 contrastive_loss: 4.5877 
2024-05-15 14:08:56.515 | INFO     | __main__:train:123 - Epoch: [165][60/63]	 loss 4.06982	 cls_loss: 0.2809 cluster_loss: 1.2148 sup_con_loss: 0.5589 contrastive_loss: 4.5942 
2024-05-15 14:08:58.326 | INFO     | __main__:train:126 - Train Epoch: 165 Avg Loss: 4.0655 
2024-05-15 14:08:58.326 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:09:30.848 | INFO     | __main__:train:135 - Train Accuracies: All 0.8135 | Old 0.8431 | New 0.7841
2024-05-15 14:09:37.824 | INFO     | __main__:train:123 - Epoch: [166][0/63]	 loss 3.97393	 cls_loss: 0.2666 cluster_loss: 1.1354 sup_con_loss: 0.4523 contrastive_loss: 4.5912 
2024-05-15 14:09:55.413 | INFO     | __main__:train:123 - Epoch: [166][20/63]	 loss 4.08784	 cls_loss: 0.2695 cluster_loss: 1.1905 sup_con_loss: 0.6729 contrastive_loss: 4.5910 
2024-05-15 14:10:12.700 | INFO     | __main__:train:123 - Epoch: [166][40/63]	 loss 4.12018	 cls_loss: 0.3162 cluster_loss: 1.2033 sup_con_loss: 0.7037 contrastive_loss: 4.5863 
2024-05-15 14:10:28.076 | INFO     | __main__:train:123 - Epoch: [166][60/63]	 loss 4.14218	 cls_loss: 0.2796 cluster_loss: 1.2391 sup_con_loss: 0.7258 contrastive_loss: 4.5922 
2024-05-15 14:10:29.714 | INFO     | __main__:train:126 - Train Epoch: 166 Avg Loss: 4.0772 
2024-05-15 14:10:29.715 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:11:00.852 | INFO     | __main__:train:135 - Train Accuracies: All 0.8136 | Old 0.8436 | New 0.7839
2024-05-15 14:11:08.823 | INFO     | __main__:train:123 - Epoch: [167][0/63]	 loss 4.06117	 cls_loss: 0.2800 cluster_loss: 1.1480 sup_con_loss: 0.6786 contrastive_loss: 4.5838 
2024-05-15 14:11:26.369 | INFO     | __main__:train:123 - Epoch: [167][20/63]	 loss 4.10440	 cls_loss: 0.2575 cluster_loss: 1.2574 sup_con_loss: 0.5926 contrastive_loss: 4.5993 
2024-05-15 14:11:43.706 | INFO     | __main__:train:123 - Epoch: [167][40/63]	 loss 4.05840	 cls_loss: 0.2695 cluster_loss: 1.2144 sup_con_loss: 0.5571 contrastive_loss: 4.5841 
2024-05-15 14:11:58.888 | INFO     | __main__:train:123 - Epoch: [167][60/63]	 loss 4.03118	 cls_loss: 0.2738 cluster_loss: 1.1356 sup_con_loss: 0.6150 contrastive_loss: 4.5876 
2024-05-15 14:12:00.583 | INFO     | __main__:train:126 - Train Epoch: 167 Avg Loss: 4.0677 
2024-05-15 14:12:00.583 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:12:33.017 | INFO     | __main__:train:135 - Train Accuracies: All 0.8123 | Old 0.8443 | New 0.7806
2024-05-15 14:12:39.896 | INFO     | __main__:train:123 - Epoch: [168][0/63]	 loss 4.29867	 cls_loss: 0.2586 cluster_loss: 1.3152 sup_con_loss: 1.0614 contrastive_loss: 4.5874 
2024-05-15 14:12:58.009 | INFO     | __main__:train:123 - Epoch: [168][20/63]	 loss 4.10648	 cls_loss: 0.2495 cluster_loss: 1.2280 sup_con_loss: 0.6788 contrastive_loss: 4.5898 
2024-05-15 14:13:15.715 | INFO     | __main__:train:123 - Epoch: [168][40/63]	 loss 4.14839	 cls_loss: 0.2932 cluster_loss: 1.2648 sup_con_loss: 0.6703 contrastive_loss: 4.5985 
2024-05-15 14:13:31.049 | INFO     | __main__:train:123 - Epoch: [168][60/63]	 loss 3.90637	 cls_loss: 0.2881 cluster_loss: 1.0467 sup_con_loss: 0.4116 contrastive_loss: 4.5864 
2024-05-15 14:13:32.749 | INFO     | __main__:train:126 - Train Epoch: 168 Avg Loss: 4.0833 
2024-05-15 14:13:32.750 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:14:05.301 | INFO     | __main__:train:135 - Train Accuracies: All 0.8151 | Old 0.8468 | New 0.7836
2024-05-15 14:14:12.617 | INFO     | __main__:train:123 - Epoch: [169][0/63]	 loss 4.05942	 cls_loss: 0.3061 cluster_loss: 1.1570 sup_con_loss: 0.6060 contrastive_loss: 4.5971 
2024-05-15 14:14:31.068 | INFO     | __main__:train:123 - Epoch: [169][20/63]	 loss 4.08433	 cls_loss: 0.2668 cluster_loss: 1.1786 sup_con_loss: 0.6962 contrastive_loss: 4.5865 
2024-05-15 14:14:48.363 | INFO     | __main__:train:123 - Epoch: [169][40/63]	 loss 4.16459	 cls_loss: 0.2638 cluster_loss: 1.2698 sup_con_loss: 0.7506 contrastive_loss: 4.5911 
2024-05-15 14:15:03.783 | INFO     | __main__:train:123 - Epoch: [169][60/63]	 loss 3.95904	 cls_loss: 0.3164 cluster_loss: 1.0722 sup_con_loss: 0.4955 contrastive_loss: 4.5815 
2024-05-15 14:15:05.454 | INFO     | __main__:train:126 - Train Epoch: 169 Avg Loss: 4.0661 
2024-05-15 14:15:05.455 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:15:38.283 | INFO     | __main__:train:135 - Train Accuracies: All 0.8140 | Old 0.8461 | New 0.7821
2024-05-15 14:15:46.291 | INFO     | __main__:train:123 - Epoch: [170][0/63]	 loss 4.01486	 cls_loss: 0.2647 cluster_loss: 1.1366 sup_con_loss: 0.5795 contrastive_loss: 4.5855 
2024-05-15 14:16:04.176 | INFO     | __main__:train:123 - Epoch: [170][20/63]	 loss 4.13397	 cls_loss: 0.2917 cluster_loss: 1.1947 sup_con_loss: 0.7786 contrastive_loss: 4.5890 
2024-05-15 14:16:21.638 | INFO     | __main__:train:123 - Epoch: [170][40/63]	 loss 4.00766	 cls_loss: 0.2843 cluster_loss: 1.0993 sup_con_loss: 0.6091 contrastive_loss: 4.5853 
2024-05-15 14:16:37.047 | INFO     | __main__:train:123 - Epoch: [170][60/63]	 loss 4.15475	 cls_loss: 0.2820 cluster_loss: 1.2241 sup_con_loss: 0.7924 contrastive_loss: 4.5893 
2024-05-15 14:16:38.721 | INFO     | __main__:train:126 - Train Epoch: 170 Avg Loss: 4.0734 
2024-05-15 14:16:38.722 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:17:10.953 | INFO     | __main__:train:135 - Train Accuracies: All 0.8138 | Old 0.8441 | New 0.7839
2024-05-15 14:17:18.341 | INFO     | __main__:train:123 - Epoch: [171][0/63]	 loss 4.01046	 cls_loss: 0.2558 cluster_loss: 1.1756 sup_con_loss: 0.4685 contrastive_loss: 4.6043 
2024-05-15 14:17:35.694 | INFO     | __main__:train:123 - Epoch: [171][20/63]	 loss 4.09152	 cls_loss: 0.2742 cluster_loss: 1.1954 sup_con_loss: 0.6696 contrastive_loss: 4.5910 
2024-05-15 14:17:53.275 | INFO     | __main__:train:123 - Epoch: [171][40/63]	 loss 4.05346	 cls_loss: 0.2731 cluster_loss: 1.1584 sup_con_loss: 0.6395 contrastive_loss: 4.5863 
2024-05-15 14:18:09.022 | INFO     | __main__:train:123 - Epoch: [171][60/63]	 loss 4.14732	 cls_loss: 0.2700 cluster_loss: 1.2518 sup_con_loss: 0.7367 contrastive_loss: 4.5867 
2024-05-15 14:18:10.716 | INFO     | __main__:train:126 - Train Epoch: 171 Avg Loss: 4.0645 
2024-05-15 14:18:10.716 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:18:43.091 | INFO     | __main__:train:135 - Train Accuracies: All 0.8127 | Old 0.8426 | New 0.7831
2024-05-15 14:18:50.024 | INFO     | __main__:train:123 - Epoch: [172][0/63]	 loss 4.07252	 cls_loss: 0.3021 cluster_loss: 1.1885 sup_con_loss: 0.5891 contrastive_loss: 4.5970 
2024-05-15 14:19:07.391 | INFO     | __main__:train:123 - Epoch: [172][20/63]	 loss 4.03800	 cls_loss: 0.2751 cluster_loss: 1.1640 sup_con_loss: 0.5707 contrastive_loss: 4.5929 
2024-05-15 14:19:25.699 | INFO     | __main__:train:123 - Epoch: [172][40/63]	 loss 4.27021	 cls_loss: 0.2779 cluster_loss: 1.3615 sup_con_loss: 0.8580 contrastive_loss: 4.5965 
2024-05-15 14:19:41.204 | INFO     | __main__:train:123 - Epoch: [172][60/63]	 loss 4.11182	 cls_loss: 0.2658 cluster_loss: 1.2566 sup_con_loss: 0.6151 contrastive_loss: 4.5949 
2024-05-15 14:19:42.905 | INFO     | __main__:train:126 - Train Epoch: 172 Avg Loss: 4.0844 
2024-05-15 14:19:42.905 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:20:15.194 | INFO     | __main__:train:135 - Train Accuracies: All 0.8147 | Old 0.8451 | New 0.7846
2024-05-15 14:20:23.209 | INFO     | __main__:train:123 - Epoch: [173][0/63]	 loss 4.11960	 cls_loss: 0.3060 cluster_loss: 1.1997 sup_con_loss: 0.7147 contrastive_loss: 4.5886 
2024-05-15 14:20:40.925 | INFO     | __main__:train:123 - Epoch: [173][20/63]	 loss 4.00918	 cls_loss: 0.2472 cluster_loss: 1.0988 sup_con_loss: 0.6577 contrastive_loss: 4.5819 
2024-05-15 14:20:58.190 | INFO     | __main__:train:123 - Epoch: [173][40/63]	 loss 4.06226	 cls_loss: 0.2964 cluster_loss: 1.1988 sup_con_loss: 0.5601 contrastive_loss: 4.5896 
2024-05-15 14:21:13.575 | INFO     | __main__:train:123 - Epoch: [173][60/63]	 loss 4.03301	 cls_loss: 0.2658 cluster_loss: 1.1689 sup_con_loss: 0.5663 contrastive_loss: 4.5877 
2024-05-15 14:21:15.249 | INFO     | __main__:train:126 - Train Epoch: 173 Avg Loss: 4.0670 
2024-05-15 14:21:15.249 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:21:47.020 | INFO     | __main__:train:135 - Train Accuracies: All 0.8145 | Old 0.8471 | New 0.7821
2024-05-15 14:21:54.781 | INFO     | __main__:train:123 - Epoch: [174][0/63]	 loss 4.05500	 cls_loss: 0.2820 cluster_loss: 1.1726 sup_con_loss: 0.6094 contrastive_loss: 4.5859 
2024-05-15 14:22:12.296 | INFO     | __main__:train:123 - Epoch: [174][20/63]	 loss 4.11324	 cls_loss: 0.2968 cluster_loss: 1.2379 sup_con_loss: 0.6425 contrastive_loss: 4.5843 
2024-05-15 14:22:29.369 | INFO     | __main__:train:123 - Epoch: [174][40/63]	 loss 4.10405	 cls_loss: 0.2979 cluster_loss: 1.1664 sup_con_loss: 0.7454 contrastive_loss: 4.5857 
2024-05-15 14:22:45.025 | INFO     | __main__:train:123 - Epoch: [174][60/63]	 loss 4.05747	 cls_loss: 0.2584 cluster_loss: 1.1735 sup_con_loss: 0.6236 contrastive_loss: 4.5939 
2024-05-15 14:22:46.762 | INFO     | __main__:train:126 - Train Epoch: 174 Avg Loss: 4.0735 
2024-05-15 14:22:46.763 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:23:19.604 | INFO     | __main__:train:135 - Train Accuracies: All 0.8131 | Old 0.8421 | New 0.7844
2024-05-15 14:23:26.715 | INFO     | __main__:train:123 - Epoch: [175][0/63]	 loss 3.95242	 cls_loss: 0.2772 cluster_loss: 1.0766 sup_con_loss: 0.5057 contrastive_loss: 4.5825 
2024-05-15 14:23:44.847 | INFO     | __main__:train:123 - Epoch: [175][20/63]	 loss 4.00012	 cls_loss: 0.2690 cluster_loss: 1.0860 sup_con_loss: 0.6241 contrastive_loss: 4.5871 
2024-05-15 14:24:02.617 | INFO     | __main__:train:123 - Epoch: [175][40/63]	 loss 3.93576	 cls_loss: 0.2834 cluster_loss: 1.0771 sup_con_loss: 0.4313 contrastive_loss: 4.5931 
2024-05-15 14:24:17.823 | INFO     | __main__:train:123 - Epoch: [175][60/63]	 loss 4.02673	 cls_loss: 0.2712 cluster_loss: 1.1328 sup_con_loss: 0.6088 contrastive_loss: 4.5883 
2024-05-15 14:24:19.535 | INFO     | __main__:train:126 - Train Epoch: 175 Avg Loss: 4.0576 
2024-05-15 14:24:19.536 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:24:51.071 | INFO     | __main__:train:135 - Train Accuracies: All 0.8128 | Old 0.8428 | New 0.7831
2024-05-15 14:24:57.624 | INFO     | __main__:train:123 - Epoch: [176][0/63]	 loss 4.08930	 cls_loss: 0.2758 cluster_loss: 1.2224 sup_con_loss: 0.6144 contrastive_loss: 4.5895 
2024-05-15 14:25:15.441 | INFO     | __main__:train:123 - Epoch: [176][20/63]	 loss 4.06784	 cls_loss: 0.2671 cluster_loss: 1.1673 sup_con_loss: 0.6604 contrastive_loss: 4.5915 
2024-05-15 14:25:33.407 | INFO     | __main__:train:123 - Epoch: [176][40/63]	 loss 4.12104	 cls_loss: 0.2599 cluster_loss: 1.2473 sup_con_loss: 0.6795 contrastive_loss: 4.5870 
2024-05-15 14:25:49.066 | INFO     | __main__:train:123 - Epoch: [176][60/63]	 loss 4.09690	 cls_loss: 0.2713 cluster_loss: 1.2019 sup_con_loss: 0.6715 contrastive_loss: 4.5934 
2024-05-15 14:25:50.798 | INFO     | __main__:train:126 - Train Epoch: 176 Avg Loss: 4.0610 
2024-05-15 14:25:50.798 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:26:23.562 | INFO     | __main__:train:135 - Train Accuracies: All 0.8138 | Old 0.8428 | New 0.7851
2024-05-15 14:26:31.410 | INFO     | __main__:train:123 - Epoch: [177][0/63]	 loss 4.12822	 cls_loss: 0.2617 cluster_loss: 1.2455 sup_con_loss: 0.7003 contrastive_loss: 4.5877 
2024-05-15 14:26:49.567 | INFO     | __main__:train:123 - Epoch: [177][20/63]	 loss 4.03753	 cls_loss: 0.2727 cluster_loss: 1.1406 sup_con_loss: 0.6221 contrastive_loss: 4.5891 
2024-05-15 14:27:06.782 | INFO     | __main__:train:123 - Epoch: [177][40/63]	 loss 4.17969	 cls_loss: 0.2726 cluster_loss: 1.2889 sup_con_loss: 0.7391 contrastive_loss: 4.5966 
2024-05-15 14:27:21.945 | INFO     | __main__:train:123 - Epoch: [177][60/63]	 loss 4.20042	 cls_loss: 0.2588 cluster_loss: 1.3063 sup_con_loss: 0.7827 contrastive_loss: 4.5950 
2024-05-15 14:27:23.618 | INFO     | __main__:train:126 - Train Epoch: 177 Avg Loss: 4.0635 
2024-05-15 14:27:23.619 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:27:55.981 | INFO     | __main__:train:135 - Train Accuracies: All 0.8133 | Old 0.8438 | New 0.7831
2024-05-15 14:28:03.776 | INFO     | __main__:train:123 - Epoch: [178][0/63]	 loss 4.09069	 cls_loss: 0.2660 cluster_loss: 1.1652 sup_con_loss: 0.7329 contrastive_loss: 4.5903 
2024-05-15 14:28:21.236 | INFO     | __main__:train:123 - Epoch: [178][20/63]	 loss 4.09389	 cls_loss: 0.2691 cluster_loss: 1.2198 sup_con_loss: 0.6428 contrastive_loss: 4.5874 
2024-05-15 14:28:38.842 | INFO     | __main__:train:123 - Epoch: [178][40/63]	 loss 4.11500	 cls_loss: 0.2845 cluster_loss: 1.2005 sup_con_loss: 0.7207 contrastive_loss: 4.5890 
2024-05-15 14:28:54.100 | INFO     | __main__:train:123 - Epoch: [178][60/63]	 loss 4.15758	 cls_loss: 0.2767 cluster_loss: 1.2532 sup_con_loss: 0.7511 contrastive_loss: 4.5896 
2024-05-15 14:28:55.773 | INFO     | __main__:train:126 - Train Epoch: 178 Avg Loss: 4.0715 
2024-05-15 14:28:55.773 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:29:28.176 | INFO     | __main__:train:135 - Train Accuracies: All 0.8138 | Old 0.8451 | New 0.7829
2024-05-15 14:29:34.494 | INFO     | __main__:train:123 - Epoch: [179][0/63]	 loss 4.06795	 cls_loss: 0.2807 cluster_loss: 1.1965 sup_con_loss: 0.5885 contrastive_loss: 4.5939 
2024-05-15 14:29:52.702 | INFO     | __main__:train:123 - Epoch: [179][20/63]	 loss 4.09865	 cls_loss: 0.2461 cluster_loss: 1.2450 sup_con_loss: 0.6226 contrastive_loss: 4.5928 
2024-05-15 14:30:10.184 | INFO     | __main__:train:123 - Epoch: [179][40/63]	 loss 4.09108	 cls_loss: 0.2631 cluster_loss: 1.1895 sup_con_loss: 0.6959 contrastive_loss: 4.5880 
2024-05-15 14:30:25.720 | INFO     | __main__:train:123 - Epoch: [179][60/63]	 loss 4.10000	 cls_loss: 0.2678 cluster_loss: 1.2180 sup_con_loss: 0.6578 contrastive_loss: 4.5913 
2024-05-15 14:30:27.421 | INFO     | __main__:train:126 - Train Epoch: 179 Avg Loss: 4.0552 
2024-05-15 14:30:27.422 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:30:59.012 | INFO     | __main__:train:135 - Train Accuracies: All 0.8137 | Old 0.8436 | New 0.7841
2024-05-15 14:31:05.892 | INFO     | __main__:train:123 - Epoch: [180][0/63]	 loss 4.17799	 cls_loss: 0.2789 cluster_loss: 1.2941 sup_con_loss: 0.7158 contrastive_loss: 4.5980 
2024-05-15 14:31:23.763 | INFO     | __main__:train:123 - Epoch: [180][20/63]	 loss 3.93507	 cls_loss: 0.2884 cluster_loss: 1.0695 sup_con_loss: 0.4534 contrastive_loss: 4.5850 
2024-05-15 14:31:41.317 | INFO     | __main__:train:123 - Epoch: [180][40/63]	 loss 4.02909	 cls_loss: 0.2612 cluster_loss: 1.1525 sup_con_loss: 0.5882 contrastive_loss: 4.5887 
2024-05-15 14:31:57.085 | INFO     | __main__:train:123 - Epoch: [180][60/63]	 loss 4.12382	 cls_loss: 0.2573 cluster_loss: 1.2168 sup_con_loss: 0.7479 contrastive_loss: 4.5863 
2024-05-15 14:31:58.745 | INFO     | __main__:train:126 - Train Epoch: 180 Avg Loss: 4.0652 
2024-05-15 14:31:58.746 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:32:32.101 | INFO     | __main__:train:135 - Train Accuracies: All 0.8138 | Old 0.8443 | New 0.7836
2024-05-15 14:32:39.955 | INFO     | __main__:train:123 - Epoch: [181][0/63]	 loss 4.05639	 cls_loss: 0.3011 cluster_loss: 1.1889 sup_con_loss: 0.5439 contrastive_loss: 4.5967 
2024-05-15 14:32:57.065 | INFO     | __main__:train:123 - Epoch: [181][20/63]	 loss 4.00416	 cls_loss: 0.2777 cluster_loss: 1.1119 sup_con_loss: 0.5849 contrastive_loss: 4.5839 
2024-05-15 14:33:14.056 | INFO     | __main__:train:123 - Epoch: [181][40/63]	 loss 4.02026	 cls_loss: 0.2600 cluster_loss: 1.1889 sup_con_loss: 0.4861 contrastive_loss: 4.5944 
2024-05-15 14:33:29.285 | INFO     | __main__:train:123 - Epoch: [181][60/63]	 loss 4.15016	 cls_loss: 0.2823 cluster_loss: 1.2612 sup_con_loss: 0.7044 contrastive_loss: 4.5923 
2024-05-15 14:33:30.933 | INFO     | __main__:train:126 - Train Epoch: 181 Avg Loss: 4.0672 
2024-05-15 14:33:30.934 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:34:03.020 | INFO     | __main__:train:135 - Train Accuracies: All 0.8140 | Old 0.8443 | New 0.7839
2024-05-15 14:34:09.984 | INFO     | __main__:train:123 - Epoch: [182][0/63]	 loss 4.02603	 cls_loss: 0.2659 cluster_loss: 1.1607 sup_con_loss: 0.5630 contrastive_loss: 4.5868 
2024-05-15 14:34:27.569 | INFO     | __main__:train:123 - Epoch: [182][20/63]	 loss 4.00136	 cls_loss: 0.2762 cluster_loss: 1.1229 sup_con_loss: 0.5386 contrastive_loss: 4.5944 
2024-05-15 14:34:45.439 | INFO     | __main__:train:123 - Epoch: [182][40/63]	 loss 4.00559	 cls_loss: 0.2729 cluster_loss: 1.1251 sup_con_loss: 0.5672 contrastive_loss: 4.5850 
2024-05-15 14:35:00.817 | INFO     | __main__:train:123 - Epoch: [182][60/63]	 loss 4.10739	 cls_loss: 0.2853 cluster_loss: 1.1807 sup_con_loss: 0.7357 contrastive_loss: 4.5886 
2024-05-15 14:35:02.494 | INFO     | __main__:train:126 - Train Epoch: 182 Avg Loss: 4.0698 
2024-05-15 14:35:02.495 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:35:34.416 | INFO     | __main__:train:135 - Train Accuracies: All 0.8138 | Old 0.8428 | New 0.7851
2024-05-15 14:35:42.471 | INFO     | __main__:train:123 - Epoch: [183][0/63]	 loss 3.98972	 cls_loss: 0.2477 cluster_loss: 1.1571 sup_con_loss: 0.4724 contrastive_loss: 4.5932 
2024-05-15 14:36:00.058 | INFO     | __main__:train:123 - Epoch: [183][20/63]	 loss 4.07928	 cls_loss: 0.2584 cluster_loss: 1.1749 sup_con_loss: 0.7103 contrastive_loss: 4.5794 
2024-05-15 14:36:17.784 | INFO     | __main__:train:123 - Epoch: [183][40/63]	 loss 4.06560	 cls_loss: 0.2771 cluster_loss: 1.1651 sup_con_loss: 0.6603 contrastive_loss: 4.5849 
2024-05-15 14:36:33.153 | INFO     | __main__:train:123 - Epoch: [183][60/63]	 loss 4.09454	 cls_loss: 0.2797 cluster_loss: 1.2308 sup_con_loss: 0.6023 contrastive_loss: 4.5936 
2024-05-15 14:36:34.838 | INFO     | __main__:train:126 - Train Epoch: 183 Avg Loss: 4.0482 
2024-05-15 14:36:34.840 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:37:07.000 | INFO     | __main__:train:135 - Train Accuracies: All 0.8135 | Old 0.8431 | New 0.7841
2024-05-15 14:37:14.657 | INFO     | __main__:train:123 - Epoch: [184][0/63]	 loss 4.02685	 cls_loss: 0.2974 cluster_loss: 1.1724 sup_con_loss: 0.4996 contrastive_loss: 4.5936 
2024-05-15 14:37:32.536 | INFO     | __main__:train:123 - Epoch: [184][20/63]	 loss 4.09782	 cls_loss: 0.2630 cluster_loss: 1.2227 sup_con_loss: 0.6511 contrastive_loss: 4.5894 
2024-05-15 14:37:50.294 | INFO     | __main__:train:123 - Epoch: [184][40/63]	 loss 3.96906	 cls_loss: 0.2804 cluster_loss: 1.1272 sup_con_loss: 0.4343 contrastive_loss: 4.5942 
2024-05-15 14:38:05.795 | INFO     | __main__:train:123 - Epoch: [184][60/63]	 loss 4.02545	 cls_loss: 0.2635 cluster_loss: 1.2018 sup_con_loss: 0.4817 contrastive_loss: 4.5899 
2024-05-15 14:38:07.474 | INFO     | __main__:train:126 - Train Epoch: 184 Avg Loss: 4.0678 
2024-05-15 14:38:07.474 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:38:38.856 | INFO     | __main__:train:135 - Train Accuracies: All 0.8132 | Old 0.8426 | New 0.7841
2024-05-15 14:38:45.994 | INFO     | __main__:train:123 - Epoch: [185][0/63]	 loss 4.03611	 cls_loss: 0.2596 cluster_loss: 1.1658 sup_con_loss: 0.5932 contrastive_loss: 4.5844 
2024-05-15 14:39:04.108 | INFO     | __main__:train:123 - Epoch: [185][20/63]	 loss 4.07023	 cls_loss: 0.2535 cluster_loss: 1.2285 sup_con_loss: 0.5680 contrastive_loss: 4.5911 
2024-05-15 14:39:22.073 | INFO     | __main__:train:123 - Epoch: [185][40/63]	 loss 4.13866	 cls_loss: 0.2634 cluster_loss: 1.2469 sup_con_loss: 0.7208 contrastive_loss: 4.5903 
2024-05-15 14:39:37.285 | INFO     | __main__:train:123 - Epoch: [185][60/63]	 loss 4.12077	 cls_loss: 0.3058 cluster_loss: 1.2192 sup_con_loss: 0.6797 contrastive_loss: 4.5898 
2024-05-15 14:39:39.015 | INFO     | __main__:train:126 - Train Epoch: 185 Avg Loss: 4.0716 
2024-05-15 14:39:39.015 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:40:11.467 | INFO     | __main__:train:135 - Train Accuracies: All 0.8140 | Old 0.8441 | New 0.7841
2024-05-15 14:40:18.713 | INFO     | __main__:train:123 - Epoch: [186][0/63]	 loss 4.10023	 cls_loss: 0.2721 cluster_loss: 1.1995 sup_con_loss: 0.6914 contrastive_loss: 4.5897 
2024-05-15 14:40:36.395 | INFO     | __main__:train:123 - Epoch: [186][20/63]	 loss 3.97129	 cls_loss: 0.2729 cluster_loss: 1.0724 sup_con_loss: 0.5751 contrastive_loss: 4.5807 
2024-05-15 14:40:53.663 | INFO     | __main__:train:123 - Epoch: [186][40/63]	 loss 4.02740	 cls_loss: 0.2844 cluster_loss: 1.1650 sup_con_loss: 0.5332 contrastive_loss: 4.5908 
2024-05-15 14:41:08.849 | INFO     | __main__:train:123 - Epoch: [186][60/63]	 loss 4.12021	 cls_loss: 0.2600 cluster_loss: 1.2514 sup_con_loss: 0.6671 contrastive_loss: 4.5882 
2024-05-15 14:41:10.517 | INFO     | __main__:train:126 - Train Epoch: 186 Avg Loss: 4.0825 
2024-05-15 14:41:10.517 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:41:41.875 | INFO     | __main__:train:135 - Train Accuracies: All 0.8140 | Old 0.8438 | New 0.7844
2024-05-15 14:41:48.806 | INFO     | __main__:train:123 - Epoch: [187][0/63]	 loss 4.04403	 cls_loss: 0.2797 cluster_loss: 1.1586 sup_con_loss: 0.6020 contrastive_loss: 4.5882 
2024-05-15 14:42:07.396 | INFO     | __main__:train:123 - Epoch: [187][20/63]	 loss 4.08478	 cls_loss: 0.2726 cluster_loss: 1.1764 sup_con_loss: 0.7019 contrastive_loss: 4.5832 
2024-05-15 14:42:25.013 | INFO     | __main__:train:123 - Epoch: [187][40/63]	 loss 4.13608	 cls_loss: 0.2928 cluster_loss: 1.2575 sup_con_loss: 0.6642 contrastive_loss: 4.5904 
2024-05-15 14:42:40.590 | INFO     | __main__:train:123 - Epoch: [187][60/63]	 loss 4.02879	 cls_loss: 0.2760 cluster_loss: 1.1332 sup_con_loss: 0.6203 contrastive_loss: 4.5823 
2024-05-15 14:42:42.273 | INFO     | __main__:train:126 - Train Epoch: 187 Avg Loss: 4.0725 
2024-05-15 14:42:42.274 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:43:14.831 | INFO     | __main__:train:135 - Train Accuracies: All 0.8138 | Old 0.8428 | New 0.7851
2024-05-15 14:43:22.597 | INFO     | __main__:train:123 - Epoch: [188][0/63]	 loss 4.11923	 cls_loss: 0.2753 cluster_loss: 1.2173 sup_con_loss: 0.7035 contrastive_loss: 4.5930 
2024-05-15 14:43:40.320 | INFO     | __main__:train:123 - Epoch: [188][20/63]	 loss 4.03302	 cls_loss: 0.2853 cluster_loss: 1.1485 sup_con_loss: 0.5850 contrastive_loss: 4.5875 
2024-05-15 14:43:57.707 | INFO     | __main__:train:123 - Epoch: [188][40/63]	 loss 4.09520	 cls_loss: 0.2685 cluster_loss: 1.2123 sup_con_loss: 0.6475 contrastive_loss: 4.5948 
2024-05-15 14:44:13.227 | INFO     | __main__:train:123 - Epoch: [188][60/63]	 loss 4.01123	 cls_loss: 0.3123 cluster_loss: 1.1185 sup_con_loss: 0.5339 contrastive_loss: 4.5970 
2024-05-15 14:44:14.946 | INFO     | __main__:train:126 - Train Epoch: 188 Avg Loss: 4.0605 
2024-05-15 14:44:14.947 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:44:46.766 | INFO     | __main__:train:135 - Train Accuracies: All 0.8145 | Old 0.8456 | New 0.7836
2024-05-15 14:44:54.187 | INFO     | __main__:train:123 - Epoch: [189][0/63]	 loss 4.08467	 cls_loss: 0.3102 cluster_loss: 1.2193 sup_con_loss: 0.5688 contrastive_loss: 4.5914 
2024-05-15 14:45:11.831 | INFO     | __main__:train:123 - Epoch: [189][20/63]	 loss 4.09632	 cls_loss: 0.2946 cluster_loss: 1.1635 sup_con_loss: 0.7317 contrastive_loss: 4.5859 
2024-05-15 14:45:29.401 | INFO     | __main__:train:123 - Epoch: [189][40/63]	 loss 4.17604	 cls_loss: 0.2692 cluster_loss: 1.3129 sup_con_loss: 0.6960 contrastive_loss: 4.5920 
2024-05-15 14:45:44.851 | INFO     | __main__:train:123 - Epoch: [189][60/63]	 loss 4.10535	 cls_loss: 0.2570 cluster_loss: 1.2092 sup_con_loss: 0.7050 contrastive_loss: 4.5887 
2024-05-15 14:45:46.608 | INFO     | __main__:train:126 - Train Epoch: 189 Avg Loss: 4.0918 
2024-05-15 14:45:46.609 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:46:19.048 | INFO     | __main__:train:135 - Train Accuracies: All 0.8140 | Old 0.8443 | New 0.7839
2024-05-15 14:46:26.076 | INFO     | __main__:train:123 - Epoch: [190][0/63]	 loss 4.10875	 cls_loss: 0.2944 cluster_loss: 1.2426 sup_con_loss: 0.6037 contrastive_loss: 4.5949 
2024-05-15 14:46:43.781 | INFO     | __main__:train:123 - Epoch: [190][20/63]	 loss 4.06672	 cls_loss: 0.2917 cluster_loss: 1.1737 sup_con_loss: 0.6290 contrastive_loss: 4.5871 
2024-05-15 14:47:01.302 | INFO     | __main__:train:123 - Epoch: [190][40/63]	 loss 3.97114	 cls_loss: 0.2658 cluster_loss: 1.0878 sup_con_loss: 0.5569 contrastive_loss: 4.5787 
2024-05-15 14:47:17.111 | INFO     | __main__:train:123 - Epoch: [190][60/63]	 loss 4.12295	 cls_loss: 0.2801 cluster_loss: 1.2417 sup_con_loss: 0.6718 contrastive_loss: 4.5887 
2024-05-15 14:47:18.785 | INFO     | __main__:train:126 - Train Epoch: 190 Avg Loss: 4.0735 
2024-05-15 14:47:18.785 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:47:50.120 | INFO     | __main__:train:135 - Train Accuracies: All 0.8137 | Old 0.8441 | New 0.7836
2024-05-15 14:47:56.601 | INFO     | __main__:train:123 - Epoch: [191][0/63]	 loss 4.04113	 cls_loss: 0.2704 cluster_loss: 1.1878 sup_con_loss: 0.5593 contrastive_loss: 4.5826 
2024-05-15 14:48:14.680 | INFO     | __main__:train:123 - Epoch: [191][20/63]	 loss 4.15181	 cls_loss: 0.2701 cluster_loss: 1.2374 sup_con_loss: 0.7818 contrastive_loss: 4.5836 
2024-05-15 14:48:32.489 | INFO     | __main__:train:123 - Epoch: [191][40/63]	 loss 4.18397	 cls_loss: 0.2636 cluster_loss: 1.2718 sup_con_loss: 0.7960 contrastive_loss: 4.5945 
2024-05-15 14:48:48.157 | INFO     | __main__:train:123 - Epoch: [191][60/63]	 loss 4.07042	 cls_loss: 0.2850 cluster_loss: 1.1778 sup_con_loss: 0.6385 contrastive_loss: 4.5871 
2024-05-15 14:48:49.861 | INFO     | __main__:train:126 - Train Epoch: 191 Avg Loss: 4.0788 
2024-05-15 14:48:49.862 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:49:22.273 | INFO     | __main__:train:135 - Train Accuracies: All 0.8137 | Old 0.8431 | New 0.7846
2024-05-15 14:49:29.130 | INFO     | __main__:train:123 - Epoch: [192][0/63]	 loss 4.08335	 cls_loss: 0.2785 cluster_loss: 1.1764 sup_con_loss: 0.6622 contrastive_loss: 4.5991 
2024-05-15 14:49:47.311 | INFO     | __main__:train:123 - Epoch: [192][20/63]	 loss 4.13250	 cls_loss: 0.2737 cluster_loss: 1.2293 sup_con_loss: 0.7220 contrastive_loss: 4.5922 
2024-05-15 14:50:04.710 | INFO     | __main__:train:123 - Epoch: [192][40/63]	 loss 4.10863	 cls_loss: 0.2656 cluster_loss: 1.1746 sup_con_loss: 0.7791 contrastive_loss: 4.5838 
2024-05-15 14:50:20.320 | INFO     | __main__:train:123 - Epoch: [192][60/63]	 loss 4.00287	 cls_loss: 0.2748 cluster_loss: 1.0966 sup_con_loss: 0.6145 contrastive_loss: 4.5827 
2024-05-15 14:50:22.028 | INFO     | __main__:train:126 - Train Epoch: 192 Avg Loss: 4.0620 
2024-05-15 14:50:22.029 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:50:55.014 | INFO     | __main__:train:135 - Train Accuracies: All 0.8143 | Old 0.8441 | New 0.7848
2024-05-15 14:51:02.928 | INFO     | __main__:train:123 - Epoch: [193][0/63]	 loss 3.99717	 cls_loss: 0.2827 cluster_loss: 1.1393 sup_con_loss: 0.5095 contrastive_loss: 4.5836 
2024-05-15 14:51:21.307 | INFO     | __main__:train:123 - Epoch: [193][20/63]	 loss 4.01412	 cls_loss: 0.2760 cluster_loss: 1.1346 sup_con_loss: 0.5552 contrastive_loss: 4.5934 
2024-05-15 14:51:38.699 | INFO     | __main__:train:123 - Epoch: [193][40/63]	 loss 4.01222	 cls_loss: 0.2715 cluster_loss: 1.1568 sup_con_loss: 0.5255 contrastive_loss: 4.5866 
2024-05-15 14:51:54.064 | INFO     | __main__:train:123 - Epoch: [193][60/63]	 loss 4.00899	 cls_loss: 0.2649 cluster_loss: 1.1342 sup_con_loss: 0.5626 contrastive_loss: 4.5879 
2024-05-15 14:51:55.813 | INFO     | __main__:train:126 - Train Epoch: 193 Avg Loss: 4.0646 
2024-05-15 14:51:55.813 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:52:28.338 | INFO     | __main__:train:135 - Train Accuracies: All 0.8132 | Old 0.8421 | New 0.7846
2024-05-15 14:52:36.713 | INFO     | __main__:train:123 - Epoch: [194][0/63]	 loss 4.06772	 cls_loss: 0.2998 cluster_loss: 1.1556 sup_con_loss: 0.6622 contrastive_loss: 4.5844 
2024-05-15 14:52:54.668 | INFO     | __main__:train:123 - Epoch: [194][20/63]	 loss 4.13178	 cls_loss: 0.2713 cluster_loss: 1.2045 sup_con_loss: 0.7596 contrastive_loss: 4.5970 
2024-05-15 14:53:12.076 | INFO     | __main__:train:123 - Epoch: [194][40/63]	 loss 4.06886	 cls_loss: 0.2746 cluster_loss: 1.1575 sup_con_loss: 0.6867 contrastive_loss: 4.5847 
2024-05-15 14:53:27.483 | INFO     | __main__:train:123 - Epoch: [194][60/63]	 loss 4.17561	 cls_loss: 0.2713 cluster_loss: 1.2675 sup_con_loss: 0.7881 contrastive_loss: 4.5861 
2024-05-15 14:53:29.167 | INFO     | __main__:train:126 - Train Epoch: 194 Avg Loss: 4.0644 
2024-05-15 14:53:29.167 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:54:01.442 | INFO     | __main__:train:135 - Train Accuracies: All 0.8135 | Old 0.8431 | New 0.7841
2024-05-15 14:54:09.137 | INFO     | __main__:train:123 - Epoch: [195][0/63]	 loss 4.05769	 cls_loss: 0.2583 cluster_loss: 1.1923 sup_con_loss: 0.6131 contrastive_loss: 4.5811 
2024-05-15 14:54:27.018 | INFO     | __main__:train:123 - Epoch: [195][20/63]	 loss 4.17708	 cls_loss: 0.2796 cluster_loss: 1.2833 sup_con_loss: 0.7389 contrastive_loss: 4.5946 
2024-05-15 14:54:44.450 | INFO     | __main__:train:123 - Epoch: [195][40/63]	 loss 4.02799	 cls_loss: 0.2751 cluster_loss: 1.0939 sup_con_loss: 0.6890 contrastive_loss: 4.5838 
2024-05-15 14:54:59.711 | INFO     | __main__:train:123 - Epoch: [195][60/63]	 loss 4.01860	 cls_loss: 0.2816 cluster_loss: 1.1682 sup_con_loss: 0.5080 contrastive_loss: 4.5892 
2024-05-15 14:55:01.437 | INFO     | __main__:train:126 - Train Epoch: 195 Avg Loss: 4.0516 
2024-05-15 14:55:01.437 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:55:33.965 | INFO     | __main__:train:135 - Train Accuracies: All 0.8136 | Old 0.8433 | New 0.7841
2024-05-15 14:55:39.938 | INFO     | __main__:train:123 - Epoch: [196][0/63]	 loss 4.03384	 cls_loss: 0.2762 cluster_loss: 1.1616 sup_con_loss: 0.5573 contrastive_loss: 4.5955 
2024-05-15 14:55:58.512 | INFO     | __main__:train:123 - Epoch: [196][20/63]	 loss 4.16741	 cls_loss: 0.2804 cluster_loss: 1.2912 sup_con_loss: 0.6924 contrastive_loss: 4.5964 
2024-05-15 14:56:16.205 | INFO     | __main__:train:123 - Epoch: [196][40/63]	 loss 4.10710	 cls_loss: 0.2759 cluster_loss: 1.2061 sup_con_loss: 0.6960 contrastive_loss: 4.5891 
2024-05-15 14:56:31.314 | INFO     | __main__:train:123 - Epoch: [196][60/63]	 loss 4.02050	 cls_loss: 0.2569 cluster_loss: 1.1298 sup_con_loss: 0.6075 contrastive_loss: 4.5901 
2024-05-15 14:56:33.014 | INFO     | __main__:train:126 - Train Epoch: 196 Avg Loss: 4.0681 
2024-05-15 14:56:33.014 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:57:05.660 | INFO     | __main__:train:135 - Train Accuracies: All 0.8137 | Old 0.8428 | New 0.7848
2024-05-15 14:57:12.088 | INFO     | __main__:train:123 - Epoch: [197][0/63]	 loss 4.19114	 cls_loss: 0.2878 cluster_loss: 1.2859 sup_con_loss: 0.7608 contrastive_loss: 4.5974 
2024-05-15 14:57:30.594 | INFO     | __main__:train:123 - Epoch: [197][20/63]	 loss 4.02222	 cls_loss: 0.2921 cluster_loss: 1.1705 sup_con_loss: 0.5092 contrastive_loss: 4.5861 
2024-05-15 14:57:47.718 | INFO     | __main__:train:123 - Epoch: [197][40/63]	 loss 4.04977	 cls_loss: 0.2810 cluster_loss: 1.1759 sup_con_loss: 0.5717 contrastive_loss: 4.5953 
2024-05-15 14:58:03.059 | INFO     | __main__:train:123 - Epoch: [197][60/63]	 loss 4.08304	 cls_loss: 0.2709 cluster_loss: 1.1812 sup_con_loss: 0.6805 contrastive_loss: 4.5881 
2024-05-15 14:58:04.723 | INFO     | __main__:train:126 - Train Epoch: 197 Avg Loss: 4.0518 
2024-05-15 14:58:04.723 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 14:58:36.581 | INFO     | __main__:train:135 - Train Accuracies: All 0.8125 | Old 0.8421 | New 0.7831
2024-05-15 14:58:43.820 | INFO     | __main__:train:123 - Epoch: [198][0/63]	 loss 3.97320	 cls_loss: 0.2749 cluster_loss: 1.1230 sup_con_loss: 0.4760 contrastive_loss: 4.5853 
2024-05-15 14:59:01.662 | INFO     | __main__:train:123 - Epoch: [198][20/63]	 loss 4.00057	 cls_loss: 0.2427 cluster_loss: 1.1729 sup_con_loss: 0.4913 contrastive_loss: 4.5865 
2024-05-15 14:59:19.754 | INFO     | __main__:train:123 - Epoch: [198][40/63]	 loss 3.99054	 cls_loss: 0.2805 cluster_loss: 1.1358 sup_con_loss: 0.4966 contrastive_loss: 4.5851 
2024-05-15 14:59:35.176 | INFO     | __main__:train:123 - Epoch: [198][60/63]	 loss 4.04520	 cls_loss: 0.2749 cluster_loss: 1.1647 sup_con_loss: 0.6010 contrastive_loss: 4.5871 
2024-05-15 14:59:36.890 | INFO     | __main__:train:126 - Train Epoch: 198 Avg Loss: 4.0569 
2024-05-15 14:59:36.890 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 15:00:09.813 | INFO     | __main__:train:135 - Train Accuracies: All 0.8143 | Old 0.8443 | New 0.7846
2024-05-15 15:00:16.209 | INFO     | __main__:train:123 - Epoch: [199][0/63]	 loss 3.98822	 cls_loss: 0.2601 cluster_loss: 1.1178 sup_con_loss: 0.5473 contrastive_loss: 4.5831 
2024-05-15 15:00:34.106 | INFO     | __main__:train:123 - Epoch: [199][20/63]	 loss 4.09792	 cls_loss: 0.2626 cluster_loss: 1.1777 sup_con_loss: 0.7424 contrastive_loss: 4.5856 
2024-05-15 15:00:51.827 | INFO     | __main__:train:123 - Epoch: [199][40/63]	 loss 4.28297	 cls_loss: 0.2771 cluster_loss: 1.3488 sup_con_loss: 0.9208 contrastive_loss: 4.5954 
2024-05-15 15:01:07.385 | INFO     | __main__:train:123 - Epoch: [199][60/63]	 loss 4.10065	 cls_loss: 0.2943 cluster_loss: 1.1906 sup_con_loss: 0.6848 contrastive_loss: 4.5908 
2024-05-15 15:01:09.059 | INFO     | __main__:train:126 - Train Epoch: 199 Avg Loss: 4.0690 
2024-05-15 15:01:09.059 | INFO     | __main__:train:128 - Testing on unlabelled examples in the training data...
2024-05-15 15:01:40.873 | INFO     | __main__:train:135 - Train Accuracies: All 0.8132 | Old 0.8441 | New 0.7826
