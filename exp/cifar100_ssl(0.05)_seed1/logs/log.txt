2024-05-13 16:52:23,764 - INFO - Training cifar100_ssl(0.05)_seed1 with the following settings:
2024-05-13 16:52:23,764 - INFO - Command-line arguments: output_dir=exp
 experiment_name=cifar100_ssl(0.05)_seed1
 seed_num=1
 evaluate=False
 dataset_name=cifar100
 backbone_name=ViT-B/16
 epochs=200
 base_lr=0.0005
 classifier_lr=0.1
 momentum=0.9
 weight_decay=0.0001
 num_workers=8
 batch_size=128
 prop_train_labels=0.05
 image_size=224
 crop_pct=0.875
 interpolation=3
 transform=imagenet
 alpha_sr=0
 alpha_ri=0.05
 alpha_rs=0.05
 alpha_rd=0.05
 pseudo_ratio=0.25
 lambda_loss=0.2
 coteaching_epoch_t=10
 coteaching_epoch_i=15
 max_kmeans_iter=10
 k_means_init=20
 interrupted_path=
 train_classes=range(0, 100)
 unlabeled_classes=[]
 num_labeled_classes=100
 num_unlabeled_classes=0
 num_classes=100
 log_path=exp/cifar100_ssl(0.05)_seed1/logs/log.txt
 model_path=exp/cifar100_ssl(0.05)_seed1/models/model.pth
 device=cuda
2024-05-13 16:52:23,769 - INFO - Loading CLIP (backbone: ViT-B/16)
2024-05-13 16:52:27,227 - INFO - Building custom CLIP
2024-05-13 16:52:33,324 - INFO - Turning off gradients in both the image and the text encoder
2024-05-13 16:52:33,329 - INFO - Parameters that require gradients: ['model.text_projection', 'model.visual.proj', 'model.visual.transformer.resblocks.11.attn.in_proj_weight', 'model.visual.transformer.resblocks.11.attn.in_proj_bias', 'model.visual.transformer.resblocks.11.attn.out_proj.weight', 'model.visual.transformer.resblocks.11.attn.out_proj.bias', 'model.visual.transformer.resblocks.11.ln_1.weight', 'model.visual.transformer.resblocks.11.ln_1.bias', 'model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'model.visual.transformer.resblocks.11.ln_2.weight', 'model.visual.transformer.resblocks.11.ln_2.bias', 'model.transformer.resblocks.11.attn.in_proj_weight', 'model.transformer.resblocks.11.attn.in_proj_bias', 'model.transformer.resblocks.11.attn.out_proj.weight', 'model.transformer.resblocks.11.attn.out_proj.bias', 'model.transformer.resblocks.11.ln_1.weight', 'model.transformer.resblocks.11.ln_1.bias', 'model.transformer.resblocks.11.mlp.c_fc.weight', 'model.transformer.resblocks.11.mlp.c_fc.bias', 'model.transformer.resblocks.11.mlp.c_proj.weight', 'model.transformer.resblocks.11.mlp.c_proj.bias', 'model.transformer.resblocks.11.ln_2.weight', 'model.transformer.resblocks.11.ln_2.bias', 'image_classifier.weight_g', 'image_classifier.weight_v', 'text_classifier.weight_g', 'text_classifier.weight_v']
2024-05-13 16:52:33,331 - INFO - Parameters in classifier with big lr: ['image_classifier.weight_g', 'image_classifier.weight_v', 'text_classifier.weight_g', 'text_classifier.weight_v']
2024-05-13 16:52:37,935 - INFO - len of train dataset: 50000
2024-05-13 16:52:37,935 - INFO - len of test dataset: 47500
2024-05-13 16:52:37,935 - INFO - Pseudo Nums: 118
2024-05-13 16:55:15,999 - INFO - len of image_to_class_map: 8878
2024-05-13 16:55:16,000 - INFO - len of image_to_class_map_i: 8817
2024-05-13 16:58:49,033 - INFO - Training cifar100_ssl(0.05)_seed1 with the following settings:
2024-05-13 16:58:49,033 - INFO - Command-line arguments: output_dir=exp
 experiment_name=cifar100_ssl(0.05)_seed1
 seed_num=1
 evaluate=False
 dataset_name=cifar100
 backbone_name=ViT-B/16
 epochs=200
 base_lr=0.0005
 classifier_lr=0.1
 momentum=0.9
 weight_decay=0.0001
 num_workers=8
 batch_size=128
 prop_train_labels=0.05
 image_size=224
 crop_pct=0.875
 interpolation=3
 transform=imagenet
 alpha_sr=0
 alpha_ri=0.05
 alpha_rs=0.05
 alpha_rd=0.05
 pseudo_ratio=0.6
 lambda_loss=0.2
 coteaching_epoch_t=10
 coteaching_epoch_i=15
 max_kmeans_iter=10
 k_means_init=20
 interrupted_path=
 train_classes=range(0, 100)
 unlabeled_classes=[]
 num_labeled_classes=100
 num_unlabeled_classes=0
 num_classes=100
 log_path=exp/cifar100_ssl(0.05)_seed1/logs/log.txt
 model_path=exp/cifar100_ssl(0.05)_seed1/models/model.pth
 device=cuda
2024-05-13 16:58:49,038 - INFO - Loading CLIP (backbone: ViT-B/16)
2024-05-13 16:58:52,449 - INFO - Building custom CLIP
2024-05-13 16:58:59,062 - INFO - Turning off gradients in both the image and the text encoder
2024-05-13 16:58:59,065 - INFO - Parameters that require gradients: ['model.text_projection', 'model.visual.proj', 'model.visual.transformer.resblocks.11.attn.in_proj_weight', 'model.visual.transformer.resblocks.11.attn.in_proj_bias', 'model.visual.transformer.resblocks.11.attn.out_proj.weight', 'model.visual.transformer.resblocks.11.attn.out_proj.bias', 'model.visual.transformer.resblocks.11.ln_1.weight', 'model.visual.transformer.resblocks.11.ln_1.bias', 'model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'model.visual.transformer.resblocks.11.ln_2.weight', 'model.visual.transformer.resblocks.11.ln_2.bias', 'model.transformer.resblocks.11.attn.in_proj_weight', 'model.transformer.resblocks.11.attn.in_proj_bias', 'model.transformer.resblocks.11.attn.out_proj.weight', 'model.transformer.resblocks.11.attn.out_proj.bias', 'model.transformer.resblocks.11.ln_1.weight', 'model.transformer.resblocks.11.ln_1.bias', 'model.transformer.resblocks.11.mlp.c_fc.weight', 'model.transformer.resblocks.11.mlp.c_fc.bias', 'model.transformer.resblocks.11.mlp.c_proj.weight', 'model.transformer.resblocks.11.mlp.c_proj.bias', 'model.transformer.resblocks.11.ln_2.weight', 'model.transformer.resblocks.11.ln_2.bias', 'image_classifier.weight_g', 'image_classifier.weight_v', 'text_classifier.weight_g', 'text_classifier.weight_v']
2024-05-13 16:58:59,067 - INFO - Parameters in classifier with big lr: ['image_classifier.weight_g', 'image_classifier.weight_v', 'text_classifier.weight_g', 'text_classifier.weight_v']
2024-05-13 16:59:03,340 - INFO - len of train dataset: 50000
2024-05-13 16:59:03,340 - INFO - len of test dataset: 47500
2024-05-13 16:59:03,340 - INFO - Pseudo Nums: 285
